{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "va6zv-gKhIu8"
      },
      "source": [
        "Name: Write Your Name Here"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Dimitar Gjorgievski"
      ],
      "metadata": {
        "id": "tuaYthb0hXnU"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Submission:\n",
        "\n",
        "1- Run all cells (this is important, the results will remain there for us to look)\n",
        "\n",
        "2- Download .ipynb\n",
        "\n",
        "3- Submit your .ipynb on Gradescope\n",
        "\n",
        "4- Double check your submitted file to make sure the submission is correct and it shows all the cell outputs\n"
      ],
      "metadata": {
        "id": "QZj93Jcmh8pV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Mounting Google Drive:\n",
        "#After running this cell a popup window will appear and requesting to select your  Google account and give the access permission.\n",
        "#You can either use your personal Google account or your UIC Google account.\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "jkGXbH9kiQEK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1134b2c-1369-49f9-bcdb-fd33fc247306"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#You need to change this path\n",
        "path=\"/content/gdrive/MyDrive/cs412/Logistic Regression/\""
      ],
      "metadata": {
        "id": "K5ljlocpiRxq"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "qB86CgDBhIu9"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib import style\n",
        "style.use('ggplot')\n",
        "%matplotlib inline\n",
        "import re\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QhFQTtLWhIu-"
      },
      "source": [
        "Numpy is library for scientific computing in Python. It has efficient implementation of n-dimensional array (tensor) manupulations, which is useful for machine learning applications."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "RZceSJk8hIu-"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLJ1LE0RhIu_"
      },
      "source": [
        "We can convert a list into numpy array (tensor)  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "UIBuEViJhIu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "383fad01-1bfc-42c1-f38a-397e0c9e8e73"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2, 4],\n",
              "       [2, 6, 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ],
      "source": [
        "b = [[1, 2, 4], [2, 6, 9]]\n",
        "a = np.array(b)\n",
        "a"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YpaBPUPEhIu_"
      },
      "source": [
        "We can check the dimensions of the array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "fUrF-jvPhIu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "92f3119c-fac3-425d-c509-cc40e489b461"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2, 3)"
            ]
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "a.shape"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UjUJI4vRhIu_"
      },
      "source": [
        "We can apply simple arithmetic operation on all element of a tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VlQlxcGzhIu_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6d1806e9-9943-4075-c32d-72b09c18c1cb"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 3,  6, 12],\n",
              "       [ 6, 18, 27]])"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "a * 3"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MHyhHZbghIvA"
      },
      "source": [
        "You can transpose a tensor\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "88ItMKO8hIvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9374434d-bc25-4519-d2c4-0d58d1a244f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(3, 2)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[1, 2],\n",
              "       [2, 6],\n",
              "       [4, 9]])"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "print(a.T.shape)\n",
        "a.T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x5TsmIfZhIvA"
      },
      "source": [
        "You can apply aggregate functions on the whole tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "0rHaLkUMhIvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "123ac657-aa92-4f1d-af7a-46c3c01d1e9b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "24"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ],
      "source": [
        "np.sum(a)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xYoIqupihIvA"
      },
      "source": [
        "or on one dimension of it"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "2t74tXHOhIvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7b3a056a-5ec4-4bbf-8bca-35aa4a998e2f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 3,  8, 13])"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ],
      "source": [
        "np.sum(a, axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "9pLDwLh8hIvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6d155d3-f178-4a35-9a1e-ec7afcfa72b9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([ 7, 17])"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "np.sum(a, axis=1)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1gQUIziphIvA"
      },
      "source": [
        "We can do element-wise arithmetic operation on two tensors (of the same size)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "hLcwq4nZhIvA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee9595e9-75b9-4d99-8e0e-f39233fcb5e1"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 2,  6, 20],\n",
              "       [ 2, 12,  9]])"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ],
      "source": [
        "c1 = np.array([[1, 2, 4], [2, 6, 9]])\n",
        "c2 = np.array([[2, 3, 5], [1, 2, 1]])\n",
        "c1 * c2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nTbsvWJxhIvA"
      },
      "source": [
        "If you want to multiply all columns of a tensor by vector (for example if you want to multiply all data features by their lables) you need a trick. This multiplication shows up in calculating the gradients."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "VDEbp8nzhIvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "840b8593-fa30-41fc-90c5-8d460f049d51"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[1 2 4]\n",
            " [2 6 9]]\n",
            "[ 1 -1]\n"
          ]
        }
      ],
      "source": [
        "a = np.array([[1, 2, 4], [2, 6, 9]])\n",
        "b = np.array([1,-1])\n",
        "print(a)\n",
        "print(b)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YHqynmerhIvB"
      },
      "source": [
        "Here we want to multiply the first row of a by 1 and the second row of a by -1. Simply multiplying a by b does not work because a and b do not have the same dimension"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "uMALphCUhIvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e69e455e-2149-406d-cddb-859a1b073e8f"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1, -2],\n",
              "       [ 2, -6],\n",
              "       [ 4, -9]])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ],
      "source": [
        "a.T * b"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qkH4wCRhIvB"
      },
      "source": [
        "To do this multiplication we first have to assume b has one column and then repeat the column of b with the number of columns in a. We use tile function to do that"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "BK6SI4OqhIvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c364a493-a46d-4de3-db4b-ac99f37e59ce"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(2, 3)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  1,  1],\n",
              "       [-1, -1, -1]])"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ],
      "source": [
        "b_repeat = np.tile(b,  (a.shape[1],1)).T\n",
        "print(b_repeat.shape)\n",
        "b_repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-37pUmo_hIvB"
      },
      "source": [
        "Now we can multiply each column of a by b:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "Dj57Pxv1hIvB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab6f17dd-bf52-41ba-8f6e-61bdb5dd2866"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[ 1,  2,  4],\n",
              "       [-2, -6, -9]])"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ],
      "source": [
        "a * b_repeat"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njjOZYvHhIvB"
      },
      "source": [
        "You can create inital random vector using numpy (using N(0,1)):"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "x8O7pRx5hIvB"
      },
      "outputs": [],
      "source": [
        "mu = 0 #mean\n",
        "sigma = 1 #standard deviation\n",
        "r = np.random.normal(mu,sigma, 1000) #draws 1000 samples from a normal distribution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f6kyKWMnhIvB"
      },
      "source": [
        "We can apply functions on tensors"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "id": "13oMFM34hIvC"
      },
      "outputs": [],
      "source": [
        "#implementation of Normal distribution\n",
        "def normal(x, mu, sigma):\n",
        "    return np.exp( -0.5 * ((x-mu)/sigma)**2)/np.sqrt(2.0*np.pi*sigma**2)\n",
        "\n",
        "#probability of samples on the Normal distribution\n",
        "probabilities = normal(r, mu, sigma)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hby6OxM2hIvC"
      },
      "source": [
        "Numpy has useful APIs for analysis. Here we plot the histogram of samples and also plot the probabilies to see if the samples follow the normal distribution."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "Ort_wanOhIvC",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 447
        },
        "outputId": "4a075585-3e07-46b9-a68d-9c38d2503543"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.collections.PathCollection at 0x7d4c793603d0>"
            ]
          },
          "metadata": {},
          "execution_count": 24
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ],
      "source": [
        "counts, bins = np.histogram(r,50,density=True)\n",
        "plt.hist(bins[:-1], bins, weights=counts)\n",
        "plt.scatter(r, probabilities, c='b', marker='.')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "1ixKIArxhIvC"
      },
      "outputs": [],
      "source": [
        "def read_data(filename):  #reading the filename\n",
        "    f = open(filename, 'r') #open file for reading\n",
        "    p = re.compile(',')\n",
        "    xdata = []\n",
        "    ydata = []\n",
        "    header = f.readline().strip() #f; the file is used to get header section\n",
        "    varnames = p.split(header) #get a list of the atttribute names\n",
        "    namehash = {}\n",
        "    for l in f: #go through rest of the file and put all the features in proper\n",
        "        li = p.split(l.strip())\n",
        "        xdata.append([float(x) for x in li[:-1]]) #add all the features to a list\n",
        "        ydata.append(float(li[-1])) #add all the class variables to the list\n",
        "\n",
        "    return np.array(xdata), np.array(ydata) #converting xdata into a matrix; converting ydata to a vector\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ES-3FSlmhIvC"
      },
      "source": [
        "Assuming our data is x is available in numpy we use numpy to implement logistic regression\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "SN454nr0hIvC"
      },
      "outputs": [],
      "source": [
        "(xtrain_whole, ytrain_whole) = read_data(path + 'spambase-train.csv') #training data\n",
        "(xtest, ytest) = read_data(path + 'spambase-test.csv')  #test data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ciUpv-cGhIvC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "93a3f12a-87d6-4d6d-dd58-4cac04f5fcbd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The shape of xtrain: (3601, 54)\n",
            "The shape of ytrain: (3601,)\n",
            "The shape of xtest: (1000, 54)\n",
            "The shape of ytest: (1000,)\n"
          ]
        }
      ],
      "source": [
        "print(\"The shape of xtrain:\", xtrain_whole.shape)\n",
        "print(\"The shape of ytrain:\", ytrain_whole.shape)\n",
        "print(\"The shape of xtest:\", xtest.shape)\n",
        "print(\"The shape of ytest:\", ytest.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QW3mxkkahIvC"
      },
      "source": [
        "before training make we normalize the input data (features)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "whv7x-gthIvD"
      },
      "outputs": [],
      "source": [
        "xmean = np.mean(xtrain_whole, axis=0)\n",
        "xstd = np.std(xtrain_whole, axis=0)\n",
        "xtrain_normal_whole = (xtrain_whole-xmean) / xstd\n",
        "xtest_normal = (xtest-xmean) / xstd #same normalization as in the slide for KNN (non comensurate cariables?)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wu6SLWKFhIvD"
      },
      "source": [
        "We need to create a validation set. We create an array of indecies and permute it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "QcTRxtqohIvD"
      },
      "outputs": [],
      "source": [
        "premute_indicies = np.random.permutation(np.arange(xtrain_whole.shape[0]))  #rearanging the data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rols6OsxhIvD"
      },
      "source": [
        "We keep the first 2600 data points as the training data and rest as the validation data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "DP87ZxqhhIvD"
      },
      "outputs": [],
      "source": [
        "xtrain_normal = xtrain_normal_whole[premute_indicies[:2600]]  #taking train features\n",
        "ytrain = ytrain_whole[premute_indicies[:2600]]  #taking train labels\n",
        "xval_normal = xtrain_normal_whole[premute_indicies[2600:]]  #taking validation features\n",
        "yval = ytrain_whole[premute_indicies[2600:]]  #taking validation labels"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OcY4MTtKhIvD"
      },
      "source": [
        "Initiallizing the weights and bias with random values from N(0,1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "id": "GSZd1VgGhIvD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eb577849-0c47-45c6-d408-ac2413cd8db5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(54,)"
            ]
          },
          "metadata": {},
          "execution_count": 31
        }
      ],
      "source": [
        "weights = np.random.normal(0, 1, xtrain_normal.shape[1]); #generate an array of weights that follow the normal distribution\n",
        "bias = np.random.normal(0,1,1)  #generate a bias that follows the normal distribution\n",
        "weights.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "2wT420TChIvD"
      },
      "outputs": [],
      "source": [
        "#the sigmoid function\n",
        "def sigmoid(v):\n",
        "    #return np.exp(-np.logaddexp(0, -v)) #numerically stable implementation of sigmoid function\n",
        "    return 1.0 / (1+np.exp(-v))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oe6TnxiBhIvD"
      },
      "source": [
        "We can use dot-product from numpy to calculate the margin and pass it to the sigmoid function"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "YzLLEHfihIvD"
      },
      "outputs": [],
      "source": [
        "#w: weight vector (numpy array of size n)\n",
        "#b: numpy array of size 1\n",
        "#returns p(y=1|x, w, b)\n",
        "def prob(x, w, b):  #finding the probability of the margin\n",
        "    return sigmoid(np.dot(x,w) + b); #Class labels are 1 and 0, probability for a sigmoid works for 1 & -1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6HJuk25jhIvD"
      },
      "source": [
        "You can also calculate $l_2$ penalty using linalg library of numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "TC70IRw9hIvD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "be802400-4568-4bbf-ba8e-aa8a5e80adf5"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "7.249744064501348"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ],
      "source": [
        "np.linalg.norm(weights) #p=2 for l; norm of the weight vector"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bVsllOx5hIvD"
      },
      "source": [
        "$$\\text{Cross Entropy Loss} = -\\frac{1}{|D|}[\\sum_{(y^i,\\mathbf{x}^i)\\in\\mathcal{D}}\n",
        " y^i \\log p(y=1|\\mathbf{x}^i;\\mathbf{w},b)  +  (1-y^i) \\log (1 - p(y=1|\\mathbf{x}^i;\\mathbf{w},b))]+\\frac{\\lambda}{2} \\|\\mathbf{w}\\|^2 $$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "iI64I76vhIvE"
      },
      "outputs": [],
      "source": [
        "#w: weight vector (numpy array of size n)\n",
        "#x: training data points (only attributes)\n",
        "#y_prob: p(y|x, w, b)\n",
        "#y_true: class variable data\n",
        "#lambda_: l2 penalty coefficient\n",
        "#returns the cross entropy loss\n",
        "def loss(w, x, y_prob, y_true, lambda_):\n",
        "    cross_entropy1 = y_true * np.log(y_prob + 1e-10) #y * logsigmoid(p)\n",
        "    cross_entropy2 = (1-y_true) * np.log(1 - y_prob + 1e-10) #1-y * logsigmoid(1-p)\n",
        "    cross_entropy_loss = -(np.sum(cross_entropy1 + cross_entropy2)/x.shape[0]) + (lambda_*np.dot(w.T,w))/2\n",
        "    return cross_entropy_loss;\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 74,
      "metadata": {
        "id": "hN3S3kGnhIvE"
      },
      "outputs": [],
      "source": [
        "#x: input variables (data of size m x n with m data point and n features)\n",
        "#w: weight vector (numpy array of size n)\n",
        "#y_prob: p(y|x, w, b)\n",
        "#y_true: class variable data\n",
        "#lambda_: l2 penalty coefficient\n",
        "#returns tuple of gradient w.r.t w and w.r.t to bias\n",
        "\n",
        "def grad_w_b(x, w, y_prob, y_true, lambda_):\n",
        "    grad_w = -(np.dot(x.T, (y_true-y_prob))/x.shape[0]) + lambda_* w\n",
        "    grad_b = -np.sum((y_true-y_prob))/x.shape[0]\n",
        "    return (grad_w,grad_b)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 87,
      "metadata": {
        "scrolled": false,
        "id": "4tGaJCDjhIvE"
      },
      "outputs": [],
      "source": [
        "\n",
        "#lambda_ is the coeffienct of l2 norm penalty\n",
        "#learning_rate is learning rate of gradient descent algorithm\n",
        "#max_iter determines the maximum number of iterations if the gradients descent does not converge.\n",
        "#continue the training while gradient > 0.1 or the number steps is less max_iter\n",
        "\n",
        "#returns model as tuple of (weights,bias)\n",
        "\n",
        "def fit(x, y_true, learning_rate, lambda_, max_iter, verbose=0):\n",
        "    weights = np.random.normal(0, 1, x.shape[1]);\n",
        "    bias = np.random.normal(0,1,1)\n",
        "    iter = 0\n",
        "\n",
        "    while iter<max_iter:  #while iter is less than max_iter\n",
        "\n",
        "      verbose = 0\n",
        "      if(iter%4==0):\n",
        "        verbose = 1\n",
        "\n",
        "      dw, db = grad_w_b(x, weights, prob(x, weights, bias), y_true, lambda_)\n",
        "\n",
        "      weights = weights - (learning_rate * dw)\n",
        "      bias = bias - (learning_rate * db)\n",
        "      curr_loss = loss(weights, x, prob(x, weights, bias), y_true, lambda_)\n",
        "\n",
        "\n",
        "      if verbose: #verbose is used for debugging purposes\n",
        "            #print iteration number, loss, l2 norm of gradients, l2 norm of weights\n",
        "            print(\"Iteration#: \", iter, \"; Loss: \", curr_loss, \"; l2 norm of gradient: \",np.linalg.norm(dw), \"; l2 norm of weights: \",  np.linalg.norm(weights))\n",
        "            pass\n",
        "\n",
        "      if(np.linalg.norm(dw) < 0.001):\n",
        "        break\n",
        "\n",
        "      iter = iter + 1\n",
        "\n",
        "    return (weights, bias)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 88,
      "metadata": {
        "id": "55TdlFeyhIvE"
      },
      "outputs": [],
      "source": [
        "def accuracy(x, y_true, model):\n",
        "    w, b = model\n",
        "    return np.sum((prob(x, w, b)>0.5).astype(np.float64) == y_true)  / y_true.shape[0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 89,
      "metadata": {
        "id": "DZlAxkr2hIvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c826da8c-11c6-4fe5-b5c2-5f7cc844cd1c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration#:  0 ; Loss:  29.252865256171795 ; l2 norm of gradient:  7.731987578810309 ; l2 norm of weights:  7.282592099766983\n",
            "Iteration#:  4 ; Loss:  29.01513887551891 ; l2 norm of gradient:  7.700562046524516 ; l2 norm of weights:  7.252149986120583\n",
            "Iteration#:  8 ; Loss:  28.779336850260556 ; l2 norm of gradient:  7.669259035855869 ; l2 norm of weights:  7.221833381855212\n",
            "Iteration#:  12 ; Loss:  28.545443947904843 ; l2 norm of gradient:  7.638078077513907 ; l2 norm of weights:  7.191641789675225\n",
            "Iteration#:  16 ; Loss:  28.313445032120097 ; l2 norm of gradient:  7.607018703736391 ; l2 norm of weights:  7.161574714202542\n",
            "Iteration#:  20 ; Loss:  28.083325059991424 ; l2 norm of gradient:  7.576080448230538 ; l2 norm of weights:  7.131631661970241\n",
            "Iteration#:  24 ; Loss:  27.855069088453856 ; l2 norm of gradient:  7.545262846120459 ; l2 norm of weights:  7.101812141416168\n",
            "Iteration#:  28 ; Loss:  27.628662275636646 ; l2 norm of gradient:  7.5145654339009536 ; l2 norm of weights:  7.072115662876518\n",
            "Iteration#:  32 ; Loss:  27.40408988305695 ; l2 norm of gradient:  7.483987749397731 ; l2 norm of weights:  7.0425417385793905\n",
            "Iteration#:  36 ; Loss:  27.181337280866043 ; l2 norm of gradient:  7.45352933173406 ; l2 norm of weights:  7.013089882638316\n",
            "Iteration#:  40 ; Loss:  26.960389946605588 ; l2 norm of gradient:  7.423189721303765 ; l2 norm of weights:  6.983759611045761\n",
            "Iteration#:  44 ; Loss:  26.741233473804158 ; l2 norm of gradient:  7.392968459750392 ; l2 norm of weights:  6.954550441666567\n",
            "Iteration#:  48 ; Loss:  26.523853570161666 ; l2 norm of gradient:  7.362865089952359 ; l2 norm of weights:  6.925461894231382\n",
            "Iteration#:  52 ; Loss:  26.30823606172338 ; l2 norm of gradient:  7.332879156013748 ; l2 norm of weights:  6.8964934903300055\n",
            "Iteration#:  56 ; Loss:  26.094366895744827 ; l2 norm of gradient:  7.303010203260447 ; l2 norm of weights:  6.867644753404722\n",
            "Iteration#:  60 ; Loss:  25.882232139893386 ; l2 norm of gradient:  7.273257778241231 ; l2 norm of weights:  6.838915208743556\n",
            "Iteration#:  64 ; Loss:  25.67181798488617 ; l2 norm of gradient:  7.24362142873333 ; l2 norm of weights:  6.810304383473501\n",
            "Iteration#:  68 ; Loss:  25.463110744745567 ; l2 norm of gradient:  7.214100703752075 ; l2 norm of weights:  6.7818118065536765\n",
            "Iteration#:  72 ; Loss:  25.256096857930718 ; l2 norm of gradient:  7.184695153564106 ; l2 norm of weights:  6.7534370087684525\n",
            "Iteration#:  76 ; Loss:  25.05076288375423 ; l2 norm of gradient:  7.15540432970366 ; l2 norm of weights:  6.725179522720513\n",
            "Iteration#:  80 ; Loss:  24.84709550365924 ; l2 norm of gradient:  7.126227784991443 ; l2 norm of weights:  6.697038882823883\n",
            "Iteration#:  84 ; Loss:  24.64508151900659 ; l2 norm of gradient:  7.0971650735556056 ; l2 norm of weights:  6.669014625296909\n",
            "Iteration#:  88 ; Loss:  24.444707850618812 ; l2 norm of gradient:  7.068215750854325 ; l2 norm of weights:  6.641106288155196\n",
            "Iteration#:  92 ; Loss:  24.24596153425266 ; l2 norm of gradient:  7.039379373699558 ; l2 norm of weights:  6.613313411204504\n",
            "Iteration#:  96 ; Loss:  24.048829719713325 ; l2 norm of gradient:  7.010655500281498 ; l2 norm of weights:  6.585635536033619\n",
            "Iteration#:  100 ; Loss:  23.853299668504782 ; l2 norm of gradient:  6.982043690193386 ; l2 norm of weights:  6.558072206007179\n",
            "Iteration#:  104 ; Loss:  23.659358750217887 ; l2 norm of gradient:  6.953543504456256 ; l2 norm of weights:  6.530622966258491\n",
            "Iteration#:  108 ; Loss:  23.466994441428607 ; l2 norm of gradient:  6.925154505543336 ; l2 norm of weights:  6.503287363682304\n",
            "Iteration#:  112 ; Loss:  23.276194321143898 ; l2 norm of gradient:  6.896876257403796 ; l2 norm of weights:  6.47606494692757\n",
            "Iteration#:  116 ; Loss:  23.08694607091218 ; l2 norm of gradient:  6.868708325485625 ; l2 norm of weights:  6.448955266390199\n",
            "Iteration#:  120 ; Loss:  22.89923746944804 ; l2 norm of gradient:  6.8406502767574455 ; l2 norm of weights:  6.421957874205775\n",
            "Iteration#:  124 ; Loss:  22.7130563923181 ; l2 norm of gradient:  6.812701679729126 ; l2 norm of weights:  6.395072324242289\n",
            "Iteration#:  128 ; Loss:  22.528390809610443 ; l2 norm of gradient:  6.784862104471101 ; l2 norm of weights:  6.3682981720928415\n",
            "Iteration#:  132 ; Loss:  22.345228783849176 ; l2 norm of gradient:  6.757131122632353 ; l2 norm of weights:  6.341634975068363\n",
            "Iteration#:  136 ; Loss:  22.163558467073834 ; l2 norm of gradient:  6.729508307457051 ; l2 norm of weights:  6.315082292190306\n",
            "Iteration#:  140 ; Loss:  21.983368101419845 ; l2 norm of gradient:  6.701993233799894 ; l2 norm of weights:  6.288639684183363\n",
            "Iteration#:  144 ; Loss:  21.8046460159778 ; l2 norm of gradient:  6.67458547814022 ; l2 norm of weights:  6.262306713468168\n",
            "Iteration#:  148 ; Loss:  21.627380626747534 ; l2 norm of gradient:  6.647284618595021 ; l2 norm of weights:  6.23608294415401\n",
            "Iteration#:  152 ; Loss:  21.451560434785122 ; l2 norm of gradient:  6.620090234930976 ; l2 norm of weights:  6.209967942031552\n",
            "Iteration#:  156 ; Loss:  21.277174025588984 ; l2 norm of gradient:  6.593001908575686 ; l2 norm of weights:  6.183961274565547\n",
            "Iteration#:  160 ; Loss:  21.104210068559908 ; l2 norm of gradient:  6.5660192226283 ; l2 norm of weights:  6.158062510887573\n",
            "Iteration#:  164 ; Loss:  20.932657315787484 ; l2 norm of gradient:  6.53914176186975 ; l2 norm of weights:  6.132271221788768\n",
            "Iteration#:  168 ; Loss:  20.762504602001812 ; l2 norm of gradient:  6.512369112772778 ; l2 norm of weights:  6.106586979712569\n",
            "Iteration#:  172 ; Loss:  20.59374084357293 ; l2 norm of gradient:  6.485700863512026 ; l2 norm of weights:  6.081009358747466\n",
            "Iteration#:  176 ; Loss:  20.426355038465914 ; l2 norm of gradient:  6.459136603974373 ; l2 norm of weights:  6.05553793461976\n",
            "Iteration#:  180 ; Loss:  20.26033626550849 ; l2 norm of gradient:  6.432675925769787 ; l2 norm of weights:  6.030172284686327\n",
            "Iteration#:  184 ; Loss:  20.09567368424145 ; l2 norm of gradient:  6.406318422242863 ; l2 norm of weights:  6.004911987927389\n",
            "Iteration#:  188 ; Loss:  19.932356534348543 ; l2 norm of gradient:  6.380063688485293 ; l2 norm of weights:  5.9797566249392915\n",
            "Iteration#:  192 ; Loss:  19.770374135679724 ; l2 norm of gradient:  6.353911321349431 ; l2 norm of weights:  5.95470577792729\n",
            "Iteration#:  196 ; Loss:  19.609715887363695 ; l2 norm of gradient:  6.327860919463151 ; l2 norm of weights:  5.929759030698337\n",
            "Iteration#:  200 ; Loss:  19.450371268005423 ; l2 norm of gradient:  6.301912083246116 ; l2 norm of weights:  5.904915968653875\n",
            "Iteration#:  204 ; Loss:  19.292329835165503 ; l2 norm of gradient:  6.276064414927613 ; l2 norm of weights:  5.8801761787826425\n",
            "Iteration#:  208 ; Loss:  19.135581224885996 ; l2 norm of gradient:  6.25031751856604 ; l2 norm of weights:  5.855539249653474\n",
            "Iteration#:  212 ; Loss:  18.980115151549786 ; l2 norm of gradient:  6.224671000070116 ; l2 norm of weights:  5.831004771408111\n",
            "Iteration#:  216 ; Loss:  18.82592140743878 ; l2 norm of gradient:  6.1991244672218535 ; l2 norm of weights:  5.806572335754023\n",
            "Iteration#:  220 ; Loss:  18.6729898623592 ; l2 norm of gradient:  6.173677529701297 ; l2 norm of weights:  5.782241535957223\n",
            "Iteration#:  224 ; Loss:  18.521310463214295 ; l2 norm of gradient:  6.14832979911302 ; l2 norm of weights:  5.758011966835106\n",
            "Iteration#:  228 ; Loss:  18.370873233736273 ; l2 norm of gradient:  6.123080889014306 ; l2 norm of weights:  5.733883224749278\n",
            "Iteration#:  232 ; Loss:  18.221668273974778 ; l2 norm of gradient:  6.097930414944934 ; l2 norm of weights:  5.709854907598419\n",
            "Iteration#:  236 ; Loss:  18.073685759893884 ; l2 norm of gradient:  6.07287799445846 ; l2 norm of weights:  5.685926614811127\n",
            "Iteration#:  240 ; Loss:  17.92691594299968 ; l2 norm of gradient:  6.047923247154818 ; l2 norm of weights:  5.662097947338804\n",
            "Iteration#:  244 ; Loss:  17.781349149757418 ; l2 norm of gradient:  6.023065794714109 ; l2 norm of weights:  5.638368507648546\n",
            "Iteration#:  248 ; Loss:  17.636975781264706 ; l2 norm of gradient:  5.998305260931321 ; l2 norm of weights:  5.614737899716051\n",
            "Iteration#:  252 ; Loss:  17.493786312743996 ; l2 norm of gradient:  5.973641271751801 ; l2 norm of weights:  5.591205729018557\n",
            "Iteration#:  256 ; Loss:  17.35177129296852 ; l2 norm of gradient:  5.949073455307191 ; l2 norm of weights:  5.567771602527799\n",
            "Iteration#:  260 ; Loss:  17.210921343953224 ; l2 norm of gradient:  5.9246014419515864 ; l2 norm of weights:  5.544435128703\n",
            "Iteration#:  264 ; Loss:  17.071227160268393 ; l2 norm of gradient:  5.900224864297605 ; l2 norm of weights:  5.521195917483897\n",
            "Iteration#:  268 ; Loss:  16.93267950866297 ; l2 norm of gradient:  5.87594335725208 ; l2 norm of weights:  5.498053580283805\n",
            "Iteration#:  272 ; Loss:  16.795269227494096 ; l2 norm of gradient:  5.851756558051068 ; l2 norm of weights:  5.475007729982721\n",
            "Iteration#:  276 ; Loss:  16.658987226230824 ; l2 norm of gradient:  5.827664106293837 ; l2 norm of weights:  5.452057980920482\n",
            "Iteration#:  280 ; Loss:  16.523824484915494 ; l2 norm of gradient:  5.803665643975515 ; l2 norm of weights:  5.429203948889972\n",
            "Iteration#:  284 ; Loss:  16.38977205367205 ; l2 norm of gradient:  5.779760815518091 ; l2 norm of weights:  5.406445251130395\n",
            "Iteration#:  288 ; Loss:  16.256821052127343 ; l2 norm of gradient:  5.755949267799402 ; l2 norm of weights:  5.383781506320599\n",
            "Iteration#:  292 ; Loss:  16.12496266889211 ; l2 norm of gradient:  5.732230650179845 ; l2 norm of weights:  5.361212334572484\n",
            "Iteration#:  296 ; Loss:  15.99418816104864 ; l2 norm of gradient:  5.708604614526437 ; l2 norm of weights:  5.338737357424477\n",
            "Iteration#:  300 ; Loss:  15.864488853611617 ; l2 norm of gradient:  5.685070815233996 ; l2 norm of weights:  5.316356197835093\n",
            "Iteration#:  304 ; Loss:  15.735856138951995 ; l2 norm of gradient:  5.661628909243086 ; l2 norm of weights:  5.294068480176576\n",
            "Iteration#:  308 ; Loss:  15.608281476312323 ; l2 norm of gradient:  5.6382785560545114 ; l2 norm of weights:  5.271873830228639\n",
            "Iteration#:  312 ; Loss:  15.481756391233144 ; l2 norm of gradient:  5.615019417740091 ; l2 norm of weights:  5.249771875172292\n",
            "Iteration#:  316 ; Loss:  15.356272475042298 ; l2 norm of gradient:  5.59185115894949 ; l2 norm of weights:  5.227762243583792\n",
            "Iteration#:  320 ; Loss:  15.231821384285603 ; l2 norm of gradient:  5.568773446912907 ; l2 norm of weights:  5.205844565428673\n",
            "Iteration#:  324 ; Loss:  15.108394840246733 ; l2 norm of gradient:  5.54578595143944 ; l2 norm of weights:  5.184018472055908\n",
            "Iteration#:  328 ; Loss:  14.985984628363216 ; l2 norm of gradient:  5.5228883449109984 ; l2 norm of weights:  5.162283596192188\n",
            "Iteration#:  332 ; Loss:  14.864582597733241 ; l2 norm of gradient:  5.500080302271624 ; l2 norm of weights:  5.140639571936306\n",
            "Iteration#:  336 ; Loss:  14.744180660582415 ; l2 norm of gradient:  5.477361501012147 ; l2 norm of weights:  5.119086034753685\n",
            "Iteration#:  340 ; Loss:  14.624770791734072 ; l2 norm of gradient:  5.45473162115013 ; l2 norm of weights:  5.097622621471021\n",
            "Iteration#:  344 ; Loss:  14.506345028110774 ; l2 norm of gradient:  5.43219034520506 ; l2 norm of weights:  5.076248970271063\n",
            "Iteration#:  348 ; Loss:  14.388895468204444 ; l2 norm of gradient:  5.409737358168807 ; l2 norm of weights:  5.054964720687524\n",
            "Iteration#:  352 ; Loss:  14.272414271579528 ; l2 norm of gradient:  5.387372347471397 ; l2 norm of weights:  5.03376951360013\n",
            "Iteration#:  356 ; Loss:  14.15689365835619 ; l2 norm of gradient:  5.36509500294217 ; l2 norm of weights:  5.012662991229807\n",
            "Iteration#:  360 ; Loss:  14.042325908727872 ; l2 norm of gradient:  5.3429050167664265 ; l2 norm of weights:  4.9916447971340006\n",
            "Iteration#:  364 ; Loss:  13.928703362458666 ; l2 norm of gradient:  5.320802083437697 ; l2 norm of weights:  4.970714576202142\n",
            "Iteration#:  368 ; Loss:  13.816018418398453 ; l2 norm of gradient:  5.298785899705798 ; l2 norm of weights:  4.949871974651253\n",
            "Iteration#:  372 ; Loss:  13.704263533992476 ; l2 norm of gradient:  5.276856164520888 ; l2 norm of weights:  4.929116640021684\n",
            "Iteration#:  376 ; Loss:  13.593431224821202 ; l2 norm of gradient:  5.25501257897371 ; l2 norm of weights:  4.908448221172992\n",
            "Iteration#:  380 ; Loss:  13.483514064111574 ; l2 norm of gradient:  5.233254846232273 ; l2 norm of weights:  4.887866368279955\n",
            "Iteration#:  384 ; Loss:  13.37450468228668 ; l2 norm of gradient:  5.211582671475265 ; l2 norm of weights:  4.867370732828728\n",
            "Iteration#:  388 ; Loss:  13.266395766497837 ; l2 norm of gradient:  5.189995761822424 ; l2 norm of weights:  4.846960967613117\n",
            "Iteration#:  392 ; Loss:  13.159180060176533 ; l2 norm of gradient:  5.168493826262228 ; l2 norm of weights:  4.8266367267309915\n",
            "Iteration#:  396 ; Loss:  13.052850362587261 ; l2 norm of gradient:  5.147076575577173 ; l2 norm of weights:  4.806397665580818\n",
            "Iteration#:  400 ; Loss:  12.947399528388933 ; l2 norm of gradient:  5.125743722266985 ; l2 norm of weights:  4.786243440858319\n",
            "Iteration#:  404 ; Loss:  12.842820467202293 ; l2 norm of gradient:  5.104494980470112 ; l2 norm of weights:  4.766173710553246\n",
            "Iteration#:  408 ; Loss:  12.739106143175398 ; l2 norm of gradient:  5.083330065883805 ; l2 norm of weights:  4.746188133946259\n",
            "Iteration#:  412 ; Loss:  12.636249574573743 ; l2 norm of gradient:  5.062248695683169 ; l2 norm of weights:  4.7262863716059265\n",
            "Iteration#:  416 ; Loss:  12.534243833356536 ; l2 norm of gradient:  5.041250588439517 ; l2 norm of weights:  4.706468085385814\n",
            "Iteration#:  420 ; Loss:  12.433082044772 ; l2 norm of gradient:  5.020335464038356 ; l2 norm of weights:  4.686732938421681\n",
            "Iteration#:  424 ; Loss:  12.332757386954714 ; l2 norm of gradient:  4.999503043597381 ; l2 norm of weights:  4.667080595128749\n",
            "Iteration#:  428 ; Loss:  12.233263090528729 ; l2 norm of gradient:  4.978753049384782 ; l2 norm of weights:  4.647510721199076\n",
            "Iteration#:  432 ; Loss:  12.134592438217922 ; l2 norm of gradient:  4.958085204738207 ; l2 norm of weights:  4.628022983598989\n",
            "Iteration#:  436 ; Loss:  12.036738764458246 ; l2 norm of gradient:  4.937499233984692 ; l2 norm of weights:  4.608617050566598\n",
            "Iteration#:  440 ; Loss:  11.939695455023516 ; l2 norm of gradient:  4.916994862361857 ; l2 norm of weights:  4.589292591609372\n",
            "Iteration#:  444 ; Loss:  11.843455946645186 ; l2 norm of gradient:  4.896571815940677 ; l2 norm of weights:  4.570049277501765\n",
            "Iteration#:  448 ; Loss:  11.748013726651804 ; l2 norm of gradient:  4.876229821550062 ; l2 norm of weights:  4.550886780282903\n",
            "Iteration#:  452 ; Loss:  11.653362332599443 ; l2 norm of gradient:  4.855968606703564 ; l2 norm of weights:  4.531804773254303\n",
            "Iteration#:  456 ; Loss:  11.559495351918136 ; l2 norm of gradient:  4.83578789952837 ; l2 norm of weights:  4.512802930977637\n",
            "Iteration#:  460 ; Loss:  11.466406421555684 ; l2 norm of gradient:  4.815687428696879 ; l2 norm of weights:  4.493880929272524\n",
            "Iteration#:  464 ; Loss:  11.374089227632265 ; l2 norm of gradient:  4.795666923361004 ; l2 norm of weights:  4.475038445214333\n",
            "Iteration#:  468 ; Loss:  11.282537505093561 ; l2 norm of gradient:  4.775726113089406 ; l2 norm of weights:  4.4562751571320245\n",
            "Iteration#:  472 ; Loss:  11.191745037373925 ; l2 norm of gradient:  4.7558647278078094 ; l2 norm of weights:  4.437590744605978\n",
            "Iteration#:  476 ; Loss:  11.10170565605968 ; l2 norm of gradient:  4.736082497742536 ; l2 norm of weights:  4.4189848884658405\n",
            "Iteration#:  480 ; Loss:  11.012413240558786 ; l2 norm of gradient:  4.716379153367369 ; l2 norm of weights:  4.400457270788359\n",
            "Iteration#:  484 ; Loss:  10.92386171777152 ; l2 norm of gradient:  4.696754425353853 ; l2 norm of weights:  4.382007574895217\n",
            "Iteration#:  488 ; Loss:  10.836045061768615 ; l2 norm of gradient:  4.67720804452509 ; l2 norm of weights:  4.363635485350846\n",
            "Iteration#:  492 ; Loss:  10.748957293470061 ; l2 norm of gradient:  4.657739741813092 ; l2 norm of weights:  4.345340687960216\n",
            "Iteration#:  496 ; Loss:  10.662592480327863 ; l2 norm of gradient:  4.638349248219731 ; l2 norm of weights:  4.327122869766613\n",
            "Iteration#:  500 ; Loss:  10.576944736011539 ; l2 norm of gradient:  4.619036294781297 ; l2 norm of weights:  4.308981719049377\n",
            "Iteration#:  504 ; Loss:  10.492008220097977 ; l2 norm of gradient:  4.599800612536654 ; l2 norm of weights:  4.2909169253216035\n",
            "Iteration#:  508 ; Loss:  10.407777137761986 ; l2 norm of gradient:  4.580641932498997 ; l2 norm of weights:  4.272928179327819\n",
            "Iteration#:  512 ; Loss:  10.324245739470657 ; l2 norm of gradient:  4.561559985631159 ; l2 norm of weights:  4.255015173041592\n",
            "Iteration#:  516 ; Loss:  10.241408320680462 ; l2 norm of gradient:  4.542554502824422 ; l2 norm of weights:  4.237177599663121\n",
            "Iteration#:  520 ; Loss:  10.1592592215356 ; l2 norm of gradient:  4.5236252148807825 ; l2 norm of weights:  4.219415153616754\n",
            "Iteration#:  524 ; Loss:  10.077792826568741 ; l2 norm of gradient:  4.50477185249858 ; l2 norm of weights:  4.201727530548464\n",
            "Iteration#:  528 ; Loss:  9.997003564404583 ; l2 norm of gradient:  4.485994146261416 ; l2 norm of weights:  4.184114427323273\n",
            "Iteration#:  532 ; Loss:  9.916885907464195 ; l2 norm of gradient:  4.467291826630263 ; l2 norm of weights:  4.166575542022597\n",
            "Iteration#:  536 ; Loss:  9.837434371671833 ; l2 norm of gradient:  4.448664623938658 ; l2 norm of weights:  4.149110573941561\n",
            "Iteration#:  540 ; Loss:  9.758643516162639 ; l2 norm of gradient:  4.4301122683908725 ; l2 norm of weights:  4.13171922358622\n",
            "Iteration#:  544 ; Loss:  9.680507942992868 ; l2 norm of gradient:  4.411634490062935 ; l2 norm of weights:  4.114401192670742\n",
            "Iteration#:  548 ; Loss:  9.60302229685068 ; l2 norm of gradient:  4.3932310189063974 ; l2 norm of weights:  4.097156184114505\n",
            "Iteration#:  552 ; Loss:  9.526181264768578 ; l2 norm of gradient:  4.374901584754678 ; l2 norm of weights:  4.079983902039146\n",
            "Iteration#:  556 ; Loss:  9.449979575836908 ; l2 norm of gradient:  4.356645917331906 ; l2 norm of weights:  4.062884051765523\n",
            "Iteration#:  560 ; Loss:  9.374412000918625 ; l2 norm of gradient:  4.338463746264083 ; l2 norm of weights:  4.045856339810627\n",
            "Iteration#:  564 ; Loss:  9.299473352365277 ; l2 norm of gradient:  4.3203548010924475 ; l2 norm of weights:  4.028900473884414\n",
            "Iteration#:  568 ; Loss:  9.225158483733715 ; l2 norm of gradient:  4.302318811288907 ; l2 norm of weights:  4.012016162886571\n",
            "Iteration#:  572 ; Loss:  9.151462289503748 ; l2 norm of gradient:  4.284355506273387 ; l2 norm of weights:  3.995203116903218\n",
            "Iteration#:  576 ; Loss:  9.078379704797083 ; l2 norm of gradient:  4.266464615432978 ; l2 norm of weights:  3.9784610472035373\n",
            "Iteration#:  580 ; Loss:  9.00590570509642 ; l2 norm of gradient:  4.248645868142725 ; l2 norm of weights:  3.9617896662363368\n",
            "Iteration#:  584 ; Loss:  8.934035305966162 ; l2 norm of gradient:  4.23089899378794 ; l2 norm of weights:  3.945188687626545\n",
            "Iteration#:  588 ; Loss:  8.862763562773091 ; l2 norm of gradient:  4.2132237217879 ; l2 norm of weights:  3.9286578261716447\n",
            "Iteration#:  592 ; Loss:  8.792085570408458 ; l2 norm of gradient:  4.195619781620806 ; l2 norm of weights:  3.912196797838036\n",
            "Iteration#:  596 ; Loss:  8.721996463010363 ; l2 norm of gradient:  4.178086902849866 ; l2 norm of weights:  3.895805319757337\n",
            "Iteration#:  600 ; Loss:  8.652491413686965 ; l2 norm of gradient:  4.160624815150392 ; l2 norm of weights:  3.879483110222625\n",
            "Iteration#:  604 ; Loss:  8.583565634240342 ; l2 norm of gradient:  4.143233248337778 ; l2 norm of weights:  3.863229888684612\n",
            "Iteration#:  608 ; Loss:  8.515214374891087 ; l2 norm of gradient:  4.125911932396266 ; l2 norm of weights:  3.847045375747766\n",
            "Iteration#:  612 ; Loss:  8.44743292400338 ; l2 norm of gradient:  4.108660597508361 ; l2 norm of weights:  3.8309292931663674\n",
            "Iteration#:  616 ; Loss:  8.380216607811024 ; l2 norm of gradient:  4.091478974084802 ; l2 norm of weights:  3.8148813638405175\n",
            "Iteration#:  620 ; Loss:  8.313560790143804 ; l2 norm of gradient:  4.074366792794989 ; l2 norm of weights:  3.7989013118120827\n",
            "Iteration#:  624 ; Loss:  8.247460872154836 ; l2 norm of gradient:  4.057323784597764 ; l2 norm of weights:  3.782988862260594\n",
            "Iteration#:  628 ; Loss:  8.181912292048171 ; l2 norm of gradient:  4.040349680772442 ; l2 norm of weights:  3.7671437414990945\n",
            "Iteration#:  632 ; Loss:  8.116910524807611 ; l2 norm of gradient:  4.023444212950009 ; l2 norm of weights:  3.7513656769699355\n",
            "Iteration#:  636 ; Loss:  8.052451081925717 ; l2 norm of gradient:  4.006607113144416 ; l2 norm of weights:  3.7356543972405274\n",
            "Iteration#:  640 ; Loss:  7.988529511133899 ; l2 norm of gradient:  3.98983811378385 ; l2 norm of weights:  3.7200096319990488\n",
            "Iteration#:  644 ; Loss:  7.925141396133018 ; l2 norm of gradient:  3.9731369477419434 ; l2 norm of weights:  3.7044311120501066\n",
            "Iteration#:  648 ; Loss:  7.862282356324825 ; l2 norm of gradient:  3.956503348368817 ; l2 norm of weights:  3.6889185693103657\n",
            "Iteration#:  652 ; Loss:  7.799948046544196 ; l2 norm of gradient:  3.9399370495219124 ; l2 norm of weights:  3.673471736804132\n",
            "Iteration#:  656 ; Loss:  7.738134156792105 ; l2 norm of gradient:  3.923437785596528 ; l2 norm of weights:  3.658090348658906\n",
            "Iteration#:  660 ; Loss:  7.676836411969376 ; l2 norm of gradient:  3.907005291556008 ; l2 norm of weights:  3.642774140100898\n",
            "Iteration#:  664 ; Loss:  7.61605057161152 ; l2 norm of gradient:  3.8906393029615134 ; l2 norm of weights:  3.6275228474505194\n",
            "Iteration#:  668 ; Loss:  7.555772429624071 ; l2 norm of gradient:  3.874339556001342 ; l2 norm of weights:  3.6123362081178327\n",
            "Iteration#:  672 ; Loss:  7.4959978140192005 ; l2 norm of gradient:  3.8581057875197247 ; l2 norm of weights:  3.5972139605979936\n",
            "Iteration#:  676 ; Loss:  7.436722586653037 ; l2 norm of gradient:  3.841937735045062 ; l2 norm of weights:  3.5821558444666493\n",
            "Iteration#:  680 ; Loss:  7.377942642963973 ; l2 norm of gradient:  3.825835136817548 ; l2 norm of weights:  3.567161600375329\n",
            "Iteration#:  684 ; Loss:  7.319653911712081 ; l2 norm of gradient:  3.8097977318161504 ; l2 norm of weights:  3.552230970046807\n",
            "Iteration#:  688 ; Loss:  7.261852354719401 ; l2 norm of gradient:  3.793825259784906 ; l2 norm of weights:  3.5373636962704524\n",
            "Iteration#:  692 ; Loss:  7.204533966611417 ; l2 norm of gradient:  3.777917461258483 ; l2 norm of weights:  3.522559522897561\n",
            "Iteration#:  696 ; Loss:  7.147694774559529 ; l2 norm of gradient:  3.7620740775870063 ; l2 norm of weights:  3.507818194836676\n",
            "Iteration#:  700 ; Loss:  7.091330838024677 ; l2 norm of gradient:  3.746294850960084 ; l2 norm of weights:  3.4931394580488977\n",
            "Iteration#:  704 ; Loss:  7.035438248502127 ; l2 norm of gradient:  3.7305795244300373 ; l2 norm of weights:  3.478523059543182\n",
            "Iteration#:  708 ; Loss:  6.980013129267399 ; l2 norm of gradient:  3.7149278419342906 ; l2 norm of weights:  3.4639687473716347\n",
            "Iteration#:  712 ; Loss:  6.925051635123443 ; l2 norm of gradient:  3.699339548316912 ; l2 norm of weights:  3.449476270624802\n",
            "Iteration#:  716 ; Loss:  6.8705499521490285 ; l2 norm of gradient:  3.6838143893492834 ; l2 norm of weights:  3.435045379426956\n",
            "Iteration#:  720 ; Loss:  6.816504297448404 ; l2 norm of gradient:  3.6683521117498814 ; l2 norm of weights:  3.42067582493138\n",
            "Iteration#:  724 ; Loss:  6.76291091890227 ; l2 norm of gradient:  3.652952463203161 ; l2 norm of weights:  3.4063673593156576\n",
            "Iteration#:  728 ; Loss:  6.709766094920063 ; l2 norm of gradient:  3.6376151923775324 ; l2 norm of weights:  3.392119735776965\n",
            "Iteration#:  732 ; Loss:  6.657066134193632 ; l2 norm of gradient:  3.622340048942409 ; l2 norm of weights:  3.3779327085273607\n",
            "Iteration#:  736 ; Loss:  6.604807375452226 ; l2 norm of gradient:  3.607126783584344 ; l2 norm of weights:  3.3638060327890984\n",
            "Iteration#:  740 ; Loss:  6.552986187218949 ; l2 norm of gradient:  3.5919751480222297 ; l2 norm of weights:  3.34973946478993\n",
            "Iteration#:  744 ; Loss:  6.501598967568656 ; l2 norm of gradient:  3.57688489502157 ; l2 norm of weights:  3.335732761758435\n",
            "Iteration#:  748 ; Loss:  6.450642143887241 ; l2 norm of gradient:  3.561855778407821 ; l2 norm of weights:  3.321785681919349\n",
            "Iteration#:  752 ; Loss:  6.400112172632498 ; l2 norm of gradient:  3.5468875530788098 ; l2 norm of weights:  3.307897984488918\n",
            "Iteration#:  756 ; Loss:  6.350005539096387 ; l2 norm of gradient:  3.5319799750162186 ; l2 norm of weights:  3.294069429670258\n",
            "Iteration#:  760 ; Loss:  6.30031875716895 ; l2 norm of gradient:  3.517132801296161 ; l2 norm of weights:  3.280299778648737\n",
            "Iteration#:  764 ; Loss:  6.251048369103641 ; l2 norm of gradient:  3.5023457900988406 ; l2 norm of weights:  3.2665887935873723\n",
            "Iteration#:  768 ; Loss:  6.202190945284341 ; l2 norm of gradient:  3.4876187007173085 ; l2 norm of weights:  3.2529362376222557\n",
            "Iteration#:  772 ; Loss:  6.153743083993891 ; l2 norm of gradient:  3.472951293565333 ; l2 norm of weights:  3.239341874857983\n",
            "Iteration#:  776 ; Loss:  6.105701411184261 ; l2 norm of gradient:  3.4583433301843813 ; l2 norm of weights:  3.2258054703631216\n",
            "Iteration#:  780 ; Loss:  6.058062580248308 ; l2 norm of gradient:  3.4437945732497393 ; l2 norm of weights:  3.2123267901656956\n",
            "Iteration#:  784 ; Loss:  6.010823271793245 ; l2 norm of gradient:  3.4293047865757753 ; l2 norm of weights:  3.198905601248695\n",
            "Iteration#:  788 ; Loss:  5.963980193415658 ; l2 norm of gradient:  3.4148737351203633 ; l2 norm of weights:  3.1855416715456086\n",
            "Iteration#:  792 ; Loss:  5.917530079478294 ; l2 norm of gradient:  3.40050118498849 ; l2 norm of weights:  3.172234769935991\n",
            "Iteration#:  796 ; Loss:  5.871469690888441 ; l2 norm of gradient:  3.3861869034350445 ; l2 norm of weights:  3.1589846662410466\n",
            "Iteration#:  800 ; Loss:  5.8257958148780595 ; l2 norm of gradient:  3.3719306588668325 ; l2 norm of weights:  3.145791131219253\n",
            "Iteration#:  804 ; Loss:  5.78050526478558 ; l2 norm of gradient:  3.3577322208438067 ; l2 norm of weights:  3.1326539365620074\n",
            "Iteration#:  808 ; Loss:  5.735594879839431 ; l2 norm of gradient:  3.3435913600795595 ; l2 norm of weights:  3.1195728548893045\n",
            "Iteration#:  812 ; Loss:  5.691061524943253 ; l2 norm of gradient:  3.3295078484410716 ; l2 norm of weights:  3.1065476597454436\n",
            "Iteration#:  816 ; Loss:  5.646902090462884 ; l2 norm of gradient:  3.315481458947756 ; l2 norm of weights:  3.0935781255947754\n",
            "Iteration#:  820 ; Loss:  5.603113492015033 ; l2 norm of gradient:  3.3015119657698064 ; l2 norm of weights:  3.08066402781747\n",
            "Iteration#:  824 ; Loss:  5.559692670257713 ; l2 norm of gradient:  3.2875991442258754 ; l2 norm of weights:  3.067805142705327\n",
            "Iteration#:  828 ; Loss:  5.516636590682398 ; l2 norm of gradient:  3.2737427707801023 ; l2 norm of weights:  3.0550012474576125\n",
            "Iteration#:  832 ; Loss:  5.473942243407945 ; l2 norm of gradient:  3.2599426230385067 ; l2 norm of weights:  3.042252120176937\n",
            "Iteration#:  836 ; Loss:  5.431606642976233 ; l2 norm of gradient:  3.2461984797447787 ; l2 norm of weights:  3.0295575398651597\n",
            "Iteration#:  840 ; Loss:  5.389626828149574 ; l2 norm of gradient:  3.2325101207754807 ; l2 norm of weights:  3.016917286419337\n",
            "Iteration#:  844 ; Loss:  5.3479998617098605 ; l2 norm of gradient:  3.2188773271346762 ; l2 norm of weights:  3.0043311406276985\n",
            "Iteration#:  848 ; Loss:  5.306722830259447 ; l2 norm of gradient:  3.2052998809480195 ; l2 norm of weights:  2.9917988841656618\n",
            "Iteration#:  852 ; Loss:  5.265792844023788 ; l2 norm of gradient:  3.1917775654563245 ; l2 norm of weights:  2.979320299591883\n",
            "Iteration#:  856 ; Loss:  5.225207036655836 ; l2 norm of gradient:  3.178310165008615 ; l2 norm of weights:  2.966895170344346\n",
            "Iteration#:  860 ; Loss:  5.1849625650421345 ; l2 norm of gradient:  3.1648974650547164 ; l2 norm of weights:  2.9545232807364803\n",
            "Iteration#:  864 ; Loss:  5.145056609110692 ; l2 norm of gradient:  3.1515392521373653 ; l2 norm of weights:  2.942204415953326\n",
            "Iteration#:  868 ; Loss:  5.105486371640554 ; l2 norm of gradient:  3.1382353138838917 ; l2 norm of weights:  2.929938362047725\n",
            "Iteration#:  872 ; Loss:  5.066249078073124 ; l2 norm of gradient:  3.1249854389974825 ; l2 norm of weights:  2.9177249059365575\n",
            "Iteration#:  876 ; Loss:  5.027341976325185 ; l2 norm of gradient:  3.111789417248041 ; l2 norm of weights:  2.905563835397008\n",
            "Iteration#:  880 ; Loss:  4.988762336603648 ; l2 norm of gradient:  3.0986470394626773 ; l2 norm of weights:  2.8934549390628717\n",
            "Iteration#:  884 ; Loss:  4.950507451222006 ; l2 norm of gradient:  3.0855580975158383 ; l2 norm of weights:  2.8813980064209\n",
            "Iteration#:  888 ; Loss:  4.912574634418466 ; l2 norm of gradient:  3.0725223843191056 ; l2 norm of weights:  2.8693928278071743\n",
            "Iteration#:  892 ; Loss:  4.874961222175798 ; l2 norm of gradient:  3.0595396938106774 ; l2 norm of weights:  2.8574391944035247\n",
            "Iteration#:  896 ; Loss:  4.837664572042849 ; l2 norm of gradient:  3.046609820944558 ; l2 norm of weights:  2.84553689823398\n",
            "Iteration#:  900 ; Loss:  4.800682062957719 ; l2 norm of gradient:  3.0337325616794777 ; l2 norm of weights:  2.8336857321612565\n",
            "Iteration#:  904 ; Loss:  4.764011095072621 ; l2 norm of gradient:  3.0209077129675554 ; l2 norm of weights:  2.8218854898832793\n",
            "Iteration#:  908 ; Loss:  4.727649089580359 ; l2 norm of gradient:  3.0081350727427334 ; l2 norm of weights:  2.81013596592974\n",
            "Iteration#:  912 ; Loss:  4.691593488542478 ; l2 norm of gradient:  2.995414439909003 ; l2 norm of weights:  2.798436955658695\n",
            "Iteration#:  916 ; Loss:  4.655841754719001 ; l2 norm of gradient:  2.9827456143284325 ; l2 norm of weights:  2.7867882552531875\n",
            "Iteration#:  920 ; Loss:  4.620391371399813 ; l2 norm of gradient:  2.9701283968090393 ; l2 norm of weights:  2.775189661717915\n",
            "Iteration#:  924 ; Loss:  4.585239842237634 ; l2 norm of gradient:  2.9575625890924986 ; l2 norm of weights:  2.7636409728759275\n",
            "Iteration#:  928 ; Loss:  4.550384691082565 ; l2 norm of gradient:  2.945047993841728 ; l2 norm of weights:  2.752141987365352\n",
            "Iteration#:  932 ; Loss:  4.515823461818231 ; l2 norm of gradient:  2.9325844146283746 ; l2 norm of weights:  2.7406925046361645\n",
            "Iteration#:  936 ; Loss:  4.481553718199467 ; l2 norm of gradient:  2.9201716559201927 ; l2 norm of weights:  2.729292324946979\n",
            "Iteration#:  940 ; Loss:  4.447573043691555 ; l2 norm of gradient:  2.9078095230683747 ; l2 norm of weights:  2.7179412493618815\n",
            "Iteration#:  944 ; Loss:  4.413879041310992 ; l2 norm of gradient:  2.895497822294819 ; l2 norm of weights:  2.7066390797472875\n",
            "Iteration#:  948 ; Loss:  4.380469333467769 ; l2 norm of gradient:  2.883236360679382 ; l2 norm of weights:  2.6953856187688348\n",
            "Iteration#:  952 ; Loss:  4.347341561809156 ; l2 norm of gradient:  2.871024946147109 ; l2 norm of weights:  2.6841806698883053\n",
            "Iteration#:  956 ; Loss:  4.314493387064962 ; l2 norm of gradient:  2.8588633874554854 ; l2 norm of weights:  2.673024037360575\n",
            "Iteration#:  960 ; Loss:  4.281922488894276 ; l2 norm of gradient:  2.846751494181718 ; l2 norm of weights:  2.6619155262305987\n",
            "Iteration#:  964 ; Loss:  4.249626565733639 ; l2 norm of gradient:  2.834689076710058 ; l2 norm of weights:  2.650854942330414\n",
            "Iteration#:  968 ; Loss:  4.217603334646672 ; l2 norm of gradient:  2.822675946219204 ; l2 norm of weights:  2.639842092276184\n",
            "Iteration#:  972 ; Loss:  4.185850531175111 ; l2 norm of gradient:  2.8107119146697777 ; l2 norm of weights:  2.6288767834652575\n",
            "Iteration#:  976 ; Loss:  4.1543659091912435 ; l2 norm of gradient:  2.798796794791914 ; l2 norm of weights:  2.6179588240732627\n",
            "Iteration#:  980 ; Loss:  4.123147240751728 ; l2 norm of gradient:  2.786930400072969 ; l2 norm of weights:  2.6070880230512223\n",
            "Iteration#:  984 ; Loss:  4.0921923159527855 ; l2 norm of gradient:  2.7751125447453657 ; l2 norm of weights:  2.596264190122695\n",
            "Iteration#:  988 ; Loss:  4.061498942786732 ; l2 norm of gradient:  2.7633430437745976 ; l2 norm of weights:  2.5854871357809417\n",
            "Iteration#:  992 ; Loss:  4.031064946999847 ; l2 norm of gradient:  2.751621712847397 ; l2 norm of weights:  2.574756671286114\n",
            "Iteration#:  996 ; Loss:  4.000888171951557 ; l2 norm of gradient:  2.7399483683601016 ; l2 norm of weights:  2.5640726086624666\n",
            "Iteration#:  1000 ; Loss:  3.9709664784749084 ; l2 norm of gradient:  2.728322827407206 ; l2 norm of weights:  2.5534347606955903\n",
            "Iteration#:  1004 ; Loss:  3.941297744738323 ; l2 norm of gradient:  2.7167449077701438 ; l2 norm of weights:  2.5428429409296656\n",
            "Iteration#:  1008 ; Loss:  3.911879866108611 ; l2 norm of gradient:  2.705214427906284 ; l2 norm of weights:  2.532296963664739\n",
            "Iteration#:  1012 ; Loss:  3.882710755015228 ; l2 norm of gradient:  2.6937312069381796 ; l2 norm of weights:  2.521796643954018\n",
            "Iteration#:  1016 ; Loss:  3.8537883408157523 ; l2 norm of gradient:  2.682295064643065 ; l2 norm of weights:  2.5113417976011787\n",
            "Iteration#:  1020 ; Loss:  3.8251105696625727 ; l2 norm of gradient:  2.6709058214426094 ; l2 norm of weights:  2.5009322411577037\n",
            "Iteration#:  1024 ; Loss:  3.796675404370763 ; l2 norm of gradient:  2.6595632983929582 ; l2 norm of weights:  2.4905677919202245\n",
            "Iteration#:  1028 ; Loss:  3.768480824287125 ; l2 norm of gradient:  2.648267317175045 ; l2 norm of weights:  2.4802482679278874\n",
            "Iteration#:  1032 ; Loss:  3.740524825160386 ; l2 norm of gradient:  2.6370177000852006 ; l2 norm of weights:  2.469973487959732\n",
            "Iteration#:  1036 ; Loss:  3.7128054190125344 ; l2 norm of gradient:  2.6258142700260607 ; l2 norm of weights:  2.4597432715320866\n",
            "Iteration#:  1040 ; Loss:  3.685320634011272 ; l2 norm of gradient:  2.6146568504977803 ; l2 norm of weights:  2.449557438895974\n",
            "Iteration#:  1044 ; Loss:  3.658068514343556 ; l2 norm of gradient:  2.6035452655895543 ; l2 norm of weights:  2.4394158110345336\n",
            "Iteration#:  1048 ; Loss:  3.6310471200902543 ; l2 norm of gradient:  2.592479339971463 ; l2 norm of weights:  2.429318209660456\n",
            "Iteration#:  1052 ; Loss:  3.6042545271018307 ; l2 norm of gradient:  2.5814588988866296 ; l2 norm of weights:  2.419264457213424\n",
            "Iteration#:  1056 ; Loss:  3.577688826875111 ; l2 norm of gradient:  2.5704837681437076 ; l2 norm of weights:  2.4092543768575747\n",
            "Iteration#:  1060 ; Loss:  3.551348126431066 ; l2 norm of gradient:  2.55955377410969 ; l2 norm of weights:  2.39928779247896\n",
            "Iteration#:  1064 ; Loss:  3.525230548193622 ; l2 norm of gradient:  2.548668743703048 ; l2 norm of weights:  2.3893645286830254\n",
            "Iteration#:  1068 ; Loss:  3.499334229869466 ; l2 norm of gradient:  2.537828504387195 ; l2 norm of weights:  2.379484410792094\n",
            "Iteration#:  1072 ; Loss:  3.4736573243288453 ; l2 norm of gradient:  2.5270328841642815 ; l2 norm of weights:  2.3696472648428584\n",
            "Iteration#:  1076 ; Loss:  3.448197999487332 ; l2 norm of gradient:  2.5162817115693104 ; l2 norm of weights:  2.3598529175838823\n",
            "Iteration#:  1080 ; Loss:  3.422954438188553 ; l2 norm of gradient:  2.5055748156645845 ; l2 norm of weights:  2.3501011964731098\n",
            "Iteration#:  1084 ; Loss:  3.3979248380878504 ; l2 norm of gradient:  2.4949120260344655 ; l2 norm of weights:  2.3403919296753757\n",
            "Iteration#:  1088 ; Loss:  3.37310741153688 ; l2 norm of gradient:  2.484293172780461 ; l2 norm of weights:  2.3307249460599277\n",
            "Iteration#:  1092 ; Loss:  3.348500385469117 ; l2 norm of gradient:  2.4737180865166195 ; l2 norm of weights:  2.321100075197952\n",
            "Iteration#:  1096 ; Loss:  3.3241020012862657 ; l2 norm of gradient:  2.4631865983652355 ; l2 norm of weights:  2.3115171473601017\n",
            "Iteration#:  1100 ; Loss:  3.2999105147455543 ; l2 norm of gradient:  2.452698539952857 ; l2 norm of weights:  2.3019759935140343\n",
            "Iteration#:  1104 ; Loss:  3.275924195847903 ; l2 norm of gradient:  2.442253743406597 ; l2 norm of weights:  2.2924764453219466\n",
            "Iteration#:  1108 ; Loss:  3.2521413287269567 ; l2 norm of gradient:  2.4318520413507265 ; l2 norm of weights:  2.28301833513812\n",
            "Iteration#:  1112 ; Loss:  3.2285602115389658 ; l2 norm of gradient:  2.421493266903561 ; l2 norm of weights:  2.2736014960064654\n",
            "Iteration#:  1116 ; Loss:  3.205179156353494 ; l2 norm of gradient:  2.4111772536746146 ; l2 norm of weights:  2.2642257616580674\n",
            "Iteration#:  1120 ; Loss:  3.181996489044977 ; l2 norm of gradient:  2.4009038357620325 ; l2 norm of weights:  2.25489096650874\n",
            "Iteration#:  1124 ; Loss:  3.1590105491850666 ; l2 norm of gradient:  2.390672847750273 ; l2 norm of weights:  2.2455969456565765\n",
            "Iteration#:  1128 ; Loss:  3.136219689935799 ; l2 norm of gradient:  2.3804841247080484 ; l2 norm of weights:  2.236343534879502\n",
            "Iteration#:  1132 ; Loss:  3.113622277943546 ; l2 norm of gradient:  2.370337502186509 ; l2 norm of weights:  2.227130570632833\n",
            "Iteration#:  1136 ; Loss:  3.091216693233759 ; l2 norm of gradient:  2.360232816217652 ; l2 norm of weights:  2.2179578900468306\n",
            "Iteration#:  1140 ; Loss:  3.0690013291064746 ; l2 norm of gradient:  2.3501699033129655 ; l2 norm of weights:  2.208825330924262\n",
            "Iteration#:  1144 ; Loss:  3.046974592032596 ; l2 norm of gradient:  2.3401486004622765 ; l2 norm of weights:  2.1997327317379547\n",
            "Iteration#:  1148 ; Loss:  3.025134901550925 ; l2 norm of gradient:  2.3301687451328092 ; l2 norm of weights:  2.190679931628363\n",
            "Iteration#:  1152 ; Loss:  3.003480690165934 ; l2 norm of gradient:  2.3202301752684367 ; l2 norm of weights:  2.181666770401121\n",
            "Iteration#:  1156 ; Loss:  2.9820104032462913 ; l2 norm of gradient:  2.310332729289114 ; l2 norm of weights:  2.1726930885246065\n",
            "Iteration#:  1160 ; Loss:  2.960722498924104 ; l2 norm of gradient:  2.3004762460904877 ; l2 norm of weights:  2.1637587271275014\n",
            "Iteration#:  1164 ; Loss:  2.939615447994884 ; l2 norm of gradient:  2.290660565043673 ; l2 norm of weights:  2.1548635279963526\n",
            "Iteration#:  1168 ; Loss:  2.9186877338182393 ; l2 norm of gradient:  2.280885525995179 ; l2 norm of weights:  2.146007333573132\n",
            "Iteration#:  1172 ; Loss:  2.897937852219259 ; l2 norm of gradient:  2.27115096926698 ; l2 norm of weights:  2.1371899869527975\n",
            "Iteration#:  1176 ; Loss:  2.877364311390611 ; l2 norm of gradient:  2.2614567356567266 ; l2 norm of weights:  2.128411331880856\n",
            "Iteration#:  1180 ; Loss:  2.856965631795326 ; l2 norm of gradient:  2.251802666438075 ; l2 norm of weights:  2.119671212750922\n",
            "Iteration#:  1184 ; Loss:  2.836740346070274 ; l2 norm of gradient:  2.2421886033611362 ; l2 norm of weights:  2.1109694746022796\n",
            "Iteration#:  1188 ; Loss:  2.8166869989303227 ; l2 norm of gradient:  2.2326143886530256 ; l2 norm of weights:  2.1023059631174434\n",
            "Iteration#:  1192 ; Loss:  2.796804147073167 ; l2 norm of gradient:  2.223079865018518 ; l2 norm of weights:  2.0936805246197205\n",
            "Iteration#:  1196 ; Loss:  2.777090359084835 ; l2 norm of gradient:  2.213584875640782 ; l2 norm of weights:  2.08509300607077\n",
            "Iteration#:  1200 ; Loss:  2.757544215345859 ; l2 norm of gradient:  2.2041292641821975 ; l2 norm of weights:  2.076543255068167\n",
            "Iteration#:  1204 ; Loss:  2.7381643079380966 ; l2 norm of gradient:  2.194712874785242 ; l2 norm of weights:  2.06803111984296\n",
            "Iteration#:  1208 ; Loss:  2.718949240552219 ; l2 norm of gradient:  2.1853355520734423 ; l2 norm of weights:  2.059556449257237\n",
            "Iteration#:  1212 ; Loss:  2.6998976283958465 ; l2 norm of gradient:  2.1759971411523735 ; l2 norm of weights:  2.051119092801686\n",
            "Iteration#:  1216 ; Loss:  2.6810080981023208 ; l2 norm of gradient:  2.166697487610713 ; l2 norm of weights:  2.042718900593157\n",
            "Iteration#:  1220 ; Loss:  2.662279287640131 ; l2 norm of gradient:  2.1574364375213246 ; l2 norm of weights:  2.0343557233722285\n",
            "Iteration#:  1224 ; Loss:  2.6437098462229702 ; l2 norm of gradient:  2.148213837442383 ; l2 norm of weights:  2.0260294125007694\n",
            "Iteration#:  1228 ; Loss:  2.625298434220419 ; l2 norm of gradient:  2.139029534418516 ; l2 norm of weights:  2.017739819959503\n",
            "Iteration#:  1232 ; Loss:  2.6070437230692702 ; l2 norm of gradient:  2.1298833759819678 ; l2 norm of weights:  2.0094867983455766\n",
            "Iteration#:  1236 ; Loss:  2.5889443951854703 ; l2 norm of gradient:  2.1207752101537767 ; l2 norm of weights:  2.001270200870126\n",
            "Iteration#:  1240 ; Loss:  2.570999143876683 ; l2 norm of gradient:  2.1117048854449556 ; l2 norm of weights:  1.993089881355845\n",
            "Iteration#:  1244 ; Loss:  2.5532066732554735 ; l2 norm of gradient:  2.102672250857674 ; l2 norm of weights:  1.9849456942345565\n",
            "Iteration#:  1248 ; Loss:  2.5355656981531003 ; l2 norm of gradient:  2.093677155886434 ; l2 norm of weights:  1.9768374945447817\n",
            "Iteration#:  1252 ; Loss:  2.5180749440339296 ; l2 norm of gradient:  2.084719450519241 ; l2 norm of weights:  1.9687651379293172\n",
            "Iteration#:  1256 ; Loss:  2.500733146910442 ; l2 norm of gradient:  2.0757989852387557 ; l2 norm of weights:  1.9607284806328062\n",
            "Iteration#:  1260 ; Loss:  2.4835390532588564 ; l2 norm of gradient:  2.0669156110234255 ; l2 norm of weights:  1.9527273794993196\n",
            "Iteration#:  1264 ; Loss:  2.4664914199353487 ; l2 norm of gradient:  2.058069179348599 ; l2 norm of weights:  1.9447616919699335\n",
            "Iteration#:  1268 ; Loss:  2.449589014092867 ; l2 norm of gradient:  2.0492595421876083 ; l2 norm of weights:  1.9368312760803117\n",
            "Iteration#:  1272 ; Loss:  2.432830613098549 ; l2 norm of gradient:  2.040486552012822 ; l2 norm of weights:  1.9289359904582914\n",
            "Iteration#:  1276 ; Loss:  2.416215004451722 ; l2 norm of gradient:  2.0317500617966693 ; l2 norm of weights:  1.9210756943214693\n",
            "Iteration#:  1280 ; Loss:  2.399740985702502 ; l2 norm of gradient:  2.0230499250126157 ; l2 norm of weights:  1.913250247474793\n",
            "Iteration#:  1284 ; Loss:  2.383407364370969 ; l2 norm of gradient:  2.0143859956361125 ; l2 norm of weights:  1.905459510308153\n",
            "Iteration#:  1288 ; Loss:  2.367212957866934 ; l2 norm of gradient:  2.005758128145499 ; l2 norm of weights:  1.8977033437939816\n",
            "Iteration#:  1292 ; Loss:  2.3511565934102827 ; l2 norm of gradient:  1.9971661775228526 ; l2 norm of weights:  1.88998160948485\n",
            "Iteration#:  1296 ; Loss:  2.3352371079518974 ; l2 norm of gradient:  1.9886099992548099 ; l2 norm of weights:  1.8822941695110744\n",
            "Iteration#:  1300 ; Loss:  2.3194533480951534 ; l2 norm of gradient:  1.980089449333322 ; l2 norm of weights:  1.8746408865783215\n",
            "Iteration#:  1304 ; Loss:  2.3038041700179916 ; l2 norm of gradient:  1.9716043842563715 ; l2 norm of weights:  1.8670216239652202\n",
            "Iteration#:  1308 ; Loss:  2.288288439395555 ; l2 norm of gradient:  1.9631546610286337 ; l2 norm of weights:  1.8594362455209765\n",
            "Iteration#:  1312 ; Loss:  2.2729050313233983 ; l2 norm of gradient:  1.9547401371620863 ; l2 norm of weights:  1.8518846156629925\n",
            "Iteration#:  1316 ; Loss:  2.257652830241255 ; l2 norm of gradient:  1.9463606706765677 ; l2 norm of weights:  1.8443665993744898\n",
            "Iteration#:  1320 ; Loss:  2.2425307298573736 ; l2 norm of gradient:  1.9380161201002792 ; l2 norm of weights:  1.8368820622021378\n",
            "Iteration#:  1324 ; Loss:  2.2275376330734065 ; l2 norm of gradient:  1.9297063444702323 ; l2 norm of weights:  1.829430870253685\n",
            "Iteration#:  1328 ; Loss:  2.2126724519098593 ; l2 norm of gradient:  1.9214312033326426 ; l2 norm of weights:  1.8220128901955968\n",
            "Iteration#:  1332 ; Loss:  2.197934107432093 ; l2 norm of gradient:  1.9131905567432679 ; l2 norm of weights:  1.8146279892506971\n",
            "Iteration#:  1336 ; Loss:  2.1833215296768724 ; l2 norm of gradient:  1.9049842652676878 ; l2 norm of weights:  1.807276035195814\n",
            "Iteration#:  1340 ; Loss:  2.1688336575794724 ; l2 norm of gradient:  1.8968121899815331 ; l2 norm of weights:  1.7999568963594335\n",
            "Iteration#:  1344 ; Loss:  2.1544694389013177 ; l2 norm of gradient:  1.888674192470653 ; l2 norm of weights:  1.7926704416193537\n",
            "Iteration#:  1348 ; Loss:  2.140227830158178 ; l2 norm of gradient:  1.88057013483123 ; l2 norm of weights:  1.7854165404003497\n",
            "Iteration#:  1352 ; Loss:  2.126107796548893 ; l2 norm of gradient:  1.8724998796698422 ; l2 norm of weights:  1.7781950626718381\n",
            "Iteration#:  1356 ; Loss:  2.1121083118846404 ; l2 norm of gradient:  1.8644632901034652 ; l2 norm of weights:  1.771005878945552\n",
            "Iteration#:  1360 ; Loss:  2.098228358518735 ; l2 norm of gradient:  1.856460229759422 ; l2 norm of weights:  1.763848860273218\n",
            "Iteration#:  1364 ; Loss:  2.084466927276966 ; l2 norm of gradient:  1.84849056277528 ; l2 norm of weights:  1.7567238782442411\n",
            "Iteration#:  1368 ; Loss:  2.070823017388454 ; l2 norm of gradient:  1.840554153798694 ; l2 norm of weights:  1.7496308049833935\n",
            "Iteration#:  1372 ; Loss:  2.057295636417044 ; l2 norm of gradient:  1.8326508679871958 ; l2 norm of weights:  1.7425695131485104\n",
            "Iteration#:  1376 ; Loss:  2.043883800193215 ; l2 norm of gradient:  1.8247805710079317 ; l2 norm of weights:  1.7355398759281921\n",
            "Iteration#:  1380 ; Loss:  2.030586532746512 ; l2 norm of gradient:  1.8169431290373514 ; l2 norm of weights:  1.7285417670395098\n",
            "Iteration#:  1384 ; Loss:  2.0174028662385 ; l2 norm of gradient:  1.809138408760843 ; l2 norm of weights:  1.7215750607257196\n",
            "Iteration#:  1388 ; Loss:  2.0043318408962274 ; l2 norm of gradient:  1.801366277372322 ; l2 norm of weights:  1.7146396317539823\n",
            "Iteration#:  1392 ; Loss:  1.991372504946204 ; l2 norm of gradient:  1.793626602573769 ; l2 norm of weights:  1.707735355413087\n",
            "Iteration#:  1396 ; Loss:  1.9785239145488918 ; l2 norm of gradient:  1.7859192525747234 ; l2 norm of weights:  1.700862107511186\n",
            "Iteration#:  1400 ; Loss:  1.9657851337336931 ; l2 norm of gradient:  1.7782440960917267 ; l2 norm of weights:  1.694019764373529\n",
            "Iteration#:  1404 ; Loss:  1.9531552343344563 ; l2 norm of gradient:  1.770601002347723 ; l2 norm of weights:  1.68720820284021\n",
            "Iteration#:  1408 ; Loss:  1.9406332959254655 ; l2 norm of gradient:  1.7629898410714144 ; l2 norm of weights:  1.6804273002639152\n",
            "Iteration#:  1412 ; Loss:  1.928218405757945 ; l2 norm of gradient:  1.7554104824965737 ; l2 norm of weights:  1.6736769345076834\n",
            "Iteration#:  1416 ; Loss:  1.9159096586970445 ; l2 norm of gradient:  1.747862797361313 ; l2 norm of weights:  1.6669569839426652\n",
            "Iteration#:  1420 ; Loss:  1.9037061571593257 ; l2 norm of gradient:  1.7403466569073134 ; l2 norm of weights:  1.6602673274458954\n",
            "Iteration#:  1424 ; Loss:  1.8916070110507328 ; l2 norm of gradient:  1.7328619328790131 ; l2 norm of weights:  1.6536078443980688\n",
            "Iteration#:  1428 ; Loss:  1.8796113377050516 ; l2 norm of gradient:  1.7254084975227564 ; l2 norm of weights:  1.6469784146813216\n",
            "Iteration#:  1432 ; Loss:  1.86771826182285 ; l2 norm of gradient:  1.7179862235859065 ; l2 norm of weights:  1.6403789186770215\n",
            "Iteration#:  1436 ; Loss:  1.8559269154109015 ; l2 norm of gradient:  1.7105949843159194 ; l2 norm of weights:  1.6338092372635638\n",
            "Iteration#:  1440 ; Loss:  1.8442364377220806 ; l2 norm of gradient:  1.7032346534593823 ; l2 norm of weights:  1.6272692518141723\n",
            "Iteration#:  1444 ; Loss:  1.8326459751957385 ; l2 norm of gradient:  1.69590510526102 ; l2 norm of weights:  1.6207588441947096\n",
            "Iteration#:  1448 ; Loss:  1.821154681398545 ; l2 norm of gradient:  1.688606214462663 ; l2 norm of weights:  1.6142778967614917\n",
            "Iteration#:  1452 ; Loss:  1.809761716965803 ; l2 norm of gradient:  1.6813378563021877 ; l2 norm of weights:  1.6078262923591107\n",
            "Iteration#:  1456 ; Loss:  1.798466249543221 ; l2 norm of gradient:  1.6740999065124214 ; l2 norm of weights:  1.601403914318262\n",
            "Iteration#:  1460 ; Loss:  1.7872674537291595 ; l2 norm of gradient:  1.6668922413200198 ; l2 norm of weights:  1.5950106464535814\n",
            "Iteration#:  1464 ; Loss:  1.7761645110173225 ; l2 norm of gradient:  1.659714737444313 ; l2 norm of weights:  1.5886463730614864\n",
            "Iteration#:  1468 ; Loss:  1.765156609739917 ; l2 norm of gradient:  1.652567272096124 ; l2 norm of weights:  1.5823109789180245\n",
            "Iteration#:  1472 ; Loss:  1.7542429450112567 ; l2 norm of gradient:  1.6454497229765586 ; l2 norm of weights:  1.5760043492767275\n",
            "Iteration#:  1476 ; Loss:  1.7434227186718236 ; l2 norm of gradient:  1.6383619682757702 ; l2 norm of weights:  1.5697263698664747\n",
            "Iteration#:  1480 ; Loss:  1.732695139232771 ; l2 norm of gradient:  1.6313038866716998 ; l2 norm of weights:  1.5634769268893616\n",
            "Iteration#:  1484 ; Loss:  1.7220594218208705 ; l2 norm of gradient:  1.6242753573287876 ; l2 norm of weights:  1.557255907018573\n",
            "Iteration#:  1488 ; Loss:  1.7115147881239063 ; l2 norm of gradient:  1.6172762598966643 ; l2 norm of weights:  1.5510631973962663\n",
            "Iteration#:  1492 ; Loss:  1.7010604663364979 ; l2 norm of gradient:  1.6103064745088198 ; l2 norm of weights:  1.5448986856314595\n",
            "Iteration#:  1496 ; Loss:  1.6906956911063653 ; l2 norm of gradient:  1.6033658817812475 ; l2 norm of weights:  1.5387622597979242\n",
            "Iteration#:  1500 ; Loss:  1.6804197034810247 ; l2 norm of gradient:  1.5964543628110703 ; l2 norm of weights:  1.53265380843209\n",
            "Iteration#:  1504 ; Loss:  1.6702317508549116 ; l2 norm of gradient:  1.589571799175142 ; l2 norm of weights:  1.526573220530951\n",
            "Iteration#:  1508 ; Loss:  1.6601310869169275 ; l2 norm of gradient:  1.582718072928635 ; l2 norm of weights:  1.5205203855499785\n",
            "Iteration#:  1512 ; Loss:  1.6501169715984172 ; l2 norm of gradient:  1.575893066603606 ; l2 norm of weights:  1.5144951934010455\n",
            "Iteration#:  1516 ; Loss:  1.6401886710215567 ; l2 norm of gradient:  1.5690966632075443 ; l2 norm of weights:  1.5084975344503515\n",
            "Iteration#:  1520 ; Loss:  1.6303454574481675 ; l2 norm of gradient:  1.5623287462219027 ; l2 norm of weights:  1.5025272995163588\n",
            "Iteration#:  1524 ; Loss:  1.6205866092289343 ; l2 norm of gradient:  1.555589199600611 ; l2 norm of weights:  1.4965843798677303\n",
            "Iteration#:  1528 ; Loss:  1.6109114107530442 ; l2 norm of gradient:  1.5488779077685806 ; l2 norm of weights:  1.4906686672212794\n",
            "Iteration#:  1532 ; Loss:  1.601319152398228 ; l2 norm of gradient:  1.5421947556201827 ; l2 norm of weights:  1.4847800537399216\n",
            "Iteration#:  1536 ; Loss:  1.5918091304812054 ; l2 norm of gradient:  1.5355396285177214 ; l2 norm of weights:  1.4789184320306352\n",
            "Iteration#:  1540 ; Loss:  1.582380647208537 ; l2 norm of gradient:  1.528912412289891 ; l2 norm of weights:  1.4730836951424275\n",
            "Iteration#:  1544 ; Loss:  1.5730330106278732 ; l2 norm of gradient:  1.5223129932302173 ; l2 norm of weights:  1.4672757365643072\n",
            "Iteration#:  1548 ; Loss:  1.5637655345795967 ; l2 norm of gradient:  1.5157412580954917 ; l2 norm of weights:  1.4614944502232636\n",
            "Iteration#:  1552 ; Loss:  1.5545775386488652 ; l2 norm of gradient:  1.5091970941041872 ; l2 norm of weights:  1.4557397304822535\n",
            "Iteration#:  1556 ; Loss:  1.5454683481180342 ; l2 norm of gradient:  1.5026803889348703 ; l2 norm of weights:  1.4500114721381892\n",
            "Iteration#:  1560 ; Loss:  1.5364372939194801 ; l2 norm of gradient:  1.4961910307245954 ; l2 norm of weights:  1.444309570419941\n",
            "Iteration#:  1564 ; Loss:  1.5274837125887946 ; l2 norm of gradient:  1.4897289080672955 ; l2 norm of weights:  1.438633920986338\n",
            "Iteration#:  1568 ; Loss:  1.5186069462183722 ; l2 norm of gradient:  1.4832939100121578 ; l2 norm of weights:  1.4329844199241815\n",
            "Iteration#:  1572 ; Loss:  1.5098063424113695 ; l2 norm of gradient:  1.4768859260619953 ; l2 norm of weights:  1.4273609637462596\n",
            "Iteration#:  1576 ; Loss:  1.5010812542360428 ; l2 norm of gradient:  1.4705048461716055 ; l2 norm of weights:  1.4217634493893716\n",
            "Iteration#:  1580 ; Loss:  1.4924310401804575 ; l2 norm of gradient:  1.4641505607461238 ; l2 norm of weights:  1.4161917742123555\n",
            "Iteration#:  1584 ; Loss:  1.483855064107573 ; l2 norm of gradient:  1.4578229606393693 ; l2 norm of weights:  1.410645835994126\n",
            "Iteration#:  1588 ; Loss:  1.4753526952106863 ; l2 norm of gradient:  1.451521937152182 ; l2 norm of weights:  1.4051255329317134\n",
            "Iteration#:  1592 ; Loss:  1.4669233079692485 ; l2 norm of gradient:  1.4452473820307543 ; l2 norm of weights:  1.3996307636383127\n",
            "Iteration#:  1596 ; Loss:  1.4585662821050387 ; l2 norm of gradient:  1.4389991874649568 ; l2 norm of weights:  1.394161427141337\n",
            "Iteration#:  1600 ; Loss:  1.450281002538698 ; l2 norm of gradient:  1.432777246086658 ; l2 norm of weights:  1.3887174228804764\n",
            "Iteration#:  1604 ; Loss:  1.4420668593466208 ; l2 norm of gradient:  1.4265814509680363 ; l2 norm of weights:  1.3832986507057654\n",
            "Iteration#:  1608 ; Loss:  1.4339232477181987 ; l2 norm of gradient:  1.420411695619891 ; l2 norm of weights:  1.3779050108756534\n",
            "Iteration#:  1612 ; Loss:  1.4258495679134149 ; l2 norm of gradient:  1.414267873989946 ; l2 norm of weights:  1.3725364040550825\n",
            "Iteration#:  1616 ; Loss:  1.4178452252207887 ; l2 norm of gradient:  1.4081498804611503 ; l2 norm of weights:  1.3671927313135723\n",
            "Iteration#:  1620 ; Loss:  1.4099096299156635 ; l2 norm of gradient:  1.402057609849973 ; l2 norm of weights:  1.3618738941233084\n",
            "Iteration#:  1624 ; Loss:  1.4020421972188384 ; l2 norm of gradient:  1.3959909574046971 ; l2 norm of weights:  1.3565797943572384\n",
            "Iteration#:  1628 ; Loss:  1.3942423472555392 ; l2 norm of gradient:  1.3899498188037076 ; l2 norm of weights:  1.351310334287174\n",
            "Iteration#:  1632 ; Loss:  1.3865095050147291 ; l2 norm of gradient:  1.3839340901537789 ; l2 norm of weights:  1.3460654165818975\n",
            "Iteration#:  1636 ; Loss:  1.3788431003087505 ; l2 norm of gradient:  1.377943667988358 ; l2 norm of weights:  1.340844944305276\n",
            "Iteration#:  1640 ; Loss:  1.371242567733302 ; l2 norm of gradient:  1.3719784492658453 ; l2 norm of weights:  1.335648820914379\n",
            "Iteration#:  1644 ; Loss:  1.3637073466277436 ; l2 norm of gradient:  1.3660383313678757 ; l2 norm of weights:  1.3304769502576048\n",
            "Iteration#:  1648 ; Loss:  1.3562368810357297 ; l2 norm of gradient:  1.3601232120975935 ; l2 norm of weights:  1.3253292365728107\n",
            "Iteration#:  1652 ; Loss:  1.3488306196661644 ; l2 norm of gradient:  1.354232989677931 ; l2 norm of weights:  1.3202055844854481\n",
            "Iteration#:  1656 ; Loss:  1.341488015854482 ; l2 norm of gradient:  1.3483675627498817 ; l2 norm of weights:  1.315105899006707\n",
            "Iteration#:  1660 ; Loss:  1.334208527524243 ; l2 norm of gradient:  1.3425268303707747 ; l2 norm of weights:  1.310030085531662\n",
            "Iteration#:  1664 ; Loss:  1.3269916171490497 ; l2 norm of gradient:  1.3367106920125478 ; l2 norm of weights:  1.3049780498374266\n",
            "Iteration#:  1668 ; Loss:  1.3198367517147749 ; l2 norm of gradient:  1.3309190475600203 ; l2 norm of weights:  1.2999496980813126\n",
            "Iteration#:  1672 ; Loss:  1.3127434026821005 ; l2 norm of gradient:  1.3251517973091647 ; l2 norm of weights:  1.2949449367989938\n",
            "Iteration#:  1676 ; Loss:  1.3057110459493702 ; l2 norm of gradient:  1.3194088419653798 ; l2 norm of weights:  1.2899636729026793\n",
            "Iteration#:  1680 ; Loss:  1.2987391618157422 ; l2 norm of gradient:  1.3136900826417635 ; l2 norm of weights:  1.2850058136792857\n",
            "Iteration#:  1684 ; Loss:  1.2918272349446522 ; l2 norm of gradient:  1.3079954208573843 ; l2 norm of weights:  1.2800712667886227\n",
            "Iteration#:  1688 ; Loss:  1.2849747543275742 ; l2 norm of gradient:  1.302324758535557 ; l2 norm of weights:  1.2751599402615779\n",
            "Iteration#:  1692 ; Loss:  1.2781812132480828 ; l2 norm of gradient:  1.2966779980021141 ; l2 norm of weights:  1.2702717424983108\n",
            "Iteration#:  1696 ; Loss:  1.2714461092462124 ; l2 norm of gradient:  1.291055041983686 ; l2 norm of weights:  1.265406582266451\n",
            "Iteration#:  1700 ; Loss:  1.2647689440831082 ; l2 norm of gradient:  1.2854557936059714 ; l2 norm of weights:  1.2605643686993022\n",
            "Iteration#:  1704 ; Loss:  1.258149223705975 ; l2 norm of gradient:  1.2798801563920217 ; l2 norm of weights:  1.255745011294052\n",
            "Iteration#:  1708 ; Loss:  1.2515864582133092 ; l2 norm of gradient:  1.2743280342605157 ; l2 norm of weights:  1.2509484199099856\n",
            "Iteration#:  1712 ; Loss:  1.2450801618204244 ; l2 norm of gradient:  1.268799331524044 ; l2 norm of weights:  1.2461745047667088\n",
            "Iteration#:  1716 ; Loss:  1.2386298528252562 ; l2 norm of gradient:  1.263293952887391 ; l2 norm of weights:  1.241423176442371\n",
            "Iteration#:  1720 ; Loss:  1.2322350535744566 ; l2 norm of gradient:  1.2578118034458214 ; l2 norm of weights:  1.2366943458718989\n",
            "Iteration#:  1724 ; Loss:  1.225895290429762 ; l2 norm of gradient:  1.2523527886833659 ; l2 norm of weights:  1.2319879243452312\n",
            "Iteration#:  1728 ; Loss:  1.2196100937346463 ; l2 norm of gradient:  1.2469168144711147 ; l2 norm of weights:  1.2273038235055636\n",
            "Iteration#:  1732 ; Loss:  1.2133789977812437 ; l2 norm of gradient:  1.2415037870655068 ; l2 norm of weights:  1.2226419553475936\n",
            "Iteration#:  1736 ; Loss:  1.2072015407775494 ; l2 norm of gradient:  1.2361136131066284 ; l2 norm of weights:  1.2180022322157755\n",
            "Iteration#:  1740 ; Loss:  1.2010772648148904 ; l2 norm of gradient:  1.2307461996165097 ; l2 norm of weights:  1.213384566802577\n",
            "Iteration#:  1744 ; Loss:  1.195005715835667 ; l2 norm of gradient:  1.2254014539974278 ; l2 norm of weights:  1.2087888721467452\n",
            "Iteration#:  1748 ; Loss:  1.1889864436013586 ; l2 norm of gradient:  1.2200792840302095 ; l2 norm of weights:  1.2042150616315737\n",
            "Iteration#:  1752 ; Loss:  1.183019001660797 ; l2 norm of gradient:  1.2147795978725413 ; l2 norm of weights:  1.199663048983178\n",
            "Iteration#:  1756 ; Loss:  1.1771029473187025 ; l2 norm of gradient:  1.2095023040572808 ; l2 norm of weights:  1.1951327482687761\n",
            "Iteration#:  1760 ; Loss:  1.1712378416044777 ; l2 norm of gradient:  1.2042473114907712 ; l2 norm of weights:  1.190624073894971\n",
            "Iteration#:  1764 ; Loss:  1.165423249241264 ; l2 norm of gradient:  1.1990145294511607 ; l2 norm of weights:  1.186136940606044\n",
            "Iteration#:  1768 ; Loss:  1.1596587386152506 ; l2 norm of gradient:  1.1938038675867262 ; l2 norm of weights:  1.1816712634822484\n",
            "Iteration#:  1772 ; Loss:  1.1539438817452412 ; l2 norm of gradient:  1.1886152359141986 ; l2 norm of weights:  1.1772269579381116\n",
            "Iteration#:  1776 ; Loss:  1.1482782542524699 ; l2 norm of gradient:  1.183448544817096 ; l2 norm of weights:  1.172803939720741\n",
            "Iteration#:  1780 ; Loss:  1.1426614353306688 ; l2 norm of gradient:  1.178303705044058 ; l2 norm of weights:  1.168402124908135\n",
            "Iteration#:  1784 ; Loss:  1.1370930077163837 ; l2 norm of gradient:  1.1731806277071857 ; l2 norm of weights:  1.1640214299075011\n",
            "Iteration#:  1788 ; Loss:  1.1315725576595348 ; l2 norm of gradient:  1.1680792242803868 ; l2 norm of weights:  1.1596617714535762\n",
            "Iteration#:  1792 ; Loss:  1.1260996748942227 ; l2 norm of gradient:  1.1629994065977243 ; l2 norm of weights:  1.1553230666069554\n",
            "Iteration#:  1796 ; Loss:  1.1206739526097749 ; l2 norm of gradient:  1.1579410868517694 ; l2 norm of weights:  1.1510052327524234\n",
            "Iteration#:  1800 ; Loss:  1.1152949874220341 ; l2 norm of gradient:  1.152904177591963 ; l2 norm of weights:  1.1467081875972935\n",
            "Iteration#:  1804 ; Loss:  1.1099623793448818 ; l2 norm of gradient:  1.1478885917229773 ; l2 norm of weights:  1.1424318491697494\n",
            "Iteration#:  1808 ; Loss:  1.104675731762 ; l2 norm of gradient:  1.1428942425030844 ; l2 norm of weights:  1.1381761358171931\n",
            "Iteration#:  1812 ; Loss:  1.0994346513988678 ; l2 norm of gradient:  1.1379210435425327 ; l2 norm of weights:  1.1339409662046003\n",
            "Iteration#:  1816 ; Loss:  1.094238748294987 ; l2 norm of gradient:  1.1329689088019246 ; l2 norm of weights:  1.1297262593128767\n",
            "Iteration#:  1820 ; Loss:  1.0890876357763402 ; l2 norm of gradient:  1.128037752590603 ; l2 norm of weights:  1.1255319344372234\n",
            "Iteration#:  1824 ; Loss:  1.083980930428077 ; l2 norm of gradient:  1.1231274895650405 ; l2 norm of weights:  1.121357911185505\n",
            "Iteration#:  1828 ; Loss:  1.0789182520674254 ; l2 norm of gradient:  1.118238034727236 ; l2 norm of weights:  1.1172041094766245\n",
            "Iteration#:  1832 ; Loss:  1.073899223716829 ; l2 norm of gradient:  1.1133693034231174 ; l2 norm of weights:  1.1130704495389014\n",
            "Iteration#:  1836 ; Loss:  1.068923471577306 ; l2 norm of gradient:  1.1085212113409486 ; l2 norm of weights:  1.108956851908459\n",
            "Iteration#:  1840 ; Loss:  1.06399062500203 ; l2 norm of gradient:  1.1036936745097443 ; l2 norm of weights:  1.1048632374276104\n",
            "Iteration#:  1844 ; Loss:  1.0591003164701274 ; l2 norm of gradient:  1.0988866092976886 ; l2 norm of weights:  1.1007895272432568\n",
            "Iteration#:  1848 ; Loss:  1.0542521815606958 ; l2 norm of gradient:  1.0940999324105618 ; l2 norm of weights:  1.0967356428052861\n",
            "Iteration#:  1852 ; Loss:  1.0494458589270341 ; l2 norm of gradient:  1.089333560890173 ; l2 norm of weights:  1.0927015058649787\n",
            "Iteration#:  1856 ; Loss:  1.0446809902710867 ; l2 norm of gradient:  1.0845874121127985 ; l2 norm of weights:  1.0886870384734173\n",
            "Iteration#:  1860 ; Loss:  1.039957220318103 ; l2 norm of gradient:  1.0798614037876253 ; l2 norm of weights:  1.084692162979904\n",
            "Iteration#:  1864 ; Loss:  1.0352741967915007 ; l2 norm of gradient:  1.0751554539552046 ; l2 norm of weights:  1.0807168020303797\n",
            "Iteration#:  1868 ; Loss:  1.0306315703879432 ; l2 norm of gradient:  1.0704694809859094 ; l2 norm of weights:  1.0767608785658516\n",
            "Iteration#:  1872 ; Loss:  1.0260289947526204 ; l2 norm of gradient:  1.065803403578398 ; l2 norm of weights:  1.0728243158208233\n",
            "Iteration#:  1876 ; Loss:  1.021466126454735 ; l2 norm of gradient:  1.061157140758087 ; l2 norm of weights:  1.0689070373217326\n",
            "Iteration#:  1880 ; Loss:  1.0169426249631903 ; l2 norm of gradient:  1.0565306118756288 ; l2 norm of weights:  1.0650089668853917\n",
            "Iteration#:  1884 ; Loss:  1.0124581526224834 ; l2 norm of gradient:  1.0519237366053962 ; l2 norm of weights:  1.0611300286174357\n",
            "Iteration#:  1888 ; Loss:  1.0080123746287932 ; l2 norm of gradient:  1.0473364349439755 ; l2 norm of weights:  1.057270146910774\n",
            "Iteration#:  1892 ; Loss:  1.0036049590062701 ; l2 norm of gradient:  1.0427686272086656 ; l2 norm of weights:  1.0534292464440478\n",
            "Iteration#:  1896 ; Loss:  0.9992355765835195 ; l2 norm of gradient:  1.038220234035983 ; l2 norm of weights:  1.0496072521800912\n",
            "Iteration#:  1900 ; Loss:  0.9949039009702816 ; l2 norm of gradient:  1.0336911763801766 ; l2 norm of weights:  1.0458040893644025\n",
            "Iteration#:  1904 ; Loss:  0.9906096085343046 ; l2 norm of gradient:  1.0291813755117467 ; l2 norm of weights:  1.0420196835236137\n",
            "Iteration#:  1908 ; Loss:  0.9863523783784074 ; l2 norm of gradient:  1.0246907530159746 ; l2 norm of weights:  1.0382539604639713\n",
            "Iteration#:  1912 ; Loss:  0.9821318923177331 ; l2 norm of gradient:  1.020219230791457 ; l2 norm of weights:  1.034506846269818\n",
            "Iteration#:  1916 ; Loss:  0.9779478348571925 ; l2 norm of gradient:  1.0157667310486478 ; l2 norm of weights:  1.0307782673020829\n",
            "Iteration#:  1920 ; Loss:  0.9737998931690907 ; l2 norm of gradient:  1.0113331763084088 ; l2 norm of weights:  1.0270681501967749\n",
            "Iteration#:  1924 ; Loss:  0.9696877570709426 ; l2 norm of gradient:  1.0069184894005678 ; l2 norm of weights:  1.0233764218634827\n",
            "Iteration#:  1928 ; Loss:  0.965611119003469 ; l2 norm of gradient:  1.0025225934624826 ; l2 norm of weights:  1.0197030094838786\n",
            "Iteration#:  1932 ; Loss:  0.9615696740087765 ; l2 norm of gradient:  0.998145411937614 ; l2 norm of weights:  1.0160478405102293\n",
            "Iteration#:  1936 ; Loss:  0.9575631197087174 ; l2 norm of gradient:  0.9937868685741064 ; l2 norm of weights:  1.0124108426639111\n",
            "Iteration#:  1940 ; Loss:  0.9535911562834283 ; l2 norm of gradient:  0.9894468874233747 ; l2 norm of weights:  1.0087919439339303\n",
            "Iteration#:  1944 ; Loss:  0.9496534864500454 ; l2 norm of gradient:  0.9851253928387013 ; l2 norm of weights:  1.0051910725754487\n",
            "Iteration#:  1948 ; Loss:  0.9457498154415998 ; l2 norm of gradient:  0.9808223094738385 ; l2 norm of weights:  1.0016081571083164\n",
            "Iteration#:  1952 ; Loss:  0.94187985098608 ; l2 norm of gradient:  0.9765375622816201 ; l2 norm of weights:  0.9980431263156073\n",
            "Iteration#:  1956 ; Loss:  0.9380433032856756 ; l2 norm of gradient:  0.9722710765125798 ; l2 norm of weights:  0.9944959092421619\n",
            "Iteration#:  1960 ; Loss:  0.9342398849961862 ; l2 norm of gradient:  0.9680227777135786 ; l2 norm of weights:  0.9909664351931343\n",
            "Iteration#:  1964 ; Loss:  0.9304693112066043 ; l2 norm of gradient:  0.9637925917264395 ; l2 norm of weights:  0.9874546337325457\n",
            "Iteration#:  1968 ; Loss:  0.9267312994188655 ; l2 norm of gradient:  0.9595804446865893 ; l2 norm of weights:  0.9839604346818427\n",
            "Iteration#:  1972 ; Loss:  0.9230255695277656 ; l2 norm of gradient:  0.9553862630217099 ; l2 norm of weights:  0.9804837681184607\n",
            "Iteration#:  1976 ; Loss:  0.9193518438010444 ; l2 norm of gradient:  0.9512099734503967 ; l2 norm of weights:  0.9770245643743939\n",
            "Iteration#:  1980 ; Loss:  0.9157098468596339 ; l2 norm of gradient:  0.9470515029808239 ; l2 norm of weights:  0.9735827540347699\n",
            "Iteration#:  1984 ; Loss:  0.9120993056580696 ; l2 norm of gradient:  0.94291077890942 ; l2 norm of weights:  0.9701582679364305\n",
            "Iteration#:  1988 ; Loss:  0.9085199494650633 ; l2 norm of gradient:  0.9387877288195492 ; l2 norm of weights:  0.9667510371665162\n",
            "Iteration#:  1992 ; Loss:  0.9049715098442364 ; l2 norm of gradient:  0.934682280580203 ; l2 norm of weights:  0.9633609930610598\n",
            "Iteration#:  1996 ; Loss:  0.9014537206350122 ; l2 norm of gradient:  0.9305943623446964 ; l2 norm of weights:  0.959988067203582\n",
            "Iteration#:  2000 ; Loss:  0.897966317933667 ; l2 norm of gradient:  0.926523902549376 ; l2 norm of weights:  0.9566321914236944\n",
            "Iteration#:  2004 ; Loss:  0.8945090400745352 ; l2 norm of gradient:  0.9224708299123338 ; l2 norm of weights:  0.953293297795708\n",
            "Iteration#:  2008 ; Loss:  0.8910816276113723 ; l2 norm of gradient:  0.9184350734321288 ; l2 norm of weights:  0.9499713186372474\n",
            "Iteration#:  2012 ; Loss:  0.8876838232988695 ; l2 norm of gradient:  0.914416562386518 ; l2 norm of weights:  0.9466661865078697\n",
            "Iteration#:  2016 ; Loss:  0.8843153720743226 ; l2 norm of gradient:  0.9104152263311951 ; l2 norm of weights:  0.9433778342076911\n",
            "Iteration#:  2020 ; Loss:  0.8809760210394495 ; l2 norm of gradient:  0.9064309950985354 ; l2 norm of weights:  0.9401061947760166\n",
            "Iteration#:  2024 ; Loss:  0.8776655194423617 ; l2 norm of gradient:  0.9024637987963514 ; l2 norm of weights:  0.9368512014899781\n",
            "Iteration#:  2028 ; Loss:  0.8743836186596801 ; l2 norm of gradient:  0.8985135678066543 ; l2 norm of weights:  0.9336127878631763\n",
            "Iteration#:  2032 ; Loss:  0.8711300721788013 ; l2 norm of gradient:  0.8945802327844253 ; l2 norm of weights:  0.9303908876443296\n",
            "Iteration#:  2036 ; Loss:  0.8679046355803086 ; l2 norm of gradient:  0.8906637246563918 ; l2 norm of weights:  0.9271854348159282\n",
            "Iteration#:  2040 ; Loss:  0.8647070665205298 ; l2 norm of gradient:  0.8867639746198163 ; l2 norm of weights:  0.9239963635928946\n",
            "Iteration#:  2044 ; Loss:  0.8615371247142355 ; l2 norm of gradient:  0.8828809141412877 ; l2 norm of weights:  0.9208236084212497\n",
            "Iteration#:  2048 ; Loss:  0.858394571917483 ; l2 norm of gradient:  0.8790144749555261 ; l2 norm of weights:  0.9176671039767841\n",
            "Iteration#:  2052 ; Loss:  0.8552791719106018 ; l2 norm of gradient:  0.8751645890641903 ; l2 norm of weights:  0.9145267851637379\n",
            "Iteration#:  2056 ; Loss:  0.852190690481315 ; l2 norm of gradient:  0.8713311887346964 ; l2 norm of weights:  0.9114025871134828\n",
            "Iteration#:  2060 ; Loss:  0.8491288954080061 ; l2 norm of gradient:  0.8675142064990448 ; l2 norm of weights:  0.9082944451832133\n",
            "Iteration#:  2064 ; Loss:  0.8460935564431179 ; l2 norm of gradient:  0.8637135751526508 ; l2 norm of weights:  0.905202294954643\n",
            "Iteration#:  2068 ; Loss:  0.8430844452966919 ; l2 norm of gradient:  0.8599292277531888 ; l2 norm of weights:  0.9021260722327062\n",
            "Iteration#:  2072 ; Loss:  0.8401013356200409 ; l2 norm of gradient:  0.8561610976194395 ; l2 norm of weights:  0.8990657130442665\n",
            "Iteration#:  2076 ; Loss:  0.8371440029895588 ; l2 norm of gradient:  0.8524091183301471 ; l2 norm of weights:  0.8960211536368324\n",
            "Iteration#:  2080 ; Loss:  0.8342122248906596 ; l2 norm of gradient:  0.848673223722884 ; l2 norm of weights:  0.892992330477276\n",
            "Iteration#:  2084 ; Loss:  0.8313057807018527 ; l2 norm of gradient:  0.8449533478929223 ; l2 norm of weights:  0.889979180250562\n",
            "Iteration#:  2088 ; Loss:  0.8284244516789472 ; l2 norm of gradient:  0.8412494251921147 ; l2 norm of weights:  0.8869816398584808\n",
            "Iteration#:  2092 ; Loss:  0.8255680209393863 ; l2 norm of gradient:  0.8375613902277804 ; l2 norm of weights:  0.8839996464183869\n",
            "Iteration#:  2096 ; Loss:  0.8227362734467107 ; l2 norm of gradient:  0.8338891778616018 ; l2 norm of weights:  0.8810331372619462\n",
            "Iteration#:  2100 ; Loss:  0.8199289959951512 ; l2 norm of gradient:  0.8302327232085253 ; l2 norm of weights:  0.878082049933888\n",
            "Iteration#:  2104 ; Loss:  0.8171459771943452 ; l2 norm of gradient:  0.8265919616356718 ; l2 norm of weights:  0.8751463221907638\n",
            "Iteration#:  2108 ; Loss:  0.8143870074541819 ; l2 norm of gradient:  0.8229668287612554 ; l2 norm of weights:  0.8722258919997129\n",
            "Iteration#:  2112 ; Loss:  0.8116518789697709 ; l2 norm of gradient:  0.8193572604535059 ; l2 norm of weights:  0.8693206975372342\n",
            "Iteration#:  2116 ; Loss:  0.8089403857065334 ; l2 norm of gradient:  0.815763192829603 ; l2 norm of weights:  0.866430677187964\n",
            "Iteration#:  2120 ; Loss:  0.8062523233854186 ; l2 norm of gradient:  0.8121845622546146 ; l2 norm of weights:  0.8635557695434626\n",
            "Iteration#:  2124 ; Loss:  0.8035874894682384 ; l2 norm of gradient:  0.8086213053404443 ; l2 norm of weights:  0.860695913401004\n",
            "Iteration#:  2128 ; Loss:  0.8009456831431265 ; l2 norm of gradient:  0.8050733589447852 ; l2 norm of weights:  0.8578510477623761\n",
            "Iteration#:  2132 ; Loss:  0.7983267053101131 ; l2 norm of gradient:  0.8015406601700809 ; l2 norm of weights:  0.8550211118326845\n",
            "Iteration#:  2136 ; Loss:  0.7957303585668214 ; l2 norm of gradient:  0.798023146362494 ; l2 norm of weights:  0.8522060450191651\n",
            "Iteration#:  2140 ; Loss:  0.7931564471942794 ; l2 norm of gradient:  0.7945207551108815 ; l2 norm of weights:  0.8494057869300026\n",
            "Iteration#:  2144 ; Loss:  0.7906047771428504 ; l2 norm of gradient:  0.7910334242457782 ; l2 norm of weights:  0.8466202773731557\n",
            "Iteration#:  2148 ; Loss:  0.7880751560182779 ; l2 norm of gradient:  0.7875610918383832 ; l2 norm of weights:  0.8438494563551904\n",
            "Iteration#:  2152 ; Loss:  0.7855673930678442 ; l2 norm of gradient:  0.7841036961995606 ; l2 norm of weights:  0.8410932640801184\n",
            "Iteration#:  2156 ; Loss:  0.7830812991666456 ; l2 norm of gradient:  0.7806611758788393 ; l2 norm of weights:  0.8383516409482444\n",
            "Iteration#:  2160 ; Loss:  0.7806166868039772 ; l2 norm of gradient:  0.7772334696634252 ; l2 norm of weights:  0.8356245275550187\n",
            "Iteration#:  2164 ; Loss:  0.7781733700698329 ; l2 norm of gradient:  0.7738205165772167 ; l2 norm of weights:  0.8329118646898982\n",
            "Iteration#:  2168 ; Loss:  0.7757511646415131 ; l2 norm of gradient:  0.7704222558798293 ; l2 norm of weights:  0.830213593335214\n",
            "Iteration#:  2172 ; Loss:  0.7733498877703442 ; l2 norm of gradient:  0.7670386270656251 ; l2 norm of weights:  0.8275296546650464\n",
            "Iteration#:  2176 ; Loss:  0.7709693582685075 ; l2 norm of gradient:  0.7636695698627515 ; l2 norm of weights:  0.8248599900441059\n",
            "Iteration#:  2180 ; Loss:  0.7686093964959753 ; l2 norm of gradient:  0.7603150242321819 ; l2 norm of weights:  0.822204541026624\n",
            "Iteration#:  2184 ; Loss:  0.7662698243475546 ; l2 norm of gradient:  0.756974930366768 ; l2 norm of weights:  0.8195632493552486\n",
            "Iteration#:  2188 ; Loss:  0.7639504652400382 ; l2 norm of gradient:  0.7536492286902959 ; l2 norm of weights:  0.8169360569599483\n",
            "Iteration#:  2192 ; Loss:  0.7616511440994596 ; l2 norm of gradient:  0.7503378598565479 ; l2 norm of weights:  0.8143229059569239\n",
            "Iteration#:  2196 ; Loss:  0.7593716873484544 ; l2 norm of gradient:  0.7470407647483732 ; l2 norm of weights:  0.8117237386475277\n",
            "Iteration#:  2200 ; Loss:  0.7571119228937238 ; l2 norm of gradient:  0.7437578844767619 ; l2 norm of weights:  0.8091384975171876\n",
            "Iteration#:  2204 ; Loss:  0.7548716801136031 ; l2 norm of gradient:  0.7404891603799287 ; l2 norm of weights:  0.8065671252343442\n",
            "Iteration#:  2208 ; Loss:  0.7526507898457301 ; l2 norm of gradient:  0.7372345340223985 ; l2 norm of weights:  0.8040095646493891\n",
            "Iteration#:  2212 ; Loss:  0.7504490843748167 ; l2 norm of gradient:  0.7339939471941025 ; l2 norm of weights:  0.8014657587936166\n",
            "Iteration#:  2216 ; Loss:  0.7482663974205193 ; l2 norm of gradient:  0.7307673419094771 ; l2 norm of weights:  0.7989356508781774\n",
            "Iteration#:  2220 ; Loss:  0.7461025641254106 ; l2 norm of gradient:  0.7275546604065702 ; l2 norm of weights:  0.7964191842930464\n",
            "Iteration#:  2224 ; Loss:  0.7439574210430488 ; l2 norm of gradient:  0.7243558451461544 ; l2 norm of weights:  0.7939163026059922\n",
            "Iteration#:  2228 ; Loss:  0.7418308061261447 ; l2 norm of gradient:  0.7211708388108428 ; l2 norm of weights:  0.7914269495615581\n",
            "Iteration#:  2232 ; Loss:  0.739722558714828 ; l2 norm of gradient:  0.7179995843042148 ; l2 norm of weights:  0.7889510690800509\n",
            "Iteration#:  2236 ; Loss:  0.7376325195250065 ; l2 norm of gradient:  0.7148420247499451 ; l2 norm of weights:  0.7864886052565345\n",
            "Iteration#:  2240 ; Loss:  0.7355605306368247 ; l2 norm of gradient:  0.7116981034909383 ; l2 norm of weights:  0.7840395023598359\n",
            "Iteration#:  2244 ; Loss:  0.7335064354832148 ; l2 norm of gradient:  0.7085677640884706 ; l2 norm of weights:  0.7816037048315548\n",
            "Iteration#:  2248 ; Loss:  0.7314700788385421 ; l2 norm of gradient:  0.7054509503213359 ; l2 norm of weights:  0.7791811572850856\n",
            "Iteration#:  2252 ; Loss:  0.7294513068073443 ; l2 norm of gradient:  0.7023476061849986 ; l2 norm of weights:  0.7767718045046433\n",
            "Iteration#:  2256 ; Loss:  0.7274499668131628 ; l2 norm of gradient:  0.6992576758907505 ; l2 norm of weights:  0.7743755914443003\n",
            "Iteration#:  2260 ; Loss:  0.7254659075874663 ; l2 norm of gradient:  0.696181103864874 ; l2 norm of weights:  0.7719924632270307\n",
            "Iteration#:  2264 ; Loss:  0.7234989791586642 ; l2 norm of gradient:  0.6931178347478107 ; l2 norm of weights:  0.7696223651437619\n",
            "Iteration#:  2268 ; Loss:  0.721549032841212 ; l2 norm of gradient:  0.690067813393335 ; l2 norm of weights:  0.7672652426524362\n",
            "Iteration#:  2272 ; Loss:  0.7196159212248043 ; l2 norm of gradient:  0.6870309848677333 ; l2 norm of weights:  0.7649210413770787\n",
            "Iteration#:  2276 ; Loss:  0.7176994981636585 ; l2 norm of gradient:  0.6840072944489872 ; l2 norm of weights:  0.7625897071068752\n",
            "Iteration#:  2280 ; Loss:  0.715799618765885 ; l2 norm of gradient:  0.6809966876259649 ; l2 norm of weights:  0.760271185795258\n",
            "Iteration#:  2284 ; Loss:  0.7139161393829453 ; l2 norm of gradient:  0.6779991100976135 ; l2 norm of weights:  0.7579654235589985\n",
            "Iteration#:  2288 ; Loss:  0.7120489175991971 ; l2 norm of gradient:  0.6750145077721592 ; l2 norm of weights:  0.755672366677312\n",
            "Iteration#:  2292 ; Loss:  0.7101978122215251 ; l2 norm of gradient:  0.6720428267663127 ; l2 norm of weights:  0.7533919615909666\n",
            "Iteration#:  2296 ; Loss:  0.7083626832690562 ; l2 norm of gradient:  0.6690840134044773 ; l2 norm of weights:  0.7511241549014038\n",
            "Iteration#:  2300 ; Loss:  0.7065433919629602 ; l2 norm of gradient:  0.6661380142179637 ; l2 norm of weights:  0.7488688933698665\n",
            "Iteration#:  2304 ; Loss:  0.7047398007163352 ; l2 norm of gradient:  0.6632047759442093 ; l2 norm of weights:  0.7466261239165357\n",
            "Iteration#:  2308 ; Loss:  0.7029517731241737 ; l2 norm of gradient:  0.660284245526001 ; l2 norm of weights:  0.7443957936196759\n",
            "Iteration#:  2312 ; Loss:  0.7011791739534123 ; l2 norm of gradient:  0.6573763701107057 ; l2 norm of weights:  0.742177849714789\n",
            "Iteration#:  2316 ; Loss:  0.6994218691330644 ; l2 norm of gradient:  0.6544810970495012 ; l2 norm of weights:  0.7399722395937769\n",
            "Iteration#:  2320 ; Loss:  0.6976797257444323 ; l2 norm of gradient:  0.6515983738966159 ; l2 norm of weights:  0.7377789108041141\n",
            "Iteration#:  2324 ; Loss:  0.6959526120114011 ; l2 norm of gradient:  0.64872814840857 ; l2 norm of weights:  0.7355978110480271\n",
            "Iteration#:  2328 ; Loss:  0.6942403972908111 ; l2 norm of gradient:  0.6458703685434224 ; l2 norm of weights:  0.7334288881816835\n",
            "Iteration#:  2332 ; Loss:  0.6925429520629109 ; l2 norm of gradient:  0.643024982460023 ; l2 norm of weights:  0.7312720902143911\n",
            "Iteration#:  2336 ; Loss:  0.6908601479218879 ; l2 norm of gradient:  0.6401919385172674 ; l2 norm of weights:  0.7291273653078034\n",
            "Iteration#:  2340 ; Loss:  0.6891918575664762 ; l2 norm of gradient:  0.6373711852733568 ; l2 norm of weights:  0.7269946617751367\n",
            "Iteration#:  2344 ; Loss:  0.687537954790644 ; l2 norm of gradient:  0.6345626714850634 ; l2 norm of weights:  0.7248739280803945\n",
            "Iteration#:  2348 ; Loss:  0.6858983144743555 ; l2 norm of gradient:  0.6317663461069977 ; l2 norm of weights:  0.7227651128376007\n",
            "Iteration#:  2352 ; Loss:  0.6842728125744092 ; l2 norm of gradient:  0.6289821582908818 ; l2 norm of weights:  0.7206681648100434\n",
            "Iteration#:  2356 ; Loss:  0.6826613261153527 ; l2 norm of gradient:  0.6262100573848264 ; l2 norm of weights:  0.7185830329095259\n",
            "Iteration#:  2360 ; Loss:  0.681063733180471 ; l2 norm of gradient:  0.623449992932612 ; l2 norm of weights:  0.7165096661956282\n",
            "Iteration#:  2364 ; Loss:  0.679479912902849 ; l2 norm of gradient:  0.6207019146729729 ; l2 norm of weights:  0.7144480138749763\n",
            "Iteration#:  2368 ; Loss:  0.6779097454565097 ; l2 norm of gradient:  0.6179657725388868 ; l2 norm of weights:  0.7123980253005219\n",
            "Iteration#:  2372 ; Loss:  0.6763531120476217 ; l2 norm of gradient:  0.6152415166568682 ; l2 norm of weights:  0.7103596499708301\n",
            "Iteration#:  2376 ; Loss:  0.6748098949057826 ; l2 norm of gradient:  0.6125290973462645 ; l2 norm of weights:  0.7083328375293775\n",
            "Iteration#:  2380 ; Loss:  0.6732799772753717 ; l2 norm of gradient:  0.6098284651185563 ; l2 norm of weights:  0.7063175377638571\n",
            "Iteration#:  2384 ; Loss:  0.671763243406976 ; l2 norm of gradient:  0.6071395706766638 ; l2 norm of weights:  0.7043137006054961\n",
            "Iteration#:  2388 ; Loss:  0.6702595785488847 ; l2 norm of gradient:  0.6044623649142523 ; l2 norm of weights:  0.7023212761283791\n",
            "Iteration#:  2392 ; Loss:  0.6687688689386558 ; l2 norm of gradient:  0.6017967989150462 ; l2 norm of weights:  0.7003402145487829\n",
            "Iteration#:  2396 ; Loss:  0.6672910017947505 ; l2 norm of gradient:  0.5991428239521442 ; l2 norm of weights:  0.69837046622452\n",
            "Iteration#:  2400 ; Loss:  0.6658258653082376 ; l2 norm of gradient:  0.5965003914873375 ; l2 norm of weights:  0.6964119816542905\n",
            "Iteration#:  2404 ; Loss:  0.6643733486345664 ; l2 norm of gradient:  0.5938694531704339 ; l2 norm of weights:  0.6944647114770447\n",
            "Iteration#:  2408 ; Loss:  0.6629333418854062 ; l2 norm of gradient:  0.591249960838583 ; l2 norm of weights:  0.6925286064713535\n",
            "Iteration#:  2412 ; Loss:  0.6615057361205544 ; l2 norm of gradient:  0.5886418665156067 ; l2 norm of weights:  0.6906036175547893\n",
            "Iteration#:  2416 ; Loss:  0.6600904233399107 ; l2 norm of gradient:  0.5860451224113326 ; l2 norm of weights:  0.6886896957833153\n",
            "Iteration#:  2420 ; Loss:  0.6586872964755182 ; l2 norm of gradient:  0.5834596809209293 ; l2 norm of weights:  0.6867867923506844\n",
            "Iteration#:  2424 ; Loss:  0.6572962493836683 ; l2 norm of gradient:  0.5808854946242487 ; l2 norm of weights:  0.6848948585878479\n",
            "Iteration#:  2428 ; Loss:  0.6559171768370733 ; l2 norm of gradient:  0.5783225162851666 ; l2 norm of weights:  0.6830138459623722\n",
            "Iteration#:  2432 ; Loss:  0.6545499745171021 ; l2 norm of gradient:  0.5757706988509311 ; l2 norm of weights:  0.6811437060778666\n",
            "Iteration#:  2436 ; Loss:  0.6531945390060793 ; l2 norm of gradient:  0.5732299954515128 ; l2 norm of weights:  0.6792843906734182\n",
            "Iteration#:  2440 ; Loss:  0.6518507677796501 ; l2 norm of gradient:  0.5707003593989551 ; l2 norm of weights:  0.6774358516230384\n",
            "Iteration#:  2444 ; Loss:  0.6505185591992063 ; l2 norm of gradient:  0.568181744186734 ; l2 norm of weights:  0.675598040935117\n",
            "Iteration#:  2448 ; Loss:  0.6491978125043755 ; l2 norm of gradient:  0.5656741034891141 ; l2 norm of weights:  0.6737709107518862\n",
            "Iteration#:  2452 ; Loss:  0.6478884278055725 ; l2 norm of gradient:  0.5631773911605136 ; l2 norm of weights:  0.6719544133488943\n",
            "Iteration#:  2456 ; Loss:  0.6465903060766118 ; l2 norm of gradient:  0.5606915612348679 ; l2 norm of weights:  0.670148501134487\n",
            "Iteration#:  2460 ; Loss:  0.6453033491473821 ; l2 norm of gradient:  0.5582165679249977 ; l2 norm of weights:  0.6683531266493009\n",
            "Iteration#:  2464 ; Loss:  0.6440274596965797 ; l2 norm of gradient:  0.5557523656219833 ; l2 norm of weights:  0.666568242565763\n",
            "Iteration#:  2468 ; Loss:  0.642762541244504 ; l2 norm of gradient:  0.5532989088945345 ; l2 norm of weights:  0.6647938016876018\n",
            "Iteration#:  2472 ; Loss:  0.641508498145911 ; l2 norm of gradient:  0.5508561524883713 ; l2 norm of weights:  0.6630297569493666\n",
            "Iteration#:  2476 ; Loss:  0.6402652355829261 ; l2 norm of gradient:  0.5484240513256021 ; l2 norm of weights:  0.6612760614159562\n",
            "Iteration#:  2480 ; Loss:  0.6390326595580167 ; l2 norm of gradient:  0.5460025605041074 ; l2 norm of weights:  0.6595326682821562\n",
            "Iteration#:  2484 ; Loss:  0.6378106768870213 ; l2 norm of gradient:  0.5435916352969249 ; l2 norm of weights:  0.6577995308721872\n",
            "Iteration#:  2488 ; Loss:  0.6365991951922377 ; l2 norm of gradient:  0.5411912311516374 ; l2 norm of weights:  0.65607660263926\n",
            "Iteration#:  2492 ; Loss:  0.6353981228955674 ; l2 norm of gradient:  0.5388013036897651 ; l2 norm of weights:  0.6543638371651411\n",
            "Iteration#:  2496 ; Loss:  0.6342073692117173 ; l2 norm of gradient:  0.5364218087061584 ; l2 norm of weights:  0.6526611881597275\n",
            "Iteration#:  2500 ; Loss:  0.6330268441414577 ; l2 norm of gradient:  0.5340527021683946 ; l2 norm of weights:  0.6509686094606297\n",
            "Iteration#:  2504 ; Loss:  0.6318564584649355 ; l2 norm of gradient:  0.5316939402161767 ; l2 norm of weights:  0.6492860550327647\n",
            "Iteration#:  2508 ; Loss:  0.6306961237350444 ; l2 norm of gradient:  0.5293454791607356 ; l2 norm of weights:  0.6476134789679567\n",
            "Iteration#:  2512 ; Loss:  0.6295457522708472 ; l2 norm of gradient:  0.527007275484233 ; l2 norm of weights:  0.6459508354845488\n",
            "Iteration#:  2516 ; Loss:  0.6284052571510563 ; l2 norm of gradient:  0.5246792858391699 ; l2 norm of weights:  0.6442980789270214\n",
            "Iteration#:  2520 ; Loss:  0.6272745522075648 ; l2 norm of gradient:  0.5223614670477934 ; l2 norm of weights:  0.6426551637656209\n",
            "Iteration#:  2524 ; Loss:  0.6261535520190333 ; l2 norm of gradient:  0.5200537761015104 ; l2 norm of weights:  0.6410220445959977\n",
            "Iteration#:  2528 ; Loss:  0.6250421719045296 ; l2 norm of gradient:  0.5177561701603006 ; l2 norm of weights:  0.6393986761388507\n",
            "Iteration#:  2532 ; Loss:  0.6239403279172204 ; l2 norm of gradient:  0.5154686065521331 ; l2 norm of weights:  0.6377850132395837\n",
            "Iteration#:  2536 ; Loss:  0.6228479368381162 ; l2 norm of gradient:  0.5131910427723854 ; l2 norm of weights:  0.6361810108679685\n",
            "Iteration#:  2540 ; Loss:  0.621764916169868 ; l2 norm of gradient:  0.5109234364832638 ; l2 norm of weights:  0.6345866241178166\n",
            "Iteration#:  2544 ; Loss:  0.620691184130614 ; l2 norm of gradient:  0.5086657455132276 ; l2 norm of weights:  0.6330018082066615\n",
            "Iteration#:  2548 ; Loss:  0.6196266596478804 ; l2 norm of gradient:  0.5064179278564154 ; l2 norm of weights:  0.6314265184754468\n",
            "Iteration#:  2552 ; Loss:  0.6185712623525303 ; l2 norm of gradient:  0.5041799416720714 ; l2 norm of weights:  0.6298607103882259\n",
            "Iteration#:  2556 ; Loss:  0.6175249125727629 ; l2 norm of gradient:  0.5019517452839769 ; l2 norm of weights:  0.6283043395318673\n",
            "Iteration#:  2560 ; Loss:  0.6164875313281651 ; l2 norm of gradient:  0.499733297179882 ; l2 norm of weights:  0.6267573616157707\n",
            "Iteration#:  2564 ; Loss:  0.6154590403238096 ; l2 norm of gradient:  0.4975245560109412 ; l2 norm of weights:  0.6252197324715906\n",
            "Iteration#:  2568 ; Loss:  0.6144393619444031 ; l2 norm of gradient:  0.49532548059114956 ; l2 norm of weights:  0.6236914080529671\n",
            "Iteration#:  2572 ; Loss:  0.6134284192484845 ; l2 norm of gradient:  0.4931360298967807 ; l2 norm of weights:  0.6221723444352677\n",
            "Iteration#:  2576 ; Loss:  0.6124261359626688 ; l2 norm of gradient:  0.49095616306582973 ; l2 norm of weights:  0.6206624978153346\n",
            "Iteration#:  2580 ; Loss:  0.6114324364759418 ; l2 norm of gradient:  0.48878583939745474 ; l2 norm of weights:  0.6191618245112412\n",
            "Iteration#:  2584 ; Loss:  0.6104472458339995 ; l2 norm of gradient:  0.4866250183514226 ; l2 norm of weights:  0.6176702809620582\n",
            "Iteration#:  2588 ; Loss:  0.6094704897336365 ; l2 norm of gradient:  0.484473659547556 ; l2 norm of weights:  0.6161878237276245\n",
            "Iteration#:  2592 ; Loss:  0.6085020945171804 ; l2 norm of gradient:  0.4823317227651828 ; l2 norm of weights:  0.6147144094883291\n",
            "Iteration#:  2596 ; Loss:  0.6075419871669729 ; l2 norm of gradient:  0.48019916794258666 ; l2 norm of weights:  0.6132499950448997\n",
            "Iteration#:  2600 ; Loss:  0.6065900952998968 ; l2 norm of gradient:  0.4780759551764607 ; l2 norm of weights:  0.6117945373181987\n",
            "Iteration#:  2604 ; Loss:  0.6056463471619479 ; l2 norm of gradient:  0.47596204472136266 ; l2 norm of weights:  0.6103479933490277\n",
            "Iteration#:  2608 ; Loss:  0.6047106716228529 ; l2 norm of gradient:  0.4738573969891714 ; l2 norm of weights:  0.6089103202979396\n",
            "Iteration#:  2612 ; Loss:  0.6037829981707328 ; l2 norm of gradient:  0.4717619725485462 ; l2 norm of weights:  0.6074814754450575\n",
            "Iteration#:  2616 ; Loss:  0.6028632569068088 ; l2 norm of gradient:  0.46967573212438696 ; l2 norm of weights:  0.6060614161899025\n",
            "Iteration#:  2620 ; Loss:  0.6019513785401547 ; l2 norm of gradient:  0.4675986365972978 ; l2 norm of weights:  0.6046501000512282\n",
            "Iteration#:  2624 ; Loss:  0.6010472943824914 ; l2 norm of gradient:  0.46553064700305086 ; l2 norm of weights:  0.6032474846668622\n",
            "Iteration#:  2628 ; Loss:  0.6001509363430249 ; l2 norm of gradient:  0.46347172453205243 ; l2 norm of weights:  0.6018535277935552\n",
            "Iteration#:  2632 ; Loss:  0.5992622369233298 ; l2 norm of gradient:  0.4614218305288115 ; l2 norm of weights:  0.6004681873068383\n",
            "Iteration#:  2636 ; Loss:  0.5983811292122716 ; l2 norm of gradient:  0.45938092649140977 ; l2 norm of weights:  0.5990914212008859\n",
            "Iteration#:  2640 ; Loss:  0.5975075468809742 ; l2 norm of gradient:  0.45734897407097264 ; l2 norm of weights:  0.5977231875883866\n",
            "Iteration#:  2644 ; Loss:  0.5966414241778294 ; l2 norm of gradient:  0.45532593507114305 ; l2 norm of weights:  0.5963634447004206\n",
            "Iteration#:  2648 ; Loss:  0.5957826959235466 ; l2 norm of gradient:  0.45331177144755685 ; l2 norm of weights:  0.5950121508863442\n",
            "Iteration#:  2652 ; Loss:  0.5949312975062453 ; l2 norm of gradient:  0.4513064453073187 ; l2 norm of weights:  0.5936692646136816\n",
            "Iteration#:  2656 ; Loss:  0.5940871648765873 ; l2 norm of gradient:  0.4493099189084817 ; l2 norm of weights:  0.5923347444680219\n",
            "Iteration#:  2660 ; Loss:  0.5932502345429517 ; l2 norm of gradient:  0.44732215465952696 ; l2 norm of weights:  0.5910085491529247\n",
            "Iteration#:  2664 ; Loss:  0.5924204435666475 ; l2 norm of gradient:  0.44534311511884556 ; l2 norm of weights:  0.5896906374898301\n",
            "Iteration#:  2668 ; Loss:  0.5915977295571688 ; l2 norm of gradient:  0.4433727629942216 ; l2 norm of weights:  0.5883809684179769\n",
            "Iteration#:  2672 ; Loss:  0.5907820306674888 ; l2 norm of gradient:  0.44141106114231876 ; l2 norm of weights:  0.5870795009943266\n",
            "Iteration#:  2676 ; Loss:  0.5899732855893919 ; l2 norm of gradient:  0.4394579725681655 ; l2 norm of weights:  0.5857861943934926\n",
            "Iteration#:  2680 ; Loss:  0.5891714335488469 ; l2 norm of gradient:  0.43751346042464406 ; l2 norm of weights:  0.5845010079076768\n",
            "Iteration#:  2684 ; Loss:  0.5883764143014185 ; l2 norm of gradient:  0.43557748801198015 ; l2 norm of weights:  0.5832239009466117\n",
            "Iteration#:  2688 ; Loss:  0.5875881681277164 ; l2 norm of gradient:  0.43365001877723536 ; l2 norm of weights:  0.5819548330375088\n",
            "Iteration#:  2692 ; Loss:  0.586806635828883 ; l2 norm of gradient:  0.4317310163137985 ; l2 norm of weights:  0.5806937638250121\n",
            "Iteration#:  2696 ; Loss:  0.5860317587221195 ; l2 norm of gradient:  0.42982044436088174 ; l2 norm of weights:  0.5794406530711573\n",
            "Iteration#:  2700 ; Loss:  0.5852634786362487 ; l2 norm of gradient:  0.4279182668030161 ; l2 norm of weights:  0.5781954606553383\n",
            "Iteration#:  2704 ; Loss:  0.5845017379073152 ; l2 norm of gradient:  0.4260244476695492 ; l2 norm of weights:  0.5769581465742766\n",
            "Iteration#:  2708 ; Loss:  0.5837464793742233 ; l2 norm of gradient:  0.4241389511341443 ; l2 norm of weights:  0.5757286709419981\n",
            "Iteration#:  2712 ; Loss:  0.5829976463744104 ; l2 norm of gradient:  0.422261741514281 ; l2 norm of weights:  0.5745069939898146\n",
            "Iteration#:  2716 ; Loss:  0.582255182739558 ; l2 norm of gradient:  0.420392783270757 ; l2 norm of weights:  0.57329307606631\n",
            "Iteration#:  2720 ; Loss:  0.5815190327913369 ; l2 norm of gradient:  0.41853204100719205 ; l2 norm of weights:  0.5720868776373321\n",
            "Iteration#:  2724 ; Loss:  0.5807891413371904 ; l2 norm of gradient:  0.41667947946953365 ; l2 norm of weights:  0.5708883592859897\n",
            "Iteration#:  2728 ; Loss:  0.5800654536661511 ; l2 norm of gradient:  0.41483506354556154 ; l2 norm of weights:  0.5696974817126533\n",
            "Iteration#:  2732 ; Loss:  0.5793479155446939 ; l2 norm of gradient:  0.4129987582643975 ; l2 norm of weights:  0.5685142057349624\n",
            "Iteration#:  2736 ; Loss:  0.5786364732126235 ; l2 norm of gradient:  0.4111705287960142 ; l2 norm of weights:  0.567338492287835\n",
            "Iteration#:  2740 ; Loss:  0.5779310733789966 ; l2 norm of gradient:  0.40935034045074464 ; l2 norm of weights:  0.5661703024234847\n",
            "Iteration#:  2744 ; Loss:  0.5772316632180783 ; l2 norm of gradient:  0.4075381586787961 ; l2 norm of weights:  0.5650095973114385\n",
            "Iteration#:  2748 ; Loss:  0.5765381903653328 ; l2 norm of gradient:  0.40573394906976246 ; l2 norm of weights:  0.5638563382385633\n",
            "Iteration#:  2752 ; Loss:  0.5758506029134474 ; l2 norm of gradient:  0.40393767735213953 ; l2 norm of weights:  0.562710486609092\n",
            "Iteration#:  2756 ; Loss:  0.5751688494083903 ; l2 norm of gradient:  0.4021493093928409 ; l2 norm of weights:  0.5615720039446571\n",
            "Iteration#:  2760 ; Loss:  0.5744928788455024 ; l2 norm of gradient:  0.4003688111967159 ; l2 norm of weights:  0.5604408518843268\n",
            "Iteration#:  2764 ; Loss:  0.5738226406656196 ; l2 norm of gradient:  0.3985961489060685 ; l2 norm of weights:  0.5593169921846447\n",
            "Iteration#:  2768 ; Loss:  0.5731580847512312 ; l2 norm of gradient:  0.39683128880017704 ; l2 norm of weights:  0.5582003867196745\n",
            "Iteration#:  2772 ; Loss:  0.5724991614226678 ; l2 norm of gradient:  0.3950741972948171 ; l2 norm of weights:  0.5570909974810464\n",
            "Iteration#:  2776 ; Loss:  0.5718458214343225 ; l2 norm of gradient:  0.3933248409417825 ; l2 norm of weights:  0.5559887865780093\n",
            "Iteration#:  2780 ; Loss:  0.5711980159709042 ; l2 norm of gradient:  0.3915831864284116 ; l2 norm of weights:  0.5548937162374838\n",
            "Iteration#:  2784 ; Loss:  0.5705556966437231 ; l2 norm of gradient:  0.38984920057711114 ; l2 norm of weights:  0.553805748804121\n",
            "Iteration#:  2788 ; Loss:  0.5699188154870047 ; l2 norm of gradient:  0.3881228503448842 ; l2 norm of weights:  0.5527248467403623\n",
            "Iteration#:  2792 ; Loss:  0.569287324954239 ; l2 norm of gradient:  0.3864041028228577 ; l2 norm of weights:  0.5516509726265035\n",
            "Iteration#:  2796 ; Loss:  0.568661177914557 ; l2 norm of gradient:  0.38469292523581217 ; l2 norm of weights:  0.5505840891607616\n",
            "Iteration#:  2800 ; Loss:  0.5680403276491404 ; l2 norm of gradient:  0.38298928494171175 ; l2 norm of weights:  0.549524159159344\n",
            "Iteration#:  2804 ; Loss:  0.56742472784766 ; l2 norm of gradient:  0.38129314943123715 ; l2 norm of weights:  0.5484711455565204\n",
            "Iteration#:  2808 ; Loss:  0.5668143326047448 ; l2 norm of gradient:  0.37960448632731836 ; l2 norm of weights:  0.5474250114046977\n",
            "Iteration#:  2812 ; Loss:  0.5662090964164808 ; l2 norm of gradient:  0.37792326338466853 ; l2 norm of weights:  0.5463857198744969\n",
            "Iteration#:  2816 ; Loss:  0.5656089741769403 ; l2 norm of gradient:  0.37624944848932024 ; l2 norm of weights:  0.5453532342548328\n",
            "Iteration#:  2820 ; Loss:  0.5650139211747386 ; l2 norm of gradient:  0.3745830096581629 ; l2 norm of weights:  0.5443275179529949\n",
            "Iteration#:  2824 ; Loss:  0.5644238930896218 ; l2 norm of gradient:  0.3729239150384798 ; l2 norm of weights:  0.5433085344947317\n",
            "Iteration#:  2828 ; Loss:  0.5638388459890831 ; l2 norm of gradient:  0.3712721329074881 ; l2 norm of weights:  0.5422962475243358\n",
            "Iteration#:  2832 ; Loss:  0.5632587363250064 ; l2 norm of gradient:  0.36962763167187956 ; l2 norm of weights:  0.5412906208047313\n",
            "Iteration#:  2836 ; Loss:  0.5626835209303401 ; l2 norm of gradient:  0.3679903798673621 ; l2 norm of weights:  0.5402916182175637\n",
            "Iteration#:  2840 ; Loss:  0.5621131570157979 ; l2 norm of gradient:  0.36636034615820295 ; l2 norm of weights:  0.5392992037632894\n",
            "Iteration#:  2844 ; Loss:  0.5615476021665885 ; l2 norm of gradient:  0.36473749933677224 ; l2 norm of weights:  0.5383133415612694\n",
            "Iteration#:  2848 ; Loss:  0.5609868143391716 ; l2 norm of gradient:  0.3631218083230898 ; l2 norm of weights:  0.5373339958498615\n",
            "Iteration#:  2852 ; Loss:  0.5604307518580437 ; l2 norm of gradient:  0.3615132421643707 ; l2 norm of weights:  0.5363611309865163\n",
            "Iteration#:  2856 ; Loss:  0.5598793734125479 ; l2 norm of gradient:  0.35991177003457286 ; l2 norm of weights:  0.5353947114478723\n",
            "Iteration#:  2860 ; Loss:  0.5593326380537147 ; l2 norm of gradient:  0.358317361233947 ; l2 norm of weights:  0.5344347018298542\n",
            "Iteration#:  2864 ; Loss:  0.5587905051911262 ; l2 norm of gradient:  0.35672998518858584 ; l2 norm of weights:  0.5334810668477693\n",
            "Iteration#:  2868 ; Loss:  0.5582529345898082 ; l2 norm of gradient:  0.35514961144997603 ; l2 norm of weights:  0.5325337713364074\n",
            "Iteration#:  2872 ; Loss:  0.5577198863671501 ; l2 norm of gradient:  0.35357620969455084 ; l2 norm of weights:  0.53159278025014\n",
            "Iteration#:  2876 ; Loss:  0.5571913209898485 ; l2 norm of gradient:  0.3520097497232427 ; l2 norm of weights:  0.5306580586630204\n",
            "Iteration#:  2880 ; Loss:  0.556667199270878 ; l2 norm of gradient:  0.3504502014610395 ; l2 norm of weights:  0.5297295717688838\n",
            "Iteration#:  2884 ; Loss:  0.5561474823664884 ; l2 norm of gradient:  0.34889753495653925 ; l2 norm of weights:  0.5288072848814488\n",
            "Iteration#:  2888 ; Loss:  0.5556321317732258 ; l2 norm of gradient:  0.3473517203815079 ; l2 norm of weights:  0.5278911634344178\n",
            "Iteration#:  2892 ; Loss:  0.5551211093249804 ; l2 norm of gradient:  0.34581272803043783 ; l2 norm of weights:  0.5269811729815792\n",
            "Iteration#:  2896 ; Loss:  0.5546143771900595 ; l2 norm of gradient:  0.3442805283201069 ; l2 norm of weights:  0.5260772791969078\n",
            "Iteration#:  2900 ; Loss:  0.5541118978682834 ; l2 norm of gradient:  0.34275509178913827 ; l2 norm of weights:  0.5251794478746663\n",
            "Iteration#:  2904 ; Loss:  0.553613634188109 ; l2 norm of gradient:  0.3412363890975641 ; l2 norm of weights:  0.5242876449295066\n",
            "Iteration#:  2908 ; Loss:  0.5531195493037758 ; l2 norm of gradient:  0.33972439102638563 ; l2 norm of weights:  0.5234018363965702\n",
            "Iteration#:  2912 ; Loss:  0.5526296066924763 ; l2 norm of gradient:  0.33821906847714023 ; l2 norm of weights:  0.5225219884315887\n",
            "Iteration#:  2916 ; Loss:  0.5521437701515514 ; l2 norm of gradient:  0.33672039247146385 ; l2 norm of weights:  0.5216480673109835\n",
            "Iteration#:  2920 ; Loss:  0.5516620037957096 ; l2 norm of gradient:  0.3352283341506586 ; l2 norm of weights:  0.5207800394319652\n",
            "Iteration#:  2924 ; Loss:  0.5511842720542691 ; l2 norm of gradient:  0.3337428647752597 ; l2 norm of weights:  0.519917871312632\n",
            "Iteration#:  2928 ; Loss:  0.550710539668424 ; l2 norm of gradient:  0.3322639557246042 ; l2 norm of weights:  0.5190615295920677\n",
            "Iteration#:  2932 ; Loss:  0.5502407716885334 ; l2 norm of gradient:  0.3307915784963999 ; l2 norm of weights:  0.5182109810304381\n",
            "Iteration#:  2936 ; Loss:  0.5497749334714348 ; l2 norm of gradient:  0.32932570470629674 ; l2 norm of weights:  0.517366192509087\n",
            "Iteration#:  2940 ; Loss:  0.5493129906777788 ; l2 norm of gradient:  0.3278663060874583 ; l2 norm of weights:  0.5165271310306312\n",
            "Iteration#:  2944 ; Loss:  0.5488549092693874 ; l2 norm of gradient:  0.32641335449013464 ; l2 norm of weights:  0.5156937637190536\n",
            "Iteration#:  2948 ; Loss:  0.5484006555066349 ; l2 norm of gradient:  0.3249668218812361 ; l2 norm of weights:  0.5148660578197959\n",
            "Iteration#:  2952 ; Loss:  0.5479501959458509 ; l2 norm of gradient:  0.3235266803439094 ; l2 norm of weights:  0.5140439806998488\n",
            "Iteration#:  2956 ; Loss:  0.5475034974367461 ; l2 norm of gradient:  0.3220929020771122 ; l2 norm of weights:  0.5132274998478421\n",
            "Iteration#:  2960 ; Loss:  0.5470605271198584 ; l2 norm of gradient:  0.32066545939519187 ; l2 norm of weights:  0.5124165828741319\n",
            "Iteration#:  2964 ; Loss:  0.5466212524240236 ; l2 norm of gradient:  0.31924432472746295 ; l2 norm of weights:  0.5116111975108875\n",
            "Iteration#:  2968 ; Loss:  0.5461856410638646 ; l2 norm of gradient:  0.31782947061778727 ; l2 norm of weights:  0.5108113116121745\n",
            "Iteration#:  2972 ; Loss:  0.5457536610373048 ; l2 norm of gradient:  0.3164208697241541 ; l2 norm of weights:  0.5100168931540391\n",
            "Iteration#:  2976 ; Loss:  0.5453252806231009 ; l2 norm of gradient:  0.31501849481826233 ; l2 norm of weights:  0.5092279102345871\n",
            "Iteration#:  2980 ; Loss:  0.5449004683783975 ; l2 norm of gradient:  0.3136223187851026 ; l2 norm of weights:  0.5084443310740637\n",
            "Iteration#:  2984 ; Loss:  0.5444791931363033 ; l2 norm of gradient:  0.3122323146225416 ; l2 norm of weights:  0.5076661240149294\n",
            "Iteration#:  2988 ; Loss:  0.5440614240034862 ; l2 norm of gradient:  0.3108484554409067 ; l2 norm of weights:  0.5068932575219347\n",
            "Iteration#:  2992 ; Loss:  0.5436471303577911 ; l2 norm of gradient:  0.30947071446257246 ; l2 norm of weights:  0.506125700182192\n",
            "Iteration#:  2996 ; Loss:  0.543236281845877 ; l2 norm of gradient:  0.3080990650215471 ; l2 norm of weights:  0.5053634207052459\n",
            "Iteration#:  3000 ; Loss:  0.542828848380874 ; l2 norm of gradient:  0.3067334805630614 ; l2 norm of weights:  0.5046063879231395\n",
            "Iteration#:  3004 ; Loss:  0.5424248001400612 ; l2 norm of gradient:  0.3053739346431571 ; l2 norm of weights:  0.5038545707904806\n",
            "Iteration#:  3008 ; Loss:  0.5420241075625643 ; l2 norm of gradient:  0.3040204009282782 ; l2 norm of weights:  0.503107938384503\n",
            "Iteration#:  3012 ; Loss:  0.5416267413470723 ; l2 norm of gradient:  0.30267285319486203 ; l2 norm of weights:  0.5023664599051272\n",
            "Iteration#:  3016 ; Loss:  0.541232672449575 ; l2 norm of gradient:  0.30133126532893095 ; l2 norm of weights:  0.501630104675016\n",
            "Iteration#:  3020 ; Loss:  0.5408418720811183 ; l2 norm of gradient:  0.29999561132568714 ; l2 norm of weights:  0.5008988421396307\n",
            "Iteration#:  3024 ; Loss:  0.5404543117055808 ; l2 norm of gradient:  0.29866586528910716 ; l2 norm of weights:  0.5001726418672804\n",
            "Iteration#:  3028 ; Loss:  0.5400699630374667 ; l2 norm of gradient:  0.2973420014315363 ; l2 norm of weights:  0.49945147354917185\n",
            "Iteration#:  3032 ; Loss:  0.5396887980397209 ; l2 norm of gradient:  0.2960239940732867 ; l2 norm of weights:  0.49873530699945456\n",
            "Iteration#:  3036 ; Loss:  0.5393107889215606 ; l2 norm of gradient:  0.2947118176422356 ; l2 norm of weights:  0.4980241121552632\n",
            "Iteration#:  3040 ; Loss:  0.5389359081363253 ; l2 norm of gradient:  0.29340544667342233 ; l2 norm of weights:  0.49731785907675696\n",
            "Iteration#:  3044 ; Loss:  0.5385641283793476 ; l2 norm of gradient:  0.2921048558086508 ; l2 norm of weights:  0.49661651794715583\n",
            "Iteration#:  3048 ; Loss:  0.5381954225858397 ; l2 norm of gradient:  0.2908100197960877 ; l2 norm of weights:  0.4959200590727734\n",
            "Iteration#:  3052 ; Loss:  0.5378297639287997 ; l2 norm of gradient:  0.28952091348986775 ; l2 norm of weights:  0.4952284528830469\n",
            "Iteration#:  3056 ; Loss:  0.5374671258169349 ; l2 norm of gradient:  0.28823751184969465 ; l2 norm of weights:  0.4945416699305631\n",
            "Iteration#:  3060 ; Loss:  0.5371074818926042 ; l2 norm of gradient:  0.28695978994044624 ; l2 norm of weights:  0.4938596808910815\n",
            "Iteration#:  3064 ; Loss:  0.5367508060297763 ; l2 norm of gradient:  0.28568772293177963 ; l2 norm of weights:  0.49318245656355414\n",
            "Iteration#:  3068 ; Loss:  0.5363970723320075 ; l2 norm of gradient:  0.28442128609773815 ; l2 norm of weights:  0.49250996787014095\n",
            "Iteration#:  3072 ; Loss:  0.536046255130436 ; l2 norm of gradient:  0.28316045481635727 ; l2 norm of weights:  0.4918421858562228\n",
            "Iteration#:  3076 ; Loss:  0.5356983289817926 ; l2 norm of gradient:  0.28190520456927515 ; l2 norm of weights:  0.49117908169041014\n",
            "Iteration#:  3080 ; Loss:  0.5353532686664297 ; l2 norm of gradient:  0.28065551094134106 ; l2 norm of weights:  0.4905206266645481\n",
            "Iteration#:  3084 ; Loss:  0.535011049186368 ; l2 norm of gradient:  0.2794113496202264 ; l2 norm of weights:  0.48986679219371865\n",
            "Iteration#:  3088 ; Loss:  0.534671645763357 ; l2 norm of gradient:  0.2781726963960355 ; l2 norm of weights:  0.48921754981623755\n",
            "Iteration#:  3092 ; Loss:  0.5343350338369558 ; l2 norm of gradient:  0.2769395271609201 ; l2 norm of weights:  0.488572871193649\n",
            "Iteration#:  3096 ; Loss:  0.5340011890626281 ; l2 norm of gradient:  0.2757118179086919 ; l2 norm of weights:  0.4879327281107159\n",
            "Iteration#:  3100 ; Loss:  0.5336700873098541 ; l2 norm of gradient:  0.27448954473443743 ; l2 norm of weights:  0.4872970924754058\n",
            "Iteration#:  3104 ; Loss:  0.5333417046602598 ; l2 norm of gradient:  0.27327268383413494 ; l2 norm of weights:  0.48666593631887384\n",
            "Iteration#:  3108 ; Loss:  0.5330160174057601 ; l2 norm of gradient:  0.2720612115042704 ; l2 norm of weights:  0.48603923179544123\n",
            "Iteration#:  3112 ; Loss:  0.532693002046721 ; l2 norm of gradient:  0.2708551041414562 ; l2 norm of weights:  0.4854169511825698\n",
            "Iteration#:  3116 ; Loss:  0.5323726352901346 ; l2 norm of gradient:  0.26965433824205043 ; l2 norm of weights:  0.48479906688083235\n",
            "Iteration#:  3120 ; Loss:  0.5320548940478115 ; l2 norm of gradient:  0.2684588904017759 ; l2 norm of weights:  0.4841855514138803\n",
            "Iteration#:  3124 ; Loss:  0.5317397554345895 ; l2 norm of gradient:  0.2672687373153426 ; l2 norm of weights:  0.48357637742840487\n",
            "Iteration#:  3128 ; Loss:  0.5314271967665555 ; l2 norm of gradient:  0.26608385577607 ; l2 norm of weights:  0.48297151769409674\n",
            "Iteration#:  3132 ; Loss:  0.5311171955592848 ; l2 norm of gradient:  0.2649042226755094 ; l2 norm of weights:  0.4823709451035997\n",
            "Iteration#:  3136 ; Loss:  0.5308097295260947 ; l2 norm of gradient:  0.26372981500306913 ; l2 norm of weights:  0.48177463267246123\n",
            "Iteration#:  3140 ; Loss:  0.5305047765763133 ; l2 norm of gradient:  0.26256060984563956 ; l2 norm of weights:  0.4811825535390788\n",
            "Iteration#:  3144 ; Loss:  0.5302023148135631 ; l2 norm of gradient:  0.2613965843872203 ; l2 norm of weights:  0.48059468096464136\n",
            "Iteration#:  3148 ; Loss:  0.5299023225340598 ; l2 norm of gradient:  0.2602377159085469 ; l2 norm of weights:  0.48001098833306766\n",
            "Iteration#:  3152 ; Loss:  0.5296047782249258 ; l2 norm of gradient:  0.2590839817867202 ; l2 norm of weights:  0.47943144915093927\n",
            "Iteration#:  3156 ; Loss:  0.5293096605625177 ; l2 norm of gradient:  0.25793535949483537 ; l2 norm of weights:  0.47885603704743024\n",
            "Iteration#:  3160 ; Loss:  0.529016948410769 ; l2 norm of gradient:  0.25679182660161326 ; l2 norm of weights:  0.478284725774232\n",
            "Iteration#:  3164 ; Loss:  0.5287266208195466 ; l2 norm of gradient:  0.25565336077103173 ; l2 norm of weights:  0.47771748920547447\n",
            "Iteration#:  3168 ; Loss:  0.5284386570230216 ; l2 norm of gradient:  0.25451993976195836 ; l2 norm of weights:  0.4771543013376423\n",
            "Iteration#:  3172 ; Loss:  0.528153036438055 ; l2 norm of gradient:  0.253391541427785 ; l2 norm of weights:  0.47659513628948735\n",
            "Iteration#:  3176 ; Loss:  0.5278697386625956 ; l2 norm of gradient:  0.25226814371606215 ; l2 norm of weights:  0.4760399683019368\n",
            "Iteration#:  3180 ; Loss:  0.5275887434740937 ; l2 norm of gradient:  0.25114972466813495 ; l2 norm of weights:  0.4754887717379963\n",
            "Iteration#:  3184 ; Loss:  0.527310030827927 ; l2 norm of gradient:  0.2500362624187804 ; l2 norm of weights:  0.4749415210826502\n",
            "Iteration#:  3188 ; Loss:  0.5270335808558418 ; l2 norm of gradient:  0.24892773519584568 ; l2 norm of weights:  0.4743981909427559\n",
            "Iteration#:  3192 ; Loss:  0.5267593738644056 ; l2 norm of gradient:  0.24782412131988643 ; l2 norm of weights:  0.47385875604693517\n",
            "Iteration#:  3196 ; Loss:  0.526487390333475 ; l2 norm of gradient:  0.24672539920380784 ; l2 norm of weights:  0.4733231912454603\n",
            "Iteration#:  3200 ; Loss:  0.5262176109146754 ; l2 norm of gradient:  0.2456315473525057 ; l2 norm of weights:  0.47279147151013634\n",
            "Iteration#:  3204 ; Loss:  0.5259500164298957 ; l2 norm of gradient:  0.24454254436250827 ; l2 norm of weights:  0.4722635719341788\n",
            "Iteration#:  3208 ; Loss:  0.5256845878697931 ; l2 norm of gradient:  0.24345836892161984 ; l2 norm of weights:  0.4717394677320874\n",
            "Iteration#:  3212 ; Loss:  0.5254213063923148 ; l2 norm of gradient:  0.242378999808565 ; l2 norm of weights:  0.4712191342395154\n",
            "Iteration#:  3216 ; Loss:  0.5251601533212286 ; l2 norm of gradient:  0.24130441589263468 ; l2 norm of weights:  0.47070254691313385\n",
            "Iteration#:  3220 ; Loss:  0.5249011101446692 ; l2 norm of gradient:  0.24023459613333153 ; l2 norm of weights:  0.4701896813304929\n",
            "Iteration#:  3224 ; Loss:  0.5246441585136954 ; l2 norm of gradient:  0.2391695195800184 ; l2 norm of weights:  0.4696805131898775\n",
            "Iteration#:  3228 ; Loss:  0.5243892802408604 ; l2 norm of gradient:  0.23810916537156643 ; l2 norm of weights:  0.46917501831016023\n",
            "Iteration#:  3232 ; Loss:  0.5241364572987948 ; l2 norm of gradient:  0.23705351273600464 ; l2 norm of weights:  0.4686731726306479\n",
            "Iteration#:  3236 ; Loss:  0.523885671818801 ; l2 norm of gradient:  0.23600254099017073 ; l2 norm of weights:  0.468174952210926\n",
            "Iteration#:  3240 ; Loss:  0.5236369060894608 ; l2 norm of gradient:  0.234956229539363 ; l2 norm of weights:  0.46768033323069724\n",
            "Iteration#:  3244 ; Loss:  0.5233901425552551 ; l2 norm of gradient:  0.23391455787699256 ; l2 norm of weights:  0.4671892919896172\n",
            "Iteration#:  3248 ; Loss:  0.5231453638151937 ; l2 norm of gradient:  0.23287750558423712 ; l2 norm of weights:  0.46670180490712393\n",
            "Iteration#:  3252 ; Loss:  0.5229025526214601 ; l2 norm of gradient:  0.23184505232969674 ; l2 norm of weights:  0.4662178485222658\n",
            "Iteration#:  3256 ; Loss:  0.5226616918780657 ; l2 norm of gradient:  0.23081717786904854 ; l2 norm of weights:  0.46573739949352266\n",
            "Iteration#:  3260 ; Loss:  0.522422764639516 ; l2 norm of gradient:  0.2297938620447045 ; l2 norm of weights:  0.4652604345986248\n",
            "Iteration#:  3264 ; Loss:  0.5221857541094894 ; l2 norm of gradient:  0.2287750847854686 ; l2 norm of weights:  0.4647869307343661\n",
            "Iteration#:  3268 ; Loss:  0.5219506436395275 ; l2 norm of gradient:  0.22776082610619663 ; l2 norm of weights:  0.46431686491641455\n",
            "Iteration#:  3272 ; Loss:  0.5217174167277341 ; l2 norm of gradient:  0.22675106610745552 ; l2 norm of weights:  0.4638502142791174\n",
            "Iteration#:  3276 ; Loss:  0.5214860570174902 ; l2 norm of gradient:  0.225745784975185 ; l2 norm of weights:  0.463386956075303\n",
            "Iteration#:  3280 ; Loss:  0.5212565482961753 ; l2 norm of gradient:  0.224744962980359 ; l2 norm of weights:  0.46292706767607805\n",
            "Iteration#:  3284 ; Loss:  0.5210288744939038 ; l2 norm of gradient:  0.2237485804786502 ; l2 norm of weights:  0.46247052657062115\n",
            "Iteration#:  3288 ; Loss:  0.5208030196822702 ; l2 norm of gradient:  0.22275661791009208 ; l2 norm of weights:  0.46201731036597243\n",
            "Iteration#:  3292 ; Loss:  0.5205789680731046 ; l2 norm of gradient:  0.22176905579874642 ; l2 norm of weights:  0.4615673967868179\n",
            "Iteration#:  3296 ; Loss:  0.5203567040172419 ; l2 norm of gradient:  0.22078587475236786 ; l2 norm of weights:  0.4611207636752724\n",
            "Iteration#:  3300 ; Loss:  0.5201362120032988 ; l2 norm of gradient:  0.2198070554620715 ; l2 norm of weights:  0.46067738899065525\n",
            "Iteration#:  3304 ; Loss:  0.519917476656463 ; l2 norm of gradient:  0.21883257870200284 ; l2 norm of weights:  0.4602372508092651\n",
            "Iteration#:  3308 ; Loss:  0.5197004827372926 ; l2 norm of gradient:  0.21786242532900493 ; l2 norm of weights:  0.45980032732414866\n",
            "Iteration#:  3312 ; Loss:  0.5194852151405257 ; l2 norm of gradient:  0.21689657628228975 ; l2 norm of weights:  0.4593665968448668\n",
            "Iteration#:  3316 ; Loss:  0.5192716588939007 ; l2 norm of gradient:  0.21593501258310996 ; l2 norm of weights:  0.458936037797256\n",
            "Iteration#:  3320 ; Loss:  0.5190597991569865 ; l2 norm of gradient:  0.21497771533443089 ; l2 norm of weights:  0.4585086287231868\n",
            "Iteration#:  3324 ; Loss:  0.5188496212200238 ; l2 norm of gradient:  0.21402466572060436 ; l2 norm of weights:  0.4580843482803175\n",
            "Iteration#:  3328 ; Loss:  0.5186411105027751 ; l2 norm of gradient:  0.21307584500704313 ; l2 norm of weights:  0.4576631752418452\n",
            "Iteration#:  3332 ; Loss:  0.5184342525533859 ; l2 norm of gradient:  0.212131234539896 ; l2 norm of weights:  0.45724508849625184\n",
            "Iteration#:  3336 ; Loss:  0.5182290330472555 ; l2 norm of gradient:  0.21119081574572537 ; l2 norm of weights:  0.4568300670470479\n",
            "Iteration#:  3340 ; Loss:  0.518025437785917 ; l2 norm of gradient:  0.21025457013118387 ; l2 norm of weights:  0.45641809001251143\n",
            "Iteration#:  3344 ; Loss:  0.5178234526959283 ; l2 norm of gradient:  0.20932247928269404 ; l2 norm of weights:  0.45600913662542375\n",
            "Iteration#:  3348 ; Loss:  0.5176230638277715 ; l2 norm of gradient:  0.20839452486612678 ; l2 norm of weights:  0.4556031862328021\n",
            "Iteration#:  3352 ; Loss:  0.5174242573547624 ; l2 norm of gradient:  0.2074706886264832 ; l2 norm of weights:  0.4552002182956275\n",
            "Iteration#:  3356 ; Loss:  0.5172270195719708 ; l2 norm of gradient:  0.2065509523875759 ; l2 norm of weights:  0.4548002123885709\n",
            "Iteration#:  3360 ; Loss:  0.5170313368951476 ; l2 norm of gradient:  0.20563529805171135 ; l2 norm of weights:  0.4544031481997136\n",
            "Iteration#:  3364 ; Loss:  0.5168371958596631 ; l2 norm of gradient:  0.20472370759937455 ; l2 norm of weights:  0.4540090055302666\n",
            "Iteration#:  3368 ; Loss:  0.5166445831194548 ; l2 norm of gradient:  0.20381616308891293 ; l2 norm of weights:  0.45361776429428463\n",
            "Iteration#:  3372 ; Loss:  0.5164534854459827 ; l2 norm of gradient:  0.20291264665622286 ; l2 norm of weights:  0.453229404518378\n",
            "Iteration#:  3376 ; Loss:  0.5162638897271957 ; l2 norm of gradient:  0.20201314051443622 ; l2 norm of weights:  0.4528439063414206\n",
            "Iteration#:  3380 ; Loss:  0.5160757829665056 ; l2 norm of gradient:  0.2011176269536082 ; l2 norm of weights:  0.4524612500142546\n",
            "Iteration#:  3384 ; Loss:  0.5158891522817706 ; l2 norm of gradient:  0.20022608834040626 ; l2 norm of weights:  0.4520814158993923\n",
            "Iteration#:  3388 ; Loss:  0.5157039849042875 ; l2 norm of gradient:  0.19933850711780035 ; l2 norm of weights:  0.45170438447071437\n",
            "Iteration#:  3392 ; Loss:  0.5155202681777936 ; l2 norm of gradient:  0.1984548658047531 ; l2 norm of weights:  0.4513301363131652\n",
            "Iteration#:  3396 ; Loss:  0.5153379895574755 ; l2 norm of gradient:  0.19757514699591278 ; l2 norm of weights:  0.4509586521224448\n",
            "Iteration#:  3400 ; Loss:  0.5151571366089879 ; l2 norm of gradient:  0.1966993333613054 ; l2 norm of weights:  0.450589912704698\n",
            "Iteration#:  3404 ; Loss:  0.5149776970074815 ; l2 norm of gradient:  0.19582740764602882 ; l2 norm of weights:  0.4502238989762005\n",
            "Iteration#:  3408 ; Loss:  0.5147996585366377 ; l2 norm of gradient:  0.19495935266994838 ; l2 norm of weights:  0.4498605919630418\n",
            "Iteration#:  3412 ; Loss:  0.5146230090877127 ; l2 norm of gradient:  0.19409515132739205 ; l2 norm of weights:  0.44949997280080534\n",
            "Iteration#:  3416 ; Loss:  0.5144477366585908 ; l2 norm of gradient:  0.1932347865868481 ; l2 norm of weights:  0.4491420227342454\n",
            "Iteration#:  3420 ; Loss:  0.5142738293528437 ; l2 norm of gradient:  0.19237824149066254 ; l2 norm of weights:  0.4487867231169617\n",
            "Iteration#:  3424 ; Loss:  0.5141012753788003 ; l2 norm of gradient:  0.19152549915473888 ; l2 norm of weights:  0.4484340554110707\n",
            "Iteration#:  3428 ; Loss:  0.5139300630486238 ; l2 norm of gradient:  0.19067654276823762 ; l2 norm of weights:  0.4480840011868743\n",
            "Iteration#:  3432 ; Loss:  0.5137601807773965 ; l2 norm of gradient:  0.1898313555932771 ; l2 norm of weights:  0.44773654212252606\n",
            "Iteration#:  3436 ; Loss:  0.5135916170822128 ; l2 norm of gradient:  0.18898992096463624 ; l2 norm of weights:  0.4473916600036939\n",
            "Iteration#:  3440 ; Loss:  0.5134243605812807 ; l2 norm of gradient:  0.18815222228945697 ; l2 norm of weights:  0.4470493367232214\n",
            "Iteration#:  3444 ; Loss:  0.5132583999930301 ; l2 norm of gradient:  0.187318243046949 ; l2 norm of weights:  0.44670955428078507\n",
            "Iteration#:  3448 ; Loss:  0.5130937241352305 ; l2 norm of gradient:  0.18648796678809385 ; l2 norm of weights:  0.44637229478255075\n",
            "Iteration#:  3452 ; Loss:  0.5129303219241146 ; l2 norm of gradient:  0.18566137713535183 ; l2 norm of weights:  0.4460375404408258\n",
            "Iteration#:  3456 ; Loss:  0.5127681823735103 ; l2 norm of gradient:  0.1848384577823686 ; l2 norm of weights:  0.4457052735737099\n",
            "Iteration#:  3460 ; Loss:  0.5126072945939818 ; l2 norm of gradient:  0.18401919249368362 ; l2 norm of weights:  0.4453754766047433\n",
            "Iteration#:  3464 ; Loss:  0.5124476477919748 ; l2 norm of gradient:  0.1832035651044382 ; l2 norm of weights:  0.44504813206255217\n",
            "Iteration#:  3468 ; Loss:  0.5122892312689734 ; l2 norm of gradient:  0.1823915595200875 ; l2 norm of weights:  0.444723222580492\n",
            "Iteration#:  3472 ; Loss:  0.5121320344206604 ; l2 norm of gradient:  0.18158315971610933 ; l2 norm of weights:  0.44440073089628956\n",
            "Iteration#:  3476 ; Loss:  0.511976046736088 ; l2 norm of gradient:  0.18077834973771786 ; l2 norm of weights:  0.4440806398516803\n",
            "Iteration#:  3480 ; Loss:  0.5118212577968541 ; l2 norm of gradient:  0.1799771136995756 ; l2 norm of weights:  0.44376293239204606\n",
            "Iteration#:  3484 ; Loss:  0.5116676572762858 ; l2 norm of gradient:  0.17917943578550788 ; l2 norm of weights:  0.44344759156604957\n",
            "Iteration#:  3488 ; Loss:  0.5115152349386322 ; l2 norm of gradient:  0.1783853002482179 ; l2 norm of weights:  0.4431346005252665\n",
            "Iteration#:  3492 ; Loss:  0.5113639806382602 ; l2 norm of gradient:  0.17759469140900236 ; l2 norm of weights:  0.44282394252381574\n",
            "Iteration#:  3496 ; Loss:  0.5112138843188617 ; l2 norm of gradient:  0.17680759365746904 ; l2 norm of weights:  0.4425156009179885\n",
            "Iteration#:  3500 ; Loss:  0.5110649360126652 ; l2 norm of gradient:  0.17602399145125427 ; l2 norm of weights:  0.44220955916587384\n",
            "Iteration#:  3504 ; Loss:  0.5109171258396543 ; l2 norm of gradient:  0.1752438693157413 ; l2 norm of weights:  0.4419058008269837\n",
            "Iteration#:  3508 ; Loss:  0.510770444006795 ; l2 norm of gradient:  0.17446721184378194 ; l2 norm of weights:  0.4416043095618753\n",
            "Iteration#:  3512 ; Loss:  0.510624880807267 ; l2 norm of gradient:  0.17369400369541546 ; l2 norm of weights:  0.44130506913177225\n",
            "Iteration#:  3516 ; Loss:  0.5104804266197044 ; l2 norm of gradient:  0.17292422959759196 ; l2 norm of weights:  0.44100806339818294\n",
            "Iteration#:  3520 ; Loss:  0.5103370719074409 ; l2 norm of gradient:  0.17215787434389487 ; l2 norm of weights:  0.44071327632251855\n",
            "Iteration#:  3524 ; Loss:  0.5101948072177638 ; l2 norm of gradient:  0.1713949227942638 ; l2 norm of weights:  0.4404206919657081\n",
            "Iteration#:  3528 ; Loss:  0.5100536231811718 ; l2 norm of gradient:  0.1706353598747205 ; l2 norm of weights:  0.44013029448781255\n",
            "Iteration#:  3532 ; Loss:  0.5099135105106419 ; l2 norm of gradient:  0.16987917057709423 ; l2 norm of weights:  0.4398420681476373\n",
            "Iteration#:  3536 ; Loss:  0.5097744600009018 ; l2 norm of gradient:  0.1691263399587487 ; l2 norm of weights:  0.4395559973023428\n",
            "Iteration#:  3540 ; Loss:  0.5096364625277079 ; l2 norm of gradient:  0.16837685314230913 ; l2 norm of weights:  0.43927206640705396\n",
            "Iteration#:  3544 ; Loss:  0.5094995090471304 ; l2 norm of gradient:  0.16763069531539157 ; l2 norm of weights:  0.4389902600144678\n",
            "Iteration#:  3548 ; Loss:  0.509363590594845 ; l2 norm of gradient:  0.1668878517303324 ; l2 norm of weights:  0.4387105627744599\n",
            "Iteration#:  3552 ; Loss:  0.5092286982854306 ; l2 norm of gradient:  0.1661483077039186 ; l2 norm of weights:  0.43843295943368965\n",
            "Iteration#:  3556 ; Loss:  0.5090948233116719 ; l2 norm of gradient:  0.1654120486171197 ; l2 norm of weights:  0.4381574348352037\n",
            "Iteration#:  3560 ; Loss:  0.5089619569438703 ; l2 norm of gradient:  0.1646790599148201 ; l2 norm of weights:  0.4378839739180383\n",
            "Iteration#:  3564 ; Loss:  0.5088300905291592 ; l2 norm of gradient:  0.1639493271055526 ; l2 norm of weights:  0.4376125617168206\n",
            "Iteration#:  3568 ; Loss:  0.508699215490826 ; l2 norm of gradient:  0.1632228357612333 ; l2 norm of weights:  0.4373431833613688\n",
            "Iteration#:  3572 ; Loss:  0.5085693233276403 ; l2 norm of gradient:  0.1624995715168953 ; l2 norm of weights:  0.4370758240762905\n",
            "Iteration#:  3576 ; Loss:  0.5084404056131866 ; l2 norm of gradient:  0.16177952007042776 ; l2 norm of weights:  0.4368104691805805\n",
            "Iteration#:  3580 ; Loss:  0.5083124539952053 ; l2 norm of gradient:  0.16106266718231052 ; l2 norm of weights:  0.4365471040872177\n",
            "Iteration#:  3584 ; Loss:  0.5081854601949367 ; l2 norm of gradient:  0.16034899867535418 ; l2 norm of weights:  0.43628571430276064\n",
            "Iteration#:  3588 ; Loss:  0.5080594160064734 ; l2 norm of gradient:  0.15963850043443786 ; l2 norm of weights:  0.43602628542694166\n",
            "Iteration#:  3592 ; Loss:  0.507934313296116 ; l2 norm of gradient:  0.1589311584062513 ; l2 norm of weights:  0.43576880315226113\n",
            "Iteration#:  3596 ; Loss:  0.507810144001737 ; l2 norm of gradient:  0.15822695859903366 ; l2 norm of weights:  0.4355132532635797\n",
            "Iteration#:  3600 ; Loss:  0.5076869001321468 ; l2 norm of gradient:  0.1575258870823169 ; l2 norm of weights:  0.43525962163771037\n",
            "Iteration#:  3604 ; Loss:  0.5075645737664697 ; l2 norm of gradient:  0.15682792998666856 ; l2 norm of weights:  0.4350078942430098\n",
            "Iteration#:  3608 ; Loss:  0.5074431570535219 ; l2 norm of gradient:  0.15613307350343528 ; l2 norm of weights:  0.4347580571389681\n",
            "Iteration#:  3612 ; Loss:  0.5073226422111958 ; l2 norm of gradient:  0.15544130388448812 ; l2 norm of weights:  0.4345100964757991\n",
            "Iteration#:  3616 ; Loss:  0.5072030215258512 ; l2 norm of gradient:  0.15475260744196798 ; l2 norm of weights:  0.43426399849402825\n",
            "Iteration#:  3620 ; Loss:  0.507084287351709 ; l2 norm of gradient:  0.15406697054803267 ; l2 norm of weights:  0.4340197495240818\n",
            "Iteration#:  3624 ; Loss:  0.506966432110254 ; l2 norm of gradient:  0.1533843796346035 ; l2 norm of weights:  0.43377733598587365\n",
            "Iteration#:  3628 ; Loss:  0.5068494482896394 ; l2 norm of gradient:  0.15270482119311526 ; l2 norm of weights:  0.43353674438839246\n",
            "Iteration#:  3632 ; Loss:  0.5067333284440985 ; l2 norm of gradient:  0.1520282817742653 ; l2 norm of weights:  0.4332979613292885\n",
            "Iteration#:  3636 ; Loss:  0.5066180651933611 ; l2 norm of gradient:  0.1513547479877629 ; l2 norm of weights:  0.43306097349445927\n",
            "Iteration#:  3640 ; Loss:  0.5065036512220751 ; l2 norm of gradient:  0.15068420650208195 ; l2 norm of weights:  0.43282576765763514\n",
            "Iteration#:  3644 ; Loss:  0.5063900792792333 ; l2 norm of gradient:  0.15001664404421236 ; l2 norm of weights:  0.4325923306799641\n",
            "Iteration#:  3648 ; Loss:  0.5062773421776047 ; l2 norm of gradient:  0.1493520473994136 ; l2 norm of weights:  0.4323606495095971\n",
            "Iteration#:  3652 ; Loss:  0.5061654327931712 ; l2 norm of gradient:  0.1486904034109689 ; l2 norm of weights:  0.4321307111812716\n",
            "Iteration#:  3656 ; Loss:  0.5060543440645701 ; l2 norm of gradient:  0.1480316989799397 ; l2 norm of weights:  0.43190250281589604\n",
            "Iteration#:  3660 ; Loss:  0.5059440689925391 ; l2 norm of gradient:  0.14737592106492203 ; l2 norm of weights:  0.4316760116201338\n",
            "Iteration#:  3664 ; Loss:  0.5058346006393695 ; l2 norm of gradient:  0.14672305668180344 ; l2 norm of weights:  0.4314512248859861\n",
            "Iteration#:  3668 ; Loss:  0.5057259321283609 ; l2 norm of gradient:  0.14607309290352025 ; l2 norm of weights:  0.4312281299903757\n",
            "Iteration#:  3672 ; Loss:  0.5056180566432836 ; l2 norm of gradient:  0.14542601685981646 ; l2 norm of weights:  0.4310067143947301\n",
            "Iteration#:  3676 ; Loss:  0.5055109674278434 ; l2 norm of gradient:  0.14478181573700333 ; l2 norm of weights:  0.43078696564456426\n",
            "Iteration#:  3680 ; Loss:  0.5054046577851525 ; l2 norm of gradient:  0.14414047677771988 ; l2 norm of weights:  0.4305688713690638\n",
            "Iteration#:  3684 ; Loss:  0.5052991210772049 ; l2 norm of gradient:  0.1435019872806943 ; l2 norm of weights:  0.4303524192806676\n",
            "Iteration#:  3688 ; Loss:  0.5051943507243557 ; l2 norm of gradient:  0.14286633460050566 ; l2 norm of weights:  0.43013759717465067\n",
            "Iteration#:  3692 ; Loss:  0.5050903402048061 ; l2 norm of gradient:  0.14223350614734748 ; l2 norm of weights:  0.4299243929287074\n",
            "Iteration#:  3696 ; Loss:  0.5049870830540916 ; l2 norm of gradient:  0.14160348938679196 ; l2 norm of weights:  0.42971279450253347\n",
            "Iteration#:  3700 ; Loss:  0.504884572864576 ; l2 norm of gradient:  0.14097627183955452 ; l2 norm of weights:  0.42950278993741\n",
            "Iteration#:  3704 ; Loss:  0.5047828032849494 ; l2 norm of gradient:  0.14035184108125964 ; l2 norm of weights:  0.4292943673557855\n",
            "Iteration#:  3708 ; Loss:  0.5046817680197294 ; l2 norm of gradient:  0.1397301847422081 ; l2 norm of weights:  0.42908751496085973\n",
            "Iteration#:  3712 ; Loss:  0.5045814608287694 ; l2 norm of gradient:  0.13911129050714366 ; l2 norm of weights:  0.4288822210361669\n",
            "Iteration#:  3716 ; Loss:  0.5044818755267687 ; l2 norm of gradient:  0.13849514611502245 ; l2 norm of weights:  0.4286784739451589\n",
            "Iteration#:  3720 ; Loss:  0.504383005982788 ; l2 norm of gradient:  0.13788173935878184 ; l2 norm of weights:  0.42847626213078954\n",
            "Iteration#:  3724 ; Loss:  0.5042848461197689 ; l2 norm of gradient:  0.13727105808511042 ; l2 norm of weights:  0.42827557411509837\n",
            "Iteration#:  3728 ; Loss:  0.504187389914058 ; l2 norm of gradient:  0.13666309019421982 ; l2 norm of weights:  0.4280763984987951\n",
            "Iteration#:  3732 ; Loss:  0.5040906313949355 ; l2 norm of gradient:  0.13605782363961613 ; l2 norm of weights:  0.4278787239608439\n",
            "Iteration#:  3736 ; Loss:  0.5039945646441459 ; l2 norm of gradient:  0.13545524642787315 ; l2 norm of weights:  0.4276825392580493\n",
            "Iteration#:  3740 ; Loss:  0.5038991837954363 ; l2 norm of gradient:  0.13485534661840579 ; l2 norm of weights:  0.4274878332246401\n",
            "Iteration#:  3744 ; Loss:  0.5038044830340946 ; l2 norm of gradient:  0.13425811232324494 ; l2 norm of weights:  0.42729459477185683\n",
            "Iteration#:  3748 ; Loss:  0.5037104565964968 ; l2 norm of gradient:  0.13366353170681228 ; l2 norm of weights:  0.42710281288753715\n",
            "Iteration#:  3752 ; Loss:  0.5036170987696529 ; l2 norm of gradient:  0.1330715929856979 ; l2 norm of weights:  0.4269124766357021\n",
            "Iteration#:  3756 ; Loss:  0.5035244038907606 ; l2 norm of gradient:  0.1324822844284365 ; l2 norm of weights:  0.4267235751561445\n",
            "Iteration#:  3760 ; Loss:  0.5034323663467619 ; l2 norm of gradient:  0.13189559435528522 ; l2 norm of weights:  0.42653609766401546\n",
            "Iteration#:  3764 ; Loss:  0.503340980573903 ; l2 norm of gradient:  0.13131151113800332 ; l2 norm of weights:  0.4263500334494137\n",
            "Iteration#:  3768 ; Loss:  0.5032502410572989 ; l2 norm of gradient:  0.1307300231996318 ; l2 norm of weights:  0.4261653718769733\n",
            "Iteration#:  3772 ; Loss:  0.503160142330501 ; l2 norm of gradient:  0.13015111901427354 ; l2 norm of weights:  0.4259821023854544\n",
            "Iteration#:  3776 ; Loss:  0.5030706789750693 ; l2 norm of gradient:  0.12957478710687528 ; l2 norm of weights:  0.42580021448733213\n",
            "Iteration#:  3780 ; Loss:  0.5029818456201485 ; l2 norm of gradient:  0.12900101605300954 ; l2 norm of weights:  0.4256196977683882\n",
            "Iteration#:  3784 ; Loss:  0.5028936369420463 ; l2 norm of gradient:  0.12842979447865785 ; l2 norm of weights:  0.4254405418873014\n",
            "Iteration#:  3788 ; Loss:  0.502806047663818 ; l2 norm of gradient:  0.1278611110599951 ; l2 norm of weights:  0.4252627365752408\n",
            "Iteration#:  3792 ; Loss:  0.5027190725548516 ; l2 norm of gradient:  0.12729495452317416 ; l2 norm of weights:  0.4250862716354577\n",
            "Iteration#:  3796 ; Loss:  0.50263270643046 ; l2 norm of gradient:  0.1267313136441113 ; l2 norm of weights:  0.4249111369428796\n",
            "Iteration#:  3800 ; Loss:  0.5025469441514737 ; l2 norm of gradient:  0.12617017724827323 ; l2 norm of weights:  0.42473732244370427\n",
            "Iteration#:  3804 ; Loss:  0.5024617806238394 ; l2 norm of gradient:  0.12561153421046392 ; l2 norm of weights:  0.4245648181549953\n",
            "Iteration#:  3808 ; Loss:  0.5023772107982202 ; l2 norm of gradient:  0.1250553734546134 ; l2 norm of weights:  0.42439361416427773\n",
            "Iteration#:  3812 ; Loss:  0.5022932296696014 ; l2 norm of gradient:  0.12450168395356602 ; l2 norm of weights:  0.42422370062913495\n",
            "Iteration#:  3816 ; Loss:  0.5022098322768982 ; l2 norm of gradient:  0.12395045472887095 ; l2 norm of weights:  0.42405506777680646\n",
            "Iteration#:  3820 ; Loss:  0.5021270137025673 ; l2 norm of gradient:  0.12340167485057248 ; l2 norm of weights:  0.4238877059037859\n",
            "Iteration#:  3824 ; Loss:  0.502044769072222 ; l2 norm of gradient:  0.12285533343700183 ; l2 norm of weights:  0.4237216053754214\n",
            "Iteration#:  3828 ; Loss:  0.501963093554251 ; l2 norm of gradient:  0.12231141965456917 ; l2 norm of weights:  0.42355675662551523\n",
            "Iteration#:  3832 ; Loss:  0.50188198235944 ; l2 norm of gradient:  0.12176992271755688 ; l2 norm of weights:  0.42339315015592516\n",
            "Iteration#:  3836 ; Loss:  0.5018014307405971 ; l2 norm of gradient:  0.12123083188791312 ; l2 norm of weights:  0.423230776536167\n",
            "Iteration#:  3840 ; Loss:  0.501721433992181 ; l2 norm of gradient:  0.12069413647504779 ; l2 norm of weights:  0.4230696264030179\n",
            "Iteration#:  3844 ; Loss:  0.5016419874499333 ; l2 norm of gradient:  0.12015982583562652 ; l2 norm of weights:  0.4229096904601201\n",
            "Iteration#:  3848 ; Loss:  0.5015630864905131 ; l2 norm of gradient:  0.11962788937336782 ; l2 norm of weights:  0.42275095947758695\n",
            "Iteration#:  3852 ; Loss:  0.5014847265311354 ; l2 norm of gradient:  0.11909831653884077 ; l2 norm of weights:  0.42259342429160845\n",
            "Iteration#:  3856 ; Loss:  0.5014069030292129 ; l2 norm of gradient:  0.11857109682926194 ; l2 norm of weights:  0.4224370758040593\n",
            "Iteration#:  3860 ; Loss:  0.5013296114820001 ; l2 norm of gradient:  0.11804621978829538 ; l2 norm of weights:  0.4222819049821068\n",
            "Iteration#:  3864 ; Loss:  0.5012528474262414 ; l2 norm of gradient:  0.11752367500585044 ; l2 norm of weights:  0.4221279028578208\n",
            "Iteration#:  3868 ; Loss:  0.5011766064378221 ; l2 norm of gradient:  0.11700345211788458 ; l2 norm of weights:  0.42197506052778444\n",
            "Iteration#:  3872 ; Loss:  0.5011008841314222 ; l2 norm of gradient:  0.11648554080620294 ; l2 norm of weights:  0.4218233691527049\n",
            "Iteration#:  3876 ; Loss:  0.5010256761601733 ; l2 norm of gradient:  0.11596993079825982 ; l2 norm of weights:  0.42167281995702793\n",
            "Iteration#:  3880 ; Loss:  0.500950978215319 ; l2 norm of gradient:  0.11545661186696343 ; l2 norm of weights:  0.4215234042285506\n",
            "Iteration#:  3884 ; Loss:  0.5008767860258776 ; l2 norm of gradient:  0.11494557383047749 ; l2 norm of weights:  0.42137511331803723\n",
            "Iteration#:  3888 ; Loss:  0.5008030953583085 ; l2 norm of gradient:  0.11443680655202695 ; l2 norm of weights:  0.4212279386388355\n",
            "Iteration#:  3892 ; Loss:  0.5007299020161811 ; l2 norm of gradient:  0.11393029993970205 ; l2 norm of weights:  0.42108187166649425\n",
            "Iteration#:  3896 ; Loss:  0.5006572018398466 ; l2 norm of gradient:  0.1134260439462651 ; l2 norm of weights:  0.4209369039383824\n",
            "Iteration#:  3900 ; Loss:  0.5005849907061128 ; l2 norm of gradient:  0.11292402856895684 ; l2 norm of weights:  0.42079302705330873\n",
            "Iteration#:  3904 ; Loss:  0.500513264527922 ; l2 norm of gradient:  0.11242424384930394 ; l2 norm of weights:  0.42065023267114365\n",
            "Iteration#:  3908 ; Loss:  0.5004420192540315 ; l2 norm of gradient:  0.11192667987292657 ; l2 norm of weights:  0.4205085125124413\n",
            "Iteration#:  3912 ; Loss:  0.5003712508686967 ; l2 norm of gradient:  0.11143132676934853 ; l2 norm of weights:  0.4203678583580639\n",
            "Iteration#:  3916 ; Loss:  0.5003009553913579 ; l2 norm of gradient:  0.11093817471180638 ; l2 norm of weights:  0.42022826204880637\n",
            "Iteration#:  3920 ; Loss:  0.5002311288763283 ; l2 norm of gradient:  0.11044721391706051 ; l2 norm of weights:  0.4200897154850234\n",
            "Iteration#:  3924 ; Loss:  0.5001617674124865 ; l2 norm of gradient:  0.10995843464520545 ; l2 norm of weights:  0.41995221062625665\n",
            "Iteration#:  3928 ; Loss:  0.5000928671229707 ; l2 norm of gradient:  0.10947182719948234 ; l2 norm of weights:  0.419815739490864\n",
            "Iteration#:  3932 ; Loss:  0.5000244241648758 ; l2 norm of gradient:  0.10898738192609311 ; l2 norm of weights:  0.41968029415565017\n",
            "Iteration#:  3936 ; Loss:  0.4999564347289529 ; l2 norm of gradient:  0.10850508921401202 ; l2 norm of weights:  0.41954586675549815\n",
            "Iteration#:  3940 ; Loss:  0.4998888950393124 ; l2 norm of gradient:  0.10802493949480065 ; l2 norm of weights:  0.4194124494830025\n",
            "Iteration#:  3944 ; Loss:  0.4998218013531284 ; l2 norm of gradient:  0.10754692324242397 ; l2 norm of weights:  0.41928003458810376\n",
            "Iteration#:  3948 ; Loss:  0.49975514996034703 ; l2 norm of gradient:  0.10707103097306532 ; l2 norm of weights:  0.4191486143777242\n",
            "Iteration#:  3952 ; Loss:  0.49968893718339613 ; l2 norm of gradient:  0.10659725324494278 ; l2 norm of weights:  0.4190181812154054\n",
            "Iteration#:  3956 ; Loss:  0.49962315937689844 ; l2 norm of gradient:  0.10612558065812727 ; l2 norm of weights:  0.41888872752094625\n",
            "Iteration#:  3960 ; Loss:  0.49955781292738693 ; l2 norm of gradient:  0.10565600385435972 ; l2 norm of weights:  0.4187602457700438\n",
            "Iteration#:  3964 ; Loss:  0.4994928942530229 ; l2 norm of gradient:  0.105188513516871 ; l2 norm of weights:  0.41863272849393396\n",
            "Iteration#:  3968 ; Loss:  0.4994283998033158 ; l2 norm of gradient:  0.1047231003702004 ; l2 norm of weights:  0.41850616827903475\n",
            "Iteration#:  3972 ; Loss:  0.49936432605884684 ; l2 norm of gradient:  0.10425975518001668 ; l2 norm of weights:  0.4183805577665905\n",
            "Iteration#:  3976 ; Loss:  0.4993006695309935 ; l2 norm of gradient:  0.10379846875293867 ; l2 norm of weights:  0.4182558896523176\n",
            "Iteration#:  3980 ; Loss:  0.49923742676165817 ; l2 norm of gradient:  0.10333923193635715 ; l2 norm of weights:  0.41813215668605186\n",
            "Iteration#:  3984 ; Loss:  0.49917459432299754 ; l2 norm of gradient:  0.10288203561825777 ; l2 norm of weights:  0.41800935167139663\n",
            "Iteration#:  3988 ; Loss:  0.49911216881715553 ; l2 norm of gradient:  0.10242687072704296 ; l2 norm of weights:  0.41788746746537375\n",
            "Iteration#:  3992 ; Loss:  0.49905014687599814 ; l2 norm of gradient:  0.10197372823135796 ; l2 norm of weights:  0.41776649697807416\n",
            "Iteration#:  3996 ; Loss:  0.4989885251608507 ; l2 norm of gradient:  0.10152259913991307 ; l2 norm of weights:  0.4176464331723117\n",
            "Iteration#:  4000 ; Loss:  0.49892730036223737 ; l2 norm of gradient:  0.10107347450131132 ; l2 norm of weights:  0.4175272690632774\n",
            "Iteration#:  4004 ; Loss:  0.49886646919962324 ; l2 norm of gradient:  0.10062634540387226 ; l2 norm of weights:  0.4174089977181952\n",
            "Iteration#:  4008 ; Loss:  0.4988060284211583 ; l2 norm of gradient:  0.10018120297546143 ; l2 norm of weights:  0.41729161225598005\n",
            "Iteration#:  4012 ; Loss:  0.49874597480342364 ; l2 norm of gradient:  0.09973803838331523 ; l2 norm of weights:  0.41717510584689643\n",
            "Iteration#:  4016 ; Loss:  0.49868630515118106 ; l2 norm of gradient:  0.0992968428338708 ; l2 norm of weights:  0.41705947171221924\n",
            "Iteration#:  4020 ; Loss:  0.4986270162971228 ; l2 norm of gradient:  0.09885760757259464 ; l2 norm of weights:  0.41694470312389553\n",
            "Iteration#:  4024 ; Loss:  0.49856810510162514 ; l2 norm of gradient:  0.09842032388381211 ; l2 norm of weights:  0.41683079340420826\n",
            "Iteration#:  4028 ; Loss:  0.49850956845250405 ; l2 norm of gradient:  0.09798498309053741 ; l2 norm of weights:  0.4167177359254413\n",
            "Iteration#:  4032 ; Loss:  0.49845140326477216 ; l2 norm of gradient:  0.0975515765543055 ; l2 norm of weights:  0.4166055241095463\n",
            "Iteration#:  4036 ; Loss:  0.4983936064803983 ; l2 norm of gradient:  0.09712009567500297 ; l2 norm of weights:  0.4164941514278102\n",
            "Iteration#:  4040 ; Loss:  0.49833617506806976 ; l2 norm of gradient:  0.09669053189070088 ; l2 norm of weights:  0.4163836114005258\n",
            "Iteration#:  4044 ; Loss:  0.498279106022956 ; l2 norm of gradient:  0.09626287667748823 ; l2 norm of weights:  0.4162738975966623\n",
            "Iteration#:  4048 ; Loss:  0.4982223963664736 ; l2 norm of gradient:  0.09583712154930499 ; l2 norm of weights:  0.416165003633538\n",
            "Iteration#:  4052 ; Loss:  0.4981660431460558 ; l2 norm of gradient:  0.09541325805777705 ; l2 norm of weights:  0.4160569231764953\n",
            "Iteration#:  4056 ; Loss:  0.4981100434349215 ; l2 norm of gradient:  0.09499127779205155 ; l2 norm of weights:  0.4159496499385759\n",
            "Iteration#:  4060 ; Loss:  0.49805439433184767 ; l2 norm of gradient:  0.0945711723786326 ; l2 norm of weights:  0.41584317768019863\n",
            "Iteration#:  4064 ; Loss:  0.49799909296094347 ; l2 norm of gradient:  0.09415293348121774 ; l2 norm of weights:  0.41573750020883815\n",
            "Iteration#:  4068 ; Loss:  0.49794413647142627 ; l2 norm of gradient:  0.09373655280053508 ; l2 norm of weights:  0.41563261137870566\n",
            "Iteration#:  4072 ; Loss:  0.4978895220373999 ; l2 norm of gradient:  0.09332202207418198 ; l2 norm of weights:  0.4155285050904309\n",
            "Iteration#:  4076 ; Loss:  0.4978352468576344 ; l2 norm of gradient:  0.09290933307646244 ; l2 norm of weights:  0.4154251752907459\n",
            "Iteration#:  4080 ; Loss:  0.49778130815534855 ; l2 norm of gradient:  0.09249847761822724 ; l2 norm of weights:  0.4153226159721701\n",
            "Iteration#:  4084 ; Loss:  0.4977277031779931 ; l2 norm of gradient:  0.09208944754671375 ; l2 norm of weights:  0.41522082117269715\n",
            "Iteration#:  4088 ; Loss:  0.49767442919703697 ; l2 norm of gradient:  0.09168223474538624 ; l2 norm of weights:  0.4151197849754836\n",
            "Iteration#:  4092 ; Loss:  0.49762148350775537 ; l2 norm of gradient:  0.09127683113377703 ; l2 norm of weights:  0.4150195015085382\n",
            "Iteration#:  4096 ; Loss:  0.49756886342901874 ; l2 norm of gradient:  0.09087322866732934 ; l2 norm of weights:  0.41491996494441424\n",
            "Iteration#:  4100 ; Loss:  0.49751656630308444 ; l2 norm of gradient:  0.09047141933723848 ; l2 norm of weights:  0.41482116949990183\n",
            "Iteration#:  4104 ; Loss:  0.49746458949539046 ; l2 norm of gradient:  0.09007139517029623 ; l2 norm of weights:  0.41472310943572294\n",
            "Iteration#:  4108 ; Loss:  0.4974129303943501 ; l2 norm of gradient:  0.08967314822873426 ; l2 norm of weights:  0.4146257790562279\n",
            "Iteration#:  4112 ; Loss:  0.4973615864111489 ; l2 norm of gradient:  0.08927667061006848 ; l2 norm of weights:  0.4145291727090926\n",
            "Iteration#:  4116 ; Loss:  0.49731055497954413 ; l2 norm of gradient:  0.08888195444694506 ; l2 norm of weights:  0.4144332847850185\n",
            "Iteration#:  4120 ; Loss:  0.4972598335556645 ; l2 norm of gradient:  0.0884889919069852 ; l2 norm of weights:  0.4143381097174332\n",
            "Iteration#:  4124 ; Loss:  0.49720941961781284 ; l2 norm of gradient:  0.08809777519263216 ; l2 norm of weights:  0.41424364198219304\n",
            "Iteration#:  4128 ; Loss:  0.49715931066627 ; l2 norm of gradient:  0.08770829654099899 ; l2 norm of weights:  0.41414987609728743\n",
            "Iteration#:  4132 ; Loss:  0.4971095042231012 ; l2 norm of gradient:  0.08732054822371474 ; l2 norm of weights:  0.4140568066225443\n",
            "Iteration#:  4136 ; Loss:  0.49705999783196264 ; l2 norm of gradient:  0.08693452254677467 ; l2 norm of weights:  0.4139644281593376\n",
            "Iteration#:  4140 ; Loss:  0.4970107890579116 ; l2 norm of gradient:  0.08655021185038815 ; l2 norm of weights:  0.413872735350296\n",
            "Iteration#:  4144 ; Loss:  0.4969618754872166 ; l2 norm of gradient:  0.08616760850882875 ; l2 norm of weights:  0.41378172287901344\n",
            "Iteration#:  4148 ; Loss:  0.4969132547271707 ; l2 norm of gradient:  0.08578670493028444 ; l2 norm of weights:  0.4136913854697608\n",
            "Iteration#:  4152 ; Loss:  0.49686492440590535 ; l2 norm of gradient:  0.08540749355670847 ; l2 norm of weights:  0.4136017178872003\n",
            "Iteration#:  4156 ; Loss:  0.4968168821722061 ; l2 norm of gradient:  0.08502996686367134 ; l2 norm of weights:  0.41351271493609953\n",
            "Iteration#:  4160 ; Loss:  0.4967691256953305 ; l2 norm of gradient:  0.08465411736021258 ; l2 norm of weights:  0.413424371461049\n",
            "Iteration#:  4164 ; Loss:  0.4967216526648274 ; l2 norm of gradient:  0.08427993758869401 ; l2 norm of weights:  0.41333668234618026\n",
            "Iteration#:  4168 ; Loss:  0.4966744607903568 ; l2 norm of gradient:  0.08390742012465247 ; l2 norm of weights:  0.4132496425148856\n",
            "Iteration#:  4172 ; Loss:  0.4966275478015132 ; l2 norm of gradient:  0.08353655757665451 ; l2 norm of weights:  0.41316324692953943\n",
            "Iteration#:  4176 ; Loss:  0.4965809114476489 ; l2 norm of gradient:  0.08316734258615155 ; l2 norm of weights:  0.4130774905912217\n",
            "Iteration#:  4180 ; Loss:  0.4965345494976998 ; l2 norm of gradient:  0.08279976782733349 ; l2 norm of weights:  0.4129923685394418\n",
            "Iteration#:  4184 ; Loss:  0.4964884597400119 ; l2 norm of gradient:  0.0824338260069862 ; l2 norm of weights:  0.41290787585186545\n",
            "Iteration#:  4188 ; Loss:  0.49644263998217075 ; l2 norm of gradient:  0.08206950986434806 ; l2 norm of weights:  0.4128240076440415\n",
            "Iteration#:  4192 ; Loss:  0.4963970880508302 ; l2 norm of gradient:  0.08170681217096556 ; l2 norm of weights:  0.412740759069132\n",
            "Iteration#:  4196 ; Loss:  0.4963518017915454 ; l2 norm of gradient:  0.08134572573055243 ; l2 norm of weights:  0.4126581253176425\n",
            "Iteration#:  4200 ; Loss:  0.4963067790686049 ; l2 norm of gradient:  0.0809862433788477 ; l2 norm of weights:  0.4125761016171545\n",
            "Iteration#:  4204 ; Loss:  0.4962620177648652 ; l2 norm of gradient:  0.08062835798347451 ; l2 norm of weights:  0.41249468323205934\n",
            "Iteration#:  4208 ; Loss:  0.49621751578158724 ; l2 norm of gradient:  0.08027206244379954 ; l2 norm of weights:  0.41241386546329356\n",
            "Iteration#:  4212 ; Loss:  0.4961732710382736 ; l2 norm of gradient:  0.07991734969079367 ; l2 norm of weights:  0.41233364364807606\n",
            "Iteration#:  4216 ; Loss:  0.4961292814725074 ; l2 norm of gradient:  0.07956421268689111 ; l2 norm of weights:  0.41225401315964655\n",
            "Iteration#:  4220 ; Loss:  0.4960855450397926 ; l2 norm of gradient:  0.07921264442585338 ; l2 norm of weights:  0.4121749694070053\n",
            "Iteration#:  4224 ; Loss:  0.49604205971339566 ; l2 norm of gradient:  0.07886263793262878 ; l2 norm of weights:  0.4120965078346552\n",
            "Iteration#:  4228 ; Loss:  0.49599882348418883 ; l2 norm of gradient:  0.07851418626321605 ; l2 norm of weights:  0.41201862392234456\n",
            "Iteration#:  4232 ; Loss:  0.49595583436049484 ; l2 norm of gradient:  0.0781672825045273 ; l2 norm of weights:  0.411941313184812\n",
            "Iteration#:  4236 ; Loss:  0.49591309036793196 ; l2 norm of gradient:  0.07782191977425172 ; l2 norm of weights:  0.4118645711715322\n",
            "Iteration#:  4240 ; Loss:  0.4958705895492628 ; l2 norm of gradient:  0.07747809122071975 ; l2 norm of weights:  0.41178839346646406\n",
            "Iteration#:  4244 ; Loss:  0.4958283299642412 ; l2 norm of gradient:  0.07713579002276753 ; l2 norm of weights:  0.4117127756877997\n",
            "Iteration#:  4248 ; Loss:  0.49578630968946313 ; l2 norm of gradient:  0.076795009389603 ; l2 norm of weights:  0.41163771348771494\n",
            "Iteration#:  4252 ; Loss:  0.49574452681821846 ; l2 norm of gradient:  0.07645574256067131 ; l2 norm of weights:  0.4115632025521221\n",
            "Iteration#:  4256 ; Loss:  0.4957029794603423 ; l2 norm of gradient:  0.0761179828055217 ; l2 norm of weights:  0.41148923860042297\n",
            "Iteration#:  4260 ; Loss:  0.49566166574206977 ; l2 norm of gradient:  0.07578172342367438 ; l2 norm of weights:  0.4114158173852649\n",
            "Iteration#:  4264 ; Loss:  0.4956205838058912 ; l2 norm of gradient:  0.07544695774448833 ; l2 norm of weights:  0.4113429346922965\n",
            "Iteration#:  4268 ; Loss:  0.49557973181040815 ; l2 norm of gradient:  0.07511367912702971 ; l2 norm of weights:  0.411270586339927\n",
            "Iteration#:  4272 ; Loss:  0.49553910793019174 ; l2 norm of gradient:  0.0747818809599404 ; l2 norm of weights:  0.4111987681790851\n",
            "Iteration#:  4276 ; Loss:  0.4954987103556413 ; l2 norm of gradient:  0.07445155666130747 ; l2 norm of weights:  0.41112747609298034\n",
            "Iteration#:  4280 ; Loss:  0.4954585372928445 ; l2 norm of gradient:  0.07412269967853304 ; l2 norm of weights:  0.411056705996866\n",
            "Iteration#:  4284 ; Loss:  0.4954185869634394 ; l2 norm of gradient:  0.07379530348820468 ; l2 norm of weights:  0.4109864538378032\n",
            "Iteration#:  4288 ; Loss:  0.4953788576044766 ; l2 norm of gradient:  0.0734693615959675 ; l2 norm of weights:  0.4109167155944261\n",
            "Iteration#:  4292 ; Loss:  0.4953393474682838 ; l2 norm of gradient:  0.07314486753639375 ; l2 norm of weights:  0.41084748727670967\n",
            "Iteration#:  4296 ; Loss:  0.4953000548223303 ; l2 norm of gradient:  0.07282181487285666 ; l2 norm of weights:  0.41077876492573745\n",
            "Iteration#:  4300 ; Loss:  0.49526097794909396 ; l2 norm of gradient:  0.07250019719740272 ; l2 norm of weights:  0.4107105446134723\n",
            "Iteration#:  4304 ; Loss:  0.4952221151459281 ; l2 norm of gradient:  0.07218000813062414 ; l2 norm of weights:  0.4106428224425271\n",
            "Iteration#:  4308 ; Loss:  0.49518346472493086 ; l2 norm of gradient:  0.07186124132153404 ; l2 norm of weights:  0.4105755945459383\n",
            "Iteration#:  4312 ; Loss:  0.4951450250128148 ; l2 norm of gradient:  0.07154389044743954 ; l2 norm of weights:  0.41050885708693996\n",
            "Iteration#:  4316 ; Loss:  0.4951067943507776 ; l2 norm of gradient:  0.07122794921381718 ; l2 norm of weights:  0.4104426062587393\n",
            "Iteration#:  4320 ; Loss:  0.4950687710943747 ; l2 norm of gradient:  0.07091341135418779 ; l2 norm of weights:  0.4103768382842943\n",
            "Iteration#:  4324 ; Loss:  0.49503095361339244 ; l2 norm of gradient:  0.0706002706299939 ; l2 norm of weights:  0.4103115494160921\n",
            "Iteration#:  4328 ; Loss:  0.49499334029172254 ; l2 norm of gradient:  0.07028852083047452 ; l2 norm of weights:  0.4102467359359289\n",
            "Iteration#:  4332 ; Loss:  0.49495592952723727 ; l2 norm of gradient:  0.06997815577254259 ; l2 norm of weights:  0.41018239415469204\n",
            "Iteration#:  4336 ; Loss:  0.49491871973166673 ; l2 norm of gradient:  0.06966916930066336 ; l2 norm of weights:  0.41011852041214186\n",
            "Iteration#:  4340 ; Loss:  0.49488170933047604 ; l2 norm of gradient:  0.06936155528673146 ; l2 norm of weights:  0.41005511107669684\n",
            "Iteration#:  4344 ; Loss:  0.4948448967627443 ; l2 norm of gradient:  0.06905530762994995 ; l2 norm of weights:  0.40999216254521903\n",
            "Iteration#:  4348 ; Loss:  0.4948082804810447 ; l2 norm of gradient:  0.0687504202567094 ; l2 norm of weights:  0.40992967124280094\n",
            "Iteration#:  4352 ; Loss:  0.49477185895132514 ; l2 norm of gradient:  0.0684468871204675 ; l2 norm of weights:  0.40986763362255396\n",
            "Iteration#:  4356 ; Loss:  0.4947356306527904 ; l2 norm of gradient:  0.06814470220162924 ; l2 norm of weights:  0.40980604616539873\n",
            "Iteration#:  4360 ; Loss:  0.4946995940777854 ; l2 norm of gradient:  0.06784385950742725 ; l2 norm of weights:  0.4097449053798555\n",
            "Iteration#:  4364 ; Loss:  0.4946637477316788 ; l2 norm of gradient:  0.06754435307180354 ; l2 norm of weights:  0.40968420780183756\n",
            "Iteration#:  4368 ; Loss:  0.4946280901327483 ; l2 norm of gradient:  0.06724617695529068 ; l2 norm of weights:  0.4096239499944445\n",
            "Iteration#:  4372 ; Loss:  0.4945926198120672 ; l2 norm of gradient:  0.06694932524489476 ; l2 norm of weights:  0.4095641285477581\n",
            "Iteration#:  4376 ; Loss:  0.494557335313391 ; l2 norm of gradient:  0.06665379205397712 ; l2 norm of weights:  0.4095047400786386\n",
            "Iteration#:  4380 ; Loss:  0.494522235193046 ; l2 norm of gradient:  0.06635957152213814 ; l2 norm of weights:  0.40944578123052283\n",
            "Iteration#:  4384 ; Loss:  0.49448731801981755 ; l2 norm of gradient:  0.0660666578151012 ; l2 norm of weights:  0.40938724867322346\n",
            "Iteration#:  4388 ; Loss:  0.49445258237484147 ; l2 norm of gradient:  0.0657750451245963 ; l2 norm of weights:  0.4093291391027301\n",
            "Iteration#:  4392 ; Loss:  0.49441802685149405 ; l2 norm of gradient:  0.06548472766824527 ; l2 norm of weights:  0.4092714492410109\n",
            "Iteration#:  4396 ; Loss:  0.49438365005528484 ; l2 norm of gradient:  0.06519569968944675 ; l2 norm of weights:  0.409214175835816\n",
            "Iteration#:  4400 ; Loss:  0.49434945060374874 ; l2 norm of gradient:  0.06490795545726187 ; l2 norm of weights:  0.40915731566048225\n",
            "Iteration#:  4404 ; Loss:  0.4943154271263414 ; l2 norm of gradient:  0.06462148926630105 ; l2 norm of weights:  0.40910086551373925\n",
            "Iteration#:  4408 ; Loss:  0.49428157826433244 ; l2 norm of gradient:  0.06433629543660987 ; l2 norm of weights:  0.40904482221951655\n",
            "Iteration#:  4412 ; Loss:  0.4942479026707026 ; l2 norm of gradient:  0.06405236831355701 ; l2 norm of weights:  0.40898918262675227\n",
            "Iteration#:  4416 ; Loss:  0.4942143990100397 ; l2 norm of gradient:  0.06376970226772158 ; l2 norm of weights:  0.4089339436092029\n",
            "Iteration#:  4420 ; Loss:  0.4941810659584369 ; l2 norm of gradient:  0.06348829169478098 ; l2 norm of weights:  0.40887910206525485\n",
            "Iteration#:  4424 ; Loss:  0.4941479022033909 ; l2 norm of gradient:  0.06320813101540053 ; l2 norm of weights:  0.40882465491773645\n",
            "Iteration#:  4428 ; Loss:  0.49411490644370104 ; l2 norm of gradient:  0.06292921467512165 ; l2 norm of weights:  0.40877059911373176\n",
            "Iteration#:  4432 ; Loss:  0.4940820773893707 ; l2 norm of gradient:  0.06265153714425221 ; l2 norm of weights:  0.40871693162439604\n",
            "Iteration#:  4436 ; Loss:  0.4940494137615078 ; l2 norm of gradient:  0.06237509291775619 ; l2 norm of weights:  0.40866364944477146\n",
            "Iteration#:  4440 ; Loss:  0.4940169142922267 ; l2 norm of gradient:  0.062099876515144425 ; l2 norm of weights:  0.4086107495936048\n",
            "Iteration#:  4444 ; Loss:  0.49398457772455195 ; l2 norm of gradient:  0.061825882480365635 ; l2 norm of weights:  0.4085582291131666\n",
            "Iteration#:  4448 ; Loss:  0.4939524028123216 ; l2 norm of gradient:  0.06155310538169812 ; l2 norm of weights:  0.4085060850690707\n",
            "Iteration#:  4452 ; Loss:  0.493920388320092 ; l2 norm of gradient:  0.06128153981164147 ; l2 norm of weights:  0.40845431455009584\n",
            "Iteration#:  4456 ; Loss:  0.4938885330230435 ; l2 norm of gradient:  0.06101118038680959 ; l2 norm of weights:  0.40840291466800793\n",
            "Iteration#:  4460 ; Loss:  0.4938568357068864 ; l2 norm of gradient:  0.06074202174782305 ; l2 norm of weights:  0.408351882557384\n",
            "Iteration#:  4464 ; Loss:  0.49382529516776874 ; l2 norm of gradient:  0.06047405855920283 ; l2 norm of weights:  0.40830121537543723\n",
            "Iteration#:  4468 ; Loss:  0.4937939102121841 ; l2 norm of gradient:  0.06020728550926396 ; l2 norm of weights:  0.40825091030184285\n",
            "Iteration#:  4472 ; Loss:  0.4937626796568802 ; l2 norm of gradient:  0.05994169731001064 ; l2 norm of weights:  0.4082009645385657\n",
            "Iteration#:  4476 ; Loss:  0.49373160232876884 ; l2 norm of gradient:  0.059677288697029755 ; l2 norm of weights:  0.40815137530968876\n",
            "Iteration#:  4480 ; Loss:  0.49370067706483645 ; l2 norm of gradient:  0.05941405442938713 ; l2 norm of weights:  0.40810213986124305\n",
            "Iteration#:  4484 ; Loss:  0.4936699027120544 ; l2 norm of gradient:  0.0591519892895231 ; l2 norm of weights:  0.40805325546103866\n",
            "Iteration#:  4488 ; Loss:  0.49363927812729286 ; l2 norm of gradient:  0.05889108808314802 ; l2 norm of weights:  0.40800471939849653\n",
            "Iteration#:  4492 ; Loss:  0.49360880217723163 ; l2 norm of gradient:  0.05863134563914021 ; l2 norm of weights:  0.4079565289844823\n",
            "Iteration#:  4496 ; Loss:  0.49357847373827524 ; l2 norm of gradient:  0.05837275680944159 ; l2 norm of weights:  0.4079086815511406\n",
            "Iteration#:  4500 ; Loss:  0.49354829169646636 ; l2 norm of gradient:  0.0581153164689563 ; l2 norm of weights:  0.40786117445173065\n",
            "Iteration#:  4504 ; Loss:  0.4935182549474011 ; l2 norm of gradient:  0.05785901951544789 ; l2 norm of weights:  0.40781400506046317\n",
            "Iteration#:  4508 ; Loss:  0.4934883623961455 ; l2 norm of gradient:  0.05760386086943819 ; l2 norm of weights:  0.40776717077233854\n",
            "Iteration#:  4512 ; Loss:  0.493458612957151 ; l2 norm of gradient:  0.05734983547410642 ; l2 norm of weights:  0.4077206690029857\n",
            "Iteration#:  4516 ; Loss:  0.49342900555417296 ; l2 norm of gradient:  0.05709693829518758 ; l2 norm of weights:  0.4076744971885028\n",
            "Iteration#:  4520 ; Loss:  0.4933995391201879 ; l2 norm of gradient:  0.05684516432087338 ; l2 norm of weights:  0.407628652785298\n",
            "Iteration#:  4524 ; Loss:  0.4933702125973128 ; l2 norm of gradient:  0.05659450856171109 ; l2 norm of weights:  0.4075831332699326\n",
            "Iteration#:  4528 ; Loss:  0.4933410249367241 ; l2 norm of gradient:  0.05634496605050561 ; l2 norm of weights:  0.4075379361389647\n",
            "Iteration#:  4532 ; Loss:  0.4933119750985787 ; l2 norm of gradient:  0.05609653184221937 ; l2 norm of weights:  0.4074930589087935\n",
            "Iteration#:  4536 ; Loss:  0.4932830620519339 ; l2 norm of gradient:  0.055849201013874805 ; l2 norm of weights:  0.40744849911550574\n",
            "Iteration#:  4540 ; Loss:  0.49325428477467 ; l2 norm of gradient:  0.0556029686644549 ; l2 norm of weights:  0.4074042543147224\n",
            "Iteration#:  4544 ; Loss:  0.49322564225341214 ; l2 norm of gradient:  0.05535782991480713 ; l2 norm of weights:  0.4073603220814468\n",
            "Iteration#:  4548 ; Loss:  0.4931971334834534 ; l2 norm of gradient:  0.05511377990754505 ; l2 norm of weights:  0.40731670000991405\n",
            "Iteration#:  4552 ; Loss:  0.4931687574686783 ; l2 norm of gradient:  0.0548708138069523 ; l2 norm of weights:  0.4072733857134412\n",
            "Iteration#:  4556 ; Loss:  0.4931405132214879 ; l2 norm of gradient:  0.05462892679888536 ; l2 norm of weights:  0.4072303768242784\n",
            "Iteration#:  4560 ; Loss:  0.4931123997627239 ; l2 norm of gradient:  0.05438811409067881 ; l2 norm of weights:  0.40718767099346137\n",
            "Iteration#:  4564 ; Loss:  0.4930844161215947 ; l2 norm of gradient:  0.05414837091104835 ; l2 norm of weights:  0.40714526589066546\n",
            "Iteration#:  4568 ; Loss:  0.4930565613356024 ; l2 norm of gradient:  0.053909692509997116 ; l2 norm of weights:  0.4071031592040595\n",
            "Iteration#:  4572 ; Loss:  0.4930288344504689 ; l2 norm of gradient:  0.0536720741587199 ; l2 norm of weights:  0.4070613486401615\n",
            "Iteration#:  4576 ; Loss:  0.49300123452006406 ; l2 norm of gradient:  0.0534355111495093 ; l2 norm of weights:  0.40701983192369606\n",
            "Iteration#:  4580 ; Loss:  0.4929737606063337 ; l2 norm of gradient:  0.05319999879566227 ; l2 norm of weights:  0.40697860679745107\n",
            "Iteration#:  4584 ; Loss:  0.49294641177922915 ; l2 norm of gradient:  0.05296553243138551 ; l2 norm of weights:  0.4069376710221368\n",
            "Iteration#:  4588 ; Loss:  0.4929191871166362 ; l2 norm of gradient:  0.052732107411704136 ; l2 norm of weights:  0.40689702237624625\n",
            "Iteration#:  4592 ; Loss:  0.49289208570430565 ; l2 norm of gradient:  0.05249971911236729 ; l2 norm of weights:  0.40685665865591497\n",
            "Iteration#:  4596 ; Loss:  0.4928651066357837 ; l2 norm of gradient:  0.05226836292975685 ; l2 norm of weights:  0.4068165776747838\n",
            "Iteration#:  4600 ; Loss:  0.49283824901234413 ; l2 norm of gradient:  0.05203803428079585 ; l2 norm of weights:  0.40677677726386074\n",
            "Iteration#:  4604 ; Loss:  0.4928115119429195 ; l2 norm of gradient:  0.05180872860285611 ; l2 norm of weights:  0.4067372552713859\n",
            "Iteration#:  4608 ; Loss:  0.4927848945440346 ; l2 norm of gradient:  0.05158044135366808 ; l2 norm of weights:  0.40669800956269503\n",
            "Iteration#:  4612 ; Loss:  0.49275839593973864 ; l2 norm of gradient:  0.05135316801123023 ; l2 norm of weights:  0.4066590380200867\n",
            "Iteration#:  4616 ; Loss:  0.49273201526154053 ; l2 norm of gradient:  0.05112690407371764 ; l2 norm of weights:  0.40662033854268786\n",
            "Iteration#:  4620 ; Loss:  0.4927057516483417 ; l2 norm of gradient:  0.050901645059394346 ; l2 norm of weights:  0.4065819090463225\n",
            "Iteration#:  4624 ; Loss:  0.49267960424637236 ; l2 norm of gradient:  0.05067738650652166 ; l2 norm of weights:  0.4065437474633802\n",
            "Iteration#:  4628 ; Loss:  0.4926535722091262 ; l2 norm of gradient:  0.05045412397327014 ; l2 norm of weights:  0.40650585174268544\n",
            "Iteration#:  4632 ; Loss:  0.4926276546972973 ; l2 norm of gradient:  0.05023185303763159 ; l2 norm of weights:  0.4064682198493687\n",
            "Iteration#:  4636 ; Loss:  0.49260185087871633 ; l2 norm of gradient:  0.0500105692973295 ; l2 norm of weights:  0.40643084976473853\n",
            "Iteration#:  4640 ; Loss:  0.4925761599282873 ; l2 norm of gradient:  0.049790268369731876 ; l2 norm of weights:  0.40639373948615287\n",
            "Iteration#:  4644 ; Loss:  0.4925505810279269 ; l2 norm of gradient:  0.04957094589176463 ; l2 norm of weights:  0.40635688702689404\n",
            "Iteration#:  4648 ; Loss:  0.49252511336650145 ; l2 norm of gradient:  0.04935259751982216 ; l2 norm of weights:  0.40632029041604206\n",
            "Iteration#:  4652 ; Loss:  0.4924997561397665 ; l2 norm of gradient:  0.049135218929683054 ; l2 norm of weights:  0.4062839476983511\n",
            "Iteration#:  4656 ; Loss:  0.4924745085503059 ; l2 norm of gradient:  0.04891880581642272 ; l2 norm of weights:  0.40624785693412485\n",
            "Iteration#:  4660 ; Loss:  0.49244936980747245 ; l2 norm of gradient:  0.048703353894326847 ; l2 norm of weights:  0.40621201619909447\n",
            "Iteration#:  4664 ; Loss:  0.4924243391273275 ; l2 norm of gradient:  0.04848885889680731 ; l2 norm of weights:  0.40617642358429656\n",
            "Iteration#:  4668 ; Loss:  0.4923994157325829 ; l2 norm of gradient:  0.04827531657631558 ; l2 norm of weights:  0.4061410771959522\n",
            "Iteration#:  4672 ; Loss:  0.4923745988525418 ; l2 norm of gradient:  0.048062722704258605 ; l2 norm of weights:  0.40610597515534713\n",
            "Iteration#:  4676 ; Loss:  0.49234988772304145 ; l2 norm of gradient:  0.047851073070914685 ; l2 norm of weights:  0.4060711155987122\n",
            "Iteration#:  4680 ; Loss:  0.4923252815863952 ; l2 norm of gradient:  0.047640363485348315 ; l2 norm of weights:  0.40603649667710573\n",
            "Iteration#:  4684 ; Loss:  0.49230077969133595 ; l2 norm of gradient:  0.04743058977532769 ; l2 norm of weights:  0.4060021165562961\n",
            "Iteration#:  4688 ; Loss:  0.4922763812929598 ; l2 norm of gradient:  0.047221747787240594 ; l2 norm of weights:  0.4059679734166446\n",
            "Iteration#:  4692 ; Loss:  0.49225208565266987 ; l2 norm of gradient:  0.04701383338601171 ; l2 norm of weights:  0.4059340654529909\n",
            "Iteration#:  4696 ; Loss:  0.49222789203812134 ; l2 norm of gradient:  0.0468068424550202 ; l2 norm of weights:  0.4059003908745378\n",
            "Iteration#:  4700 ; Loss:  0.49220379972316586 ; l2 norm of gradient:  0.04660077089601693 ; l2 norm of weights:  0.4058669479047372\n",
            "Iteration#:  4704 ; Loss:  0.49217980798779803 ; l2 norm of gradient:  0.04639561462904352 ; l2 norm of weights:  0.4058337347811777\n",
            "Iteration#:  4708 ; Loss:  0.4921559161181009 ; l2 norm of gradient:  0.04619136959234964 ; l2 norm of weights:  0.4058007497554718\n",
            "Iteration#:  4712 ; Loss:  0.4921321234061923 ; l2 norm of gradient:  0.04598803174231289 ; l2 norm of weights:  0.4057679910931454\n",
            "Iteration#:  4716 ; Loss:  0.49210842915017267 ; l2 norm of gradient:  0.045785597053357566 ; l2 norm of weights:  0.4057354570735266\n",
            "Iteration#:  4720 ; Loss:  0.4920848326540716 ; l2 norm of gradient:  0.0455840615178745 ; l2 norm of weights:  0.4057031459896367\n",
            "Iteration#:  4724 ; Loss:  0.49206133322779677 ; l2 norm of gradient:  0.04538342114614025 ; l2 norm of weights:  0.4056710561480812\n",
            "Iteration#:  4728 ; Loss:  0.49203793018708125 ; l2 norm of gradient:  0.045183671966238514 ; l2 norm of weights:  0.4056391858689416\n",
            "Iteration#:  4732 ; Loss:  0.49201462285343345 ; l2 norm of gradient:  0.04498481002397978 ; l2 norm of weights:  0.4056075334856688\n",
            "Iteration#:  4736 ; Loss:  0.4919914105540856 ; l2 norm of gradient:  0.04478683138282294 ; l2 norm of weights:  0.40557609734497657\n",
            "Iteration#:  4740 ; Loss:  0.49196829262194397 ; l2 norm of gradient:  0.044589732123796286 ; l2 norm of weights:  0.40554487580673576\n",
            "Iteration#:  4744 ; Loss:  0.4919452683955391 ; l2 norm of gradient:  0.044393508345419185 ; l2 norm of weights:  0.4055138672438699\n",
            "Iteration#:  4748 ; Loss:  0.49192233721897616 ; l2 norm of gradient:  0.04419815616362459 ; l2 norm of weights:  0.4054830700422516\n",
            "Iteration#:  4752 ; Loss:  0.49189949844188613 ; l2 norm of gradient:  0.04400367171168036 ; l2 norm of weights:  0.4054524826005987\n",
            "Iteration#:  4756 ; Loss:  0.49187675141937787 ; l2 norm of gradient:  0.04381005114011337 ; l2 norm of weights:  0.4054221033303726\n",
            "Iteration#:  4760 ; Loss:  0.49185409551198905 ; l2 norm of gradient:  0.043617290616631575 ; l2 norm of weights:  0.40539193065567625\n",
            "Iteration#:  4764 ; Loss:  0.49183153008563957 ; l2 norm of gradient:  0.043425386326047304 ; l2 norm of weights:  0.40536196301315414\n",
            "Iteration#:  4768 ; Loss:  0.4918090545115834 ; l2 norm of gradient:  0.04323433447020199 ; l2 norm of weights:  0.40533219885189103\n",
            "Iteration#:  4772 ; Loss:  0.49178666816636274 ; l2 norm of gradient:  0.04304413126788927 ; l2 norm of weights:  0.40530263663331423\n",
            "Iteration#:  4776 ; Loss:  0.4917643704317609 ; l2 norm of gradient:  0.04285477295477999 ; l2 norm of weights:  0.405273274831094\n",
            "Iteration#:  4780 ; Loss:  0.49174216069475657 ; l2 norm of gradient:  0.04266625578334633 ; l2 norm of weights:  0.40524411193104637\n",
            "Iteration#:  4784 ; Loss:  0.4917200383474788 ; l2 norm of gradient:  0.04247857602278761 ; l2 norm of weights:  0.4052151464310361\n",
            "Iteration#:  4788 ; Loss:  0.4916980027871608 ; l2 norm of gradient:  0.04229172995895554 ; l2 norm of weights:  0.4051863768408803\n",
            "Iteration#:  4792 ; Loss:  0.4916760534160961 ; l2 norm of gradient:  0.04210571389427919 ; l2 norm of weights:  0.4051578016822534\n",
            "Iteration#:  4796 ; Loss:  0.49165418964159385 ; l2 norm of gradient:  0.04192052414769216 ; l2 norm of weights:  0.40512941948859177\n",
            "Iteration#:  4800 ; Loss:  0.4916324108759349 ; l2 norm of gradient:  0.04173615705455818 ; l2 norm of weights:  0.4051012288050004\n",
            "Iteration#:  4804 ; Loss:  0.4916107165363281 ; l2 norm of gradient:  0.04155260896659821 ; l2 norm of weights:  0.4050732281881589\n",
            "Iteration#:  4808 ; Loss:  0.49158910604486694 ; l2 norm of gradient:  0.04136987625181764 ; l2 norm of weights:  0.4050454162062296\n",
            "Iteration#:  4812 ; Loss:  0.49156757882848795 ; l2 norm of gradient:  0.041187955294433067 ; l2 norm of weights:  0.40501779143876554\n",
            "Iteration#:  4816 ; Loss:  0.49154613431892624 ; l2 norm of gradient:  0.04100684249480053 ; l2 norm of weights:  0.40499035247661896\n",
            "Iteration#:  4820 ; Loss:  0.4915247719526755 ; l2 norm of gradient:  0.04082653426934347 ; l2 norm of weights:  0.40496309792185115\n",
            "Iteration#:  4824 ; Loss:  0.49150349117094483 ; l2 norm of gradient:  0.040647027050480594 ; l2 norm of weights:  0.40493602638764303\n",
            "Iteration#:  4828 ; Loss:  0.49148229141961797 ; l2 norm of gradient:  0.0404683172865551 ; l2 norm of weights:  0.40490913649820537\n",
            "Iteration#:  4832 ; Loss:  0.4914611721492129 ; l2 norm of gradient:  0.04029040144176354 ; l2 norm of weights:  0.4048824268886913\n",
            "Iteration#:  4836 ; Loss:  0.49144013281484 ; l2 norm of gradient:  0.040113275996084816 ; l2 norm of weights:  0.4048558962051077\n",
            "Iteration#:  4840 ; Loss:  0.4914191728761632 ; l2 norm of gradient:  0.03993693744520979 ; l2 norm of weights:  0.4048295431042294\n",
            "Iteration#:  4844 ; Loss:  0.4913982917973591 ; l2 norm of gradient:  0.039761382300471534 ; l2 norm of weights:  0.40480336625351165\n",
            "Iteration#:  4848 ; Loss:  0.4913774890470779 ; l2 norm of gradient:  0.03958660708877531 ; l2 norm of weights:  0.4047773643310053\n",
            "Iteration#:  4852 ; Loss:  0.4913567640984044 ; l2 norm of gradient:  0.039412608352529306 ; l2 norm of weights:  0.4047515360252717\n",
            "Iteration#:  4856 ; Loss:  0.4913361164288185 ; l2 norm of gradient:  0.03923938264957504 ; l2 norm of weights:  0.4047258800352981\n",
            "Iteration#:  4860 ; Loss:  0.49131554552015766 ; l2 norm of gradient:  0.03906692655311897 ; l2 norm of weights:  0.4047003950704143\n",
            "Iteration#:  4864 ; Loss:  0.49129505085857805 ; l2 norm of gradient:  0.038895236651663785 ; l2 norm of weights:  0.4046750798502094\n",
            "Iteration#:  4868 ; Loss:  0.49127463193451665 ; l2 norm of gradient:  0.0387243095489397 ; l2 norm of weights:  0.40464993310444997\n",
            "Iteration#:  4872 ; Loss:  0.49125428824265494 ; l2 norm of gradient:  0.03855414186383758 ; l2 norm of weights:  0.4046249535729975\n",
            "Iteration#:  4876 ; Loss:  0.49123401928188004 ; l2 norm of gradient:  0.03838473023034049 ; l2 norm of weights:  0.40460014000572825\n",
            "Iteration#:  4880 ; Loss:  0.4912138245552494 ; l2 norm of gradient:  0.03821607129745609 ; l2 norm of weights:  0.40457549116245184\n",
            "Iteration#:  4884 ; Loss:  0.4911937035699535 ; l2 norm of gradient:  0.0380481617291506 ; l2 norm of weights:  0.40455100581283276\n",
            "Iteration#:  4888 ; Loss:  0.49117365583728007 ; l2 norm of gradient:  0.037880998204281285 ; l2 norm of weights:  0.40452668273630965\n",
            "Iteration#:  4892 ; Loss:  0.49115368087257794 ; l2 norm of gradient:  0.03771457741652983 ; l2 norm of weights:  0.4045025207220181\n",
            "Iteration#:  4896 ; Loss:  0.4911337781952219 ; l2 norm of gradient:  0.0375488960743373 ; l2 norm of weights:  0.40447851856871164\n",
            "Iteration#:  4900 ; Loss:  0.4911139473285771 ; l2 norm of gradient:  0.03738395090083699 ; l2 norm of weights:  0.404454675084685\n",
            "Iteration#:  4904 ; Loss:  0.49109418779996444 ; l2 norm of gradient:  0.03721973863378963 ; l2 norm of weights:  0.4044309890876976\n",
            "Iteration#:  4908 ; Loss:  0.491074499140626 ; l2 norm of gradient:  0.03705625602551798 ; l2 norm of weights:  0.4044074594048967\n",
            "Iteration#:  4912 ; Loss:  0.49105488088569027 ; l2 norm of gradient:  0.036893499842841564 ; l2 norm of weights:  0.4043840848727424\n",
            "Iteration#:  4916 ; Loss:  0.491035332574139 ; l2 norm of gradient:  0.03673146686701238 ; l2 norm of weights:  0.4043608643369327\n",
            "Iteration#:  4920 ; Loss:  0.4910158537487728 ; l2 norm of gradient:  0.03657015389365021 ; l2 norm of weights:  0.4043377966523287\n",
            "Iteration#:  4924 ; Loss:  0.4909964439561784 ; l2 norm of gradient:  0.03640955773267844 ; l2 norm of weights:  0.4043148806828817\n",
            "Iteration#:  4928 ; Loss:  0.49097710274669465 ; l2 norm of gradient:  0.036249675208260744 ; l2 norm of weights:  0.40429211530155884\n",
            "Iteration#:  4932 ; Loss:  0.4909578296743808 ; l2 norm of gradient:  0.03609050315873692 ; l2 norm of weights:  0.4042694993902716\n",
            "Iteration#:  4936 ; Loss:  0.4909386242969832 ; l2 norm of gradient:  0.03593203843655981 ; l2 norm of weights:  0.4042470318398035\n",
            "Iteration#:  4940 ; Loss:  0.4909194861759035 ; l2 norm of gradient:  0.03577427790823247 ; l2 norm of weights:  0.4042247115497377\n",
            "Iteration#:  4944 ; Loss:  0.4909004148761664 ; l2 norm of gradient:  0.035617218454245446 ; l2 norm of weights:  0.40420253742838774\n",
            "Iteration#:  4948 ; Loss:  0.49088140996638874 ; l2 norm of gradient:  0.035460856969013604 ; l2 norm of weights:  0.4041805083927258\n",
            "Iteration#:  4952 ; Loss:  0.4908624710187469 ; l2 norm of gradient:  0.03530519036081578 ; l2 norm of weights:  0.4041586233683136\n",
            "Iteration#:  4956 ; Loss:  0.4908435976089469 ; l2 norm of gradient:  0.03515021555173098 ; l2 norm of weights:  0.4041368812892331\n",
            "Iteration#:  4960 ; Loss:  0.49082478931619283 ; l2 norm of gradient:  0.03499592947757768 ; l2 norm of weights:  0.40411528109801764\n",
            "Iteration#:  4964 ; Loss:  0.49080604572315667 ; l2 norm of gradient:  0.034842329087852245 ; l2 norm of weights:  0.40409382174558395\n",
            "Iteration#:  4968 ; Loss:  0.490787366415948 ; l2 norm of gradient:  0.03468941134566828 ; l2 norm of weights:  0.4040725021911644\n",
            "Iteration#:  4972 ; Loss:  0.4907687509840838 ; l2 norm of gradient:  0.03453717322769549 ; l2 norm of weights:  0.40405132140224026\n",
            "Iteration#:  4976 ; Loss:  0.49075019902045874 ; l2 norm of gradient:  0.03438561172409924 ; l2 norm of weights:  0.40403027835447475\n",
            "Iteration#:  4980 ; Loss:  0.4907317101213161 ; l2 norm of gradient:  0.03423472383848039 ; l2 norm of weights:  0.40400937203164733\n",
            "Iteration#:  4984 ; Loss:  0.49071328388621793 ; l2 norm of gradient:  0.03408450658781513 ; l2 norm of weights:  0.40398860142558796\n",
            "Iteration#:  4988 ; Loss:  0.4906949199180165 ; l2 norm of gradient:  0.03393495700239551 ; l2 norm of weights:  0.4039679655361126\n",
            "Iteration#:  4992 ; Loss:  0.4906766178228255 ; l2 norm of gradient:  0.033786072125769645 ; l2 norm of weights:  0.4039474633709581\n",
            "Iteration#:  4996 ; Loss:  0.4906583772099917 ; l2 norm of gradient:  0.03363784901468337 ; l2 norm of weights:  0.40392709394571896\n",
            "Iteration#:  5000 ; Loss:  0.49064019769206657 ; l2 norm of gradient:  0.0334902847390201 ; l2 norm of weights:  0.40390685628378337\n",
            "Iteration#:  5004 ; Loss:  0.4906220788847785 ; l2 norm of gradient:  0.03334337638174328 ; l2 norm of weights:  0.4038867494162705\n",
            "Iteration#:  5008 ; Loss:  0.49060402040700446 ; l2 norm of gradient:  0.03319712103883729 ; l2 norm of weights:  0.40386677238196816\n",
            "Iteration#:  5012 ; Loss:  0.4905860218807441 ; l2 norm of gradient:  0.03305151581924962 ; l2 norm of weights:  0.40384692422727075\n",
            "Iteration#:  5016 ; Loss:  0.4905680829310905 ; l2 norm of gradient:  0.032906557844833276 ; l2 norm of weights:  0.4038272040061177\n",
            "Iteration#:  5020 ; Loss:  0.49055020318620496 ; l2 norm of gradient:  0.03276224425028832 ; l2 norm of weights:  0.4038076107799331\n",
            "Iteration#:  5024 ; Loss:  0.4905323822772892 ; l2 norm of gradient:  0.03261857218310539 ; l2 norm of weights:  0.4037881436175642\n",
            "Iteration#:  5028 ; Loss:  0.49051461983855915 ; l2 norm of gradient:  0.032475538803508126 ; l2 norm of weights:  0.4037688015952227\n",
            "Iteration#:  5032 ; Loss:  0.4904969155072189 ; l2 norm of gradient:  0.03233314128439654 ; l2 norm of weights:  0.4037495837964244\n",
            "Iteration#:  5036 ; Loss:  0.49047926892343446 ; l2 norm of gradient:  0.03219137681129002 ; l2 norm of weights:  0.40373048931193\n",
            "Iteration#:  5040 ; Loss:  0.49046167973030785 ; l2 norm of gradient:  0.03205024258227189 ; l2 norm of weights:  0.40371151723968757\n",
            "Iteration#:  5044 ; Loss:  0.49044414757385185 ; l2 norm of gradient:  0.03190973580793239 ; l2 norm of weights:  0.4036926666847735\n",
            "Iteration#:  5048 ; Loss:  0.4904266721029643 ; l2 norm of gradient:  0.03176985371131279 ; l2 norm of weights:  0.4036739367593353\n",
            "Iteration#:  5052 ; Loss:  0.4904092529694032 ; l2 norm of gradient:  0.031630593527850956 ; l2 norm of weights:  0.4036553265825343\n",
            "Iteration#:  5056 ; Loss:  0.49039188982776183 ; l2 norm of gradient:  0.03149195250532459 ; l2 norm of weights:  0.4036368352804891\n",
            "Iteration#:  5060 ; Loss:  0.4903745823354438 ; l2 norm of gradient:  0.03135392790379711 ; l2 norm of weights:  0.40361846198621915\n",
            "Iteration#:  5064 ; Loss:  0.49035733015263894 ; l2 norm of gradient:  0.031216516995562785 ; l2 norm of weights:  0.4036002058395893\n",
            "Iteration#:  5068 ; Loss:  0.4903401329422989 ; l2 norm of gradient:  0.031079717065090818 ; l2 norm of weights:  0.40358206598725366\n",
            "Iteration#:  5072 ; Loss:  0.49032299037011295 ; l2 norm of gradient:  0.03094352540897323 ; l2 norm of weights:  0.4035640415826018\n",
            "Iteration#:  5076 ; Loss:  0.4903059021044843 ; l2 norm of gradient:  0.030807939335868232 ; l2 norm of weights:  0.40354613178570337\n",
            "Iteration#:  5080 ; Loss:  0.49028886781650666 ; l2 norm of gradient:  0.030672956166448738 ; l2 norm of weights:  0.4035283357632544\n",
            "Iteration#:  5084 ; Loss:  0.4902718871799404 ; l2 norm of gradient:  0.030538573233346925 ; l2 norm of weights:  0.4035106526885238\n",
            "Iteration#:  5088 ; Loss:  0.49025495987119 ; l2 norm of gradient:  0.030404787881101503 ; l2 norm of weights:  0.40349308174130005\n",
            "Iteration#:  5092 ; Loss:  0.4902380855692803 ; l2 norm of gradient:  0.03027159746610489 ; l2 norm of weights:  0.40347562210783855\n",
            "Iteration#:  5096 ; Loss:  0.4902212639558343 ; l2 norm of gradient:  0.030138999356549883 ; l2 norm of weights:  0.40345827298080916\n",
            "Iteration#:  5100 ; Loss:  0.4902044947150501 ; l2 norm of gradient:  0.0300069909323764 ; l2 norm of weights:  0.40344103355924443\n",
            "Iteration#:  5104 ; Loss:  0.49018777753367926 ; l2 norm of gradient:  0.02987556958522059 ; l2 norm of weights:  0.403423903048488\n",
            "Iteration#:  5108 ; Loss:  0.4901711121010036 ; l2 norm of gradient:  0.029744732718360978 ; l2 norm of weights:  0.403406880660144\n",
            "Iteration#:  5112 ; Loss:  0.49015449810881395 ; l2 norm of gradient:  0.029614477746667735 ; l2 norm of weights:  0.40338996561202534\n",
            "Iteration#:  5116 ; Loss:  0.49013793525138816 ; l2 norm of gradient:  0.02948480209654987 ; l2 norm of weights:  0.4033731571281046\n",
            "Iteration#:  5120 ; Loss:  0.4901214232254695 ; l2 norm of gradient:  0.02935570320590426 ; l2 norm of weights:  0.4033564544384637\n",
            "Iteration#:  5124 ; Loss:  0.49010496173024476 ; l2 norm of gradient:  0.029227178524064593 ; l2 norm of weights:  0.4033398567792443\n",
            "Iteration#:  5128 ; Loss:  0.490088550467324 ; l2 norm of gradient:  0.02909922551174979 ; l2 norm of weights:  0.4033233633925988\n",
            "Iteration#:  5132 ; Loss:  0.4900721891407187 ; l2 norm of gradient:  0.028971841641013324 ; l2 norm of weights:  0.4033069735266421\n",
            "Iteration#:  5136 ; Loss:  0.4900558774568212 ; l2 norm of gradient:  0.028845024395192938 ; l2 norm of weights:  0.4032906864354028\n",
            "Iteration#:  5140 ; Loss:  0.49003961512438393 ; l2 norm of gradient:  0.028718771268859696 ; l2 norm of weights:  0.40327450137877574\n",
            "Iteration#:  5144 ; Loss:  0.4900234018544991 ; l2 norm of gradient:  0.028593079767768065 ; l2 norm of weights:  0.4032584176224742\n",
            "Iteration#:  5148 ; Loss:  0.49000723736057816 ; l2 norm of gradient:  0.028467947408806205 ; l2 norm of weights:  0.403242434437983\n",
            "Iteration#:  5152 ; Loss:  0.48999112135833217 ; l2 norm of gradient:  0.02834337171994608 ; l2 norm of weights:  0.40322655110251177\n",
            "Iteration#:  5156 ; Loss:  0.4899750535657506 ; l2 norm of gradient:  0.02821935024019355 ; l2 norm of weights:  0.4032107668989484\n",
            "Iteration#:  5160 ; Loss:  0.4899590337030832 ; l2 norm of gradient:  0.028095880519540144 ; l2 norm of weights:  0.4031950811158133\n",
            "Iteration#:  5164 ; Loss:  0.4899430614928194 ; l2 norm of gradient:  0.02797296011891288 ; l2 norm of weights:  0.40317949304721373\n",
            "Iteration#:  5168 ; Loss:  0.48992713665966914 ; l2 norm of gradient:  0.027850586610125915 ; l2 norm of weights:  0.4031640019927985\n",
            "Iteration#:  5172 ; Loss:  0.4899112589305432 ; l2 norm of gradient:  0.02772875757583221 ; l2 norm of weights:  0.40314860725771307\n",
            "Iteration#:  5176 ; Loss:  0.4898954280345349 ; l2 norm of gradient:  0.027607470609474504 ; l2 norm of weights:  0.40313330815255544\n",
            "Iteration#:  5180 ; Loss:  0.4898796437029009 ; l2 norm of gradient:  0.027486723315237608 ; l2 norm of weights:  0.4031181039933312\n",
            "Iteration#:  5184 ; Loss:  0.48986390566904164 ; l2 norm of gradient:  0.027366513308000384 ; l2 norm of weights:  0.4031029941014107\n",
            "Iteration#:  5188 ; Loss:  0.4898482136684841 ; l2 norm of gradient:  0.02724683821328783 ; l2 norm of weights:  0.4030879778034848\n",
            "Iteration#:  5192 ; Loss:  0.4898325674388622 ; l2 norm of gradient:  0.027127695667223478 ; l2 norm of weights:  0.4030730544315223\n",
            "Iteration#:  5196 ; Loss:  0.4898169667198996 ; l2 norm of gradient:  0.02700908331648248 ; l2 norm of weights:  0.40305822332272695\n",
            "Iteration#:  5200 ; Loss:  0.4898014112533905 ; l2 norm of gradient:  0.026890998818243802 ; l2 norm of weights:  0.4030434838194951\n",
            "Iteration#:  5204 ; Loss:  0.4897859007831824 ; l2 norm of gradient:  0.026773439840143777 ; l2 norm of weights:  0.40302883526937383\n",
            "Iteration#:  5208 ; Loss:  0.4897704350551582 ; l2 norm of gradient:  0.026656404060229746 ; l2 norm of weights:  0.4030142770250191\n",
            "Iteration#:  5212 ; Loss:  0.48975501381721886 ; l2 norm of gradient:  0.026539889166912696 ; l2 norm of weights:  0.4029998084441541\n",
            "Iteration#:  5216 ; Loss:  0.4897396368192651 ; l2 norm of gradient:  0.026423892858921716 ; l2 norm of weights:  0.40298542888952893\n",
            "Iteration#:  5220 ; Loss:  0.48972430381318066 ; l2 norm of gradient:  0.02630841284525756 ; l2 norm of weights:  0.4029711377288791\n",
            "Iteration#:  5224 ; Loss:  0.48970901455281546 ; l2 norm of gradient:  0.02619344684514666 ; l2 norm of weights:  0.4029569343348857\n",
            "Iteration#:  5228 ; Loss:  0.4896937687939678 ; l2 norm of gradient:  0.02607899258799612 ; l2 norm of weights:  0.4029428180851351\n",
            "Iteration#:  5232 ; Loss:  0.48967856629436807 ; l2 norm of gradient:  0.025965047813347047 ; l2 norm of weights:  0.4029287883620795\n",
            "Iteration#:  5236 ; Loss:  0.489663406813662 ; l2 norm of gradient:  0.02585161027083079 ; l2 norm of weights:  0.4029148445529971\n",
            "Iteration#:  5240 ; Loss:  0.4896482901133934 ; l2 norm of gradient:  0.02573867772012258 ; l2 norm of weights:  0.4029009860499534\n",
            "Iteration#:  5244 ; Loss:  0.4896332159569891 ; l2 norm of gradient:  0.025626247930897437 ; l2 norm of weights:  0.40288721224976226\n",
            "Iteration#:  5248 ; Loss:  0.4896181841097414 ; l2 norm of gradient:  0.02551431868278551 ; l2 norm of weights:  0.4028735225539475\n",
            "Iteration#:  5252 ; Loss:  0.4896031943387923 ; l2 norm of gradient:  0.025402887765326735 ; l2 norm of weights:  0.40285991636870483\n",
            "Iteration#:  5256 ; Loss:  0.4895882464131181 ; l2 norm of gradient:  0.025291952977927915 ; l2 norm of weights:  0.40284639310486386\n",
            "Iteration#:  5260 ; Loss:  0.48957334010351267 ; l2 norm of gradient:  0.025181512129817372 ; l2 norm of weights:  0.40283295217785053\n",
            "Iteration#:  5264 ; Loss:  0.4895584751825724 ; l2 norm of gradient:  0.0250715630400024 ; l2 norm of weights:  0.4028195930076506\n",
            "Iteration#:  5268 ; Loss:  0.48954365142468 ; l2 norm of gradient:  0.024962103537223983 ; l2 norm of weights:  0.40280631501877145\n",
            "Iteration#:  5272 ; Loss:  0.48952886860598993 ; l2 norm of gradient:  0.024853131459914767 ; l2 norm of weights:  0.4027931176402068\n",
            "Iteration#:  5276 ; Loss:  0.48951412650441256 ; l2 norm of gradient:  0.02474464465615535 ; l2 norm of weights:  0.40278000030539934\n",
            "Iteration#:  5280 ; Loss:  0.4894994248995987 ; l2 norm of gradient:  0.024636640983630692 ; l2 norm of weights:  0.40276696245220556\n",
            "Iteration#:  5284 ; Loss:  0.48948476357292514 ; l2 norm of gradient:  0.024529118309587445 ; l2 norm of weights:  0.40275400352285945\n",
            "Iteration#:  5288 ; Loss:  0.48947014230747976 ; l2 norm of gradient:  0.024422074510792136 ; l2 norm of weights:  0.40274112296393705\n",
            "Iteration#:  5292 ; Loss:  0.48945556088804637 ; l2 norm of gradient:  0.024315507473486934 ; l2 norm of weights:  0.40272832022632193\n",
            "Iteration#:  5296 ; Loss:  0.4894410191010903 ; l2 norm of gradient:  0.024209415093349174 ; l2 norm of weights:  0.40271559476516955\n",
            "Iteration#:  5300 ; Loss:  0.4894265167347437 ; l2 norm of gradient:  0.024103795275447672 ; l2 norm of weights:  0.4027029460398731\n",
            "Iteration#:  5304 ; Loss:  0.4894120535787916 ; l2 norm of gradient:  0.0239986459342015 ; l2 norm of weights:  0.40269037351402936\n",
            "Iteration#:  5308 ; Loss:  0.4893976294246575 ; l2 norm of gradient:  0.023893964993338655 ; l2 norm of weights:  0.40267787665540405\n",
            "Iteration#:  5312 ; Loss:  0.4893832440653886 ; l2 norm of gradient:  0.023789750385853778 ; l2 norm of weights:  0.4026654549358989\n",
            "Iteration#:  5316 ; Loss:  0.48936889729564265 ; l2 norm of gradient:  0.023686000053966438 ; l2 norm of weights:  0.40265310783151753\n",
            "Iteration#:  5320 ; Loss:  0.489354588911674 ; l2 norm of gradient:  0.023582711949081174 ; l2 norm of weights:  0.40264083482233254\n",
            "Iteration#:  5324 ; Loss:  0.489340318711319 ; l2 norm of gradient:  0.02347988403174531 ; l2 norm of weights:  0.40262863539245264\n",
            "Iteration#:  5328 ; Loss:  0.48932608649398357 ; l2 norm of gradient:  0.02337751427160888 ; l2 norm of weights:  0.40261650902998997\n",
            "Iteration#:  5332 ; Loss:  0.48931189206062864 ; l2 norm of gradient:  0.023275600647383446 ; l2 norm of weights:  0.4026044552270274\n",
            "Iteration#:  5336 ; Loss:  0.4892977352137574 ; l2 norm of gradient:  0.023174141146801833 ; l2 norm of weights:  0.4025924734795869\n",
            "Iteration#:  5340 ; Loss:  0.48928361575740187 ; l2 norm of gradient:  0.023073133766578054 ; l2 norm of weights:  0.4025805632875973\n",
            "Iteration#:  5344 ; Loss:  0.48926953349710944 ; l2 norm of gradient:  0.022972576512366616 ; l2 norm of weights:  0.4025687241548629\n",
            "Iteration#:  5348 ; Loss:  0.4892554882399302 ; l2 norm of gradient:  0.022872467398723024 ; l2 norm of weights:  0.4025569555890319\n",
            "Iteration#:  5352 ; Loss:  0.4892414797944042 ; l2 norm of gradient:  0.02277280444906432 ; l2 norm of weights:  0.4025452571015657\n",
            "Iteration#:  5356 ; Loss:  0.4892275079705477 ; l2 norm of gradient:  0.022673585695628704 ; l2 norm of weights:  0.4025336282077076\n",
            "Iteration#:  5360 ; Loss:  0.48921357257984144 ; l2 norm of gradient:  0.02257480917943628 ; l2 norm of weights:  0.40252206842645266\n",
            "Iteration#:  5364 ; Loss:  0.4891996734352179 ; l2 norm of gradient:  0.022476472950251118 ; l2 norm of weights:  0.40251057728051676\n",
            "Iteration#:  5368 ; Loss:  0.489185810351048 ; l2 norm of gradient:  0.02237857506653994 ; l2 norm of weights:  0.4024991542963075\n",
            "Iteration#:  5372 ; Loss:  0.48917198314312993 ; l2 norm of gradient:  0.022281113595435244 ; l2 norm of weights:  0.4024877990038931\n",
            "Iteration#:  5376 ; Loss:  0.489158191628676 ; l2 norm of gradient:  0.022184086612695685 ; l2 norm of weights:  0.40247651093697373\n",
            "Iteration#:  5380 ; Loss:  0.48914443562630083 ; l2 norm of gradient:  0.02208749220266728 ; l2 norm of weights:  0.40246528963285205\n",
            "Iteration#:  5384 ; Loss:  0.48913071495600924 ; l2 norm of gradient:  0.021991328458246313 ; l2 norm of weights:  0.40245413463240354\n",
            "Iteration#:  5388 ; Loss:  0.48911702943918467 ; l2 norm of gradient:  0.021895593480838905 ; l2 norm of weights:  0.4024430454800485\n",
            "Iteration#:  5392 ; Loss:  0.48910337889857625 ; l2 norm of gradient:  0.021800285380325603 ; l2 norm of weights:  0.4024320217237224\n",
            "Iteration#:  5396 ; Loss:  0.4890897631582886 ; l2 norm of gradient:  0.021705402275021806 ; l2 norm of weights:  0.4024210629148482\n",
            "Iteration#:  5400 ; Loss:  0.4890761820437689 ; l2 norm of gradient:  0.021610942291640345 ; l2 norm of weights:  0.4024101686083081\n",
            "Iteration#:  5404 ; Loss:  0.48906263538179623 ; l2 norm of gradient:  0.02151690356525434 ; l2 norm of weights:  0.4023993383624149\n",
            "Iteration#:  5408 ; Loss:  0.48904912300046954 ; l2 norm of gradient:  0.02142328423925974 ; l2 norm of weights:  0.40238857173888565\n",
            "Iteration#:  5412 ; Loss:  0.4890356447291967 ; l2 norm of gradient:  0.02133008246533824 ; l2 norm of weights:  0.4023778683028126\n",
            "Iteration#:  5416 ; Loss:  0.4890222003986833 ; l2 norm of gradient:  0.021237296403419603 ; l2 norm of weights:  0.4023672276226369\n",
            "Iteration#:  5420 ; Loss:  0.489008789840921 ; l2 norm of gradient:  0.02114492422164595 ; l2 norm of weights:  0.4023566492701214\n",
            "Iteration#:  5424 ; Loss:  0.48899541288917736 ; l2 norm of gradient:  0.02105296409633375 ; l2 norm of weights:  0.40234613282032367\n",
            "Iteration#:  5428 ; Loss:  0.4889820693779841 ; l2 norm of gradient:  0.02096141421193842 ; l2 norm of weights:  0.4023356778515695\n",
            "Iteration#:  5432 ; Loss:  0.48896875914312676 ; l2 norm of gradient:  0.020870272761017244 ; l2 norm of weights:  0.40232528394542627\n",
            "Iteration#:  5436 ; Loss:  0.48895548202163375 ; l2 norm of gradient:  0.02077953794419386 ; l2 norm of weights:  0.40231495068667733\n",
            "Iteration#:  5440 ; Loss:  0.4889422378517657 ; l2 norm of gradient:  0.02068920797012136 ; l2 norm of weights:  0.4023046776632954\n",
            "Iteration#:  5444 ; Loss:  0.4889290264730052 ; l2 norm of gradient:  0.020599281055447116 ; l2 norm of weights:  0.40229446446641726\n",
            "Iteration#:  5448 ; Loss:  0.4889158477260458 ; l2 norm of gradient:  0.020509755424776607 ; l2 norm of weights:  0.4022843106903181\n",
            "Iteration#:  5452 ; Loss:  0.48890270145278203 ; l2 norm of gradient:  0.02042062931063845 ; l2 norm of weights:  0.402274215932386\n",
            "Iteration#:  5456 ; Loss:  0.4888895874962994 ; l2 norm of gradient:  0.020331900953447852 ; l2 norm of weights:  0.4022641797930971\n",
            "Iteration#:  5460 ; Loss:  0.4888765057008634 ; l2 norm of gradient:  0.02024356860147277 ; l2 norm of weights:  0.4022542018759906\n",
            "Iteration#:  5464 ; Loss:  0.48886345591190994 ; l2 norm of gradient:  0.020155630510798238 ; l2 norm of weights:  0.40224428178764393\n",
            "Iteration#:  5468 ; Loss:  0.4888504379760355 ; l2 norm of gradient:  0.020068084945290787 ; l2 norm of weights:  0.4022344191376483\n",
            "Iteration#:  5472 ; Loss:  0.488837451740987 ; l2 norm of gradient:  0.019980930176564594 ; l2 norm of weights:  0.4022246135385847\n",
            "Iteration#:  5476 ; Loss:  0.48882449705565145 ; l2 norm of gradient:  0.019894164483946628 ; l2 norm of weights:  0.4022148646059989\n",
            "Iteration#:  5480 ; Loss:  0.4888115737700474 ; l2 norm of gradient:  0.01980778615444183 ; l2 norm of weights:  0.4022051719583789\n",
            "Iteration#:  5484 ; Loss:  0.488798681735314 ; l2 norm of gradient:  0.019721793482699094 ; l2 norm of weights:  0.40219553521712986\n",
            "Iteration#:  5488 ; Loss:  0.48878582080370236 ; l2 norm of gradient:  0.019636184770977 ; l2 norm of weights:  0.4021859540065516\n",
            "Iteration#:  5492 ; Loss:  0.4887729908285656 ; l2 norm of gradient:  0.01955095832911022 ; l2 norm of weights:  0.40217642795381453\n",
            "Iteration#:  5496 ; Loss:  0.4887601916643494 ; l2 norm of gradient:  0.019466112474474582 ; l2 norm of weights:  0.40216695668893704\n",
            "Iteration#:  5500 ; Loss:  0.48874742316658304 ; l2 norm of gradient:  0.019381645531954924 ; l2 norm of weights:  0.40215753984476227\n",
            "Iteration#:  5504 ; Loss:  0.48873468519187013 ; l2 norm of gradient:  0.019297555833910088 ; l2 norm of weights:  0.40214817705693556\n",
            "Iteration#:  5508 ; Loss:  0.4887219775978786 ; l2 norm of gradient:  0.019213841720140318 ; l2 norm of weights:  0.40213886796388154\n",
            "Iteration#:  5512 ; Loss:  0.48870930024333287 ; l2 norm of gradient:  0.019130501537853676 ; l2 norm of weights:  0.40212961220678195\n",
            "Iteration#:  5516 ; Loss:  0.4886966529880041 ; l2 norm of gradient:  0.01904753364163323 ; l2 norm of weights:  0.40212040942955374\n",
            "Iteration#:  5520 ; Loss:  0.48868403569270147 ; l2 norm of gradient:  0.018964936393403863 ; l2 norm of weights:  0.4021112592788263\n",
            "Iteration#:  5524 ; Loss:  0.48867144821926295 ; l2 norm of gradient:  0.018882708162399488 ; l2 norm of weights:  0.4021021614039204\n",
            "Iteration#:  5528 ; Loss:  0.48865889043054733 ; l2 norm of gradient:  0.01880084732513007 ; l2 norm of weights:  0.40209311545682597\n",
            "Iteration#:  5532 ; Loss:  0.48864636219042495 ; l2 norm of gradient:  0.01871935226534996 ; l2 norm of weights:  0.402084121092181\n",
            "Iteration#:  5536 ; Loss:  0.48863386336376907 ; l2 norm of gradient:  0.018638221374024797 ; l2 norm of weights:  0.40207517796724984\n",
            "Iteration#:  5540 ; Loss:  0.48862139381644754 ; l2 norm of gradient:  0.01855745304929911 ; l2 norm of weights:  0.40206628574190234\n",
            "Iteration#:  5544 ; Loss:  0.4886089534153144 ; l2 norm of gradient:  0.01847704569646505 ; l2 norm of weights:  0.402057444078593\n",
            "Iteration#:  5548 ; Loss:  0.48859654202820124 ; l2 norm of gradient:  0.018396997727929396 ; l2 norm of weights:  0.4020486526423398\n",
            "Iteration#:  5552 ; Loss:  0.488584159523909 ; l2 norm of gradient:  0.01831730756318256 ; l2 norm of weights:  0.40203991110070375\n",
            "Iteration#:  5556 ; Loss:  0.4885718057721997 ; l2 norm of gradient:  0.018237973628767244 ; l2 norm of weights:  0.40203121912376866\n",
            "Iteration#:  5560 ; Loss:  0.4885594806437886 ; l2 norm of gradient:  0.018158994358245188 ; l2 norm of weights:  0.40202257638412037\n",
            "Iteration#:  5564 ; Loss:  0.4885471840103355 ; l2 norm of gradient:  0.01808036819216766 ; l2 norm of weights:  0.4020139825568273\n",
            "Iteration#:  5568 ; Loss:  0.4885349157444371 ; l2 norm of gradient:  0.01800209357804306 ; l2 norm of weights:  0.4020054373194198\n",
            "Iteration#:  5572 ; Loss:  0.48852267571961905 ; l2 norm of gradient:  0.017924168970306752 ; l2 norm of weights:  0.4019969403518707\n",
            "Iteration#:  5576 ; Loss:  0.4885104638103279 ; l2 norm of gradient:  0.017846592830289235 ; l2 norm of weights:  0.40198849133657594\n",
            "Iteration#:  5580 ; Loss:  0.4884982798919236 ; l2 norm of gradient:  0.01776936362618503 ; l2 norm of weights:  0.4019800899583345\n",
            "Iteration#:  5584 ; Loss:  0.48848612384067114 ; l2 norm of gradient:  0.017692479833023547 ; l2 norm of weights:  0.4019717359043296\n",
            "Iteration#:  5588 ; Loss:  0.48847399553373333 ; l2 norm of gradient:  0.017615939932636906 ; l2 norm of weights:  0.4019634288641092\n",
            "Iteration#:  5592 ; Loss:  0.4884618948491636 ; l2 norm of gradient:  0.017539742413630405 ; l2 norm of weights:  0.40195516852956725\n",
            "Iteration#:  5596 ; Loss:  0.4884498216658971 ; l2 norm of gradient:  0.01746388577135203 ; l2 norm of weights:  0.401946954594925\n",
            "Iteration#:  5600 ; Loss:  0.48843777586374504 ; l2 norm of gradient:  0.01738836850786172 ; l2 norm of weights:  0.40193878675671174\n",
            "Iteration#:  5604 ; Loss:  0.48842575732338567 ; l2 norm of gradient:  0.017313189131902718 ; l2 norm of weights:  0.4019306647137469\n",
            "Iteration#:  5608 ; Loss:  0.4884137659263575 ; l2 norm of gradient:  0.017238346158870082 ; l2 norm of weights:  0.4019225881671211\n",
            "Iteration#:  5612 ; Loss:  0.48840180155505264 ; l2 norm of gradient:  0.017163838110782034 ; l2 norm of weights:  0.40191455682017846\n",
            "Iteration#:  5616 ; Loss:  0.48838986409270874 ; l2 norm of gradient:  0.01708966351624988 ; l2 norm of weights:  0.4019065703784982\n",
            "Iteration#:  5620 ; Loss:  0.48837795342340196 ; l2 norm of gradient:  0.01701582091044833 ; l2 norm of weights:  0.4018986285498769\n",
            "Iteration#:  5624 ; Loss:  0.4883660694320401 ; l2 norm of gradient:  0.016942308835087024 ; l2 norm of weights:  0.4018907310443104\n",
            "Iteration#:  5628 ; Loss:  0.48835421200435597 ; l2 norm of gradient:  0.01686912583837998 ; l2 norm of weights:  0.4018828775739765\n",
            "Iteration#:  5632 ; Loss:  0.4883423810268993 ; l2 norm of gradient:  0.016796270475018107 ; l2 norm of weights:  0.40187506785321747\n",
            "Iteration#:  5636 ; Loss:  0.4883305763870308 ; l2 norm of gradient:  0.016723741306138205 ; l2 norm of weights:  0.40186730159852224\n",
            "Iteration#:  5640 ; Loss:  0.48831879797291505 ; l2 norm of gradient:  0.01665153689929575 ; l2 norm of weights:  0.40185957852850984\n",
            "Iteration#:  5644 ; Loss:  0.48830704567351363 ; l2 norm of gradient:  0.016579655828435846 ; l2 norm of weights:  0.4018518983639119\n",
            "Iteration#:  5648 ; Loss:  0.48829531937857806 ; l2 norm of gradient:  0.016508096673864284 ; l2 norm of weights:  0.4018442608275558\n",
            "Iteration#:  5652 ; Loss:  0.4882836189786442 ; l2 norm of gradient:  0.016436858022218843 ; l2 norm of weights:  0.401836665644348\n",
            "Iteration#:  5656 ; Loss:  0.48827194436502414 ; l2 norm of gradient:  0.016365938466442128 ; l2 norm of weights:  0.40182911254125747\n",
            "Iteration#:  5660 ; Loss:  0.4882602954298011 ; l2 norm of gradient:  0.016295336605751814 ; l2 norm of weights:  0.40182160124729854\n",
            "Iteration#:  5664 ; Loss:  0.4882486720658218 ; l2 norm of gradient:  0.01622505104561364 ; l2 norm of weights:  0.40181413149351564\n",
            "Iteration#:  5668 ; Loss:  0.4882370741666907 ; l2 norm of gradient:  0.016155080397713215 ; l2 norm of weights:  0.40180670301296595\n",
            "Iteration#:  5672 ; Loss:  0.48822550162676326 ; l2 norm of gradient:  0.016085423279927635 ; l2 norm of weights:  0.40179931554070414\n",
            "Iteration#:  5676 ; Loss:  0.48821395434113996 ; l2 norm of gradient:  0.016016078316298494 ; l2 norm of weights:  0.4017919688137656\n",
            "Iteration#:  5680 ; Loss:  0.48820243220565956 ; l2 norm of gradient:  0.01594704413700422 ; l2 norm of weights:  0.4017846625711514\n",
            "Iteration#:  5684 ; Loss:  0.4881909351168932 ; l2 norm of gradient:  0.015878319378332034 ; l2 norm of weights:  0.4017773965538119\n",
            "Iteration#:  5688 ; Loss:  0.4881794629721386 ; l2 norm of gradient:  0.015809902682650794 ; l2 norm of weights:  0.40177017050463176\n",
            "Iteration#:  5692 ; Loss:  0.4881680156694127 ; l2 norm of gradient:  0.01574179269838429 ; l2 norm of weights:  0.401762984168414\n",
            "Iteration#:  5696 ; Loss:  0.4881565931074473 ; l2 norm of gradient:  0.015673988079983693 ; l2 norm of weights:  0.4017558372918646\n",
            "Iteration#:  5700 ; Loss:  0.4881451951856815 ; l2 norm of gradient:  0.015606487487900363 ; l2 norm of weights:  0.401748729623578\n",
            "Iteration#:  5704 ; Loss:  0.488133821804257 ; l2 norm of gradient:  0.015539289588559465 ; l2 norm of weights:  0.40174166091402125\n",
            "Iteration#:  5708 ; Loss:  0.4881224728640111 ; l2 norm of gradient:  0.015472393054332716 ; l2 norm of weights:  0.4017346309155193\n",
            "Iteration#:  5712 ; Loss:  0.48811114826647184 ; l2 norm of gradient:  0.015405796563512259 ; l2 norm of weights:  0.40172763938224054\n",
            "Iteration#:  5716 ; Loss:  0.4880998479138516 ; l2 norm of gradient:  0.015339498800283962 ; l2 norm of weights:  0.4017206860701814\n",
            "Iteration#:  5720 ; Loss:  0.48808857170904146 ; l2 norm of gradient:  0.01527349845470091 ; l2 norm of weights:  0.4017137707371523\n",
            "Iteration#:  5724 ; Loss:  0.4880773195556056 ; l2 norm of gradient:  0.015207794222656643 ; l2 norm of weights:  0.4017068931427629\n",
            "Iteration#:  5728 ; Loss:  0.4880660913577757 ; l2 norm of gradient:  0.01514238480586008 ; l2 norm of weights:  0.4017000530484079\n",
            "Iteration#:  5732 ; Loss:  0.48805488702044497 ; l2 norm of gradient:  0.015077268911809041 ; l2 norm of weights:  0.40169325021725266\n",
            "Iteration#:  5736 ; Loss:  0.4880437064491633 ; l2 norm of gradient:  0.015012445253763641 ; l2 norm of weights:  0.4016864844142192\n",
            "Iteration#:  5740 ; Loss:  0.48803254955013115 ; l2 norm of gradient:  0.014947912550720985 ; l2 norm of weights:  0.40167975540597217\n",
            "Iteration#:  5744 ; Loss:  0.48802141623019435 ; l2 norm of gradient:  0.014883669527390326 ; l2 norm of weights:  0.4016730629609049\n",
            "Iteration#:  5748 ; Loss:  0.4880103063968385 ; l2 norm of gradient:  0.014819714914165321 ; l2 norm of weights:  0.4016664068491256\n",
            "Iteration#:  5752 ; Loss:  0.487999219958184 ; l2 norm of gradient:  0.014756047447100757 ; l2 norm of weights:  0.40165978684244386\n",
            "Iteration#:  5756 ; Loss:  0.4879881568229805 ; l2 norm of gradient:  0.014692665867885753 ; l2 norm of weights:  0.4016532027143568\n",
            "Iteration#:  5760 ; Loss:  0.48797711690060164 ; l2 norm of gradient:  0.014629568923819357 ; l2 norm of weights:  0.4016466542400361\n",
            "Iteration#:  5764 ; Loss:  0.48796610010103936 ; l2 norm of gradient:  0.014566755367784897 ; l2 norm of weights:  0.4016401411963141\n",
            "Iteration#:  5768 ; Loss:  0.4879551063349002 ; l2 norm of gradient:  0.01450422395822487 ; l2 norm of weights:  0.40163366336167117\n",
            "Iteration#:  5772 ; Loss:  0.4879441355133982 ; l2 norm of gradient:  0.014441973459116521 ; l2 norm of weights:  0.40162722051622185\n",
            "Iteration#:  5776 ; Loss:  0.4879331875483517 ; l2 norm of gradient:  0.014380002639946491 ; l2 norm of weights:  0.4016208124417027\n",
            "Iteration#:  5780 ; Loss:  0.4879222623521765 ; l2 norm of gradient:  0.014318310275686676 ; l2 norm of weights:  0.40161443892145876\n",
            "Iteration#:  5784 ; Loss:  0.4879113598378826 ; l2 norm of gradient:  0.014256895146768676 ; l2 norm of weights:  0.40160809974043116\n",
            "Iteration#:  5788 ; Loss:  0.4879004799190678 ; l2 norm of gradient:  0.01419575603906085 ; l2 norm of weights:  0.4016017946851439\n",
            "Iteration#:  5792 ; Loss:  0.4878896225099142 ; l2 norm of gradient:  0.014134891743841884 ; l2 norm of weights:  0.401595523543692\n",
            "Iteration#:  5796 ; Loss:  0.48787878752518166 ; l2 norm of gradient:  0.014074301057778613 ; l2 norm of weights:  0.4015892861057283\n",
            "Iteration#:  5800 ; Loss:  0.48786797488020456 ; l2 norm of gradient:  0.014013982782900171 ; l2 norm of weights:  0.4015830821624518\n",
            "Iteration#:  5804 ; Loss:  0.4878571844908861 ; l2 norm of gradient:  0.01395393572657528 ; l2 norm of weights:  0.4015769115065945\n",
            "Iteration#:  5808 ; Loss:  0.48784641627369396 ; l2 norm of gradient:  0.013894158701486796 ; l2 norm of weights:  0.4015707739324102\n",
            "Iteration#:  5812 ; Loss:  0.4878356701456552 ; l2 norm of gradient:  0.013834650525609461 ; l2 norm of weights:  0.4015646692356617\n",
            "Iteration#:  5816 ; Loss:  0.48782494602435233 ; l2 norm of gradient:  0.013775410022184862 ; l2 norm of weights:  0.4015585972136088\n",
            "Iteration#:  5820 ; Loss:  0.4878142438279178 ; l2 norm of gradient:  0.013716436019698694 ; l2 norm of weights:  0.4015525576649972\n",
            "Iteration#:  5824 ; Loss:  0.4878035634750299 ; l2 norm of gradient:  0.013657727351856481 ; l2 norm of weights:  0.40154655039004566\n",
            "Iteration#:  5828 ; Loss:  0.4877929048849085 ; l2 norm of gradient:  0.013599282857560653 ; l2 norm of weights:  0.4015405751904353\n",
            "Iteration#:  5832 ; Loss:  0.4877822679773101 ; l2 norm of gradient:  0.013541101380887162 ; l2 norm of weights:  0.4015346318692973\n",
            "Iteration#:  5836 ; Loss:  0.48777165267252354 ; l2 norm of gradient:  0.013483181771061862 ; l2 norm of weights:  0.4015287202312017\n",
            "Iteration#:  5840 ; Loss:  0.48776105889136545 ; l2 norm of gradient:  0.013425522882437913 ; l2 norm of weights:  0.4015228400821458\n",
            "Iteration#:  5844 ; Loss:  0.48775048655517617 ; l2 norm of gradient:  0.013368123574472248 ; l2 norm of weights:  0.4015169912295433\n",
            "Iteration#:  5848 ; Loss:  0.4877399355858154 ; l2 norm of gradient:  0.013310982711702798 ; l2 norm of weights:  0.40151117348221255\n",
            "Iteration#:  5852 ; Loss:  0.48772940590565717 ; l2 norm of gradient:  0.01325409916372627 ; l2 norm of weights:  0.40150538665036567\n",
            "Iteration#:  5856 ; Loss:  0.4877188974375869 ; l2 norm of gradient:  0.013197471805174351 ; l2 norm of weights:  0.4014996305455977\n",
            "Iteration#:  5860 ; Loss:  0.48770841010499566 ; l2 norm of gradient:  0.013141099515691793 ; l2 norm of weights:  0.40149390498087506\n",
            "Iteration#:  5864 ; Loss:  0.48769794383177745 ; l2 norm of gradient:  0.013084981179913791 ; l2 norm of weights:  0.4014882097705256\n",
            "Iteration#:  5868 ; Loss:  0.48768749854232385 ; l2 norm of gradient:  0.013029115687443244 ; l2 norm of weights:  0.40148254473022676\n",
            "Iteration#:  5872 ; Loss:  0.4876770741615206 ; l2 norm of gradient:  0.012973501932829028 ; l2 norm of weights:  0.40147690967699595\n",
            "Iteration#:  5876 ; Loss:  0.4876666706147435 ; l2 norm of gradient:  0.012918138815542846 ; l2 norm of weights:  0.4014713044291793\n",
            "Iteration#:  5880 ; Loss:  0.48765628782785375 ; l2 norm of gradient:  0.012863025239957969 ; l2 norm of weights:  0.4014657288064412\n",
            "Iteration#:  5884 ; Loss:  0.4876459257271949 ; l2 norm of gradient:  0.012808160115326992 ; l2 norm of weights:  0.40146018262975414\n",
            "Iteration#:  5888 ; Loss:  0.48763558423958764 ; l2 norm of gradient:  0.012753542355759338 ; l2 norm of weights:  0.40145466572138805\n",
            "Iteration#:  5892 ; Loss:  0.48762526329232736 ; l2 norm of gradient:  0.01269917088019993 ; l2 norm of weights:  0.4014491779049007\n",
            "Iteration#:  5896 ; Loss:  0.48761496281317895 ; l2 norm of gradient:  0.012645044612407551 ; l2 norm of weights:  0.4014437190051265\n",
            "Iteration#:  5900 ; Loss:  0.4876046827303737 ; l2 norm of gradient:  0.012591162480932418 ; l2 norm of weights:  0.4014382888481673\n",
            "Iteration#:  5904 ; Loss:  0.487594422972605 ; l2 norm of gradient:  0.012537523419095632 ; l2 norm of weights:  0.4014328872613822\n",
            "Iteration#:  5908 ; Loss:  0.487584183469025 ; l2 norm of gradient:  0.012484126364967115 ; l2 norm of weights:  0.4014275140733772\n",
            "Iteration#:  5912 ; Loss:  0.48757396414924004 ; l2 norm of gradient:  0.012430970261344232 ; l2 norm of weights:  0.40142216911399603\n",
            "Iteration#:  5916 ; Loss:  0.48756376494330833 ; l2 norm of gradient:  0.012378054055730699 ; l2 norm of weights:  0.4014168522143099\n",
            "Iteration#:  5920 ; Loss:  0.4875535857817344 ; l2 norm of gradient:  0.012325376700314833 ; l2 norm of weights:  0.40141156320660776\n",
            "Iteration#:  5924 ; Loss:  0.487543426595467 ; l2 norm of gradient:  0.012272937151949654 ; l2 norm of weights:  0.40140630192438725\n",
            "Iteration#:  5928 ; Loss:  0.48753328731589474 ; l2 norm of gradient:  0.012220734372130634 ; l2 norm of weights:  0.4014010682023447\n",
            "Iteration#:  5932 ; Loss:  0.48752316787484257 ; l2 norm of gradient:  0.01216876732697566 ; l2 norm of weights:  0.40139586187636594\n",
            "Iteration#:  5936 ; Loss:  0.48751306820456813 ; l2 norm of gradient:  0.012117034987203127 ; l2 norm of weights:  0.4013906827835166\n",
            "Iteration#:  5940 ; Loss:  0.4875029882377584 ; l2 norm of gradient:  0.012065536328112803 ; l2 norm of weights:  0.4013855307620333\n",
            "Iteration#:  5944 ; Loss:  0.48749292790752585 ; l2 norm of gradient:  0.012014270329563733 ; l2 norm of weights:  0.4013804056513141\n",
            "Iteration#:  5948 ; Loss:  0.48748288714740556 ; l2 norm of gradient:  0.011963235975954176 ; l2 norm of weights:  0.40137530729190946\n",
            "Iteration#:  5952 ; Loss:  0.4874728658913512 ; l2 norm of gradient:  0.011912432256201177 ; l2 norm of weights:  0.401370235525513\n",
            "Iteration#:  5956 ; Loss:  0.4874628640737315 ; l2 norm of gradient:  0.011861858163720885 ; l2 norm of weights:  0.40136519019495315\n",
            "Iteration#:  5960 ; Loss:  0.48745288162932787 ; l2 norm of gradient:  0.011811512696406618 ; l2 norm of weights:  0.4013601711441834\n",
            "Iteration#:  5964 ; Loss:  0.48744291849332966 ; l2 norm of gradient:  0.01176139485661026 ; l2 norm of weights:  0.4013551782182739\n",
            "Iteration#:  5968 ; Loss:  0.4874329746013316 ; l2 norm of gradient:  0.011711503651121332 ; l2 norm of weights:  0.4013502112634025\n",
            "Iteration#:  5972 ; Loss:  0.4874230498893306 ; l2 norm of gradient:  0.011661838091147217 ; l2 norm of weights:  0.40134527012684673\n",
            "Iteration#:  5976 ; Loss:  0.48741314429372207 ; l2 norm of gradient:  0.011612397192292756 ; l2 norm of weights:  0.4013403546569738\n",
            "Iteration#:  5980 ; Loss:  0.48740325775129684 ; l2 norm of gradient:  0.011563179974541543 ; l2 norm of weights:  0.40133546470323367\n",
            "Iteration#:  5984 ; Loss:  0.48739339019923805 ; l2 norm of gradient:  0.011514185462234587 ; l2 norm of weights:  0.4013306001161492\n",
            "Iteration#:  5988 ; Loss:  0.48738354157511754 ; l2 norm of gradient:  0.011465412684051992 ; l2 norm of weights:  0.4013257607473084\n",
            "Iteration#:  5992 ; Loss:  0.4873737118168935 ; l2 norm of gradient:  0.01141686067299256 ; l2 norm of weights:  0.4013209464493563\n",
            "Iteration#:  5996 ; Loss:  0.4873639008629064 ; l2 norm of gradient:  0.011368528466354815 ; l2 norm of weights:  0.40131615707598584\n",
            "Iteration#:  6000 ; Loss:  0.48735410865187634 ; l2 norm of gradient:  0.011320415105716708 ; l2 norm of weights:  0.40131139248193054\n",
            "Iteration#:  6004 ; Loss:  0.4873443351229002 ; l2 norm of gradient:  0.011272519636917264 ; l2 norm of weights:  0.4013066525229559\n",
            "Iteration#:  6008 ; Loss:  0.4873345802154479 ; l2 norm of gradient:  0.01122484111003708 ; l2 norm of weights:  0.4013019370558511\n",
            "Iteration#:  6012 ; Loss:  0.48732484386936015 ; l2 norm of gradient:  0.011177378579378557 ; l2 norm of weights:  0.40129724593842186\n",
            "Iteration#:  6016 ; Loss:  0.4873151260248448 ; l2 norm of gradient:  0.011130131103447646 ; l2 norm of weights:  0.4012925790294814\n",
            "Iteration#:  6020 ; Loss:  0.48730542662247445 ; l2 norm of gradient:  0.011083097744933762 ; l2 norm of weights:  0.40128793618884334\n",
            "Iteration#:  6024 ; Loss:  0.4872957456031827 ; l2 norm of gradient:  0.011036277570692466 ; l2 norm of weights:  0.40128331727731337\n",
            "Iteration#:  6028 ; Loss:  0.4872860829082621 ; l2 norm of gradient:  0.010989669651724644 ; l2 norm of weights:  0.4012787221566821\n",
            "Iteration#:  6032 ; Loss:  0.48727643847936064 ; l2 norm of gradient:  0.010943273063159665 ; l2 norm of weights:  0.40127415068971645\n",
            "Iteration#:  6036 ; Loss:  0.48726681225847945 ; l2 norm of gradient:  0.01089708688423493 ; l2 norm of weights:  0.40126960274015333\n",
            "Iteration#:  6040 ; Loss:  0.4872572041879691 ; l2 norm of gradient:  0.010851110198278757 ; l2 norm of weights:  0.40126507817269064\n",
            "Iteration#:  6044 ; Loss:  0.4872476142105273 ; l2 norm of gradient:  0.01080534209269098 ; l2 norm of weights:  0.4012605768529807\n",
            "Iteration#:  6048 ; Loss:  0.48723804226919654 ; l2 norm of gradient:  0.010759781658924676 ; l2 norm of weights:  0.4012560986476226\n",
            "Iteration#:  6052 ; Loss:  0.4872284883073602 ; l2 norm of gradient:  0.010714427992467784 ; l2 norm of weights:  0.40125164342415476\n",
            "Iteration#:  6056 ; Loss:  0.4872189522687409 ; l2 norm of gradient:  0.010669280192825337 ; l2 norm of weights:  0.40124721105104744\n",
            "Iteration#:  6060 ; Loss:  0.4872094340973969 ; l2 norm of gradient:  0.010624337363500222 ; l2 norm of weights:  0.40124280139769575\n",
            "Iteration#:  6064 ; Loss:  0.48719993373772036 ; l2 norm of gradient:  0.010579598611976009 ; l2 norm of weights:  0.4012384143344124\n",
            "Iteration#:  6068 ; Loss:  0.48719045113443343 ; l2 norm of gradient:  0.010535063049698371 ; l2 norm of weights:  0.4012340497324204\n",
            "Iteration#:  6072 ; Loss:  0.48718098623258643 ; l2 norm of gradient:  0.010490729792057306 ; l2 norm of weights:  0.401229707463846\n",
            "Iteration#:  6076 ; Loss:  0.48717153897755555 ; l2 norm of gradient:  0.010446597958369159 ; l2 norm of weights:  0.401225387401712\n",
            "Iteration#:  6080 ; Loss:  0.48716210931503884 ; l2 norm of gradient:  0.010402666671858836 ; l2 norm of weights:  0.4012210894199302\n",
            "Iteration#:  6084 ; Loss:  0.48715269719105525 ; l2 norm of gradient:  0.010358935059641975 ; l2 norm of weights:  0.40121681339329474\n",
            "Iteration#:  6088 ; Loss:  0.48714330255194077 ; l2 norm of gradient:  0.01031540225270752 ; l2 norm of weights:  0.4012125591974756\n",
            "Iteration#:  6092 ; Loss:  0.48713392534434674 ; l2 norm of gradient:  0.010272067385899721 ; l2 norm of weights:  0.4012083267090112\n",
            "Iteration#:  6096 ; Loss:  0.4871245655152369 ; l2 norm of gradient:  0.01022892959790112 ; l2 norm of weights:  0.4012041158053021\n",
            "Iteration#:  6100 ; Loss:  0.48711522301188515 ; l2 norm of gradient:  0.010185988031214694 ; l2 norm of weights:  0.401199926364604\n",
            "Iteration#:  6104 ; Loss:  0.4871058977818724 ; l2 norm of gradient:  0.010143241832146582 ; l2 norm of weights:  0.4011957582660212\n",
            "Iteration#:  6108 ; Loss:  0.4870965897730852 ; l2 norm of gradient:  0.010100690150788622 ; l2 norm of weights:  0.4011916113895003\n",
            "Iteration#:  6112 ; Loss:  0.48708729893371244 ; l2 norm of gradient:  0.010058332141002138 ; l2 norm of weights:  0.40118748561582307\n",
            "Iteration#:  6116 ; Loss:  0.4870780252122433 ; l2 norm of gradient:  0.010016166960398833 ; l2 norm of weights:  0.4011833808266006\n",
            "Iteration#:  6120 ; Loss:  0.4870687685574647 ; l2 norm of gradient:  0.00997419377032584 ; l2 norm of weights:  0.40117929690426635\n",
            "Iteration#:  6124 ; Loss:  0.48705952891845883 ; l2 norm of gradient:  0.00993241173584747 ; l2 norm of weights:  0.40117523373207015\n",
            "Iteration#:  6128 ; Loss:  0.48705030624460155 ; l2 norm of gradient:  0.009890820025728138 ; l2 norm of weights:  0.4011711911940717\n",
            "Iteration#:  6132 ; Loss:  0.4870411004855588 ; l2 norm of gradient:  0.009849417812416562 ; l2 norm of weights:  0.40116716917513423\n",
            "Iteration#:  6136 ; Loss:  0.48703191159128534 ; l2 norm of gradient:  0.009808204272027875 ; l2 norm of weights:  0.4011631675609186\n",
            "Iteration#:  6140 ; Loss:  0.4870227395120218 ; l2 norm of gradient:  0.009767178584327735 ; l2 norm of weights:  0.40115918623787683\n",
            "Iteration#:  6144 ; Loss:  0.487013584198293 ; l2 norm of gradient:  0.009726339932715062 ; l2 norm of weights:  0.40115522509324586\n",
            "Iteration#:  6148 ; Loss:  0.4870044456009051 ; l2 norm of gradient:  0.00968568750420602 ; l2 norm of weights:  0.401151284015042\n",
            "Iteration#:  6152 ; Loss:  0.48699532367094356 ; l2 norm of gradient:  0.00964522048941673 ; l2 norm of weights:  0.4011473628920543\n",
            "Iteration#:  6156 ; Loss:  0.48698621835977135 ; l2 norm of gradient:  0.00960493808254789 ; l2 norm of weights:  0.4011434616138392\n",
            "Iteration#:  6160 ; Loss:  0.4869771296190262 ; l2 norm of gradient:  0.009564839481367361 ; l2 norm of weights:  0.40113958007071376\n",
            "Iteration#:  6164 ; Loss:  0.4869680574006185 ; l2 norm of gradient:  0.009524923887194952 ; l2 norm of weights:  0.4011357181537508\n",
            "Iteration#:  6168 ; Loss:  0.4869590016567295 ; l2 norm of gradient:  0.009485190504884987 ; l2 norm of weights:  0.4011318757547724\n",
            "Iteration#:  6172 ; Loss:  0.48694996233980864 ; l2 norm of gradient:  0.009445638542811125 ; l2 norm of weights:  0.4011280527663442\n",
            "Iteration#:  6176 ; Loss:  0.48694093940257216 ; l2 norm of gradient:  0.009406267212849883 ; l2 norm of weights:  0.40112424908176986\n",
            "Iteration#:  6180 ; Loss:  0.486931932798 ; l2 norm of gradient:  0.009367075730364691 ; l2 norm of weights:  0.40112046459508516\n",
            "Iteration#:  6184 ; Loss:  0.48692294247933476 ; l2 norm of gradient:  0.009328063314189929 ; l2 norm of weights:  0.40111669920105253\n",
            "Iteration#:  6188 ; Loss:  0.48691396840007883 ; l2 norm of gradient:  0.00928922918661447 ; l2 norm of weights:  0.4011129527951554\n",
            "Iteration#:  6192 ; Loss:  0.486905010513993 ; l2 norm of gradient:  0.009250572573367583 ; l2 norm of weights:  0.4011092252735926\n",
            "Iteration#:  6196 ; Loss:  0.48689606877509345 ; l2 norm of gradient:  0.009212092703600823 ; l2 norm of weights:  0.4011055165332728\n",
            "Iteration#:  6200 ; Loss:  0.4868871431376511 ; l2 norm of gradient:  0.009173788809874222 ; l2 norm of weights:  0.4011018264718094\n",
            "Iteration#:  6204 ; Loss:  0.4868782335561883 ; l2 norm of gradient:  0.00913566012813967 ; l2 norm of weights:  0.40109815498751467\n",
            "Iteration#:  6208 ; Loss:  0.4868693399854779 ; l2 norm of gradient:  0.00909770589772615 ; l2 norm of weights:  0.4010945019793944\n",
            "Iteration#:  6212 ; Loss:  0.4868604623805406 ; l2 norm of gradient:  0.009059925361323159 ; l2 norm of weights:  0.401090867347143\n",
            "Iteration#:  6216 ; Loss:  0.486851600696643 ; l2 norm of gradient:  0.009022317764966474 ; l2 norm of weights:  0.40108725099113796\n",
            "Iteration#:  6220 ; Loss:  0.4868427548892963 ; l2 norm of gradient:  0.008984882358021815 ; l2 norm of weights:  0.40108365281243424\n",
            "Iteration#:  6224 ; Loss:  0.4868339249142537 ; l2 norm of gradient:  0.00894761839317038 ; l2 norm of weights:  0.40108007271275986\n",
            "Iteration#:  6228 ; Loss:  0.486825110727509 ; l2 norm of gradient:  0.008910525126392859 ; l2 norm of weights:  0.40107651059450994\n",
            "Iteration#:  6232 ; Loss:  0.48681631228529415 ; l2 norm of gradient:  0.008873601816954929 ; l2 norm of weights:  0.40107296636074213\n",
            "Iteration#:  6236 ; Loss:  0.4868075295440782 ; l2 norm of gradient:  0.0088368477273915 ; l2 norm of weights:  0.4010694399151713\n",
            "Iteration#:  6240 ; Loss:  0.48679876246056486 ; l2 norm of gradient:  0.008800262123492462 ; l2 norm of weights:  0.4010659311621646\n",
            "Iteration#:  6244 ; Loss:  0.48679001099169034 ; l2 norm of gradient:  0.008763844274287239 ; l2 norm of weights:  0.40106244000673613\n",
            "Iteration#:  6248 ; Loss:  0.48678127509462277 ; l2 norm of gradient:  0.008727593452029482 ; l2 norm of weights:  0.40105896635454275\n",
            "Iteration#:  6252 ; Loss:  0.48677255472675895 ; l2 norm of gradient:  0.008691508932183437 ; l2 norm of weights:  0.40105551011187823\n",
            "Iteration#:  6256 ; Loss:  0.48676384984572374 ; l2 norm of gradient:  0.008655589993407655 ; l2 norm of weights:  0.40105207118566916\n",
            "Iteration#:  6260 ; Loss:  0.4867551604093674 ; l2 norm of gradient:  0.008619835917541554 ; l2 norm of weights:  0.4010486494834695\n",
            "Iteration#:  6264 ; Loss:  0.4867464863757644 ; l2 norm of gradient:  0.008584245989589975 ; l2 norm of weights:  0.4010452449134564\n",
            "Iteration#:  6268 ; Loss:  0.4867378277032116 ; l2 norm of gradient:  0.008548819497708835 ; l2 norm of weights:  0.4010418573844247\n",
            "Iteration#:  6272 ; Loss:  0.4867291843502264 ; l2 norm of gradient:  0.00851355573319066 ; l2 norm of weights:  0.40103848680578297\n",
            "Iteration#:  6276 ; Loss:  0.486720556275545 ; l2 norm of gradient:  0.008478453990450259 ; l2 norm of weights:  0.4010351330875484\n",
            "Iteration#:  6280 ; Loss:  0.48671194343812085 ; l2 norm of gradient:  0.008443513567009476 ; l2 norm of weights:  0.40103179614034207\n",
            "Iteration#:  6284 ; Loss:  0.48670334579712254 ; l2 norm of gradient:  0.008408733763484319 ; l2 norm of weights:  0.40102847587538476\n",
            "Iteration#:  6288 ; Loss:  0.48669476331193323 ; l2 norm of gradient:  0.008374113883569036 ; l2 norm of weights:  0.4010251722044918\n",
            "Iteration#:  6292 ; Loss:  0.48668619594214746 ; l2 norm of gradient:  0.008339653234022821 ; l2 norm of weights:  0.40102188504006925\n",
            "Iteration#:  6296 ; Loss:  0.4866776436475708 ; l2 norm of gradient:  0.008305351124655865 ; l2 norm of weights:  0.4010186142951088\n",
            "Iteration#:  6300 ; Loss:  0.4866691063882176 ; l2 norm of gradient:  0.008271206868314469 ; l2 norm of weights:  0.4010153598831836\n",
            "Iteration#:  6304 ; Loss:  0.48666058412430946 ; l2 norm of gradient:  0.008237219780867113 ; l2 norm of weights:  0.40101212171844375\n",
            "Iteration#:  6308 ; Loss:  0.48665207681627387 ; l2 norm of gradient:  0.008203389181191732 ; l2 norm of weights:  0.4010088997156119\n",
            "Iteration#:  6312 ; Loss:  0.4866435844247422 ; l2 norm of gradient:  0.008169714391159374 ; l2 norm of weights:  0.401005693789979\n",
            "Iteration#:  6316 ; Loss:  0.4866351069105485 ; l2 norm of gradient:  0.008136194735622508 ; l2 norm of weights:  0.4010025038574\n",
            "Iteration#:  6320 ; Loss:  0.486626644234728 ; l2 norm of gradient:  0.008102829542400262 ; l2 norm of weights:  0.40099932983428915\n",
            "Iteration#:  6324 ; Loss:  0.4866181963585151 ; l2 norm of gradient:  0.008069618142264822 ; l2 norm of weights:  0.4009961716376165\n",
            "Iteration#:  6328 ; Loss:  0.4866097632433424 ; l2 norm of gradient:  0.008036559868927356 ; l2 norm of weights:  0.40099302918490304\n",
            "Iteration#:  6332 ; Loss:  0.4866013448508385 ; l2 norm of gradient:  0.008003654059025086 ; l2 norm of weights:  0.40098990239421667\n",
            "Iteration#:  6336 ; Loss:  0.4865929411428276 ; l2 norm of gradient:  0.00797090005210674 ; l2 norm of weights:  0.40098679118416847\n",
            "Iteration#:  6340 ; Loss:  0.48658455208132684 ; l2 norm of gradient:  0.007938297190620417 ; l2 norm of weights:  0.4009836954739079\n",
            "Iteration#:  6344 ; Loss:  0.48657617762854527 ; l2 norm of gradient:  0.007905844819898391 ; l2 norm of weights:  0.4009806151831191\n",
            "Iteration#:  6348 ; Loss:  0.4865678177468827 ; l2 norm of gradient:  0.00787354228814477 ; l2 norm of weights:  0.4009775502320173\n",
            "Iteration#:  6352 ; Loss:  0.4865594723989279 ; l2 norm of gradient:  0.007841388946422502 ; l2 norm of weights:  0.4009745005413436\n",
            "Iteration#:  6356 ; Loss:  0.4865511415474572 ; l2 norm of gradient:  0.007809384148638594 ; l2 norm of weights:  0.4009714660323622\n",
            "Iteration#:  6360 ; Loss:  0.486542825155433 ; l2 norm of gradient:  0.007777527251532063 ; l2 norm of weights:  0.400968446626856\n",
            "Iteration#:  6364 ; Loss:  0.48653452318600293 ; l2 norm of gradient:  0.007745817614660624 ; l2 norm of weights:  0.4009654422471221\n",
            "Iteration#:  6368 ; Loss:  0.48652623560249736 ; l2 norm of gradient:  0.007714254600386891 ; l2 norm of weights:  0.40096245281596904\n",
            "Iteration#:  6372 ; Loss:  0.4865179623684289 ; l2 norm of gradient:  0.00768283757386598 ; l2 norm of weights:  0.40095947825671213\n",
            "Iteration#:  6376 ; Loss:  0.4865097034474909 ; l2 norm of gradient:  0.007651565903031947 ; l2 norm of weights:  0.40095651849316977\n",
            "Iteration#:  6380 ; Loss:  0.48650145880355566 ; l2 norm of gradient:  0.0076204389585853615 ; l2 norm of weights:  0.4009535734496599\n",
            "Iteration#:  6384 ; Loss:  0.48649322840067366 ; l2 norm of gradient:  0.007589456113979698 ; l2 norm of weights:  0.400950643050996\n",
            "Iteration#:  6388 ; Loss:  0.4864850122030716 ; l2 norm of gradient:  0.007558616745408817 ; l2 norm of weights:  0.40094772722248334\n",
            "Iteration#:  6392 ; Loss:  0.4864768101751516 ; l2 norm of gradient:  0.007527920231794476 ; l2 norm of weights:  0.4009448258899156\n",
            "Iteration#:  6396 ; Loss:  0.48646862228148946 ; l2 norm of gradient:  0.007497365954772967 ; l2 norm of weights:  0.4009419389795707\n",
            "Iteration#:  6400 ; Loss:  0.48646044848683373 ; l2 norm of gradient:  0.007466953298682449 ; l2 norm of weights:  0.4009390664182076\n",
            "Iteration#:  6404 ; Loss:  0.48645228875610413 ; l2 norm of gradient:  0.00743668165055087 ; l2 norm of weights:  0.40093620813306236\n",
            "Iteration#:  6408 ; Loss:  0.4864441430543903 ; l2 norm of gradient:  0.007406550400082629 ; l2 norm of weights:  0.40093336405184493\n",
            "Iteration#:  6412 ; Loss:  0.486436011346951 ; l2 norm of gradient:  0.007376558939646586 ; l2 norm of weights:  0.4009305341027351\n",
            "Iteration#:  6416 ; Loss:  0.4864278935992118 ; l2 norm of gradient:  0.007346706664263028 ; l2 norm of weights:  0.40092771821437934\n",
            "Iteration#:  6420 ; Loss:  0.4864197897767648 ; l2 norm of gradient:  0.0073169929715916175 ; l2 norm of weights:  0.4009249163158872\n",
            "Iteration#:  6424 ; Loss:  0.486411699845367 ; l2 norm of gradient:  0.007287417261918927 ; l2 norm of weights:  0.40092212833682794\n",
            "Iteration#:  6428 ; Loss:  0.48640362377093926 ; l2 norm of gradient:  0.007257978938145781 ; l2 norm of weights:  0.40091935420722663\n",
            "Iteration#:  6432 ; Loss:  0.48639556151956465 ; l2 norm of gradient:  0.007228677405775535 ; l2 norm of weights:  0.4009165938575615\n",
            "Iteration#:  6436 ; Loss:  0.48638751305748773 ; l2 norm of gradient:  0.0071995120729013325 ; l2 norm of weights:  0.4009138472187598\n",
            "Iteration#:  6440 ; Loss:  0.4863794783511129 ; l2 norm of gradient:  0.007170482350193606 ; l2 norm of weights:  0.4009111142221948\n",
            "Iteration#:  6444 ; Loss:  0.4863714573670039 ; l2 norm of gradient:  0.007141587650889068 ; l2 norm of weights:  0.4009083947996827\n",
            "Iteration#:  6448 ; Loss:  0.48636345007188153 ; l2 norm of gradient:  0.007112827390777297 ; l2 norm of weights:  0.4009056888834786\n",
            "Iteration#:  6452 ; Loss:  0.48635545643262357 ; l2 norm of gradient:  0.007084200988189552 ; l2 norm of weights:  0.400902996406274\n",
            "Iteration#:  6456 ; Loss:  0.4863474764162632 ; l2 norm of gradient:  0.007055707863986549 ; l2 norm of weights:  0.4009003173011931\n",
            "Iteration#:  6460 ; Loss:  0.48633950998998765 ; l2 norm of gradient:  0.007027347441546542 ; l2 norm of weights:  0.40089765150178963\n",
            "Iteration#:  6464 ; Loss:  0.4863315571211371 ; l2 norm of gradient:  0.006999119146752846 ; l2 norm of weights:  0.40089499894204356\n",
            "Iteration#:  6468 ; Loss:  0.486323617777204 ; l2 norm of gradient:  0.0069710224079829915 ; l2 norm of weights:  0.4008923595563584\n",
            "Iteration#:  6472 ; Loss:  0.4863156919258314 ; l2 norm of gradient:  0.006943056656095991 ; l2 norm of weights:  0.4008897332795571\n",
            "Iteration#:  6476 ; Loss:  0.4863077795348122 ; l2 norm of gradient:  0.00691522132442154 ; l2 norm of weights:  0.4008871200468803\n",
            "Iteration#:  6480 ; Loss:  0.4862998805720875 ; l2 norm of gradient:  0.006887515848747395 ; l2 norm of weights:  0.4008845197939816\n",
            "Iteration#:  6484 ; Loss:  0.4862919950057466 ; l2 norm of gradient:  0.006859939667307837 ; l2 norm of weights:  0.40088193245692594\n",
            "Iteration#:  6488 ; Loss:  0.4862841228040247 ; l2 norm of gradient:  0.0068324922207728955 ; l2 norm of weights:  0.40087935797218555\n",
            "Iteration#:  6492 ; Loss:  0.4862762639353026 ; l2 norm of gradient:  0.006805172952235741 ; l2 norm of weights:  0.4008767962766375\n",
            "Iteration#:  6496 ; Loss:  0.48626841836810525 ; l2 norm of gradient:  0.00677798130720192 ; l2 norm of weights:  0.4008742473075603\n",
            "Iteration#:  6500 ; Loss:  0.48626058607110095 ; l2 norm of gradient:  0.006750916733577404 ; l2 norm of weights:  0.4008717110026314\n",
            "Iteration#:  6504 ; Loss:  0.4862527670131 ; l2 norm of gradient:  0.006723978681657423 ; l2 norm of weights:  0.4008691872999236\n",
            "Iteration#:  6508 ; Loss:  0.48624496116305405 ; l2 norm of gradient:  0.0066971666041149925 ; l2 norm of weights:  0.40086667613790256\n",
            "Iteration#:  6512 ; Loss:  0.48623716849005466 ; l2 norm of gradient:  0.006670479955989639 ; l2 norm of weights:  0.4008641774554238\n",
            "Iteration#:  6516 ; Loss:  0.4862293889633327 ; l2 norm of gradient:  0.006643918194676027 ; l2 norm of weights:  0.4008616911917298\n",
            "Iteration#:  6520 ; Loss:  0.486221622552257 ; l2 norm of gradient:  0.006617480779913132 ; l2 norm of weights:  0.40085921728644713\n",
            "Iteration#:  6524 ; Loss:  0.4862138692263334 ; l2 norm of gradient:  0.006591167173772403 ; l2 norm of weights:  0.40085675567958357\n",
            "Iteration#:  6528 ; Loss:  0.486206128955204 ; l2 norm of gradient:  0.006564976840646946 ; l2 norm of weights:  0.4008543063115253\n",
            "Iteration#:  6532 ; Loss:  0.4861984017086458 ; l2 norm of gradient:  0.006538909247240759 ; l2 norm of weights:  0.4008518691230341\n",
            "Iteration#:  6536 ; Loss:  0.4861906874565699 ; l2 norm of gradient:  0.0065129638625569496 ; l2 norm of weights:  0.40084944405524464\n",
            "Iteration#:  6540 ; Loss:  0.4861829861690208 ; l2 norm of gradient:  0.006487140157887616 ; l2 norm of weights:  0.4008470310496618\n",
            "Iteration#:  6544 ; Loss:  0.4861752978161748 ; l2 norm of gradient:  0.006461437606801626 ; l2 norm of weights:  0.40084463004815757\n",
            "Iteration#:  6548 ; Loss:  0.48616762236833966 ; l2 norm of gradient:  0.006435855685135541 ; l2 norm of weights:  0.4008422409929687\n",
            "Iteration#:  6552 ; Loss:  0.4861599597959536 ; l2 norm of gradient:  0.006410393870980915 ; l2 norm of weights:  0.4008398638266941\n",
            "Iteration#:  6556 ; Loss:  0.48615231006958365 ; l2 norm of gradient:  0.006385051644674719 ; l2 norm of weights:  0.40083749849229167\n",
            "Iteration#:  6560 ; Loss:  0.4861446731599256 ; l2 norm of gradient:  0.006359828488787938 ; l2 norm of weights:  0.40083514493307604\n",
            "Iteration#:  6564 ; Loss:  0.48613704903780275 ; l2 norm of gradient:  0.006334723888114953 ; l2 norm of weights:  0.4008328030927159\n",
            "Iteration#:  6568 ; Loss:  0.486129437674165 ; l2 norm of gradient:  0.006309737329663101 ; l2 norm of weights:  0.4008304729152315\n",
            "Iteration#:  6572 ; Loss:  0.48612183904008743 ; l2 norm of gradient:  0.006284868302641798 ; l2 norm of weights:  0.40082815434499175\n",
            "Iteration#:  6576 ; Loss:  0.48611425310677064 ; l2 norm of gradient:  0.006260116298451923 ; l2 norm of weights:  0.40082584732671195\n",
            "Iteration#:  6580 ; Loss:  0.48610667984553835 ; l2 norm of gradient:  0.00623548081067567 ; l2 norm of weights:  0.4008235518054509\n",
            "Iteration#:  6584 ; Loss:  0.48609911922783783 ; l2 norm of gradient:  0.006210961335065499 ; l2 norm of weights:  0.4008212677266088\n",
            "Iteration#:  6588 ; Loss:  0.4860915712252381 ; l2 norm of gradient:  0.006186557369533342 ; l2 norm of weights:  0.4008189950359248\n",
            "Iteration#:  6592 ; Loss:  0.4860840358094296 ; l2 norm of gradient:  0.006162268414141872 ; l2 norm of weights:  0.4008167336794737\n",
            "Iteration#:  6596 ; Loss:  0.4860765129522233 ; l2 norm of gradient:  0.006138093971092046 ; l2 norm of weights:  0.40081448360366445\n",
            "Iteration#:  6600 ; Loss:  0.4860690026255492 ; l2 norm of gradient:  0.006114033544714442 ; l2 norm of weights:  0.40081224475523736\n",
            "Iteration#:  6604 ; Loss:  0.4860615048014564 ; l2 norm of gradient:  0.006090086641457534 ; l2 norm of weights:  0.4008100170812614\n",
            "Iteration#:  6608 ; Loss:  0.4860540194521117 ; l2 norm of gradient:  0.006066252769878839 ; l2 norm of weights:  0.4008078005291322\n",
            "Iteration#:  6612 ; Loss:  0.4860465465497986 ; l2 norm of gradient:  0.006042531440633431 ; l2 norm of weights:  0.4008055950465693\n",
            "Iteration#:  6616 ; Loss:  0.4860390860669176 ; l2 norm of gradient:  0.0060189221664648 ; l2 norm of weights:  0.4008034005816144\n",
            "Iteration#:  6620 ; Loss:  0.48603163797598364 ; l2 norm of gradient:  0.005995424462194113 ; l2 norm of weights:  0.400801217082628\n",
            "Iteration#:  6624 ; Loss:  0.4860242022496267 ; l2 norm of gradient:  0.005972037844710369 ; l2 norm of weights:  0.4007990444982881\n",
            "Iteration#:  6628 ; Loss:  0.48601677886059025 ; l2 norm of gradient:  0.0059487618329603816 ; l2 norm of weights:  0.4007968827775873\n",
            "Iteration#:  6632 ; Loss:  0.48600936778173087 ; l2 norm of gradient:  0.005925595947939033 ; l2 norm of weights:  0.40079473186983067\n",
            "Iteration#:  6636 ; Loss:  0.4860019689860172 ; l2 norm of gradient:  0.005902539712678172 ; l2 norm of weights:  0.4007925917246334\n",
            "Iteration#:  6640 ; Loss:  0.4859945824465294 ; l2 norm of gradient:  0.005879592652238756 ; l2 norm of weights:  0.4007904622919187\n",
            "Iteration#:  6644 ; Loss:  0.4859872081364578 ; l2 norm of gradient:  0.005856754293698837 ; l2 norm of weights:  0.4007883435219154\n",
            "Iteration#:  6648 ; Loss:  0.48597984602910294 ; l2 norm of gradient:  0.005834024166145291 ; l2 norm of weights:  0.4007862353651557\n",
            "Iteration#:  6652 ; Loss:  0.4859724960978742 ; l2 norm of gradient:  0.005811401800663 ; l2 norm of weights:  0.4007841377724733\n",
            "Iteration#:  6656 ; Loss:  0.4859651583162893 ; l2 norm of gradient:  0.0057888867303260835 ; l2 norm of weights:  0.4007820506950008\n",
            "Iteration#:  6660 ; Loss:  0.48595783265797343 ; l2 norm of gradient:  0.005766478490187459 ; l2 norm of weights:  0.4007799740841675\n",
            "Iteration#:  6664 ; Loss:  0.48595051909665865 ; l2 norm of gradient:  0.005744176617268999 ; l2 norm of weights:  0.40077790789169787\n",
            "Iteration#:  6668 ; Loss:  0.4859432176061829 ; l2 norm of gradient:  0.005721980650553181 ; l2 norm of weights:  0.40077585206960886\n",
            "Iteration#:  6672 ; Loss:  0.48593592816048964 ; l2 norm of gradient:  0.0056998901309718115 ; l2 norm of weights:  0.4007738065702077\n",
            "Iteration#:  6676 ; Loss:  0.4859286507336267 ; l2 norm of gradient:  0.005677904601397994 ; l2 norm of weights:  0.40077177134609027\n",
            "Iteration#:  6680 ; Loss:  0.4859213852997459 ; l2 norm of gradient:  0.005656023606635706 ; l2 norm of weights:  0.40076974635013835\n",
            "Iteration#:  6684 ; Loss:  0.48591413183310195 ; l2 norm of gradient:  0.005634246693410647 ; l2 norm of weights:  0.40076773153551853\n",
            "Iteration#:  6688 ; Loss:  0.48590689030805245 ; l2 norm of gradient:  0.005612573410360859 ; l2 norm of weights:  0.4007657268556791\n",
            "Iteration#:  6692 ; Loss:  0.4858996606990561 ; l2 norm of gradient:  0.005591003308027186 ; l2 norm of weights:  0.4007637322643489\n",
            "Iteration#:  6696 ; Loss:  0.4858924429806732 ; l2 norm of gradient:  0.005569535938844263 ; l2 norm of weights:  0.4007617477155344\n",
            "Iteration#:  6700 ; Loss:  0.48588523712756404 ; l2 norm of gradient:  0.005548170857130935 ; l2 norm of weights:  0.4007597731635188\n",
            "Iteration#:  6704 ; Loss:  0.4858780431144887 ; l2 norm of gradient:  0.005526907619081089 ; l2 norm of weights:  0.40075780856285886\n",
            "Iteration#:  6708 ; Loss:  0.48587086091630605 ; l2 norm of gradient:  0.005505745782754475 ; l2 norm of weights:  0.4007558538683837\n",
            "Iteration#:  6712 ; Loss:  0.4858636905079734 ; l2 norm of gradient:  0.005484684908067524 ; l2 norm of weights:  0.40075390903519287\n",
            "Iteration#:  6716 ; Loss:  0.4858565318645456 ; l2 norm of gradient:  0.005463724556784496 ; l2 norm of weights:  0.40075197401865376\n",
            "Iteration#:  6720 ; Loss:  0.48584938496117447 ; l2 norm of gradient:  0.005442864292507457 ; l2 norm of weights:  0.40075004877440057\n",
            "Iteration#:  6724 ; Loss:  0.4858422497731082 ; l2 norm of gradient:  0.005422103680668524 ; l2 norm of weights:  0.40074813325833136\n",
            "Iteration#:  6728 ; Loss:  0.4858351262756905 ; l2 norm of gradient:  0.0054014422885199295 ; l2 norm of weights:  0.4007462274266072\n",
            "Iteration#:  6732 ; Loss:  0.48582801444436 ; l2 norm of gradient:  0.005380879685125488 ; l2 norm of weights:  0.40074433123564945\n",
            "Iteration#:  6736 ; Loss:  0.48582091425464985 ; l2 norm of gradient:  0.005360415441351277 ; l2 norm of weights:  0.4007424446421386\n",
            "Iteration#:  6740 ; Loss:  0.48581382568218695 ; l2 norm of gradient:  0.0053400491298574776 ; l2 norm of weights:  0.4007405676030117\n",
            "Iteration#:  6744 ; Loss:  0.4858067487026911 ; l2 norm of gradient:  0.00531978032508844 ; l2 norm of weights:  0.4007387000754609\n",
            "Iteration#:  6748 ; Loss:  0.48579968329197437 ; l2 norm of gradient:  0.00529960860326472 ; l2 norm of weights:  0.4007368420169319\n",
            "Iteration#:  6752 ; Loss:  0.48579262942594115 ; l2 norm of gradient:  0.0052795335423743445 ; l2 norm of weights:  0.40073499338512153\n",
            "Iteration#:  6756 ; Loss:  0.48578558708058683 ; l2 norm of gradient:  0.005259554722163326 ; l2 norm of weights:  0.40073315413797644\n",
            "Iteration#:  6760 ; Loss:  0.4857785562319972 ; l2 norm of gradient:  0.005239671724127418 ; l2 norm of weights:  0.40073132423369107\n",
            "Iteration#:  6764 ; Loss:  0.4857715368563485 ; l2 norm of gradient:  0.005219884131503975 ; l2 norm of weights:  0.4007295036307061\n",
            "Iteration#:  6768 ; Loss:  0.48576452892990585 ; l2 norm of gradient:  0.005200191529262428 ; l2 norm of weights:  0.4007276922877063\n",
            "Iteration#:  6772 ; Loss:  0.4857575324290234 ; l2 norm of gradient:  0.0051805935040961995 ; l2 norm of weights:  0.4007258901636191\n",
            "Iteration#:  6776 ; Loss:  0.4857505473301435 ; l2 norm of gradient:  0.005161089644414047 ; l2 norm of weights:  0.40072409721761304\n",
            "Iteration#:  6780 ; Loss:  0.4857435736097962 ; l2 norm of gradient:  0.005141679540331633 ; l2 norm of weights:  0.40072231340909564\n",
            "Iteration#:  6784 ; Loss:  0.48573661124459855 ; l2 norm of gradient:  0.005122362783662741 ; l2 norm of weights:  0.40072053869771174\n",
            "Iteration#:  6788 ; Loss:  0.4857296602112539 ; l2 norm of gradient:  0.005103138967911725 ; l2 norm of weights:  0.40071877304334225\n",
            "Iteration#:  6792 ; Loss:  0.4857227204865517 ; l2 norm of gradient:  0.005084007688263928 ; l2 norm of weights:  0.400717016406102\n",
            "Iteration#:  6796 ; Loss:  0.48571579204736653 ; l2 norm of gradient:  0.005064968541578144 ; l2 norm of weights:  0.4007152687463382\n",
            "Iteration#:  6800 ; Loss:  0.48570887487065806 ; l2 norm of gradient:  0.005046021126378074 ; l2 norm of weights:  0.40071353002462917\n",
            "Iteration#:  6804 ; Loss:  0.4857019689334698 ; l2 norm of gradient:  0.005027165042844316 ; l2 norm of weights:  0.400711800201782\n",
            "Iteration#:  6808 ; Loss:  0.485695074212929 ; l2 norm of gradient:  0.005008399892805265 ; l2 norm of weights:  0.4007100792388314\n",
            "Iteration#:  6812 ; Loss:  0.4856881906862463 ; l2 norm of gradient:  0.004989725279730365 ; l2 norm of weights:  0.40070836709703817\n",
            "Iteration#:  6816 ; Loss:  0.48568131833071443 ; l2 norm of gradient:  0.004971140808720299 ; l2 norm of weights:  0.4007066637378874\n",
            "Iteration#:  6820 ; Loss:  0.4856744571237085 ; l2 norm of gradient:  0.004952646086500414 ; l2 norm of weights:  0.40070496912308684\n",
            "Iteration#:  6824 ; Loss:  0.4856676070426849 ; l2 norm of gradient:  0.004934240721411209 ; l2 norm of weights:  0.40070328321456544\n",
            "Iteration#:  6828 ; Loss:  0.48566076806518116 ; l2 norm of gradient:  0.0049159243234015906 ; l2 norm of weights:  0.4007016059744717\n",
            "Iteration#:  6832 ; Loss:  0.485653940168815 ; l2 norm of gradient:  0.004897696504020301 ; l2 norm of weights:  0.4006999373651722\n",
            "Iteration#:  6836 ; Loss:  0.48564712333128435 ; l2 norm of gradient:  0.004879556876407309 ; l2 norm of weights:  0.4006982773492503\n",
            "Iteration#:  6840 ; Loss:  0.48564031753036624 ; l2 norm of gradient:  0.004861505055287024 ; l2 norm of weights:  0.40069662588950394\n",
            "Iteration#:  6844 ; Loss:  0.4856335227439166 ; l2 norm of gradient:  0.00484354065695991 ; l2 norm of weights:  0.4006949829489449\n",
            "Iteration#:  6848 ; Loss:  0.48562673894986996 ; l2 norm of gradient:  0.0048256632992941416 ; l2 norm of weights:  0.40069334849079663\n",
            "Iteration#:  6852 ; Loss:  0.4856199661262385 ; l2 norm of gradient:  0.004807872601718681 ; l2 norm of weights:  0.4006917224784935\n",
            "Iteration#:  6856 ; Loss:  0.48561320425111176 ; l2 norm of gradient:  0.004790168185214738 ; l2 norm of weights:  0.40069010487567847\n",
            "Iteration#:  6860 ; Loss:  0.48560645330265634 ; l2 norm of gradient:  0.004772549672308663 ; l2 norm of weights:  0.40068849564620235\n",
            "Iteration#:  6864 ; Loss:  0.4855997132591149 ; l2 norm of gradient:  0.00475501668706379 ; l2 norm of weights:  0.40068689475412195\n",
            "Iteration#:  6868 ; Loss:  0.48559298409880614 ; l2 norm of gradient:  0.004737568855072763 ; l2 norm of weights:  0.40068530216369863\n",
            "Iteration#:  6872 ; Loss:  0.4855862658001242 ; l2 norm of gradient:  0.0047202058034501865 ; l2 norm of weights:  0.4006837178393972\n",
            "Iteration#:  6876 ; Loss:  0.4855795583415383 ; l2 norm of gradient:  0.0047029271608248005 ; l2 norm of weights:  0.4006821417458842\n",
            "Iteration#:  6880 ; Loss:  0.48557286170159153 ; l2 norm of gradient:  0.004685732557332174 ; l2 norm of weights:  0.4006805738480264\n",
            "Iteration#:  6884 ; Loss:  0.4855661758589016 ; l2 norm of gradient:  0.004668621624606763 ; l2 norm of weights:  0.40067901411088974\n",
            "Iteration#:  6888 ; Loss:  0.48555950079215926 ; l2 norm of gradient:  0.004651593995775196 ; l2 norm of weights:  0.4006774624997377\n",
            "Iteration#:  6892 ; Loss:  0.48555283648012837 ; l2 norm of gradient:  0.004634649305447468 ; l2 norm of weights:  0.4006759189800301\n",
            "Iteration#:  6896 ; Loss:  0.4855461829016456 ; l2 norm of gradient:  0.004617787189711032 ; l2 norm of weights:  0.4006743835174215\n",
            "Iteration#:  6900 ; Loss:  0.4855395400356196 ; l2 norm of gradient:  0.004601007286122703 ; l2 norm of weights:  0.40067285607775993\n",
            "Iteration#:  6904 ; Loss:  0.4855329078610305 ; l2 norm of gradient:  0.004584309233701348 ; l2 norm of weights:  0.4006713366270857\n",
            "Iteration#:  6908 ; Loss:  0.4855262863569298 ; l2 norm of gradient:  0.004567692672920768 ; l2 norm of weights:  0.4006698251316299\n",
            "Iteration#:  6912 ; Loss:  0.4855196755024399 ; l2 norm of gradient:  0.004551157245701668 ; l2 norm of weights:  0.40066832155781323\n",
            "Iteration#:  6916 ; Loss:  0.48551307527675314 ; l2 norm of gradient:  0.0045347025954060585 ; l2 norm of weights:  0.4006668258722442\n",
            "Iteration#:  6920 ; Loss:  0.4855064856591322 ; l2 norm of gradient:  0.0045183283668281905 ; l2 norm of weights:  0.4006653380417188\n",
            "Iteration#:  6924 ; Loss:  0.48549990662890863 ; l2 norm of gradient:  0.004502034206188961 ; l2 norm of weights:  0.40066385803321797\n",
            "Iteration#:  6928 ; Loss:  0.4854933381654836 ; l2 norm of gradient:  0.004485819761127712 ; l2 norm of weights:  0.4006623858139074\n",
            "Iteration#:  6932 ; Loss:  0.48548678024832653 ; l2 norm of gradient:  0.004469684680696018 ; l2 norm of weights:  0.40066092135113557\n",
            "Iteration#:  6936 ; Loss:  0.4854802328569751 ; l2 norm of gradient:  0.004453628615349477 ; l2 norm of weights:  0.4006594646124329\n",
            "Iteration#:  6940 ; Loss:  0.48547369597103446 ; l2 norm of gradient:  0.004437651216942257 ; l2 norm of weights:  0.4006580155655101\n",
            "Iteration#:  6944 ; Loss:  0.48546716957017755 ; l2 norm of gradient:  0.004421752138718718 ; l2 norm of weights:  0.40065657417825723\n",
            "Iteration#:  6948 ; Loss:  0.48546065363414376 ; l2 norm of gradient:  0.004405931035307408 ; l2 norm of weights:  0.40065514041874256\n",
            "Iteration#:  6952 ; Loss:  0.48545414814273935 ; l2 norm of gradient:  0.004390187562713723 ; l2 norm of weights:  0.40065371425521085\n",
            "Iteration#:  6956 ; Loss:  0.48544765307583626 ; l2 norm of gradient:  0.004374521378313146 ; l2 norm of weights:  0.40065229565608257\n",
            "Iteration#:  6960 ; Loss:  0.4854411684133726 ; l2 norm of gradient:  0.004358932140844371 ; l2 norm of weights:  0.4006508845899527\n",
            "Iteration#:  6964 ; Loss:  0.4854346941353512 ; l2 norm of gradient:  0.004343419510402937 ; l2 norm of weights:  0.400649481025589\n",
            "Iteration#:  6968 ; Loss:  0.48542823022184023 ; l2 norm of gradient:  0.0043279831484334405 ; l2 norm of weights:  0.4006480849319316\n",
            "Iteration#:  6972 ; Loss:  0.485421776652972 ; l2 norm of gradient:  0.004312622717724416 ; l2 norm of weights:  0.40064669627809135\n",
            "Iteration#:  6976 ; Loss:  0.4854153334089432 ; l2 norm of gradient:  0.004297337882400027 ; l2 norm of weights:  0.40064531503334855\n",
            "Iteration#:  6980 ; Loss:  0.4854089004700142 ; l2 norm of gradient:  0.004282128307914484 ; l2 norm of weights:  0.40064394116715213\n",
            "Iteration#:  6984 ; Loss:  0.48540247781650825 ; l2 norm of gradient:  0.004266993661045137 ; l2 norm of weights:  0.40064257464911823\n",
            "Iteration#:  6988 ; Loss:  0.4853960654288122 ; l2 norm of gradient:  0.004251933609885834 ; l2 norm of weights:  0.4006412154490292\n",
            "Iteration#:  6992 ; Loss:  0.485389663287375 ; l2 norm of gradient:  0.00423694782384045 ; l2 norm of weights:  0.40063986353683256\n",
            "Iteration#:  6996 ; Loss:  0.4853832713727077 ; l2 norm of gradient:  0.004222035973616536 ; l2 norm of weights:  0.40063851888263935\n",
            "Iteration#:  7000 ; Loss:  0.48537688966538356 ; l2 norm of gradient:  0.004207197731218374 ; l2 norm of weights:  0.4006371814567239\n",
            "Iteration#:  7004 ; Loss:  0.4853705181460371 ; l2 norm of gradient:  0.004192432769941206 ; l2 norm of weights:  0.4006358512295218\n",
            "Iteration#:  7008 ; Loss:  0.4853641567953636 ; l2 norm of gradient:  0.004177740764364647 ; l2 norm of weights:  0.40063452817162937\n",
            "Iteration#:  7012 ; Loss:  0.48535780559411956 ; l2 norm of gradient:  0.004163121390346037 ; l2 norm of weights:  0.4006332122538025\n",
            "Iteration#:  7016 ; Loss:  0.4853514645231216 ; l2 norm of gradient:  0.004148574325014254 ; l2 norm of weights:  0.4006319034469552\n",
            "Iteration#:  7020 ; Loss:  0.48534513356324616 ; l2 norm of gradient:  0.004134099246763912 ; l2 norm of weights:  0.40063060172215914\n",
            "Iteration#:  7024 ; Loss:  0.48533881269542933 ; l2 norm of gradient:  0.004119695835248357 ; l2 norm of weights:  0.40062930705064204\n",
            "Iteration#:  7028 ; Loss:  0.4853325019006668 ; l2 norm of gradient:  0.004105363771373884 ; l2 norm of weights:  0.4006280194037867\n",
            "Iteration#:  7032 ; Loss:  0.48532620116001274 ; l2 norm of gradient:  0.004091102737293779 ; l2 norm of weights:  0.4006267387531302\n",
            "Iteration#:  7036 ; Loss:  0.48531991045458034 ; l2 norm of gradient:  0.004076912416401559 ; l2 norm of weights:  0.40062546507036295\n",
            "Iteration#:  7040 ; Loss:  0.4853136297655409 ; l2 norm of gradient:  0.00406279249332579 ; l2 norm of weights:  0.40062419832732693\n",
            "Iteration#:  7044 ; Loss:  0.48530735907412315 ; l2 norm of gradient:  0.004048742653922943 ; l2 norm of weights:  0.4006229384960154\n",
            "Iteration#:  7048 ; Loss:  0.48530109836161384 ; l2 norm of gradient:  0.004034762585272339 ; l2 norm of weights:  0.4006216855485717\n",
            "Iteration#:  7052 ; Loss:  0.4852948476093569 ; l2 norm of gradient:  0.004020851975669396 ; l2 norm of weights:  0.400620439457288\n",
            "Iteration#:  7056 ; Loss:  0.4852886067987532 ; l2 norm of gradient:  0.004007010514620426 ; l2 norm of weights:  0.4006192001946045\n",
            "Iteration#:  7060 ; Loss:  0.4852823759112599 ; l2 norm of gradient:  0.003993237892835918 ; l2 norm of weights:  0.40061796773310837\n",
            "Iteration#:  7064 ; Loss:  0.48527615492839027 ; l2 norm of gradient:  0.00397953380222527 ; l2 norm of weights:  0.4006167420455329\n",
            "Iteration#:  7068 ; Loss:  0.4852699438317141 ; l2 norm of gradient:  0.003965897935890766 ; l2 norm of weights:  0.4006155231047563\n",
            "Iteration#:  7072 ; Loss:  0.4852637426028561 ; l2 norm of gradient:  0.003952329988121847 ; l2 norm of weights:  0.4006143108838008\n",
            "Iteration#:  7076 ; Loss:  0.4852575512234967 ; l2 norm of gradient:  0.00393882965438898 ; l2 norm of weights:  0.40061310535583167\n",
            "Iteration#:  7080 ; Loss:  0.48525136967537086 ; l2 norm of gradient:  0.003925396631338386 ; l2 norm of weights:  0.40061190649415646\n",
            "Iteration#:  7084 ; Loss:  0.4852451979402686 ; l2 norm of gradient:  0.0039120306167862474 ; l2 norm of weights:  0.4006107142722237\n",
            "Iteration#:  7088 ; Loss:  0.4852390360000338 ; l2 norm of gradient:  0.00389873130971258 ; l2 norm of weights:  0.40060952866362237\n",
            "Iteration#:  7092 ; Loss:  0.48523288383656465 ; l2 norm of gradient:  0.003885498410256363 ; l2 norm of weights:  0.40060834964208025\n",
            "Iteration#:  7096 ; Loss:  0.4852267414318128 ; l2 norm of gradient:  0.0038723316197091613 ; l2 norm of weights:  0.40060717718146405\n",
            "Iteration#:  7100 ; Loss:  0.48522060876778356 ; l2 norm of gradient:  0.0038592306405102073 ; l2 norm of weights:  0.40060601125577744\n",
            "Iteration#:  7104 ; Loss:  0.48521448582653515 ; l2 norm of gradient:  0.0038461951762407544 ; l2 norm of weights:  0.40060485183916084\n",
            "Iteration#:  7108 ; Loss:  0.4852083725901785 ; l2 norm of gradient:  0.003833224931618234 ; l2 norm of weights:  0.4006036989058902\n",
            "Iteration#:  7112 ; Loss:  0.48520226904087704 ; l2 norm of gradient:  0.0038203196124911186 ; l2 norm of weights:  0.4006025524303762\n",
            "Iteration#:  7116 ; Loss:  0.4851961751608467 ; l2 norm of gradient:  0.003807478925833381 ; l2 norm of weights:  0.400601412387163\n",
            "Iteration#:  7120 ; Loss:  0.48519009093235477 ; l2 norm of gradient:  0.003794702579739423 ; l2 norm of weights:  0.40060027875092813\n",
            "Iteration#:  7124 ; Loss:  0.48518401633772057 ; l2 norm of gradient:  0.003781990283418098 ; l2 norm of weights:  0.4005991514964809\n",
            "Iteration#:  7128 ; Loss:  0.4851779513593144 ; l2 norm of gradient:  0.0037693417471883614 ; l2 norm of weights:  0.4005980305987616\n",
            "Iteration#:  7132 ; Loss:  0.4851718959795581 ; l2 norm of gradient:  0.0037567566824726266 ; l2 norm of weights:  0.40059691603284114\n",
            "Iteration#:  7136 ; Loss:  0.48516585018092356 ; l2 norm of gradient:  0.003744234801793209 ; l2 norm of weights:  0.4005958077739195\n",
            "Iteration#:  7140 ; Loss:  0.48515981394593366 ; l2 norm of gradient:  0.0037317758187655234 ; l2 norm of weights:  0.40059470579732553\n",
            "Iteration#:  7144 ; Loss:  0.48515378725716124 ; l2 norm of gradient:  0.003719379448094301 ; l2 norm of weights:  0.40059361007851535\n",
            "Iteration#:  7148 ; Loss:  0.485147770097229 ; l2 norm of gradient:  0.0037070454055672486 ; l2 norm of weights:  0.4005925205930724\n",
            "Iteration#:  7152 ; Loss:  0.48514176244880913 ; l2 norm of gradient:  0.0036947734080508762 ; l2 norm of weights:  0.40059143731670566\n",
            "Iteration#:  7156 ; Loss:  0.48513576429462396 ; l2 norm of gradient:  0.003682563173484718 ; l2 norm of weights:  0.40059036022524974\n",
            "Iteration#:  7160 ; Loss:  0.48512977561744375 ; l2 norm of gradient:  0.003670414420877248 ; l2 norm of weights:  0.4005892892946633\n",
            "Iteration#:  7164 ; Loss:  0.48512379640008835 ; l2 norm of gradient:  0.003658326870299848 ; l2 norm of weights:  0.4005882245010287\n",
            "Iteration#:  7168 ; Loss:  0.48511782662542574 ; l2 norm of gradient:  0.0036463002428821552 ; l2 norm of weights:  0.4005871658205508\n",
            "Iteration#:  7172 ; Loss:  0.4851118662763727 ; l2 norm of gradient:  0.003634334260808128 ; l2 norm of weights:  0.40058611322955673\n",
            "Iteration#:  7176 ; Loss:  0.48510591533589337 ; l2 norm of gradient:  0.0036224286473096456 ; l2 norm of weights:  0.40058506670449445\n",
            "Iteration#:  7180 ; Loss:  0.4850999737870002 ; l2 norm of gradient:  0.0036105831266623465 ; l2 norm of weights:  0.4005840262219323\n",
            "Iteration#:  7184 ; Loss:  0.48509404161275266 ; l2 norm of gradient:  0.0035987974241813193 ; l2 norm of weights:  0.40058299175855816\n",
            "Iteration#:  7188 ; Loss:  0.4850881187962578 ; l2 norm of gradient:  0.0035870712662154276 ; l2 norm of weights:  0.4005819632911785\n",
            "Iteration#:  7192 ; Loss:  0.4850822053206696 ; l2 norm of gradient:  0.0035754043801431057 ; l2 norm of weights:  0.4005809407967183\n",
            "Iteration#:  7196 ; Loss:  0.48507630116918854 ; l2 norm of gradient:  0.003563796494367778 ; l2 norm of weights:  0.400579924252219\n",
            "Iteration#:  7200 ; Loss:  0.4850704063250617 ; l2 norm of gradient:  0.003552247338312434 ; l2 norm of weights:  0.40057891363483894\n",
            "Iteration#:  7204 ; Loss:  0.4850645207715828 ; l2 norm of gradient:  0.003540756642415832 ; l2 norm of weights:  0.4005779089218521\n",
            "Iteration#:  7208 ; Loss:  0.48505864449209085 ; l2 norm of gradient:  0.003529324138127506 ; l2 norm of weights:  0.4005769100906471\n",
            "Iteration#:  7212 ; Loss:  0.485052777469971 ; l2 norm of gradient:  0.0035179495579032013 ; l2 norm of weights:  0.4005759171187271\n",
            "Iteration#:  7216 ; Loss:  0.48504691968865365 ; l2 norm of gradient:  0.0035066326352003767 ; l2 norm of weights:  0.4005749299837083\n",
            "Iteration#:  7220 ; Loss:  0.4850410711316151 ; l2 norm of gradient:  0.0034953731044737313 ; l2 norm of weights:  0.40057394866332\n",
            "Iteration#:  7224 ; Loss:  0.4850352317823756 ; l2 norm of gradient:  0.0034841707011708254 ; l2 norm of weights:  0.400572973135403\n",
            "Iteration#:  7228 ; Loss:  0.485029401624501 ; l2 norm of gradient:  0.003473025161727439 ; l2 norm of weights:  0.40057200337790977\n",
            "Iteration#:  7232 ; Loss:  0.48502358064160145 ; l2 norm of gradient:  0.0034619362235630943 ; l2 norm of weights:  0.4005710393689029\n",
            "Iteration#:  7236 ; Loss:  0.4850177688173315 ; l2 norm of gradient:  0.0034509036250772052 ; l2 norm of weights:  0.4005700810865551\n",
            "Iteration#:  7240 ; Loss:  0.4850119661353894 ; l2 norm of gradient:  0.0034399271056441225 ; l2 norm of weights:  0.40056912850914794\n",
            "Iteration#:  7244 ; Loss:  0.48500617257951767 ; l2 norm of gradient:  0.0034290064056094766 ; l2 norm of weights:  0.4005681816150713\n",
            "Iteration#:  7248 ; Loss:  0.48500038813350244 ; l2 norm of gradient:  0.0034181412662852503 ; l2 norm of weights:  0.40056724038282293\n",
            "Iteration#:  7252 ; Loss:  0.4849946127811727 ; l2 norm of gradient:  0.0034073314299461585 ; l2 norm of weights:  0.40056630479100747\n",
            "Iteration#:  7256 ; Loss:  0.4849888465064013 ; l2 norm of gradient:  0.0033965766398248187 ; l2 norm of weights:  0.40056537481833593\n",
            "Iteration#:  7260 ; Loss:  0.4849830892931034 ; l2 norm of gradient:  0.0033858766401083738 ; l2 norm of weights:  0.4005644504436248\n",
            "Iteration#:  7264 ; Loss:  0.4849773411252374 ; l2 norm of gradient:  0.0033752311759334464 ; l2 norm of weights:  0.40056353164579545\n",
            "Iteration#:  7268 ; Loss:  0.48497160198680395 ; l2 norm of gradient:  0.003364639993383104 ; l2 norm of weights:  0.40056261840387364\n",
            "Iteration#:  7272 ; Loss:  0.4849658718618459 ; l2 norm of gradient:  0.003354102839481714 ; l2 norm of weights:  0.4005617106969887\n",
            "Iteration#:  7276 ; Loss:  0.48496015073444837 ; l2 norm of gradient:  0.0033436194621917817 ; l2 norm of weights:  0.40056080850437287\n",
            "Iteration#:  7280 ; Loss:  0.4849544385887381 ; l2 norm of gradient:  0.0033331896104095296 ; l2 norm of weights:  0.4005599118053604\n",
            "Iteration#:  7284 ; Loss:  0.4849487354088837 ; l2 norm of gradient:  0.0033228130339607344 ; l2 norm of weights:  0.4005590205793874\n",
            "Iteration#:  7288 ; Loss:  0.4849430411790951 ; l2 norm of gradient:  0.0033124894835975314 ; l2 norm of weights:  0.400558134805991\n",
            "Iteration#:  7292 ; Loss:  0.4849373558836233 ; l2 norm of gradient:  0.003302218710993968 ; l2 norm of weights:  0.4005572544648084\n",
            "Iteration#:  7296 ; Loss:  0.4849316795067603 ; l2 norm of gradient:  0.0032920004687420814 ; l2 norm of weights:  0.4005563795355766\n",
            "Iteration#:  7300 ; Loss:  0.48492601203283947 ; l2 norm of gradient:  0.003281834510348391 ; l2 norm of weights:  0.4005555099981315\n",
            "Iteration#:  7304 ; Loss:  0.484920353446234 ; l2 norm of gradient:  0.003271720590229741 ; l2 norm of weights:  0.40055464583240746\n",
            "Iteration#:  7308 ; Loss:  0.48491470373135775 ; l2 norm of gradient:  0.0032616584637100454 ; l2 norm of weights:  0.40055378701843675\n",
            "Iteration#:  7312 ; Loss:  0.4849090628726649 ; l2 norm of gradient:  0.003251647887016062 ; l2 norm of weights:  0.4005529335363486\n",
            "Iteration#:  7316 ; Loss:  0.4849034308546496 ; l2 norm of gradient:  0.0032416886172739008 ; l2 norm of weights:  0.4005520853663687\n",
            "Iteration#:  7320 ; Loss:  0.48489780766184554 ; l2 norm of gradient:  0.0032317804125057423 ; l2 norm of weights:  0.400551242488819\n",
            "Iteration#:  7324 ; Loss:  0.48489219327882616 ; l2 norm of gradient:  0.0032219230316250526 ; l2 norm of weights:  0.40055040488411636\n",
            "Iteration#:  7328 ; Loss:  0.48488658769020443 ; l2 norm of gradient:  0.0032121162344344325 ; l2 norm of weights:  0.4005495725327727\n",
            "Iteration#:  7332 ; Loss:  0.48488099088063247 ; l2 norm of gradient:  0.003202359781621194 ; l2 norm of weights:  0.4005487454153938\n",
            "Iteration#:  7336 ; Loss:  0.484875402834801 ; l2 norm of gradient:  0.003192653434753472 ; l2 norm of weights:  0.4005479235126789\n",
            "Iteration#:  7340 ; Loss:  0.48486982353744 ; l2 norm of gradient:  0.0031829969562782883 ; l2 norm of weights:  0.4005471068054205\n",
            "Iteration#:  7344 ; Loss:  0.4848642529733179 ; l2 norm of gradient:  0.003173390109515842 ; l2 norm of weights:  0.400546295274503\n",
            "Iteration#:  7348 ; Loss:  0.4848586911272419 ; l2 norm of gradient:  0.0031638326586579976 ; l2 norm of weights:  0.4005454889009028\n",
            "Iteration#:  7352 ; Loss:  0.4848531379840566 ; l2 norm of gradient:  0.003154324368764122 ; l2 norm of weights:  0.40054468766568746\n",
            "Iteration#:  7356 ; Loss:  0.48484759352864554 ; l2 norm of gradient:  0.003144865005757228 ; l2 norm of weights:  0.40054389155001513\n",
            "Iteration#:  7360 ; Loss:  0.48484205774592964 ; l2 norm of gradient:  0.0031354543364215033 ; l2 norm of weights:  0.40054310053513403\n",
            "Iteration#:  7364 ; Loss:  0.4848365306208679 ; l2 norm of gradient:  0.0031260921283983376 ; l2 norm of weights:  0.4005423146023817\n",
            "Iteration#:  7368 ; Loss:  0.4848310121384563 ; l2 norm of gradient:  0.0031167781501834487 ; l2 norm of weights:  0.40054153373318463\n",
            "Iteration#:  7372 ; Loss:  0.48482550228372856 ; l2 norm of gradient:  0.00310751217112342 ; l2 norm of weights:  0.40054075790905774\n",
            "Iteration#:  7376 ; Loss:  0.4848200010417554 ; l2 norm of gradient:  0.0030982939614126854 ; l2 norm of weights:  0.40053998711160366\n",
            "Iteration#:  7380 ; Loss:  0.4848145083976446 ; l2 norm of gradient:  0.003089123292089449 ; l2 norm of weights:  0.4005392213225124\n",
            "Iteration#:  7384 ; Loss:  0.4848090243365407 ; l2 norm of gradient:  0.0030799999350339833 ; l2 norm of weights:  0.40053846052356046\n",
            "Iteration#:  7388 ; Loss:  0.4848035488436246 ; l2 norm of gradient:  0.003070923662964401 ; l2 norm of weights:  0.40053770469661054\n",
            "Iteration#:  7392 ; Loss:  0.48479808190411405 ; l2 norm of gradient:  0.0030618942494340575 ; l2 norm of weights:  0.40053695382361126\n",
            "Iteration#:  7396 ; Loss:  0.4847926235032628 ; l2 norm of gradient:  0.0030529114688279736 ; l2 norm of weights:  0.40053620788659583\n",
            "Iteration#:  7400 ; Loss:  0.48478717362636103 ; l2 norm of gradient:  0.0030439750963602887 ; l2 norm of weights:  0.4005354668676825\n",
            "Iteration#:  7404 ; Loss:  0.4847817322587343 ; l2 norm of gradient:  0.0030350849080711144 ; l2 norm of weights:  0.4005347307490732\n",
            "Iteration#:  7408 ; Loss:  0.48477629938574435 ; l2 norm of gradient:  0.003026240680823261 ; l2 norm of weights:  0.40053399951305363\n",
            "Iteration#:  7412 ; Loss:  0.48477087499278854 ; l2 norm of gradient:  0.00301744219229964 ; l2 norm of weights:  0.4005332731419921\n",
            "Iteration#:  7416 ; Loss:  0.4847654590652993 ; l2 norm of gradient:  0.0030086892210001116 ; l2 norm of weights:  0.4005325516183399\n",
            "Iteration#:  7420 ; Loss:  0.48476005158874463 ; l2 norm of gradient:  0.002999981546238799 ; l2 norm of weights:  0.40053183492462996\n",
            "Iteration#:  7424 ; Loss:  0.48475465254862765 ; l2 norm of gradient:  0.002991318948140684 ; l2 norm of weights:  0.4005311230434768\n",
            "Iteration#:  7428 ; Loss:  0.4847492619304864 ; l2 norm of gradient:  0.002982701207639553 ; l2 norm of weights:  0.4005304159575758\n",
            "Iteration#:  7432 ; Loss:  0.48474387971989324 ; l2 norm of gradient:  0.0029741281064745054 ; l2 norm of weights:  0.400529713649703\n",
            "Iteration#:  7436 ; Loss:  0.4847385059024558 ; l2 norm of gradient:  0.0029655994271875334 ; l2 norm of weights:  0.40052901610271413\n",
            "Iteration#:  7440 ; Loss:  0.48473314046381577 ; l2 norm of gradient:  0.0029571149531204713 ; l2 norm of weights:  0.4005283232995445\n",
            "Iteration#:  7444 ; Loss:  0.4847277833896494 ; l2 norm of gradient:  0.0029486744684123108 ; l2 norm of weights:  0.4005276352232085\n",
            "Iteration#:  7448 ; Loss:  0.48472243466566667 ; l2 norm of gradient:  0.0029402777579969623 ; l2 norm of weights:  0.40052695185679904\n",
            "Iteration#:  7452 ; Loss:  0.4847170942776119 ; l2 norm of gradient:  0.002931924607600208 ; l2 norm of weights:  0.40052627318348677\n",
            "Iteration#:  7456 ; Loss:  0.4847117622112632 ; l2 norm of gradient:  0.0029236148037365224 ; l2 norm of weights:  0.4005255991865202\n",
            "Iteration#:  7460 ; Loss:  0.4847064384524322 ; l2 norm of gradient:  0.0029153481337073125 ; l2 norm of weights:  0.40052492984922466\n",
            "Iteration#:  7464 ; Loss:  0.4847011229869643 ; l2 norm of gradient:  0.0029071243855980375 ; l2 norm of weights:  0.4005242651550023\n",
            "Iteration#:  7468 ; Loss:  0.484695815800738 ; l2 norm of gradient:  0.002898943348275081 ; l2 norm of weights:  0.4005236050873312\n",
            "Iteration#:  7472 ; Loss:  0.484690516879665 ; l2 norm of gradient:  0.002890804811384201 ; l2 norm of weights:  0.4005229496297653\n",
            "Iteration#:  7476 ; Loss:  0.4846852262096906 ; l2 norm of gradient:  0.0028827085653472047 ; l2 norm of weights:  0.4005222987659335\n",
            "Iteration#:  7480 ; Loss:  0.48467994377679235 ; l2 norm of gradient:  0.0028746544013596746 ; l2 norm of weights:  0.4005216524795395\n",
            "Iteration#:  7484 ; Loss:  0.48467466956698085 ; l2 norm of gradient:  0.0028666421113885737 ; l2 norm of weights:  0.4005210107543616\n",
            "Iteration#:  7488 ; Loss:  0.4846694035662995 ; l2 norm of gradient:  0.0028586714881697547 ; l2 norm of weights:  0.4005203735742514\n",
            "Iteration#:  7492 ; Loss:  0.4846641457608239 ; l2 norm of gradient:  0.0028507423252054427 ; l2 norm of weights:  0.40051974092313436\n",
            "Iteration#:  7496 ; Loss:  0.48465889613666224 ; l2 norm of gradient:  0.0028428544167622394 ; l2 norm of weights:  0.40051911278500857\n",
            "Iteration#:  7500 ; Loss:  0.4846536546799547 ; l2 norm of gradient:  0.0028350075578678353 ; l2 norm of weights:  0.4005184891439449\n",
            "Iteration#:  7504 ; Loss:  0.48464842137687375 ; l2 norm of gradient:  0.0028272015443095073 ; l2 norm of weights:  0.40051786998408606\n",
            "Iteration#:  7508 ; Loss:  0.4846431962136237 ; l2 norm of gradient:  0.002819436172631809 ; l2 norm of weights:  0.4005172552896464\n",
            "Iteration#:  7512 ; Loss:  0.48463797917644036 ; l2 norm of gradient:  0.002811711240133515 ; l2 norm of weights:  0.4005166450449116\n",
            "Iteration#:  7516 ; Loss:  0.4846327702515917 ; l2 norm of gradient:  0.00280402654486618 ; l2 norm of weights:  0.40051603923423795\n",
            "Iteration#:  7520 ; Loss:  0.4846275694253767 ; l2 norm of gradient:  0.002796381885631077 ; l2 norm of weights:  0.40051543784205224\n",
            "Iteration#:  7524 ; Loss:  0.48462237668412583 ; l2 norm of gradient:  0.0027887770619779437 ; l2 norm of weights:  0.40051484085285105\n",
            "Iteration#:  7528 ; Loss:  0.4846171920142013 ; l2 norm of gradient:  0.0027812118742015298 ; l2 norm of weights:  0.4005142482512005\n",
            "Iteration#:  7532 ; Loss:  0.48461201540199556 ; l2 norm of gradient:  0.0027736861233406637 ; l2 norm of weights:  0.4005136600217359\n",
            "Iteration#:  7536 ; Loss:  0.4846068468339326 ; l2 norm of gradient:  0.00276619961117529 ; l2 norm of weights:  0.40051307614916104\n",
            "Iteration#:  7540 ; Loss:  0.48460168629646716 ; l2 norm of gradient:  0.0027587521402245513 ; l2 norm of weights:  0.40051249661824795\n",
            "Iteration#:  7544 ; Loss:  0.4845965337760845 ; l2 norm of gradient:  0.002751343513744427 ; l2 norm of weights:  0.40051192141383674\n",
            "Iteration#:  7548 ; Loss:  0.4845913892593007 ; l2 norm of gradient:  0.0027439735357264908 ; l2 norm of weights:  0.4005113505208347\n",
            "Iteration#:  7552 ; Loss:  0.4845862527326621 ; l2 norm of gradient:  0.002736642010894494 ; l2 norm of weights:  0.4005107839242164\n",
            "Iteration#:  7556 ; Loss:  0.48458112418274524 ; l2 norm of gradient:  0.0027293487447034087 ; l2 norm of weights:  0.4005102216090229\n",
            "Iteration#:  7560 ; Loss:  0.48457600359615716 ; l2 norm of gradient:  0.0027220935433367515 ; l2 norm of weights:  0.40050966356036144\n",
            "Iteration#:  7564 ; Loss:  0.4845708909595349 ; l2 norm of gradient:  0.002714876213705269 ; l2 norm of weights:  0.4005091097634053\n",
            "Iteration#:  7568 ; Loss:  0.48456578625954494 ; l2 norm of gradient:  0.002707696563443672 ; l2 norm of weights:  0.40050856020339315\n",
            "Iteration#:  7572 ; Loss:  0.4845606894828841 ; l2 norm of gradient:  0.002700554400910555 ; l2 norm of weights:  0.4005080148656285\n",
            "Iteration#:  7576 ; Loss:  0.4845556006162789 ; l2 norm of gradient:  0.0026934495351840395 ; l2 norm of weights:  0.40050747373547996\n",
            "Iteration#:  7580 ; Loss:  0.4845505196464849 ; l2 norm of gradient:  0.0026863817760624286 ; l2 norm of weights:  0.40050693679838\n",
            "Iteration#:  7584 ; Loss:  0.4845454465602875 ; l2 norm of gradient:  0.002679350934060113 ; l2 norm of weights:  0.40050640403982546\n",
            "Iteration#:  7588 ; Loss:  0.48454038134450145 ; l2 norm of gradient:  0.0026723568204068066 ; l2 norm of weights:  0.40050587544537636\n",
            "Iteration#:  7592 ; Loss:  0.48453532398597043 ; l2 norm of gradient:  0.0026653992470458444 ; l2 norm of weights:  0.40050535100065593\n",
            "Iteration#:  7596 ; Loss:  0.4845302744715674 ; l2 norm of gradient:  0.0026584780266309836 ; l2 norm of weights:  0.40050483069135046\n",
            "Iteration#:  7600 ; Loss:  0.48452523278819404 ; l2 norm of gradient:  0.0026515929725264664 ; l2 norm of weights:  0.40050431450320845\n",
            "Iteration#:  7604 ; Loss:  0.48452019892278103 ; l2 norm of gradient:  0.0026447438988036124 ; l2 norm of weights:  0.40050380242204037\n",
            "Iteration#:  7608 ; Loss:  0.4845151728622878 ; l2 norm of gradient:  0.0026379306202397616 ; l2 norm of weights:  0.4005032944337186\n",
            "Iteration#:  7612 ; Loss:  0.48451015459370217 ; l2 norm of gradient:  0.002631152952316296 ; l2 norm of weights:  0.4005027905241768\n",
            "Iteration#:  7616 ; Loss:  0.48450514410404066 ; l2 norm of gradient:  0.002624410711216916 ; l2 norm of weights:  0.4005022906794095\n",
            "Iteration#:  7620 ; Loss:  0.4845001413803481 ; l2 norm of gradient:  0.0026177037138258676 ; l2 norm of weights:  0.400501794885472\n",
            "Iteration#:  7624 ; Loss:  0.4844951464096974 ; l2 norm of gradient:  0.002611031777726405 ; l2 norm of weights:  0.4005013031284796\n",
            "Iteration#:  7628 ; Loss:  0.48449015917918964 ; l2 norm of gradient:  0.0026043947211982768 ; l2 norm of weights:  0.4005008153946079\n",
            "Iteration#:  7632 ; Loss:  0.4844851796759543 ; l2 norm of gradient:  0.0025977923632172035 ; l2 norm of weights:  0.40050033167009164\n",
            "Iteration#:  7636 ; Loss:  0.48448020788714824 ; l2 norm of gradient:  0.0025912245234526927 ; l2 norm of weights:  0.4004998519412251\n",
            "Iteration#:  7640 ; Loss:  0.48447524379995643 ; l2 norm of gradient:  0.0025846910222654443 ; l2 norm of weights:  0.40049937619436127\n",
            "Iteration#:  7644 ; Loss:  0.4844702874015915 ; l2 norm of gradient:  0.0025781916807071214 ; l2 norm of weights:  0.40049890441591174\n",
            "Iteration#:  7648 ; Loss:  0.48446533867929353 ; l2 norm of gradient:  0.0025717263205181262 ; l2 norm of weights:  0.4004984365923462\n",
            "Iteration#:  7652 ; Loss:  0.4844603976203301 ; l2 norm of gradient:  0.002565294764125993 ; l2 norm of weights:  0.4004979727101922\n",
            "Iteration#:  7656 ; Loss:  0.4844554642119963 ; l2 norm of gradient:  0.0025588968346433325 ; l2 norm of weights:  0.400497512756035\n",
            "Iteration#:  7660 ; Loss:  0.48445053844161434 ; l2 norm of gradient:  0.0025525323558669866 ; l2 norm of weights:  0.4004970567165168\n",
            "Iteration#:  7664 ; Loss:  0.4844456202965335 ; l2 norm of gradient:  0.0025462011522759485 ; l2 norm of weights:  0.40049660457833675\n",
            "Iteration#:  7668 ; Loss:  0.4844407097641303 ; l2 norm of gradient:  0.0025399030490297274 ; l2 norm of weights:  0.4004961563282505\n",
            "Iteration#:  7672 ; Loss:  0.4844358068318079 ; l2 norm of gradient:  0.0025336378719674466 ; l2 norm of weights:  0.4004957119530698\n",
            "Iteration#:  7676 ; Loss:  0.48443091148699674 ; l2 norm of gradient:  0.0025274054476055532 ; l2 norm of weights:  0.40049527143966257\n",
            "Iteration#:  7680 ; Loss:  0.4844260237171537 ; l2 norm of gradient:  0.0025212056031364565 ; l2 norm of weights:  0.4004948347749521\n",
            "Iteration#:  7684 ; Loss:  0.48442114350976206 ; l2 norm of gradient:  0.0025150381664273097 ; l2 norm of weights:  0.4004944019459168\n",
            "Iteration#:  7688 ; Loss:  0.48441627085233196 ; l2 norm of gradient:  0.0025089029660185294 ; l2 norm of weights:  0.4004939729395902\n",
            "Iteration#:  7692 ; Loss:  0.4844114057323998 ; l2 norm of gradient:  0.002502799831121646 ; l2 norm of weights:  0.4004935477430603\n",
            "Iteration#:  7696 ; Loss:  0.48440654813752865 ; l2 norm of gradient:  0.0024967285916186526 ; l2 norm of weights:  0.40049312634346945\n",
            "Iteration#:  7700 ; Loss:  0.484401698055307 ; l2 norm of gradient:  0.0024906890780599746 ; l2 norm of weights:  0.40049270872801396\n",
            "Iteration#:  7704 ; Loss:  0.4843968554733503 ; l2 norm of gradient:  0.0024846811216630254 ; l2 norm of weights:  0.40049229488394394\n",
            "Iteration#:  7708 ; Loss:  0.4843920203792996 ; l2 norm of gradient:  0.002478704554311263 ; l2 norm of weights:  0.4004918847985626\n",
            "Iteration#:  7712 ; Loss:  0.48438719276082176 ; l2 norm of gradient:  0.0024727592085524164 ; l2 norm of weights:  0.40049147845922645\n",
            "Iteration#:  7716 ; Loss:  0.4843823726056099 ; l2 norm of gradient:  0.002466844917596901 ; l2 norm of weights:  0.40049107585334465\n",
            "Iteration#:  7720 ; Loss:  0.4843775599013823 ; l2 norm of gradient:  0.0024609615153162526 ; l2 norm of weights:  0.4004906769683789\n",
            "Iteration#:  7724 ; Loss:  0.4843727546358832 ; l2 norm of gradient:  0.0024551088362428715 ; l2 norm of weights:  0.4004902817918429\n",
            "Iteration#:  7728 ; Loss:  0.4843679567968825 ; l2 norm of gradient:  0.0024492867155667427 ; l2 norm of weights:  0.40048989031130233\n",
            "Iteration#:  7732 ; Loss:  0.484363166372175 ; l2 norm of gradient:  0.002443494989136166 ; l2 norm of weights:  0.40048950251437454\n",
            "Iteration#:  7736 ; Loss:  0.4843583833495816 ; l2 norm of gradient:  0.0024377334934544575 ; l2 norm of weights:  0.4004891183887281\n",
            "Iteration#:  7740 ; Loss:  0.4843536077169477 ; l2 norm of gradient:  0.00243200206567986 ; l2 norm of weights:  0.40048873792208245\n",
            "Iteration#:  7744 ; Loss:  0.48434883946214435 ; l2 norm of gradient:  0.0024263005436237885 ; l2 norm of weights:  0.4004883611022078\n",
            "Iteration#:  7748 ; Loss:  0.4843440785730675 ; l2 norm of gradient:  0.0024206287657491336 ; l2 norm of weights:  0.40048798791692486\n",
            "Iteration#:  7752 ; Loss:  0.4843393250376379 ; l2 norm of gradient:  0.0024149865711697344 ; l2 norm of weights:  0.4004876183541045\n",
            "Iteration#:  7756 ; Loss:  0.48433457884380127 ; l2 norm of gradient:  0.002409373799647999 ; l2 norm of weights:  0.4004872524016672\n",
            "Iteration#:  7760 ; Loss:  0.4843298399795285 ; l2 norm of gradient:  0.0024037902915946658 ; l2 norm of weights:  0.40048689004758337\n",
            "Iteration#:  7764 ; Loss:  0.4843251084328145 ; l2 norm of gradient:  0.0023982358880667008 ; l2 norm of weights:  0.4004865312798724\n",
            "Iteration#:  7768 ; Loss:  0.48432038419167933 ; l2 norm of gradient:  0.0023927104307658906 ; l2 norm of weights:  0.4004861760866029\n",
            "Iteration#:  7772 ; Loss:  0.48431566724416736 ; l2 norm of gradient:  0.002387213762038767 ; l2 norm of weights:  0.4004858244558922\n",
            "Iteration#:  7776 ; Loss:  0.48431095757834713 ; l2 norm of gradient:  0.0023817457248735473 ; l2 norm of weights:  0.40048547637590604\n",
            "Iteration#:  7780 ; Loss:  0.4843062551823119 ; l2 norm of gradient:  0.0023763061629002626 ; l2 norm of weights:  0.4004851318348584\n",
            "Iteration#:  7784 ; Loss:  0.4843015600441791 ; l2 norm of gradient:  0.002370894920388986 ; l2 norm of weights:  0.4004847908210112\n",
            "Iteration#:  7788 ; Loss:  0.4842968721520903 ; l2 norm of gradient:  0.0023655118422478932 ; l2 norm of weights:  0.40048445332267396\n",
            "Iteration#:  7792 ; Loss:  0.484292191494211 ; l2 norm of gradient:  0.002360156774023506 ; l2 norm of weights:  0.40048411932820377\n",
            "Iteration#:  7796 ; Loss:  0.4842875180587307 ; l2 norm of gradient:  0.0023548295618979614 ; l2 norm of weights:  0.40048378882600477\n",
            "Iteration#:  7800 ; Loss:  0.4842828518338631 ; l2 norm of gradient:  0.0023495300526882885 ; l2 norm of weights:  0.4004834618045279\n",
            "Iteration#:  7804 ; Loss:  0.48427819280784545 ; l2 norm of gradient:  0.002344258093845559 ; l2 norm of weights:  0.4004831382522708\n",
            "Iteration#:  7808 ; Loss:  0.4842735409689387 ; l2 norm of gradient:  0.002339013533453003 ; l2 norm of weights:  0.4004828181577776\n",
            "Iteration#:  7812 ; Loss:  0.48426889630542774 ; l2 norm of gradient:  0.002333796220225336 ; l2 norm of weights:  0.40048250150963827\n",
            "Iteration#:  7816 ; Loss:  0.48426425880562063 ; l2 norm of gradient:  0.002328606003506595 ; l2 norm of weights:  0.4004821882964889\n",
            "Iteration#:  7820 ; Loss:  0.48425962845784937 ; l2 norm of gradient:  0.0023234427332702195 ; l2 norm of weights:  0.400481878507011\n",
            "Iteration#:  7824 ; Loss:  0.48425500525046883 ; l2 norm of gradient:  0.0023183062601168244 ; l2 norm of weights:  0.40048157212993146\n",
            "Iteration#:  7828 ; Loss:  0.4842503891718578 ; l2 norm of gradient:  0.0023131964352732315 ; l2 norm of weights:  0.4004812691540224\n",
            "Iteration#:  7832 ; Loss:  0.48424578021041775 ; l2 norm of gradient:  0.002308113110591247 ; l2 norm of weights:  0.4004809695681006\n",
            "Iteration#:  7836 ; Loss:  0.4842411783545736 ; l2 norm of gradient:  0.0023030561385469677 ; l2 norm of weights:  0.4004806733610276\n",
            "Iteration#:  7840 ; Loss:  0.4842365835927735 ; l2 norm of gradient:  0.002298025372238231 ; l2 norm of weights:  0.40048038052170926\n",
            "Iteration#:  7844 ; Loss:  0.48423199591348826 ; l2 norm of gradient:  0.0022930206653849184 ; l2 norm of weights:  0.4004800910390956\n",
            "Iteration#:  7848 ; Loss:  0.4842274153052118 ; l2 norm of gradient:  0.0022880418723270107 ; l2 norm of weights:  0.40047980490218044\n",
            "Iteration#:  7852 ; Loss:  0.4842228417564608 ; l2 norm of gradient:  0.002283088848023078 ; l2 norm of weights:  0.40047952210000126\n",
            "Iteration#:  7856 ; Loss:  0.4842182752557748 ; l2 norm of gradient:  0.0022781614480497283 ; l2 norm of weights:  0.400479242621639\n",
            "Iteration#:  7860 ; Loss:  0.48421371579171596 ; l2 norm of gradient:  0.002273259528599916 ; l2 norm of weights:  0.4004789664562177\n",
            "Iteration#:  7864 ; Loss:  0.48420916335286907 ; l2 norm of gradient:  0.0022683829464822188 ; l2 norm of weights:  0.40047869359290444\n",
            "Iteration#:  7868 ; Loss:  0.4842046179278415 ; l2 norm of gradient:  0.0022635315591192666 ; l2 norm of weights:  0.40047842402090883\n",
            "Iteration#:  7872 ; Loss:  0.484200079505263 ; l2 norm of gradient:  0.002258705224546246 ; l2 norm of weights:  0.40047815772948325\n",
            "Iteration#:  7876 ; Loss:  0.4841955480737856 ; l2 norm of gradient:  0.002253903801410558 ; l2 norm of weights:  0.40047789470792194\n",
            "Iteration#:  7880 ; Loss:  0.48419102362208394 ; l2 norm of gradient:  0.002249127148969816 ; l2 norm of weights:  0.40047763494556143\n",
            "Iteration#:  7884 ; Loss:  0.4841865061388547 ; l2 norm of gradient:  0.002244375127091401 ; l2 norm of weights:  0.40047737843177983\n",
            "Iteration#:  7888 ; Loss:  0.4841819956128168 ; l2 norm of gradient:  0.0022396475962504925 ; l2 norm of weights:  0.4004771251559972\n",
            "Iteration#:  7892 ; Loss:  0.48417749203271093 ; l2 norm of gradient:  0.0022349444175290938 ; l2 norm of weights:  0.40047687510767443\n",
            "Iteration#:  7896 ; Loss:  0.4841729953873003 ; l2 norm of gradient:  0.0022302654526151 ; l2 norm of weights:  0.40047662827631375\n",
            "Iteration#:  7900 ; Loss:  0.4841685056653697 ; l2 norm of gradient:  0.0022256105638013292 ; l2 norm of weights:  0.40047638465145846\n",
            "Iteration#:  7904 ; Loss:  0.484164022855726 ; l2 norm of gradient:  0.0022209796139834813 ; l2 norm of weights:  0.4004761442226923\n",
            "Iteration#:  7908 ; Loss:  0.48415954694719754 ; l2 norm of gradient:  0.0022163724666597814 ; l2 norm of weights:  0.40047590697963953\n",
            "Iteration#:  7912 ; Loss:  0.48415507792863477 ; l2 norm of gradient:  0.0022117889859293266 ; l2 norm of weights:  0.4004756729119647\n",
            "Iteration#:  7916 ; Loss:  0.4841506157889096 ; l2 norm of gradient:  0.002207229036490722 ; l2 norm of weights:  0.40047544200937213\n",
            "Iteration#:  7920 ; Loss:  0.4841461605169155 ; l2 norm of gradient:  0.0022026924836415743 ; l2 norm of weights:  0.4004752142616066\n",
            "Iteration#:  7924 ; Loss:  0.4841417121015675 ; l2 norm of gradient:  0.002198179193276623 ; l2 norm of weights:  0.4004749896584518\n",
            "Iteration#:  7928 ; Loss:  0.4841372705318019 ; l2 norm of gradient:  0.0021936890318869197 ; l2 norm of weights:  0.40047476818973105\n",
            "Iteration#:  7932 ; Loss:  0.48413283579657673 ; l2 norm of gradient:  0.00218922186655852 ; l2 norm of weights:  0.40047454984530695\n",
            "Iteration#:  7936 ; Loss:  0.4841284078848711 ; l2 norm of gradient:  0.0021847775649712496 ; l2 norm of weights:  0.4004743346150811\n",
            "Iteration#:  7940 ; Loss:  0.4841239867856852 ; l2 norm of gradient:  0.002180355995397429 ; l2 norm of weights:  0.4004741224889936\n",
            "Iteration#:  7944 ; Loss:  0.4841195724880409 ; l2 norm of gradient:  0.002175957026700953 ; l2 norm of weights:  0.40047391345702343\n",
            "Iteration#:  7948 ; Loss:  0.48411516498098045 ; l2 norm of gradient:  0.002171580528335668 ; l2 norm of weights:  0.40047370750918776\n",
            "Iteration#:  7952 ; Loss:  0.4841107642535678 ; l2 norm of gradient:  0.0021672263703448255 ; l2 norm of weights:  0.40047350463554177\n",
            "Iteration#:  7956 ; Loss:  0.48410637029488746 ; l2 norm of gradient:  0.0021628944233590074 ; l2 norm of weights:  0.400473304826179\n",
            "Iteration#:  7960 ; Loss:  0.48410198309404473 ; l2 norm of gradient:  0.0021585845585955672 ; l2 norm of weights:  0.4004731080712304\n",
            "Iteration#:  7964 ; Loss:  0.4840976026401662 ; l2 norm of gradient:  0.0021542966478571815 ; l2 norm of weights:  0.4004729143608645\n",
            "Iteration#:  7968 ; Loss:  0.4840932289223991 ; l2 norm of gradient:  0.0021500305635306703 ; l2 norm of weights:  0.4004727236852875\n",
            "Iteration#:  7972 ; Loss:  0.484088861929911 ; l2 norm of gradient:  0.0021457861785856083 ; l2 norm of weights:  0.4004725360347425\n",
            "Iteration#:  7976 ; Loss:  0.4840845016518905 ; l2 norm of gradient:  0.002141563366573796 ; l2 norm of weights:  0.40047235139950943\n",
            "Iteration#:  7980 ; Loss:  0.48408014807754635 ; l2 norm of gradient:  0.0021373620016265633 ; l2 norm of weights:  0.40047216976990535\n",
            "Iteration#:  7984 ; Loss:  0.4840758011961084 ; l2 norm of gradient:  0.0021331819584554748 ; l2 norm of weights:  0.4004719911362836\n",
            "Iteration#:  7988 ; Loss:  0.48407146099682663 ; l2 norm of gradient:  0.0021290231123494963 ; l2 norm of weights:  0.4004718154890343\n",
            "Iteration#:  7992 ; Loss:  0.484067127468971 ; l2 norm of gradient:  0.0021248853391747967 ; l2 norm of weights:  0.40047164281858333\n",
            "Iteration#:  7996 ; Loss:  0.48406280060183254 ; l2 norm of gradient:  0.002120768515372509 ; l2 norm of weights:  0.40047147311539283\n",
            "Iteration#:  8000 ; Loss:  0.48405848038472193 ; l2 norm of gradient:  0.0021166725179587636 ; l2 norm of weights:  0.4004713063699607\n",
            "Iteration#:  8004 ; Loss:  0.4840541668069704 ; l2 norm of gradient:  0.002112597224522371 ; l2 norm of weights:  0.40047114257282046\n",
            "Iteration#:  8008 ; Loss:  0.48404985985792914 ; l2 norm of gradient:  0.0021085425132239414 ; l2 norm of weights:  0.40047098171454126\n",
            "Iteration#:  8012 ; Loss:  0.4840455595269695 ; l2 norm of gradient:  0.0021045082627947215 ; l2 norm of weights:  0.4004708237857273\n",
            "Iteration#:  8016 ; Loss:  0.4840412658034827 ; l2 norm of gradient:  0.0021004943525354328 ; l2 norm of weights:  0.40047066877701815\n",
            "Iteration#:  8020 ; Loss:  0.48403697867688017 ; l2 norm of gradient:  0.00209650066231445 ; l2 norm of weights:  0.40047051667908784\n",
            "Iteration#:  8024 ; Loss:  0.4840326981365928 ; l2 norm of gradient:  0.00209252707256762 ; l2 norm of weights:  0.4004703674826456\n",
            "Iteration#:  8028 ; Loss:  0.48402842417207176 ; l2 norm of gradient:  0.0020885734642958273 ; l2 norm of weights:  0.400470221178435\n",
            "Iteration#:  8032 ; Loss:  0.48402415677278743 ; l2 norm of gradient:  0.0020846397190640246 ; l2 norm of weights:  0.400470077757234\n",
            "Iteration#:  8036 ; Loss:  0.4840198959282307 ; l2 norm of gradient:  0.002080725719000802 ; l2 norm of weights:  0.4004699372098548\n",
            "Iteration#:  8040 ; Loss:  0.4840156416279114 ; l2 norm of gradient:  0.002076831346796118 ; l2 norm of weights:  0.4004697995271435\n",
            "Iteration#:  8044 ; Loss:  0.4840113938613592 ; l2 norm of gradient:  0.0020729564857002785 ; l2 norm of weights:  0.4004696646999802\n",
            "Iteration#:  8048 ; Loss:  0.48400715261812327 ; l2 norm of gradient:  0.002069101019522715 ; l2 norm of weights:  0.4004695327192787\n",
            "Iteration#:  8052 ; Loss:  0.4840029178877723 ; l2 norm of gradient:  0.0020652648326313365 ; l2 norm of weights:  0.4004694035759861\n",
            "Iteration#:  8056 ; Loss:  0.4839986896598944 ; l2 norm of gradient:  0.002061447809949455 ; l2 norm of weights:  0.40046927726108317\n",
            "Iteration#:  8060 ; Loss:  0.4839944679240969 ; l2 norm of gradient:  0.0020576498369565036 ; l2 norm of weights:  0.4004691537655834\n",
            "Iteration#:  8064 ; Loss:  0.4839902526700066 ; l2 norm of gradient:  0.0020538707996860996 ; l2 norm of weights:  0.40046903308053367\n",
            "Iteration#:  8068 ; Loss:  0.4839860438872695 ; l2 norm of gradient:  0.0020501105847234844 ; l2 norm of weights:  0.40046891519701355\n",
            "Iteration#:  8072 ; Loss:  0.48398184156555074 ; l2 norm of gradient:  0.0020463690792063586 ; l2 norm of weights:  0.40046880010613517\n",
            "Iteration#:  8076 ; Loss:  0.48397764569453444 ; l2 norm of gradient:  0.0020426461708215643 ; l2 norm of weights:  0.40046868779904327\n",
            "Iteration#:  8080 ; Loss:  0.48397345626392413 ; l2 norm of gradient:  0.0020389417478051445 ; l2 norm of weights:  0.400468578266915\n",
            "Iteration#:  8084 ; Loss:  0.48396927326344213 ; l2 norm of gradient:  0.0020352556989401186 ; l2 norm of weights:  0.40046847150095954\n",
            "Iteration#:  8088 ; Loss:  0.48396509668282967 ; l2 norm of gradient:  0.0020315879135560724 ; l2 norm of weights:  0.4004683674924181\n",
            "Iteration#:  8092 ; Loss:  0.4839609265118471 ; l2 norm of gradient:  0.0020279382815266826 ; l2 norm of weights:  0.40046826623256376\n",
            "Iteration#:  8096 ; Loss:  0.48395676274027344 ; l2 norm of gradient:  0.0020243066932694996 ; l2 norm of weights:  0.40046816771270133\n",
            "Iteration#:  8100 ; Loss:  0.4839526053579067 ; l2 norm of gradient:  0.0020206930397435596 ; l2 norm of weights:  0.40046807192416717\n",
            "Iteration#:  8104 ; Loss:  0.48394845435456324 ; l2 norm of gradient:  0.002017097212448964 ; l2 norm of weights:  0.4004679788583288\n",
            "Iteration#:  8108 ; Loss:  0.48394430972007874 ; l2 norm of gradient:  0.0020135191034249722 ; l2 norm of weights:  0.4004678885065852\n",
            "Iteration#:  8112 ; Loss:  0.48394017144430684 ; l2 norm of gradient:  0.002009958605248783 ; l2 norm of weights:  0.40046780086036626\n",
            "Iteration#:  8116 ; Loss:  0.48393603951712016 ; l2 norm of gradient:  0.0020064156110336188 ; l2 norm of weights:  0.4004677159111329\n",
            "Iteration#:  8120 ; Loss:  0.48393191392840984 ; l2 norm of gradient:  0.0020028900144288663 ; l2 norm of weights:  0.40046763365037663\n",
            "Iteration#:  8124 ; Loss:  0.48392779466808533 ; l2 norm of gradient:  0.001999381709616996 ; l2 norm of weights:  0.40046755406961976\n",
            "Iteration#:  8128 ; Loss:  0.4839236817260747 ; l2 norm of gradient:  0.001995890591313096 ; l2 norm of weights:  0.40046747716041475\n",
            "Iteration#:  8132 ; Loss:  0.48391957509232425 ; l2 norm of gradient:  0.0019924165547630813 ; l2 norm of weights:  0.4004674029143447\n",
            "Iteration#:  8136 ; Loss:  0.4839154747567986 ; l2 norm of gradient:  0.001988959495742782 ; l2 norm of weights:  0.4004673313230225\n",
            "Iteration#:  8140 ; Loss:  0.4839113807094807 ; l2 norm of gradient:  0.0019855193105562945 ; l2 norm of weights:  0.4004672623780913\n",
            "Iteration#:  8144 ; Loss:  0.4839072929403716 ; l2 norm of gradient:  0.0019820958960339815 ; l2 norm of weights:  0.40046719607122405\n",
            "Iteration#:  8148 ; Loss:  0.4839032114394908 ; l2 norm of gradient:  0.0019786891495320496 ; l2 norm of weights:  0.4004671323941233\n",
            "Iteration#:  8152 ; Loss:  0.4838991361968757 ; l2 norm of gradient:  0.001975298968930967 ; l2 norm of weights:  0.40046707133852116\n",
            "Iteration#:  8156 ; Loss:  0.48389506720258174 ; l2 norm of gradient:  0.001971925252632902 ; l2 norm of weights:  0.40046701289617925\n",
            "Iteration#:  8160 ; Loss:  0.48389100444668237 ; l2 norm of gradient:  0.0019685678995619814 ; l2 norm of weights:  0.4004669570588884\n",
            "Iteration#:  8164 ; Loss:  0.4838869479192693 ; l2 norm of gradient:  0.0019652268091616878 ; l2 norm of weights:  0.40046690381846844\n",
            "Iteration#:  8168 ; Loss:  0.4838828976104517 ; l2 norm of gradient:  0.001961901881393729 ; l2 norm of weights:  0.40046685316676833\n",
            "Iteration#:  8172 ; Loss:  0.483878853510357 ; l2 norm of gradient:  0.001958593016736534 ; l2 norm of weights:  0.40046680509566585\n",
            "Iteration#:  8176 ; Loss:  0.48387481560913015 ; l2 norm of gradient:  0.001955300116184284 ; l2 norm of weights:  0.40046675959706735\n",
            "Iteration#:  8180 ; Loss:  0.48387078389693416 ; l2 norm of gradient:  0.0019520230812448377 ; l2 norm of weights:  0.4004667166629079\n",
            "Iteration#:  8184 ; Loss:  0.4838667583639493 ; l2 norm of gradient:  0.0019487618139383907 ; l2 norm of weights:  0.40046667628515076\n",
            "Iteration#:  8188 ; Loss:  0.48386273900037435 ; l2 norm of gradient:  0.0019455162167962743 ; l2 norm of weights:  0.4004666384557877\n",
            "Iteration#:  8192 ; Loss:  0.48385872579642475 ; l2 norm of gradient:  0.001942286192859166 ; l2 norm of weights:  0.4004666031668386\n",
            "Iteration#:  8196 ; Loss:  0.4838547187423341 ; l2 norm of gradient:  0.0019390716456762938 ; l2 norm of weights:  0.40046657041035105\n",
            "Iteration#:  8200 ; Loss:  0.48385071782835337 ; l2 norm of gradient:  0.001935872479302618 ; l2 norm of weights:  0.40046654017840094\n",
            "Iteration#:  8204 ; Loss:  0.48384672304475107 ; l2 norm of gradient:  0.0019326885982991827 ; l2 norm of weights:  0.4004665124630915\n",
            "Iteration#:  8208 ; Loss:  0.4838427343818131 ; l2 norm of gradient:  0.0019295199077298299 ; l2 norm of weights:  0.40046648725655376\n",
            "Iteration#:  8212 ; Loss:  0.48383875182984265 ; l2 norm of gradient:  0.001926366313160433 ; l2 norm of weights:  0.4004664645509462\n",
            "Iteration#:  8216 ; Loss:  0.48383477537916036 ; l2 norm of gradient:  0.0019232277206582761 ; l2 norm of weights:  0.4004664443384546\n",
            "Iteration#:  8220 ; Loss:  0.48383080502010434 ; l2 norm of gradient:  0.0019201040367887302 ; l2 norm of weights:  0.40046642661129184\n",
            "Iteration#:  8224 ; Loss:  0.4838268407430297 ; l2 norm of gradient:  0.001916995168615557 ; l2 norm of weights:  0.4004664113616981\n",
            "Iteration#:  8228 ; Loss:  0.48382288253830874 ; l2 norm of gradient:  0.0019139010236980363 ; l2 norm of weights:  0.4004663985819403\n",
            "Iteration#:  8232 ; Loss:  0.48381893039633095 ; l2 norm of gradient:  0.0019108215100904124 ; l2 norm of weights:  0.40046638826431213\n",
            "Iteration#:  8236 ; Loss:  0.48381498430750314 ; l2 norm of gradient:  0.0019077565363393213 ; l2 norm of weights:  0.40046638040113425\n",
            "Iteration#:  8240 ; Loss:  0.48381104426224913 ; l2 norm of gradient:  0.0019047060114836076 ; l2 norm of weights:  0.4004663749847536\n",
            "Iteration#:  8244 ; Loss:  0.4838071102510094 ; l2 norm of gradient:  0.0019016698450513975 ; l2 norm of weights:  0.40046637200754354\n",
            "Iteration#:  8248 ; Loss:  0.4838031822642418 ; l2 norm of gradient:  0.001898647947059841 ; l2 norm of weights:  0.4004663714619039\n",
            "Iteration#:  8252 ; Loss:  0.48379926029242093 ; l2 norm of gradient:  0.0018956402280123803 ; l2 norm of weights:  0.4004663733402606\n",
            "Iteration#:  8256 ; Loss:  0.48379534432603843 ; l2 norm of gradient:  0.0018926465988982316 ; l2 norm of weights:  0.40046637763506554\n",
            "Iteration#:  8260 ; Loss:  0.4837914343556026 ; l2 norm of gradient:  0.001889666971189863 ; l2 norm of weights:  0.40046638433879655\n",
            "Iteration#:  8264 ; Loss:  0.4837875303716388 ; l2 norm of gradient:  0.0018867012568424938 ; l2 norm of weights:  0.4004663934439575\n",
            "Iteration#:  8268 ; Loss:  0.4837836323646889 ; l2 norm of gradient:  0.0018837493682915655 ; l2 norm of weights:  0.4004664049430777\n",
            "Iteration#:  8272 ; Loss:  0.48377974032531146 ; l2 norm of gradient:  0.00188081121845173 ; l2 norm of weights:  0.4004664188287121\n",
            "Iteration#:  8276 ; Loss:  0.4837758542440819 ; l2 norm of gradient:  0.0018778867207150998 ; l2 norm of weights:  0.4004664350934409\n",
            "Iteration#:  8280 ; Loss:  0.48377197411159223 ; l2 norm of gradient:  0.0018749757889496438 ; l2 norm of weights:  0.40046645372986994\n",
            "Iteration#:  8284 ; Loss:  0.48376809991845104 ; l2 norm of gradient:  0.0018720783374979516 ; l2 norm of weights:  0.40046647473063013\n",
            "Iteration#:  8288 ; Loss:  0.4837642316552832 ; l2 norm of gradient:  0.0018691942811748664 ; l2 norm of weights:  0.4004664980883773\n",
            "Iteration#:  8292 ; Loss:  0.4837603693127307 ; l2 norm of gradient:  0.001866323535266864 ; l2 norm of weights:  0.4004665237957926\n",
            "Iteration#:  8296 ; Loss:  0.4837565128814513 ; l2 norm of gradient:  0.001863466015529678 ; l2 norm of weights:  0.40046655184558155\n",
            "Iteration#:  8300 ; Loss:  0.4837526623521195 ; l2 norm of gradient:  0.0018606216381872058 ; l2 norm of weights:  0.4004665822304748\n",
            "Iteration#:  8304 ; Loss:  0.4837488177154262 ; l2 norm of gradient:  0.0018577903199295035 ; l2 norm of weights:  0.40046661494322744\n",
            "Iteration#:  8308 ; Loss:  0.4837449789620787 ; l2 norm of gradient:  0.0018549719779115757 ; l2 norm of weights:  0.4004666499766191\n",
            "Iteration#:  8312 ; Loss:  0.4837411460828003 ; l2 norm of gradient:  0.0018521665297514935 ; l2 norm of weights:  0.40046668732345386\n",
            "Iteration#:  8316 ; Loss:  0.4837373190683309 ; l2 norm of gradient:  0.0018493738935288805 ; l2 norm of weights:  0.4004667269765598\n",
            "Iteration#:  8320 ; Loss:  0.4837334979094262 ; l2 norm of gradient:  0.0018465939877833676 ; l2 norm of weights:  0.4004667689287895\n",
            "Iteration#:  8324 ; Loss:  0.48372968259685867 ; l2 norm of gradient:  0.0018438267315125084 ; l2 norm of weights:  0.40046681317301935\n",
            "Iteration#:  8328 ; Loss:  0.48372587312141624 ; l2 norm of gradient:  0.001841072044171132 ; l2 norm of weights:  0.40046685970214985\n",
            "Iteration#:  8332 ; Loss:  0.48372206947390345 ; l2 norm of gradient:  0.0018383298456687571 ; l2 norm of weights:  0.40046690850910527\n",
            "Iteration#:  8336 ; Loss:  0.4837182716451406 ; l2 norm of gradient:  0.0018356000563681818 ; l2 norm of weights:  0.40046695958683337\n",
            "Iteration#:  8340 ; Loss:  0.483714479625964 ; l2 norm of gradient:  0.0018328825970845655 ; l2 norm of weights:  0.4004670129283059\n",
            "Iteration#:  8344 ; Loss:  0.48371069340722606 ; l2 norm of gradient:  0.0018301773890824213 ; l2 norm of weights:  0.40046706852651776\n",
            "Iteration#:  8348 ; Loss:  0.4837069129797952 ; l2 norm of gradient:  0.0018274843540753221 ; l2 norm of weights:  0.40046712637448745\n",
            "Iteration#:  8352 ; Loss:  0.4837031383345556 ; l2 norm of gradient:  0.0018248034142238205 ; l2 norm of weights:  0.4004671864652568\n",
            "Iteration#:  8356 ; Loss:  0.483699369462407 ; l2 norm of gradient:  0.0018221344921329921 ; l2 norm of weights:  0.4004672487918905\n",
            "Iteration#:  8360 ; Loss:  0.4836956063542657 ; l2 norm of gradient:  0.001819477510852071 ; l2 norm of weights:  0.4004673133474767\n",
            "Iteration#:  8364 ; Loss:  0.48369184900106293 ; l2 norm of gradient:  0.0018168323938721307 ; l2 norm of weights:  0.40046738012512617\n",
            "Iteration#:  8368 ; Loss:  0.48368809739374613 ; l2 norm of gradient:  0.001814199065124604 ; l2 norm of weights:  0.4004674491179727\n",
            "Iteration#:  8372 ; Loss:  0.4836843515232787 ; l2 norm of gradient:  0.001811577448978891 ; l2 norm of weights:  0.40046752031917293\n",
            "Iteration#:  8376 ; Loss:  0.4836806113806389 ; l2 norm of gradient:  0.0018089674702420567 ; l2 norm of weights:  0.40046759372190593\n",
            "Iteration#:  8380 ; Loss:  0.4836768769568212 ; l2 norm of gradient:  0.001806369054156242 ; l2 norm of weights:  0.40046766931937344\n",
            "Iteration#:  8384 ; Loss:  0.48367314824283575 ; l2 norm of gradient:  0.001803782126397065 ; l2 norm of weights:  0.4004677471047997\n",
            "Iteration#:  8388 ; Loss:  0.4836694252297078 ; l2 norm of gradient:  0.0018012066130718744 ; l2 norm of weights:  0.400467827071431\n",
            "Iteration#:  8392 ; Loss:  0.4836657079084783 ; l2 norm of gradient:  0.001798642440718859 ; l2 norm of weights:  0.40046790921253633\n",
            "Iteration#:  8396 ; Loss:  0.48366199627020356 ; l2 norm of gradient:  0.001796089536304479 ; l2 norm of weights:  0.4004679935214065\n",
            "Iteration#:  8400 ; Loss:  0.4836582903059559 ; l2 norm of gradient:  0.001793547827222169 ; l2 norm of weights:  0.40046807999135436\n",
            "Iteration#:  8404 ; Loss:  0.48365459000682226 ; l2 norm of gradient:  0.0017910172412903807 ; l2 norm of weights:  0.40046816861571477\n",
            "Iteration#:  8408 ; Loss:  0.48365089536390543 ; l2 norm of gradient:  0.0017884977067515171 ; l2 norm of weights:  0.4004682593878446\n",
            "Iteration#:  8412 ; Loss:  0.48364720636832326 ; l2 norm of gradient:  0.0017859891522695342 ; l2 norm of weights:  0.40046835230112204\n",
            "Iteration#:  8416 ; Loss:  0.48364352301120894 ; l2 norm of gradient:  0.0017834915069288601 ; l2 norm of weights:  0.4004684473489473\n",
            "Iteration#:  8420 ; Loss:  0.48363984528371096 ; l2 norm of gradient:  0.0017810047002322742 ; l2 norm of weights:  0.40046854452474206\n",
            "Iteration#:  8424 ; Loss:  0.48363617317699314 ; l2 norm of gradient:  0.0017785286620994095 ; l2 norm of weights:  0.4004686438219494\n",
            "Iteration#:  8428 ; Loss:  0.4836325066822344 ; l2 norm of gradient:  0.0017760633228651314 ; l2 norm of weights:  0.40046874523403364\n",
            "Iteration#:  8432 ; Loss:  0.4836288457906286 ; l2 norm of gradient:  0.0017736086132777908 ; l2 norm of weights:  0.4004688487544807\n",
            "Iteration#:  8436 ; Loss:  0.48362519049338504 ; l2 norm of gradient:  0.0017711644644970532 ; l2 norm of weights:  0.40046895437679714\n",
            "Iteration#:  8440 ; Loss:  0.4836215407817277 ; l2 norm of gradient:  0.0017687308080931573 ; l2 norm of weights:  0.40046906209451133\n",
            "Iteration#:  8444 ; Loss:  0.4836178966468958 ; l2 norm of gradient:  0.001766307576044723 ; l2 norm of weights:  0.40046917190117176\n",
            "Iteration#:  8448 ; Loss:  0.4836142580801438 ; l2 norm of gradient:  0.0017638947007366196 ; l2 norm of weights:  0.40046928379034835\n",
            "Iteration#:  8452 ; Loss:  0.4836106250727408 ; l2 norm of gradient:  0.0017614921149591374 ; l2 norm of weights:  0.4004693977556318\n",
            "Iteration#:  8456 ; Loss:  0.4836069976159709 ; l2 norm of gradient:  0.0017590997519056465 ; l2 norm of weights:  0.40046951379063334\n",
            "Iteration#:  8460 ; Loss:  0.4836033757011332 ; l2 norm of gradient:  0.0017567175451710217 ; l2 norm of weights:  0.40046963188898477\n",
            "Iteration#:  8464 ; Loss:  0.48359975931954136 ; l2 norm of gradient:  0.0017543454287504215 ; l2 norm of weights:  0.4004697520443386\n",
            "Iteration#:  8468 ; Loss:  0.4835961484625244 ; l2 norm of gradient:  0.0017519833370364857 ; l2 norm of weights:  0.40046987425036773\n",
            "Iteration#:  8472 ; Loss:  0.48359254312142563 ; l2 norm of gradient:  0.001749631204819233 ; l2 norm of weights:  0.4004699985007651\n",
            "Iteration#:  8476 ; Loss:  0.48358894328760327 ; l2 norm of gradient:  0.0017472889672827414 ; l2 norm of weights:  0.4004701247892444\n",
            "Iteration#:  8480 ; Loss:  0.4835853489524304 ; l2 norm of gradient:  0.0017449565600044278 ; l2 norm of weights:  0.400470253109539\n",
            "Iteration#:  8484 ; Loss:  0.4835817601072945 ; l2 norm of gradient:  0.0017426339189532151 ; l2 norm of weights:  0.4004703834554026\n",
            "Iteration#:  8488 ; Loss:  0.4835781767435981 ; l2 norm of gradient:  0.001740320980487859 ; l2 norm of weights:  0.40047051582060894\n",
            "Iteration#:  8492 ; Loss:  0.4835745988527581 ; l2 norm of gradient:  0.0017380176813545783 ; l2 norm of weights:  0.4004706501989514\n",
            "Iteration#:  8496 ; Loss:  0.48357102642620586 ; l2 norm of gradient:  0.0017357239586860758 ; l2 norm of weights:  0.4004707865842434\n",
            "Iteration#:  8500 ; Loss:  0.4835674594553878 ; l2 norm of gradient:  0.0017334397499999313 ; l2 norm of weights:  0.40047092497031783\n",
            "Iteration#:  8504 ; Loss:  0.4835638979317643 ; l2 norm of gradient:  0.0017311649931963779 ; l2 norm of weights:  0.40047106535102756\n",
            "Iteration#:  8508 ; Loss:  0.4835603418468104 ; l2 norm of gradient:  0.001728899626556593 ; l2 norm of weights:  0.4004712077202447\n",
            "Iteration#:  8512 ; Loss:  0.4835567911920158 ; l2 norm of gradient:  0.0017266435887417087 ; l2 norm of weights:  0.40047135207186096\n",
            "Iteration#:  8516 ; Loss:  0.4835532459588845 ; l2 norm of gradient:  0.0017243968187902242 ; l2 norm of weights:  0.4004714983997875\n",
            "Iteration#:  8520 ; Loss:  0.48354970613893467 ; l2 norm of gradient:  0.001722159256117106 ; l2 norm of weights:  0.4004716466979546\n",
            "Iteration#:  8524 ; Loss:  0.4835461717236993 ; l2 norm of gradient:  0.0017199308405111493 ; l2 norm of weights:  0.4004717969603118\n",
            "Iteration#:  8528 ; Loss:  0.48354264270472525 ; l2 norm of gradient:  0.0017177115121341628 ; l2 norm of weights:  0.4004719491808279\n",
            "Iteration#:  8532 ; Loss:  0.48353911907357383 ; l2 norm of gradient:  0.0017155012115189934 ; l2 norm of weights:  0.40047210335349076\n",
            "Iteration#:  8536 ; Loss:  0.4835356008218208 ; l2 norm of gradient:  0.0017132998795675641 ; l2 norm of weights:  0.40047225947230686\n",
            "Iteration#:  8540 ; Loss:  0.48353208794105595 ; l2 norm of gradient:  0.0017111074575493909 ; l2 norm of weights:  0.4004724175313019\n",
            "Iteration#:  8544 ; Loss:  0.48352858042288327 ; l2 norm of gradient:  0.0017089238870998684 ; l2 norm of weights:  0.40047257752452037\n",
            "Iteration#:  8548 ; Loss:  0.4835250782589209 ; l2 norm of gradient:  0.0017067491102189724 ; l2 norm of weights:  0.4004727394460253\n",
            "Iteration#:  8552 ; Loss:  0.4835215814408014 ; l2 norm of gradient:  0.0017045830692683486 ; l2 norm of weights:  0.4004729032898986\n",
            "Iteration#:  8556 ; Loss:  0.48351808996017087 ; l2 norm of gradient:  0.0017024257069707823 ; l2 norm of weights:  0.4004730690502405\n",
            "Iteration#:  8560 ; Loss:  0.4835146038086903 ; l2 norm of gradient:  0.001700276966408702 ; l2 norm of weights:  0.4004732367211698\n",
            "Iteration#:  8564 ; Loss:  0.48351112297803384 ; l2 norm of gradient:  0.0016981367910211727 ; l2 norm of weights:  0.4004734062968238\n",
            "Iteration#:  8568 ; Loss:  0.4835076474598902 ; l2 norm of gradient:  0.0016960051246032632 ; l2 norm of weights:  0.40047357777135795\n",
            "Iteration#:  8572 ; Loss:  0.48350417724596195 ; l2 norm of gradient:  0.0016938819113041384 ; l2 norm of weights:  0.4004737511389462\n",
            "Iteration#:  8576 ; Loss:  0.48350071232796543 ; l2 norm of gradient:  0.0016917670956250919 ; l2 norm of weights:  0.4004739263937803\n",
            "Iteration#:  8580 ; Loss:  0.4834972526976312 ; l2 norm of gradient:  0.0016896606224180515 ; l2 norm of weights:  0.40047410353007057\n",
            "Iteration#:  8584 ; Loss:  0.48349379834670336 ; l2 norm of gradient:  0.001687562436884206 ; l2 norm of weights:  0.4004742825420448\n",
            "Iteration#:  8588 ; Loss:  0.48349034926694034 ; l2 norm of gradient:  0.0016854724845719083 ; l2 norm of weights:  0.4004744634239493\n",
            "Iteration#:  8592 ; Loss:  0.4834869054501141 ; l2 norm of gradient:  0.001683390711374808 ; l2 norm of weights:  0.4004746461700478\n",
            "Iteration#:  8596 ; Loss:  0.48348346688801 ; l2 norm of gradient:  0.0016813170635308129 ; l2 norm of weights:  0.4004748307746219\n",
            "Iteration#:  8600 ; Loss:  0.48348003357242814 ; l2 norm of gradient:  0.001679251487619791 ; l2 norm of weights:  0.40047501723197115\n",
            "Iteration#:  8604 ; Loss:  0.48347660549518134 ; l2 norm of gradient:  0.0016771939305623457 ; l2 norm of weights:  0.40047520553641247\n",
            "Iteration#:  8608 ; Loss:  0.4834731826480968 ; l2 norm of gradient:  0.0016751443396179689 ; l2 norm of weights:  0.40047539568228047\n",
            "Iteration#:  8612 ; Loss:  0.48346976502301525 ; l2 norm of gradient:  0.0016731026623832978 ; l2 norm of weights:  0.40047558766392727\n",
            "Iteration#:  8616 ; Loss:  0.4834663526117908 ; l2 norm of gradient:  0.001671068846790381 ; l2 norm of weights:  0.4004757814757223\n",
            "Iteration#:  8620 ; Loss:  0.48346294540629153 ; l2 norm of gradient:  0.0016690428411055987 ; l2 norm of weights:  0.4004759771120526\n",
            "Iteration#:  8624 ; Loss:  0.48345954339839886 ; l2 norm of gradient:  0.0016670245939268138 ; l2 norm of weights:  0.400476174567322\n",
            "Iteration#:  8628 ; Loss:  0.4834561465800079 ; l2 norm of gradient:  0.0016650140541832079 ; l2 norm of weights:  0.4004763738359522\n",
            "Iteration#:  8632 ; Loss:  0.4834527549430272 ; l2 norm of gradient:  0.0016630111711320402 ; l2 norm of weights:  0.4004765749123813\n",
            "Iteration#:  8636 ; Loss:  0.48344936847937897 ; l2 norm of gradient:  0.001661015894358644 ; l2 norm of weights:  0.40047677779106505\n",
            "Iteration#:  8640 ; Loss:  0.48344598718099874 ; l2 norm of gradient:  0.0016590281737731774 ; l2 norm of weights:  0.4004769824664759\n",
            "Iteration#:  8644 ; Loss:  0.4834426110398353 ; l2 norm of gradient:  0.0016570479596102043 ; l2 norm of weights:  0.40047718893310313\n",
            "Iteration#:  8648 ; Loss:  0.48343924004785155 ; l2 norm of gradient:  0.0016550752024265902 ; l2 norm of weights:  0.40047739718545333\n",
            "Iteration#:  8652 ; Loss:  0.48343587419702294 ; l2 norm of gradient:  0.0016531098530994212 ; l2 norm of weights:  0.40047760721804926\n",
            "Iteration#:  8656 ; Loss:  0.48343251347933874 ; l2 norm of gradient:  0.0016511518628250167 ; l2 norm of weights:  0.40047781902543084\n",
            "Iteration#:  8660 ; Loss:  0.48342915788680146 ; l2 norm of gradient:  0.0016492011831169809 ; l2 norm of weights:  0.4004780326021544\n",
            "Iteration#:  8664 ; Loss:  0.4834258074114269 ; l2 norm of gradient:  0.0016472577658044375 ; l2 norm of weights:  0.40047824794279296\n",
            "Iteration#:  8668 ; Loss:  0.4834224620452442 ; l2 norm of gradient:  0.001645321563030916 ; l2 norm of weights:  0.40047846504193607\n",
            "Iteration#:  8672 ; Loss:  0.48341912178029556 ; l2 norm of gradient:  0.0016433925272523419 ; l2 norm of weights:  0.40047868389418956\n",
            "Iteration#:  8676 ; Loss:  0.48341578660863666 ; l2 norm of gradient:  0.0016414706112352377 ; l2 norm of weights:  0.40047890449417567\n",
            "Iteration#:  8680 ; Loss:  0.4834124565223362 ; l2 norm of gradient:  0.0016395557680551837 ; l2 norm of weights:  0.40047912683653303\n",
            "Iteration#:  8684 ; Loss:  0.4834091315134761 ; l2 norm of gradient:  0.0016376479510957149 ; l2 norm of weights:  0.40047935091591663\n",
            "Iteration#:  8688 ; Loss:  0.4834058115741512 ; l2 norm of gradient:  0.001635747114046079 ; l2 norm of weights:  0.40047957672699724\n",
            "Iteration#:  8692 ; Loss:  0.48340249669646984 ; l2 norm of gradient:  0.0016338532108998358 ; l2 norm of weights:  0.400479804264462\n",
            "Iteration#:  8696 ; Loss:  0.4833991868725532 ; l2 norm of gradient:  0.0016319661959528864 ; l2 norm of weights:  0.40048003352301415\n",
            "Iteration#:  8700 ; Loss:  0.48339588209453543 ; l2 norm of gradient:  0.0016300860238032263 ; l2 norm of weights:  0.4004802644973728\n",
            "Iteration#:  8704 ; Loss:  0.48339258235456395 ; l2 norm of gradient:  0.0016282126493475059 ; l2 norm of weights:  0.40048049718227285\n",
            "Iteration#:  8708 ; Loss:  0.4833892876447989 ; l2 norm of gradient:  0.0016263460277802582 ; l2 norm of weights:  0.4004807315724652\n",
            "Iteration#:  8712 ; Loss:  0.48338599795741377 ; l2 norm of gradient:  0.0016244861145928828 ; l2 norm of weights:  0.40048096766271657\n",
            "Iteration#:  8716 ; Loss:  0.4833827132845947 ; l2 norm of gradient:  0.0016226328655708265 ; l2 norm of weights:  0.4004812054478093\n",
            "Iteration#:  8720 ; Loss:  0.4833794336185404 ; l2 norm of gradient:  0.0016207862367929617 ; l2 norm of weights:  0.4004814449225413\n",
            "Iteration#:  8724 ; Loss:  0.48337615895146346 ; l2 norm of gradient:  0.0016189461846298923 ; l2 norm of weights:  0.40048168608172624\n",
            "Iteration#:  8728 ; Loss:  0.4833728892755884 ; l2 norm of gradient:  0.001617112665741833 ; l2 norm of weights:  0.40048192892019324\n",
            "Iteration#:  8732 ; Loss:  0.4833696245831531 ; l2 norm of gradient:  0.0016152856370772286 ; l2 norm of weights:  0.4004821734327869\n",
            "Iteration#:  8736 ; Loss:  0.483366364866408 ; l2 norm of gradient:  0.001613465055872091 ; l2 norm of weights:  0.40048241961436726\n",
            "Iteration#:  8740 ; Loss:  0.48336311011761635 ; l2 norm of gradient:  0.0016116508796464217 ; l2 norm of weights:  0.4004826674598096\n",
            "Iteration#:  8744 ; Loss:  0.4833598603290544 ; l2 norm of gradient:  0.0016098430662050095 ; l2 norm of weights:  0.40048291696400456\n",
            "Iteration#:  8748 ; Loss:  0.4833566154930107 ; l2 norm of gradient:  0.0016080415736339636 ; l2 norm of weights:  0.4004831681218579\n",
            "Iteration#:  8752 ; Loss:  0.48335337560178687 ; l2 norm of gradient:  0.0016062463603003286 ; l2 norm of weights:  0.4004834209282906\n",
            "Iteration#:  8756 ; Loss:  0.4833501406476972 ; l2 norm of gradient:  0.0016044573848502225 ; l2 norm of weights:  0.400483675378239\n",
            "Iteration#:  8760 ; Loss:  0.4833469106230684 ; l2 norm of gradient:  0.0016026746062064929 ; l2 norm of weights:  0.4004839314666539\n",
            "Iteration#:  8764 ; Loss:  0.48334368552023976 ; l2 norm of gradient:  0.0016008979835688089 ; l2 norm of weights:  0.40048418918850154\n",
            "Iteration#:  8768 ; Loss:  0.4833404653315637 ; l2 norm of gradient:  0.001599127476410299 ; l2 norm of weights:  0.400484448538763\n",
            "Iteration#:  8772 ; Loss:  0.4833372500494047 ; l2 norm of gradient:  0.0015973630444776393 ; l2 norm of weights:  0.4004847095124341\n",
            "Iteration#:  8776 ; Loss:  0.48333403966613986 ; l2 norm of gradient:  0.0015956046477882226 ; l2 norm of weights:  0.40048497210452555\n",
            "Iteration#:  8780 ; Loss:  0.4833308341741591 ; l2 norm of gradient:  0.0015938522466295113 ; l2 norm of weights:  0.40048523631006266\n",
            "Iteration#:  8784 ; Loss:  0.48332763356586456 ; l2 norm of gradient:  0.001592105801556833 ; l2 norm of weights:  0.4004855021240857\n",
            "Iteration#:  8788 ; Loss:  0.4833244378336708 ; l2 norm of gradient:  0.0015903652733927924 ; l2 norm of weights:  0.4004857695416493\n",
            "Iteration#:  8792 ; Loss:  0.48332124697000517 ; l2 norm of gradient:  0.0015886306232244679 ; l2 norm of weights:  0.4004860385578227\n",
            "Iteration#:  8796 ; Loss:  0.4833180609673071 ; l2 norm of gradient:  0.0015869018124031284 ; l2 norm of weights:  0.40048630916768996\n",
            "Iteration#:  8800 ; Loss:  0.48331487981802873 ; l2 norm of gradient:  0.0015851788025420437 ; l2 norm of weights:  0.4004865813663491\n",
            "Iteration#:  8804 ; Loss:  0.48331170351463415 ; l2 norm of gradient:  0.001583461555514984 ; l2 norm of weights:  0.4004868551489129\n",
            "Iteration#:  8808 ; Loss:  0.48330853204960034 ; l2 norm of gradient:  0.0015817500334553381 ; l2 norm of weights:  0.4004871305105083\n",
            "Iteration#:  8812 ; Loss:  0.48330536541541624 ; l2 norm of gradient:  0.001580044198753712 ; l2 norm of weights:  0.40048740744627676\n",
            "Iteration#:  8816 ; Loss:  0.4833022036045831 ; l2 norm of gradient:  0.0015783440140571812 ; l2 norm of weights:  0.4004876859513736\n",
            "Iteration#:  8820 ; Loss:  0.48329904660961454 ; l2 norm of gradient:  0.001576649442267259 ; l2 norm of weights:  0.4004879660209686\n",
            "Iteration#:  8824 ; Loss:  0.48329589442303644 ; l2 norm of gradient:  0.0015749604465392572 ; l2 norm of weights:  0.4004882476502455\n",
            "Iteration#:  8828 ; Loss:  0.483292747037387 ; l2 norm of gradient:  0.0015732769902794368 ; l2 norm of weights:  0.40048853083440233\n",
            "Iteration#:  8832 ; Loss:  0.4832896044452164 ; l2 norm of gradient:  0.0015715990371449684 ; l2 norm of weights:  0.4004888155686508\n",
            "Iteration#:  8836 ; Loss:  0.4832864666390871 ; l2 norm of gradient:  0.0015699265510417417 ; l2 norm of weights:  0.40048910184821696\n",
            "Iteration#:  8840 ; Loss:  0.4832833336115738 ; l2 norm of gradient:  0.0015682594961230912 ; l2 norm of weights:  0.40048938966834047\n",
            "Iteration#:  8844 ; Loss:  0.48328020535526317 ; l2 norm of gradient:  0.0015665978367878184 ; l2 norm of weights:  0.4004896790242748\n",
            "Iteration#:  8848 ; Loss:  0.4832770818627542 ; l2 norm of gradient:  0.0015649415376801576 ; l2 norm of weights:  0.4004899699112875\n",
            "Iteration#:  8852 ; Loss:  0.48327396312665766 ; l2 norm of gradient:  0.001563290563686151 ; l2 norm of weights:  0.40049026232465956\n",
            "Iteration#:  8856 ; Loss:  0.48327084913959684 ; l2 norm of gradient:  0.0015616448799346439 ; l2 norm of weights:  0.400490556259686\n",
            "Iteration#:  8860 ; Loss:  0.48326773989420646 ; l2 norm of gradient:  0.0015600044517936034 ; l2 norm of weights:  0.4004908517116751\n",
            "Iteration#:  8864 ; Loss:  0.4832646353831338 ; l2 norm of gradient:  0.0015583692448707543 ; l2 norm of weights:  0.400491148675949\n",
            "Iteration#:  8868 ; Loss:  0.4832615355990379 ; l2 norm of gradient:  0.0015567392250102478 ; l2 norm of weights:  0.4004914471478433\n",
            "Iteration#:  8872 ; Loss:  0.4832584405345897 ; l2 norm of gradient:  0.0015551143582929442 ; l2 norm of weights:  0.4004917471227068\n",
            "Iteration#:  8876 ; Loss:  0.4832553501824722 ; l2 norm of gradient:  0.001553494611033467 ; l2 norm of weights:  0.4004920485959022\n",
            "Iteration#:  8880 ; Loss:  0.4832522645353804 ; l2 norm of gradient:  0.0015518799497801848 ; l2 norm of weights:  0.4004923515628055\n",
            "Iteration#:  8884 ; Loss:  0.48324918358602076 ; l2 norm of gradient:  0.0015502703413123621 ; l2 norm of weights:  0.4004926560188055\n",
            "Iteration#:  8888 ; Loss:  0.4832461073271122 ; l2 norm of gradient:  0.0015486657526408642 ; l2 norm of weights:  0.400492961959305\n",
            "Iteration#:  8892 ; Loss:  0.4832430357513851 ; l2 norm of gradient:  0.001547066151004364 ; l2 norm of weights:  0.4004932693797195\n",
            "Iteration#:  8896 ; Loss:  0.4832399688515818 ; l2 norm of gradient:  0.0015454715038694439 ; l2 norm of weights:  0.40049357827547793\n",
            "Iteration#:  8900 ; Loss:  0.48323690662045643 ; l2 norm of gradient:  0.0015438817789287544 ; l2 norm of weights:  0.4004938886420222\n",
            "Iteration#:  8904 ; Loss:  0.4832338490507748 ; l2 norm of gradient:  0.0015422969440996323 ; l2 norm of weights:  0.4004942004748074\n",
            "Iteration#:  8908 ; Loss:  0.48323079613531483 ; l2 norm of gradient:  0.001540716967523453 ; l2 norm of weights:  0.4004945137693016\n",
            "Iteration#:  8912 ; Loss:  0.4832277478668656 ; l2 norm of gradient:  0.0015391418175626425 ; l2 norm of weights:  0.40049482852098595\n",
            "Iteration#:  8916 ; Loss:  0.4832247042382286 ; l2 norm of gradient:  0.001537571462801508 ; l2 norm of weights:  0.4004951447253545\n",
            "Iteration#:  8920 ; Loss:  0.48322166524221627 ; l2 norm of gradient:  0.001536005872042598 ; l2 norm of weights:  0.4004954623779139\n",
            "Iteration#:  8924 ; Loss:  0.4832186308716532 ; l2 norm of gradient:  0.0015344450143072686 ; l2 norm of weights:  0.40049578147418397\n",
            "Iteration#:  8928 ; Loss:  0.48321560111937556 ; l2 norm of gradient:  0.0015328888588328022 ; l2 norm of weights:  0.40049610200969726\n",
            "Iteration#:  8932 ; Loss:  0.483212575978231 ; l2 norm of gradient:  0.0015313373750726496 ; l2 norm of weights:  0.40049642397999885\n",
            "Iteration#:  8936 ; Loss:  0.4832095554410789 ; l2 norm of gradient:  0.001529790532693712 ; l2 norm of weights:  0.4004967473806467\n",
            "Iteration#:  8940 ; Loss:  0.4832065395007902 ; l2 norm of gradient:  0.0015282483015755847 ; l2 norm of weights:  0.4004970722072114\n",
            "Iteration#:  8944 ; Loss:  0.48320352815024736 ; l2 norm of gradient:  0.0015267106518095175 ; l2 norm of weights:  0.40049739845527615\n",
            "Iteration#:  8948 ; Loss:  0.4832005213823443 ; l2 norm of gradient:  0.0015251775536964452 ; l2 norm of weights:  0.4004977261204365\n",
            "Iteration#:  8952 ; Loss:  0.4831975191899866 ; l2 norm of gradient:  0.0015236489777463915 ; l2 norm of weights:  0.4004980551983006\n",
            "Iteration#:  8956 ; Loss:  0.4831945215660911 ; l2 norm of gradient:  0.0015221248946765877 ; l2 norm of weights:  0.4004983856844893\n",
            "Iteration#:  8960 ; Loss:  0.48319152850358665 ; l2 norm of gradient:  0.0015206052754105815 ; l2 norm of weights:  0.4004987175746354\n",
            "Iteration#:  8964 ; Loss:  0.4831885399954128 ; l2 norm of gradient:  0.0015190900910765804 ; l2 norm of weights:  0.40049905086438453\n",
            "Iteration#:  8968 ; Loss:  0.48318555603452107 ; l2 norm of gradient:  0.0015175793130062123 ; l2 norm of weights:  0.4004993855493943\n",
            "Iteration#:  8972 ; Loss:  0.4831825766138741 ; l2 norm of gradient:  0.0015160729127343237 ; l2 norm of weights:  0.4004997216253346\n",
            "Iteration#:  8976 ; Loss:  0.4831796017264461 ; l2 norm of gradient:  0.0015145708619956437 ; l2 norm of weights:  0.4005000590878879\n",
            "Iteration#:  8980 ; Loss:  0.48317663136522254 ; l2 norm of gradient:  0.001513073132725213 ; l2 norm of weights:  0.4005003979327483\n",
            "Iteration#:  8984 ; Loss:  0.4831736655232002 ; l2 norm of gradient:  0.0015115796970561696 ; l2 norm of weights:  0.40050073815562254\n",
            "Iteration#:  8988 ; Loss:  0.48317070419338715 ; l2 norm of gradient:  0.0015100905273195244 ; l2 norm of weights:  0.40050107975222915\n",
            "Iteration#:  8992 ; Loss:  0.4831677473688029 ; l2 norm of gradient:  0.00150860559604205 ; l2 norm of weights:  0.4005014227182988\n",
            "Iteration#:  8996 ; Loss:  0.4831647950424782 ; l2 norm of gradient:  0.0015071248759448872 ; l2 norm of weights:  0.40050176704957424\n",
            "Iteration#:  9000 ; Loss:  0.483161847207455 ; l2 norm of gradient:  0.0015056483399430397 ; l2 norm of weights:  0.40050211274181\n",
            "Iteration#:  9004 ; Loss:  0.4831589038567863 ; l2 norm of gradient:  0.001504175961143776 ; l2 norm of weights:  0.4005024597907726\n",
            "Iteration#:  9008 ; Loss:  0.48315596498353663 ; l2 norm of gradient:  0.0015027077128450137 ; l2 norm of weights:  0.40050280819224054\n",
            "Iteration#:  9012 ; Loss:  0.4831530305807816 ; l2 norm of gradient:  0.0015012435685351326 ; l2 norm of weights:  0.40050315794200386\n",
            "Iteration#:  9016 ; Loss:  0.483150100641608 ; l2 norm of gradient:  0.0014997835018904532 ; l2 norm of weights:  0.4005035090358648\n",
            "Iteration#:  9020 ; Loss:  0.48314717515911365 ; l2 norm of gradient:  0.001498327486775328 ; l2 norm of weights:  0.400503861469637\n",
            "Iteration#:  9024 ; Loss:  0.48314425412640755 ; l2 norm of gradient:  0.001496875497239773 ; l2 norm of weights:  0.400504215239146\n",
            "Iteration#:  9028 ; Loss:  0.48314133753660987 ; l2 norm of gradient:  0.0014954275075190601 ; l2 norm of weights:  0.4005045703402287\n",
            "Iteration#:  9032 ; Loss:  0.4831384253828518 ; l2 norm of gradient:  0.001493983492031958 ; l2 norm of weights:  0.40050492676873406\n",
            "Iteration#:  9036 ; Loss:  0.4831355176582756 ; l2 norm of gradient:  0.001492543425380615 ; l2 norm of weights:  0.4005052845205222\n",
            "Iteration#:  9040 ; Loss:  0.4831326143560347 ; l2 norm of gradient:  0.001491107282347457 ; l2 norm of weights:  0.40050564359146507\n",
            "Iteration#:  9044 ; Loss:  0.4831297154692934 ; l2 norm of gradient:  0.0014896750378962105 ; l2 norm of weights:  0.4005060039774459\n",
            "Iteration#:  9048 ; Loss:  0.48312682099122695 ; l2 norm of gradient:  0.0014882466671689934 ; l2 norm of weights:  0.4005063656743595\n",
            "Iteration#:  9052 ; Loss:  0.48312393091502187 ; l2 norm of gradient:  0.0014868221454861646 ; l2 norm of weights:  0.40050672867811205\n",
            "Iteration#:  9056 ; Loss:  0.4831210452338751 ; l2 norm of gradient:  0.0014854014483449644 ; l2 norm of weights:  0.40050709298462106\n",
            "Iteration#:  9060 ; Loss:  0.48311816394099516 ; l2 norm of gradient:  0.0014839845514175714 ; l2 norm of weights:  0.40050745858981524\n",
            "Iteration#:  9064 ; Loss:  0.4831152870296012 ; l2 norm of gradient:  0.0014825714305514688 ; l2 norm of weights:  0.4005078254896349\n",
            "Iteration#:  9068 ; Loss:  0.4831124144929231 ; l2 norm of gradient:  0.001481162061766959 ; l2 norm of weights:  0.4005081936800314\n",
            "Iteration#:  9072 ; Loss:  0.483109546324202 ; l2 norm of gradient:  0.0014797564212564876 ; l2 norm of weights:  0.4005085631569673\n",
            "Iteration#:  9076 ; Loss:  0.48310668251668953 ; l2 norm of gradient:  0.0014783544853838421 ; l2 norm of weights:  0.4005089339164164\n",
            "Iteration#:  9080 ; Loss:  0.48310382306364835 ; l2 norm of gradient:  0.001476956230682413 ; l2 norm of weights:  0.4005093059543634\n",
            "Iteration#:  9084 ; Loss:  0.48310096795835195 ; l2 norm of gradient:  0.0014755616338549904 ; l2 norm of weights:  0.4005096792668044\n",
            "Iteration#:  9088 ; Loss:  0.48309811719408463 ; l2 norm of gradient:  0.0014741706717710118 ; l2 norm of weights:  0.40051005384974636\n",
            "Iteration#:  9092 ; Loss:  0.4830952707641412 ; l2 norm of gradient:  0.001472783321467136 ; l2 norm of weights:  0.4005104296992073\n",
            "Iteration#:  9096 ; Loss:  0.48309242866182767 ; l2 norm of gradient:  0.0014713995601458088 ; l2 norm of weights:  0.4005108068112161\n",
            "Iteration#:  9100 ; Loss:  0.48308959088046044 ; l2 norm of gradient:  0.0014700193651735062 ; l2 norm of weights:  0.4005111851818127\n",
            "Iteration#:  9104 ; Loss:  0.48308675741336704 ; l2 norm of gradient:  0.0014686427140796264 ; l2 norm of weights:  0.400511564807048\n",
            "Iteration#:  9108 ; Loss:  0.483083928253885 ; l2 norm of gradient:  0.0014672695845562183 ; l2 norm of weights:  0.40051194568298343\n",
            "Iteration#:  9112 ; Loss:  0.4830811033953633 ; l2 norm of gradient:  0.001465899954456819 ; l2 norm of weights:  0.40051232780569157\n",
            "Iteration#:  9116 ; Loss:  0.48307828283116105 ; l2 norm of gradient:  0.0014645338017939351 ; l2 norm of weights:  0.40051271117125553\n",
            "Iteration#:  9120 ; Loss:  0.4830754665546481 ; l2 norm of gradient:  0.0014631711047402617 ; l2 norm of weights:  0.4005130957757693\n",
            "Iteration#:  9124 ; Loss:  0.48307265455920545 ; l2 norm of gradient:  0.0014618118416250777 ; l2 norm of weights:  0.4005134816153377\n",
            "Iteration#:  9128 ; Loss:  0.48306984683822374 ; l2 norm of gradient:  0.00146045599093583 ; l2 norm of weights:  0.4005138686860757\n",
            "Iteration#:  9132 ; Loss:  0.4830670433851051 ; l2 norm of gradient:  0.0014591035313154495 ; l2 norm of weights:  0.4005142569841095\n",
            "Iteration#:  9136 ; Loss:  0.48306424419326166 ; l2 norm of gradient:  0.0014577544415608763 ; l2 norm of weights:  0.4005146465055755\n",
            "Iteration#:  9140 ; Loss:  0.48306144925611644 ; l2 norm of gradient:  0.001456408700623516 ; l2 norm of weights:  0.40051503724662085\n",
            "Iteration#:  9144 ; Loss:  0.4830586585671027 ; l2 norm of gradient:  0.001455066287607187 ; l2 norm of weights:  0.4005154292034031\n",
            "Iteration#:  9148 ; Loss:  0.4830558721196643 ; l2 norm of gradient:  0.0014537271817674526 ; l2 norm of weights:  0.4005158223720901\n",
            "Iteration#:  9152 ; Loss:  0.4830530899072557 ; l2 norm of gradient:  0.001452391362510686 ; l2 norm of weights:  0.40051621674886073\n",
            "Iteration#:  9156 ; Loss:  0.4830503119233419 ; l2 norm of gradient:  0.001451058809392566 ; l2 norm of weights:  0.4005166123299036\n",
            "Iteration#:  9160 ; Loss:  0.48304753816139795 ; l2 norm of gradient:  0.001449729502117202 ; l2 norm of weights:  0.4005170091114181\n",
            "Iteration#:  9164 ; Loss:  0.48304476861490964 ; l2 norm of gradient:  0.0014484034205370006 ; l2 norm of weights:  0.4005174070896137\n",
            "Iteration#:  9168 ; Loss:  0.48304200327737323 ; l2 norm of gradient:  0.001447080544650385 ; l2 norm of weights:  0.40051780626071043\n",
            "Iteration#:  9172 ; Loss:  0.4830392421422954 ; l2 norm of gradient:  0.0014457608546015574 ; l2 norm of weights:  0.40051820662093834\n",
            "Iteration#:  9176 ; Loss:  0.4830364852031927 ; l2 norm of gradient:  0.0014444443306792822 ; l2 norm of weights:  0.4005186081665379\n",
            "Iteration#:  9180 ; Loss:  0.4830337324535927 ; l2 norm of gradient:  0.001443130953316 ; l2 norm of weights:  0.40051901089375946\n",
            "Iteration#:  9184 ; Loss:  0.4830309838870329 ; l2 norm of gradient:  0.0014418207030866945 ; l2 norm of weights:  0.40051941479886394\n",
            "Iteration#:  9188 ; Loss:  0.4830282394970613 ; l2 norm of gradient:  0.0014405135607082945 ; l2 norm of weights:  0.4005198198781221\n",
            "Iteration#:  9192 ; Loss:  0.48302549927723615 ; l2 norm of gradient:  0.0014392095070382038 ; l2 norm of weights:  0.40052022612781496\n",
            "Iteration#:  9196 ; Loss:  0.483022763221126 ; l2 norm of gradient:  0.00143790852307352 ; l2 norm of weights:  0.40052063354423323\n",
            "Iteration#:  9200 ; Loss:  0.4830200313223095 ; l2 norm of gradient:  0.0014366105899503289 ; l2 norm of weights:  0.4005210421236781\n",
            "Iteration#:  9204 ; Loss:  0.4830173035743757 ; l2 norm of gradient:  0.0014353156889422122 ; l2 norm of weights:  0.4005214518624604\n",
            "Iteration#:  9208 ; Loss:  0.483014579970924 ; l2 norm of gradient:  0.0014340238014599763 ; l2 norm of weights:  0.4005218627569012\n",
            "Iteration#:  9212 ; Loss:  0.48301186050556366 ; l2 norm of gradient:  0.0014327349090501233 ; l2 norm of weights:  0.4005222748033312\n",
            "Iteration#:  9216 ; Loss:  0.4830091451719146 ; l2 norm of gradient:  0.0014314489933943144 ; l2 norm of weights:  0.4005226879980911\n",
            "Iteration#:  9220 ; Loss:  0.4830064339636065 ; l2 norm of gradient:  0.0014301660363082204 ; l2 norm of weights:  0.4005231023375314\n",
            "Iteration#:  9224 ; Loss:  0.4830037268742793 ; l2 norm of gradient:  0.0014288860197408766 ; l2 norm of weights:  0.40052351781801254\n",
            "Iteration#:  9228 ; Loss:  0.48300102389758315 ; l2 norm of gradient:  0.0014276089257729725 ; l2 norm of weights:  0.40052393443590456\n",
            "Iteration#:  9232 ; Loss:  0.4829983250271785 ; l2 norm of gradient:  0.0014263347366169458 ; l2 norm of weights:  0.40052435218758753\n",
            "Iteration#:  9236 ; Loss:  0.4829956302567353 ; l2 norm of gradient:  0.0014250634346156712 ; l2 norm of weights:  0.4005247710694508\n",
            "Iteration#:  9240 ; Loss:  0.4829929395799342 ; l2 norm of gradient:  0.0014237950022413985 ; l2 norm of weights:  0.40052519107789364\n",
            "Iteration#:  9244 ; Loss:  0.48299025299046583 ; l2 norm of gradient:  0.0014225294220947936 ; l2 norm of weights:  0.40052561220932514\n",
            "Iteration#:  9248 ; Loss:  0.48298757048203045 ; l2 norm of gradient:  0.0014212666769045856 ; l2 norm of weights:  0.40052603446016366\n",
            "Iteration#:  9252 ; Loss:  0.48298489204833883 ; l2 norm of gradient:  0.0014200067495261542 ; l2 norm of weights:  0.40052645782683755\n",
            "Iteration#:  9256 ; Loss:  0.48298221768311134 ; l2 norm of gradient:  0.0014187496229404071 ; l2 norm of weights:  0.40052688230578415\n",
            "Iteration#:  9260 ; Loss:  0.48297954738007876 ; l2 norm of gradient:  0.0014174952802541653 ; l2 norm of weights:  0.4005273078934508\n",
            "Iteration#:  9264 ; Loss:  0.48297688113298143 ; l2 norm of gradient:  0.001416243704697723 ; l2 norm of weights:  0.40052773458629415\n",
            "Iteration#:  9268 ; Loss:  0.48297421893557 ; l2 norm of gradient:  0.0014149948796248815 ; l2 norm of weights:  0.4005281623807803\n",
            "Iteration#:  9272 ; Loss:  0.48297156078160486 ; l2 norm of gradient:  0.0014137487885118546 ; l2 norm of weights:  0.4005285912733848\n",
            "Iteration#:  9276 ; Loss:  0.48296890666485626 ; l2 norm of gradient:  0.001412505414956535 ; l2 norm of weights:  0.4005290212605925\n",
            "Iteration#:  9280 ; Loss:  0.48296625657910464 ; l2 norm of gradient:  0.0014112647426778484 ; l2 norm of weights:  0.4005294523388978\n",
            "Iteration#:  9284 ; Loss:  0.4829636105181401 ; l2 norm of gradient:  0.0014100267555137644 ; l2 norm of weights:  0.4005298845048041\n",
            "Iteration#:  9288 ; Loss:  0.4829609684757626 ; l2 norm of gradient:  0.0014087914374220165 ; l2 norm of weights:  0.40053031775482467\n",
            "Iteration#:  9292 ; Loss:  0.482958330445782 ; l2 norm of gradient:  0.0014075587724781252 ; l2 norm of weights:  0.4005307520854813\n",
            "Iteration#:  9296 ; Loss:  0.4829556964220182 ; l2 norm of gradient:  0.001406328744875645 ; l2 norm of weights:  0.4005311874933056\n",
            "Iteration#:  9300 ; Loss:  0.4829530663983004 ; l2 norm of gradient:  0.0014051013389236378 ; l2 norm of weights:  0.400531623974838\n",
            "Iteration#:  9304 ; Loss:  0.4829504403684683 ; l2 norm of gradient:  0.0014038765390479728 ; l2 norm of weights:  0.4005320615266285\n",
            "Iteration#:  9308 ; Loss:  0.48294781832637085 ; l2 norm of gradient:  0.001402654329788445 ; l2 norm of weights:  0.40053250014523595\n",
            "Iteration#:  9312 ; Loss:  0.48294520026586674 ; l2 norm of gradient:  0.001401434695799977 ; l2 norm of weights:  0.4005329398272284\n",
            "Iteration#:  9316 ; Loss:  0.4829425861808248 ; l2 norm of gradient:  0.0014002176218497445 ; l2 norm of weights:  0.4005333805691828\n",
            "Iteration#:  9320 ; Loss:  0.48293997606512346 ; l2 norm of gradient:  0.0013990030928180321 ; l2 norm of weights:  0.4005338223676856\n",
            "Iteration#:  9324 ; Loss:  0.4829373699126506 ; l2 norm of gradient:  0.0013977910936968862 ; l2 norm of weights:  0.40053426521933166\n",
            "Iteration#:  9328 ; Loss:  0.48293476771730404 ; l2 norm of gradient:  0.0013965816095885873 ; l2 norm of weights:  0.40053470912072553\n",
            "Iteration#:  9332 ; Loss:  0.48293216947299134 ; l2 norm of gradient:  0.001395374625706411 ; l2 norm of weights:  0.4005351540684801\n",
            "Iteration#:  9336 ; Loss:  0.48292957517362955 ; l2 norm of gradient:  0.0013941701273718536 ; l2 norm of weights:  0.4005356000592176\n",
            "Iteration#:  9340 ; Loss:  0.48292698481314544 ; l2 norm of gradient:  0.0013929681000161983 ; l2 norm of weights:  0.40053604708956897\n",
            "Iteration#:  9344 ; Loss:  0.4829243983854753 ; l2 norm of gradient:  0.0013917685291777971 ; l2 norm of weights:  0.40053649515617396\n",
            "Iteration#:  9348 ; Loss:  0.48292181588456534 ; l2 norm of gradient:  0.0013905714005018918 ; l2 norm of weights:  0.4005369442556816\n",
            "Iteration#:  9352 ; Loss:  0.48291923730437103 ; l2 norm of gradient:  0.0013893766997401628 ; l2 norm of weights:  0.400537394384749\n",
            "Iteration#:  9356 ; Loss:  0.4829166626388576 ; l2 norm of gradient:  0.0013881844127500071 ; l2 norm of weights:  0.4005378455400428\n",
            "Iteration#:  9360 ; Loss:  0.48291409188199963 ; l2 norm of gradient:  0.001386994525492986 ; l2 norm of weights:  0.40053829771823796\n",
            "Iteration#:  9364 ; Loss:  0.4829115250277817 ; l2 norm of gradient:  0.0013858070240351818 ; l2 norm of weights:  0.40053875091601826\n",
            "Iteration#:  9368 ; Loss:  0.48290896207019746 ; l2 norm of gradient:  0.0013846218945456582 ; l2 norm of weights:  0.4005392051300763\n",
            "Iteration#:  9372 ; Loss:  0.4829064030032503 ; l2 norm of gradient:  0.001383439123295782 ; l2 norm of weights:  0.4005396603571132\n",
            "Iteration#:  9376 ; Loss:  0.4829038478209529 ; l2 norm of gradient:  0.0013822586966589698 ; l2 norm of weights:  0.4005401165938388\n",
            "Iteration#:  9380 ; Loss:  0.4829012965173279 ; l2 norm of gradient:  0.0013810806011096282 ; l2 norm of weights:  0.4005405738369716\n",
            "Iteration#:  9384 ; Loss:  0.4828987490864068 ; l2 norm of gradient:  0.0013799048232224183 ; l2 norm of weights:  0.4005410320832386\n",
            "Iteration#:  9388 ; Loss:  0.4828962055222309 ; l2 norm of gradient:  0.0013787313496713988 ; l2 norm of weights:  0.40054149132937544\n",
            "Iteration#:  9392 ; Loss:  0.48289366581885085 ; l2 norm of gradient:  0.0013775601672297634 ; l2 norm of weights:  0.40054195157212624\n",
            "Iteration#:  9396 ; Loss:  0.4828911299703268 ; l2 norm of gradient:  0.0013763912627685053 ; l2 norm of weights:  0.40054241280824365\n",
            "Iteration#:  9400 ; Loss:  0.48288859797072814 ; l2 norm of gradient:  0.0013752246232564985 ; l2 norm of weights:  0.4005428750344889\n",
            "Iteration#:  9404 ; Loss:  0.48288606981413373 ; l2 norm of gradient:  0.0013740602357587364 ; l2 norm of weights:  0.4005433382476314\n",
            "Iteration#:  9408 ; Loss:  0.48288354549463186 ; l2 norm of gradient:  0.0013728980874370778 ; l2 norm of weights:  0.4005438024444493\n",
            "Iteration#:  9412 ; Loss:  0.48288102500631996 ; l2 norm of gradient:  0.0013717381655478868 ; l2 norm of weights:  0.40054426762172896\n",
            "Iteration#:  9416 ; Loss:  0.48287850834330504 ; l2 norm of gradient:  0.0013705804574425868 ; l2 norm of weights:  0.4005447337762651\n",
            "Iteration#:  9420 ; Loss:  0.4828759954997033 ; l2 norm of gradient:  0.0013694249505666256 ; l2 norm of weights:  0.400545200904861\n",
            "Iteration#:  9424 ; Loss:  0.4828734864696403 ; l2 norm of gradient:  0.0013682716324584835 ; l2 norm of weights:  0.40054566900432803\n",
            "Iteration#:  9428 ; Loss:  0.48287098124725075 ; l2 norm of gradient:  0.0013671204907493956 ; l2 norm of weights:  0.40054613807148587\n",
            "Iteration#:  9432 ; Loss:  0.48286847982667874 ; l2 norm of gradient:  0.0013659715131626295 ; l2 norm of weights:  0.4005466081031626\n",
            "Iteration#:  9436 ; Loss:  0.4828659822020778 ; l2 norm of gradient:  0.0013648246875122892 ; l2 norm of weights:  0.40054707909619447\n",
            "Iteration#:  9440 ; Loss:  0.4828634883676101 ; l2 norm of gradient:  0.001363680001703389 ; l2 norm of weights:  0.40054755104742584\n",
            "Iteration#:  9444 ; Loss:  0.4828609983174479 ; l2 norm of gradient:  0.0013625374437311291 ; l2 norm of weights:  0.40054802395370953\n",
            "Iteration#:  9448 ; Loss:  0.4828585120457719 ; l2 norm of gradient:  0.0013613970016795867 ; l2 norm of weights:  0.4005484978119062\n",
            "Iteration#:  9452 ; Loss:  0.4828560295467723 ; l2 norm of gradient:  0.001360258663721558 ; l2 norm of weights:  0.40054897261888495\n",
            "Iteration#:  9456 ; Loss:  0.48285355081464854 ; l2 norm of gradient:  0.001359122418117713 ; l2 norm of weights:  0.40054944837152257\n",
            "Iteration#:  9460 ; Loss:  0.48285107584360915 ; l2 norm of gradient:  0.0013579882532163265 ; l2 norm of weights:  0.4005499250667044\n",
            "Iteration#:  9464 ; Loss:  0.4828486046278718 ; l2 norm of gradient:  0.0013568561574523036 ; l2 norm of weights:  0.40055040270132364\n",
            "Iteration#:  9468 ; Loss:  0.48284613716166314 ; l2 norm of gradient:  0.0013557261193464086 ; l2 norm of weights:  0.40055088127228133\n",
            "Iteration#:  9472 ; Loss:  0.48284367343921925 ; l2 norm of gradient:  0.0013545981275052065 ; l2 norm of weights:  0.40055136077648684\n",
            "Iteration#:  9476 ; Loss:  0.482841213454785 ; l2 norm of gradient:  0.00135347217061972 ; l2 norm of weights:  0.40055184121085746\n",
            "Iteration#:  9480 ; Loss:  0.4828387572026146 ; l2 norm of gradient:  0.0013523482374652696 ; l2 norm of weights:  0.40055232257231826\n",
            "Iteration#:  9484 ; Loss:  0.48283630467697114 ; l2 norm of gradient:  0.001351226316900863 ; l2 norm of weights:  0.4005528048578023\n",
            "Iteration#:  9488 ; Loss:  0.4828338558721267 ; l2 norm of gradient:  0.0013501063978684418 ; l2 norm of weights:  0.40055328806425067\n",
            "Iteration#:  9492 ; Loss:  0.4828314107823626 ; l2 norm of gradient:  0.0013489884693920324 ; l2 norm of weights:  0.4005537721886123\n",
            "Iteration#:  9496 ; Loss:  0.4828289694019692 ; l2 norm of gradient:  0.0013478725205778397 ; l2 norm of weights:  0.4005542572278438\n",
            "Iteration#:  9500 ; Loss:  0.4828265317252456 ; l2 norm of gradient:  0.0013467585406128292 ; l2 norm of weights:  0.40055474317890993\n",
            "Iteration#:  9504 ; Loss:  0.4828240977465 ; l2 norm of gradient:  0.0013456465187648432 ; l2 norm of weights:  0.40055523003878296\n",
            "Iteration#:  9508 ; Loss:  0.48282166746004973 ; l2 norm of gradient:  0.0013445364443812984 ; l2 norm of weights:  0.40055571780444316\n",
            "Iteration#:  9512 ; Loss:  0.4828192408602209 ; l2 norm of gradient:  0.0013434283068891695 ; l2 norm of weights:  0.4005562064728783\n",
            "Iteration#:  9516 ; Loss:  0.4828168179413486 ; l2 norm of gradient:  0.0013423220957945326 ; l2 norm of weights:  0.40055669604108424\n",
            "Iteration#:  9520 ; Loss:  0.48281439869777704 ; l2 norm of gradient:  0.0013412178006810668 ; l2 norm of weights:  0.40055718650606437\n",
            "Iteration#:  9524 ; Loss:  0.48281198312385887 ; l2 norm of gradient:  0.001340115411210283 ; l2 norm of weights:  0.40055767786482954\n",
            "Iteration#:  9528 ; Loss:  0.4828095712139562 ; l2 norm of gradient:  0.0013390149171208245 ; l2 norm of weights:  0.4005581701143987\n",
            "Iteration#:  9532 ; Loss:  0.4828071629624397 ; l2 norm of gradient:  0.001337916308228018 ; l2 norm of weights:  0.40055866325179823\n",
            "Iteration#:  9536 ; Loss:  0.4828047583636889 ; l2 norm of gradient:  0.0013368195744225813 ; l2 norm of weights:  0.40055915727406205\n",
            "Iteration#:  9540 ; Loss:  0.4828023574120922 ; l2 norm of gradient:  0.0013357247056708236 ; l2 norm of weights:  0.4005596521782317\n",
            "Iteration#:  9544 ; Loss:  0.48279996010204695 ; l2 norm of gradient:  0.0013346316920142525 ; l2 norm of weights:  0.4005601479613564\n",
            "Iteration#:  9548 ; Loss:  0.4827975664279594 ; l2 norm of gradient:  0.0013335405235677324 ; l2 norm of weights:  0.400560644620493\n",
            "Iteration#:  9552 ; Loss:  0.4827951763842442 ; l2 norm of gradient:  0.001332451190520781 ; l2 norm of weights:  0.4005611421527055\n",
            "Iteration#:  9556 ; Loss:  0.4827927899653252 ; l2 norm of gradient:  0.0013313636831352833 ; l2 norm of weights:  0.40056164055506566\n",
            "Iteration#:  9560 ; Loss:  0.4827904071656349 ; l2 norm of gradient:  0.0013302779917459128 ; l2 norm of weights:  0.4005621398246529\n",
            "Iteration#:  9564 ; Loss:  0.48278802797961434 ; l2 norm of gradient:  0.0013291941067598895 ; l2 norm of weights:  0.4005626399585535\n",
            "Iteration#:  9568 ; Loss:  0.4827856524017138 ; l2 norm of gradient:  0.001328112018654833 ; l2 norm of weights:  0.400563140953862\n",
            "Iteration#:  9572 ; Loss:  0.48278328042639185 ; l2 norm of gradient:  0.001327031717980794 ; l2 norm of weights:  0.40056364280767953\n",
            "Iteration#:  9576 ; Loss:  0.4827809120481159 ; l2 norm of gradient:  0.0013259531953571967 ; l2 norm of weights:  0.4005641455171151\n",
            "Iteration#:  9580 ; Loss:  0.48277854726136216 ; l2 norm of gradient:  0.001324876441473775 ; l2 norm of weights:  0.4005646490792851\n",
            "Iteration#:  9584 ; Loss:  0.4827761860606154 ; l2 norm of gradient:  0.0013238014470898308 ; l2 norm of weights:  0.40056515349131283\n",
            "Iteration#:  9588 ; Loss:  0.4827738284403692 ; l2 norm of gradient:  0.0013227282030330281 ; l2 norm of weights:  0.4005656587503295\n",
            "Iteration#:  9592 ; Loss:  0.4827714743951256 ; l2 norm of gradient:  0.0013216567001999324 ; l2 norm of weights:  0.4005661648534731\n",
            "Iteration#:  9596 ; Loss:  0.4827691239193955 ; l2 norm of gradient:  0.001320586929554721 ; l2 norm of weights:  0.4005666717978892\n",
            "Iteration#:  9600 ; Loss:  0.4827667770076983 ; l2 norm of gradient:  0.0013195188821289504 ; l2 norm of weights:  0.4005671795807305\n",
            "Iteration#:  9604 ; Loss:  0.4827644336545621 ; l2 norm of gradient:  0.0013184525490211269 ; l2 norm of weights:  0.400567688199157\n",
            "Iteration#:  9608 ; Loss:  0.4827620938545236 ; l2 norm of gradient:  0.001317387921396143 ; l2 norm of weights:  0.4005681976503358\n",
            "Iteration#:  9612 ; Loss:  0.4827597576021278 ; l2 norm of gradient:  0.0013163249904846802 ; l2 norm of weights:  0.4005687079314412\n",
            "Iteration#:  9616 ; Loss:  0.48275742489192874 ; l2 norm of gradient:  0.0013152637475825095 ; l2 norm of weights:  0.40056921903965503\n",
            "Iteration#:  9620 ; Loss:  0.4827550957184887 ; l2 norm of gradient:  0.0013142041840504353 ; l2 norm of weights:  0.4005697309721655\n",
            "Iteration#:  9624 ; Loss:  0.4827527700763786 ; l2 norm of gradient:  0.001313146291314189 ; l2 norm of weights:  0.40057024372616873\n",
            "Iteration#:  9628 ; Loss:  0.48275044796017796 ; l2 norm of gradient:  0.0013120900608628483 ; l2 norm of weights:  0.4005707572988674\n",
            "Iteration#:  9632 ; Loss:  0.4827481293644746 ; l2 norm of gradient:  0.0013110354842486586 ; l2 norm of weights:  0.40057127168747164\n",
            "Iteration#:  9636 ; Loss:  0.48274581428386504 ; l2 norm of gradient:  0.0013099825530875295 ; l2 norm of weights:  0.4005717868891983\n",
            "Iteration#:  9640 ; Loss:  0.48274350271295424 ; l2 norm of gradient:  0.001308931259057287 ; l2 norm of weights:  0.4005723029012715\n",
            "Iteration#:  9644 ; Loss:  0.4827411946463555 ; l2 norm of gradient:  0.0013078815938981302 ; l2 norm of weights:  0.40057281972092235\n",
            "Iteration#:  9648 ; Loss:  0.4827388900786908 ; l2 norm of gradient:  0.0013068335494115454 ; l2 norm of weights:  0.40057333734538875\n",
            "Iteration#:  9652 ; Loss:  0.48273658900459043 ; l2 norm of gradient:  0.001305787117459997 ; l2 norm of weights:  0.40057385577191595\n",
            "Iteration#:  9656 ; Loss:  0.4827342914186931 ; l2 norm of gradient:  0.0013047422899673273 ; l2 norm of weights:  0.4005743749977558\n",
            "Iteration#:  9660 ; Loss:  0.4827319973156461 ; l2 norm of gradient:  0.0013036990589162909 ; l2 norm of weights:  0.40057489502016713\n",
            "Iteration#:  9664 ; Loss:  0.48272970669010495 ; l2 norm of gradient:  0.0013026574163502605 ; l2 norm of weights:  0.4005754158364159\n",
            "Iteration#:  9668 ; Loss:  0.4827274195367334 ; l2 norm of gradient:  0.0013016173543717445 ; l2 norm of weights:  0.40057593744377495\n",
            "Iteration#:  9672 ; Loss:  0.4827251358502039 ; l2 norm of gradient:  0.0013005788651417349 ; l2 norm of weights:  0.40057645983952356\n",
            "Iteration#:  9676 ; Loss:  0.4827228556251971 ; l2 norm of gradient:  0.0012995419408799915 ; l2 norm of weights:  0.4005769830209483\n",
            "Iteration#:  9680 ; Loss:  0.4827205788564022 ; l2 norm of gradient:  0.0012985065738636886 ; l2 norm of weights:  0.4005775069853425\n",
            "Iteration#:  9684 ; Loss:  0.4827183055385163 ; l2 norm of gradient:  0.0012974727564281723 ; l2 norm of weights:  0.40057803173000606\n",
            "Iteration#:  9688 ; Loss:  0.48271603566624505 ; l2 norm of gradient:  0.001296440480964953 ; l2 norm of weights:  0.40057855725224595\n",
            "Iteration#:  9692 ; Loss:  0.48271376923430276 ; l2 norm of gradient:  0.0012954097399229991 ; l2 norm of weights:  0.4005790835493757\n",
            "Iteration#:  9696 ; Loss:  0.4827115062374113 ; l2 norm of gradient:  0.0012943805258070593 ; l2 norm of weights:  0.40057961061871583\n",
            "Iteration#:  9700 ; Loss:  0.4827092466703017 ; l2 norm of gradient:  0.0012933528311776703 ; l2 norm of weights:  0.40058013845759344\n",
            "Iteration#:  9704 ; Loss:  0.48270699052771215 ; l2 norm of gradient:  0.001292326648650699 ; l2 norm of weights:  0.4005806670633421\n",
            "Iteration#:  9708 ; Loss:  0.48270473780439005 ; l2 norm of gradient:  0.001291301970897072 ; l2 norm of weights:  0.4005811964333024\n",
            "Iteration#:  9712 ; Loss:  0.48270248849509073 ; l2 norm of gradient:  0.0012902787906420867 ; l2 norm of weights:  0.40058172656482166\n",
            "Iteration#:  9716 ; Loss:  0.48270024259457756 ; l2 norm of gradient:  0.0012892571006653507 ; l2 norm of weights:  0.40058225745525355\n",
            "Iteration#:  9720 ; Loss:  0.4826980000976223 ; l2 norm of gradient:  0.0012882368937995286 ; l2 norm of weights:  0.40058278910195855\n",
            "Iteration#:  9724 ; Loss:  0.48269576099900496 ; l2 norm of gradient:  0.0012872181629312012 ; l2 norm of weights:  0.4005833215023037\n",
            "Iteration#:  9728 ; Loss:  0.48269352529351334 ; l2 norm of gradient:  0.0012862009009999133 ; l2 norm of weights:  0.4005838546536625\n",
            "Iteration#:  9732 ; Loss:  0.48269129297594393 ; l2 norm of gradient:  0.0012851851009971218 ; l2 norm of weights:  0.4005843885534154\n",
            "Iteration#:  9736 ; Loss:  0.4826890640411012 ; l2 norm of gradient:  0.0012841707559666587 ; l2 norm of weights:  0.400584923198949\n",
            "Iteration#:  9740 ; Loss:  0.48268683848379745 ; l2 norm of gradient:  0.0012831578590039175 ; l2 norm of weights:  0.40058545858765665\n",
            "Iteration#:  9744 ; Loss:  0.48268461629885345 ; l2 norm of gradient:  0.0012821464032557054 ; l2 norm of weights:  0.4005859947169382\n",
            "Iteration#:  9748 ; Loss:  0.48268239748109815 ; l2 norm of gradient:  0.001281136381919686 ; l2 norm of weights:  0.4005865315841998\n",
            "Iteration#:  9752 ; Loss:  0.4826801820253682 ; l2 norm of gradient:  0.001280127788243926 ; l2 norm of weights:  0.4005870691868544\n",
            "Iteration#:  9756 ; Loss:  0.4826779699265087 ; l2 norm of gradient:  0.0012791206155266888 ; l2 norm of weights:  0.40058760752232103\n",
            "Iteration#:  9760 ; Loss:  0.4826757611793725 ; l2 norm of gradient:  0.0012781148571159636 ; l2 norm of weights:  0.4005881465880255\n",
            "Iteration#:  9764 ; Loss:  0.482673555778821 ; l2 norm of gradient:  0.00127711050640907 ; l2 norm of weights:  0.40058868638139994\n",
            "Iteration#:  9768 ; Loss:  0.48267135371972303 ; l2 norm of gradient:  0.0012761075568522282 ; l2 norm of weights:  0.4005892268998827\n",
            "Iteration#:  9772 ; Loss:  0.4826691549969561 ; l2 norm of gradient:  0.0012751060019406274 ; l2 norm of weights:  0.40058976814091884\n",
            "Iteration#:  9776 ; Loss:  0.4826669596054051 ; l2 norm of gradient:  0.0012741058352170168 ; l2 norm of weights:  0.4005903101019594\n",
            "Iteration#:  9780 ; Loss:  0.48266476753996335 ; l2 norm of gradient:  0.0012731070502724846 ; l2 norm of weights:  0.40059085278046197\n",
            "Iteration#:  9784 ; Loss:  0.4826625787955321 ; l2 norm of gradient:  0.0012721096407458713 ; l2 norm of weights:  0.40059139617389056\n",
            "Iteration#:  9788 ; Loss:  0.4826603933670205 ; l2 norm of gradient:  0.0012711136003224798 ; l2 norm of weights:  0.40059194027971534\n",
            "Iteration#:  9792 ; Loss:  0.4826582112493455 ; l2 norm of gradient:  0.0012701189227349973 ; l2 norm of weights:  0.4005924850954128\n",
            "Iteration#:  9796 ; Loss:  0.48265603243743244 ; l2 norm of gradient:  0.0012691256017620318 ; l2 norm of weights:  0.40059303061846574\n",
            "Iteration#:  9800 ; Loss:  0.48265385692621426 ; l2 norm of gradient:  0.0012681336312284984 ; l2 norm of weights:  0.4005935768463632\n",
            "Iteration#:  9804 ; Loss:  0.48265168471063186 ; l2 norm of gradient:  0.0012671430050048968 ; l2 norm of weights:  0.4005941237766003\n",
            "Iteration#:  9808 ; Loss:  0.4826495157856341 ; l2 norm of gradient:  0.001266153717007703 ; l2 norm of weights:  0.40059467140667876\n",
            "Iteration#:  9812 ; Loss:  0.4826473501461779 ; l2 norm of gradient:  0.0012651657611973063 ; l2 norm of weights:  0.4005952197341061\n",
            "Iteration#:  9816 ; Loss:  0.4826451877872277 ; l2 norm of gradient:  0.0012641791315796543 ; l2 norm of weights:  0.4005957687563962\n",
            "Iteration#:  9820 ; Loss:  0.48264302870375614 ; l2 norm of gradient:  0.0012631938222045491 ; l2 norm of weights:  0.40059631847106913\n",
            "Iteration#:  9824 ; Loss:  0.48264087289074376 ; l2 norm of gradient:  0.0012622098271659215 ; l2 norm of weights:  0.40059686887565105\n",
            "Iteration#:  9828 ; Loss:  0.4826387203431786 ; l2 norm of gradient:  0.0012612271406016449 ; l2 norm of weights:  0.40059741996767434\n",
            "Iteration#:  9832 ; Loss:  0.4826365710560568 ; l2 norm of gradient:  0.0012602457566922899 ; l2 norm of weights:  0.40059797174467726\n",
            "Iteration#:  9836 ; Loss:  0.4826344250243822 ; l2 norm of gradient:  0.001259265669661874 ; l2 norm of weights:  0.4005985242042044\n",
            "Iteration#:  9840 ; Loss:  0.48263228224316657 ; l2 norm of gradient:  0.0012582868737770988 ; l2 norm of weights:  0.4005990773438063\n",
            "Iteration#:  9844 ; Loss:  0.48263014270742943 ; l2 norm of gradient:  0.0012573093633464745 ; l2 norm of weights:  0.40059963116103975\n",
            "Iteration#:  9848 ; Loss:  0.48262800641219794 ; l2 norm of gradient:  0.0012563331327211593 ; l2 norm of weights:  0.4006001856534672\n",
            "Iteration#:  9852 ; Loss:  0.4826258733525075 ; l2 norm of gradient:  0.0012553581762931902 ; l2 norm of weights:  0.4006007408186576\n",
            "Iteration#:  9856 ; Loss:  0.48262374352340054 ; l2 norm of gradient:  0.00125438448849716 ; l2 norm of weights:  0.40060129665418565\n",
            "Iteration#:  9860 ; Loss:  0.48262161691992767 ; l2 norm of gradient:  0.0012534120638073945 ; l2 norm of weights:  0.4006018531576319\n",
            "Iteration#:  9864 ; Loss:  0.4826194935371474 ; l2 norm of gradient:  0.0012524408967397077 ; l2 norm of weights:  0.4006024103265831\n",
            "Iteration#:  9868 ; Loss:  0.4826173733701257 ; l2 norm of gradient:  0.0012514709818501803 ; l2 norm of weights:  0.400602968158632\n",
            "Iteration#:  9872 ; Loss:  0.4826152564139362 ; l2 norm of gradient:  0.001250502313734761 ; l2 norm of weights:  0.40060352665137705\n",
            "Iteration#:  9876 ; Loss:  0.48261314266366034 ; l2 norm of gradient:  0.0012495348870292513 ; l2 norm of weights:  0.4006040858024228\n",
            "Iteration#:  9880 ; Loss:  0.4826110321143872 ; l2 norm of gradient:  0.0012485686964095017 ; l2 norm of weights:  0.40060464560937975\n",
            "Iteration#:  9884 ; Loss:  0.48260892476121375 ; l2 norm of gradient:  0.0012476037365899294 ; l2 norm of weights:  0.40060520606986405\n",
            "Iteration#:  9888 ; Loss:  0.4826068205992442 ; l2 norm of gradient:  0.0012466400023236242 ; l2 norm of weights:  0.4006057671814981\n",
            "Iteration#:  9892 ; Loss:  0.48260471962359064 ; l2 norm of gradient:  0.0012456774884026022 ; l2 norm of weights:  0.4006063289419096\n",
            "Iteration#:  9896 ; Loss:  0.48260262182937286 ; l2 norm of gradient:  0.0012447161896574801 ; l2 norm of weights:  0.4006068913487327\n",
            "Iteration#:  9900 ; Loss:  0.48260052721171826 ; l2 norm of gradient:  0.001243756100956202 ; l2 norm of weights:  0.40060745439960704\n",
            "Iteration#:  9904 ; Loss:  0.48259843576576145 ; l2 norm of gradient:  0.001242797217204553 ; l2 norm of weights:  0.40060801809217805\n",
            "Iteration#:  9908 ; Loss:  0.4825963474866454 ; l2 norm of gradient:  0.0012418395333464072 ; l2 norm of weights:  0.40060858242409714\n",
            "Iteration#:  9912 ; Loss:  0.48259426236951986 ; l2 norm of gradient:  0.0012408830443617327 ; l2 norm of weights:  0.40060914739302134\n",
            "Iteration#:  9916 ; Loss:  0.4825921804095427 ; l2 norm of gradient:  0.0012399277452676306 ; l2 norm of weights:  0.4006097129966134\n",
            "Iteration#:  9920 ; Loss:  0.48259010160187926 ; l2 norm of gradient:  0.0012389736311184132 ; l2 norm of weights:  0.4006102792325421\n",
            "Iteration#:  9924 ; Loss:  0.482588025941702 ; l2 norm of gradient:  0.0012380206970037467 ; l2 norm of weights:  0.40061084609848147\n",
            "Iteration#:  9928 ; Loss:  0.4825859534241914 ; l2 norm of gradient:  0.0012370689380495534 ; l2 norm of weights:  0.40061141359211166\n",
            "Iteration#:  9932 ; Loss:  0.48258388404453556 ; l2 norm of gradient:  0.0012361183494174666 ; l2 norm of weights:  0.40061198171111845\n",
            "Iteration#:  9936 ; Loss:  0.4825818177979294 ; l2 norm of gradient:  0.0012351689263051281 ; l2 norm of weights:  0.40061255045319305\n",
            "Iteration#:  9940 ; Loss:  0.482579754679576 ; l2 norm of gradient:  0.001234220663944165 ; l2 norm of weights:  0.40061311981603276\n",
            "Iteration#:  9944 ; Loss:  0.48257769468468564 ; l2 norm of gradient:  0.0012332735576019145 ; l2 norm of weights:  0.4006136897973401\n",
            "Iteration#:  9948 ; Loss:  0.4825756378084762 ; l2 norm of gradient:  0.00123232760257998 ; l2 norm of weights:  0.40061426039482334\n",
            "Iteration#:  9952 ; Loss:  0.48257358404617273 ; l2 norm of gradient:  0.0012313827942146568 ; l2 norm of weights:  0.4006148316061965\n",
            "Iteration#:  9956 ; Loss:  0.48257153339300807 ; l2 norm of gradient:  0.0012304391278757146 ; l2 norm of weights:  0.40061540342917923\n",
            "Iteration#:  9960 ; Loss:  0.4825694858442223 ; l2 norm of gradient:  0.001229496598967031 ; l2 norm of weights:  0.4006159758614965\n",
            "Iteration#:  9964 ; Loss:  0.48256744139506325 ; l2 norm of gradient:  0.0012285552029262733 ; l2 norm of weights:  0.40061654890087917\n",
            "Iteration#:  9968 ; Loss:  0.4825654000407855 ; l2 norm of gradient:  0.0012276149352241563 ; l2 norm of weights:  0.40061712254506326\n",
            "Iteration#:  9972 ; Loss:  0.48256336177665166 ; l2 norm of gradient:  0.001226675791364075 ; l2 norm of weights:  0.4006176967917909\n",
            "Iteration#:  9976 ; Loss:  0.4825613265979315 ; l2 norm of gradient:  0.0012257377668828142 ; l2 norm of weights:  0.4006182716388091\n",
            "Iteration#:  9980 ; Loss:  0.48255929449990204 ; l2 norm of gradient:  0.0012248008573493838 ; l2 norm of weights:  0.40061884708387085\n",
            "Iteration#:  9984 ; Loss:  0.48255726547784783 ; l2 norm of gradient:  0.001223865058364932 ; l2 norm of weights:  0.4006194231247344\n",
            "Iteration#:  9988 ; Loss:  0.48255523952706053 ; l2 norm of gradient:  0.0012229303655625647 ; l2 norm of weights:  0.4006199997591635\n",
            "Iteration#:  9992 ; Loss:  0.4825532166428398 ; l2 norm of gradient:  0.001221996774607513 ; l2 norm of weights:  0.40062057698492753\n",
            "Iteration#:  9996 ; Loss:  0.4825511968204917 ; l2 norm of gradient:  0.0012210642811963715 ; l2 norm of weights:  0.40062115479980126\n"
          ]
        }
      ],
      "source": [
        "learning_rate = 0.001\n",
        "lambda_ = 1.0\n",
        "\n",
        "model = fit(xtrain_normal, ytrain, learning_rate, lambda_, 10000, verbose=1) #keep the verbose on here for your submissions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 90,
      "metadata": {
        "id": "-AiarpzOhIvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b6252248-1286-4950-ba09-a9f0a86c7c0f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train accuracy:  0.8792307692307693\n"
          ]
        }
      ],
      "source": [
        "print(\"Train accuracy: \", accuracy(xtrain_normal, ytrain, model))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 91,
      "metadata": {
        "id": "nrU6Tr7mhIvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8d56c93c-dba1-40c2-ced3-ae65688a6e37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Iteration#:  8 ; Loss:  7.373068773552692 ; l2 norm of gradient:  1.6145215074289525 ; l2 norm of weights:  8.659211719568797\n",
            "Iteration#:  12 ; Loss:  7.372980073673069 ; l2 norm of gradient:  1.6145128889836273 ; l2 norm of weights:  8.659160515113623\n",
            "Iteration#:  16 ; Loss:  7.372891372372219 ; l2 norm of gradient:  1.6145042705369226 ; l2 norm of weights:  8.659109311054198\n",
            "Iteration#:  20 ; Loss:  7.37280267302574 ; l2 norm of gradient:  1.6144956520888358 ; l2 norm of weights:  8.659058107390528\n",
            "Iteration#:  24 ; Loss:  7.372713974885084 ; l2 norm of gradient:  1.6144870336393655 ; l2 norm of weights:  8.659006904122604\n",
            "Iteration#:  28 ; Loss:  7.372625276706742 ; l2 norm of gradient:  1.6144784151885085 ; l2 norm of weights:  8.65895570125043\n",
            "Iteration#:  32 ; Loss:  7.37253657780893 ; l2 norm of gradient:  1.614469796736264 ; l2 norm of weights:  8.658904498774007\n",
            "Iteration#:  36 ; Loss:  7.3724478810266 ; l2 norm of gradient:  1.614461178282631 ; l2 norm of weights:  8.658853296693326\n",
            "Iteration#:  40 ; Loss:  7.37235918178491 ; l2 norm of gradient:  1.6144525598276063 ; l2 norm of weights:  8.658802095008395\n",
            "Iteration#:  44 ; Loss:  7.372270486928434 ; l2 norm of gradient:  1.614443941371189 ; l2 norm of weights:  8.65875089371921\n",
            "Iteration#:  48 ; Loss:  7.372181789221532 ; l2 norm of gradient:  1.614435322913377 ; l2 norm of weights:  8.65869969282577\n",
            "Iteration#:  52 ; Loss:  7.372093097971331 ; l2 norm of gradient:  1.6144267044541676 ; l2 norm of weights:  8.658648492328075\n",
            "Iteration#:  56 ; Loss:  7.372004405256809 ; l2 norm of gradient:  1.6144180859935595 ; l2 norm of weights:  8.658597292226123\n",
            "Iteration#:  60 ; Loss:  7.371915713239792 ; l2 norm of gradient:  1.614409467531552 ; l2 norm of weights:  8.658546092519911\n",
            "Iteration#:  64 ; Loss:  7.371827015634008 ; l2 norm of gradient:  1.6144008490681425 ; l2 norm of weights:  8.658494893209443\n",
            "Iteration#:  68 ; Loss:  7.3717383231391835 ; l2 norm of gradient:  1.614392230603328 ; l2 norm of weights:  8.658443694294716\n",
            "Iteration#:  72 ; Loss:  7.371649631807136 ; l2 norm of gradient:  1.6143836121371082 ; l2 norm of weights:  8.658392495775729\n",
            "Iteration#:  76 ; Loss:  7.371560939580889 ; l2 norm of gradient:  1.6143749936694805 ; l2 norm of weights:  8.658341297652482\n",
            "Iteration#:  80 ; Loss:  7.371472248791672 ; l2 norm of gradient:  1.614366375200444 ; l2 norm of weights:  8.658290099924974\n",
            "Iteration#:  84 ; Loss:  7.371383556176877 ; l2 norm of gradient:  1.614357756729996 ; l2 norm of weights:  8.658238902593205\n",
            "Iteration#:  88 ; Loss:  7.371294862527593 ; l2 norm of gradient:  1.6143491382581352 ; l2 norm of weights:  8.65818770565717\n",
            "Iteration#:  92 ; Loss:  7.37120617067385 ; l2 norm of gradient:  1.6143405197848584 ; l2 norm of weights:  8.658136509116872\n",
            "Iteration#:  96 ; Loss:  7.3711174866676465 ; l2 norm of gradient:  1.6143319013101647 ; l2 norm of weights:  8.658085312972313\n",
            "Iteration#:  100 ; Loss:  7.37102879829169 ; l2 norm of gradient:  1.614323282834053 ; l2 norm of weights:  8.658034117223485\n",
            "Iteration#:  104 ; Loss:  7.370940115232817 ; l2 norm of gradient:  1.6143146643565214 ; l2 norm of weights:  8.657982921870394\n",
            "Iteration#:  108 ; Loss:  7.370851416955018 ; l2 norm of gradient:  1.6143060458775669 ; l2 norm of weights:  8.657931726913034\n",
            "Iteration#:  112 ; Loss:  7.370762731654285 ; l2 norm of gradient:  1.6142974273971888 ; l2 norm of weights:  8.657880532351406\n",
            "Iteration#:  116 ; Loss:  7.370674044336276 ; l2 norm of gradient:  1.6142888089153835 ; l2 norm of weights:  8.657829338185513\n",
            "Iteration#:  120 ; Loss:  7.370585359186865 ; l2 norm of gradient:  1.6142801904321513 ; l2 norm of weights:  8.65777814441535\n",
            "Iteration#:  124 ; Loss:  7.370496671630119 ; l2 norm of gradient:  1.6142715719474892 ; l2 norm of weights:  8.657726951040916\n",
            "Iteration#:  128 ; Loss:  7.370407987105086 ; l2 norm of gradient:  1.614262953461396 ; l2 norm of weights:  8.657675758062211\n",
            "Iteration#:  132 ; Loss:  7.3703192998594576 ; l2 norm of gradient:  1.6142543349738676 ; l2 norm of weights:  8.657624565479233\n",
            "Iteration#:  136 ; Loss:  7.370230619950251 ; l2 norm of gradient:  1.6142457164849058 ; l2 norm of weights:  8.657573373291987\n",
            "Iteration#:  140 ; Loss:  7.370141934200556 ; l2 norm of gradient:  1.6142370979945069 ; l2 norm of weights:  8.657522181500465\n",
            "Iteration#:  144 ; Loss:  7.370053252933734 ; l2 norm of gradient:  1.614228479502669 ; l2 norm of weights:  8.657470990104668\n",
            "Iteration#:  148 ; Loss:  7.369964568039256 ; l2 norm of gradient:  1.61421986100939 ; l2 norm of weights:  8.6574197991046\n",
            "Iteration#:  152 ; Loss:  7.369875886435396 ; l2 norm of gradient:  1.6142112425146686 ; l2 norm of weights:  8.657368608500255\n",
            "Iteration#:  156 ; Loss:  7.369787203418355 ; l2 norm of gradient:  1.6142026240185032 ; l2 norm of weights:  8.657317418291635\n",
            "Iteration#:  160 ; Loss:  7.369698525262571 ; l2 norm of gradient:  1.6141940055208914 ; l2 norm of weights:  8.657266228478735\n",
            "Iteration#:  164 ; Loss:  7.36960983991002 ; l2 norm of gradient:  1.614185387021831 ; l2 norm of weights:  8.65721503906156\n",
            "Iteration#:  168 ; Loss:  7.3695211604012645 ; l2 norm of gradient:  1.6141767685213206 ; l2 norm of weights:  8.657163850040105\n",
            "Iteration#:  172 ; Loss:  7.369432479583805 ; l2 norm of gradient:  1.6141681500193588 ; l2 norm of weights:  8.657112661414372\n",
            "Iteration#:  176 ; Loss:  7.369343800637314 ; l2 norm of gradient:  1.6141595315159443 ; l2 norm of weights:  8.657061473184358\n",
            "Iteration#:  180 ; Loss:  7.369255122205292 ; l2 norm of gradient:  1.6141509130110734 ; l2 norm of weights:  8.657010285350063\n",
            "Iteration#:  184 ; Loss:  7.3691664415147144 ; l2 norm of gradient:  1.6141422945047452 ; l2 norm of weights:  8.656959097911487\n",
            "Iteration#:  188 ; Loss:  7.369077766451756 ; l2 norm of gradient:  1.6141336759969571 ; l2 norm of weights:  8.656907910868627\n",
            "Iteration#:  192 ; Loss:  7.368989086751851 ; l2 norm of gradient:  1.6141250574877097 ; l2 norm of weights:  8.656856724221488\n",
            "Iteration#:  196 ; Loss:  7.3689004129469025 ; l2 norm of gradient:  1.6141164389769982 ; l2 norm of weights:  8.65680553797006\n",
            "Iteration#:  200 ; Loss:  7.368811736104638 ; l2 norm of gradient:  1.6141078204648218 ; l2 norm of weights:  8.65675435211435\n",
            "Iteration#:  204 ; Loss:  7.368723059218624 ; l2 norm of gradient:  1.6140992019511806 ; l2 norm of weights:  8.656703166654353\n",
            "Iteration#:  208 ; Loss:  7.368634384482607 ; l2 norm of gradient:  1.614090583436069 ; l2 norm of weights:  8.65665198159007\n",
            "Iteration#:  212 ; Loss:  7.368545710842791 ; l2 norm of gradient:  1.6140819649194884 ; l2 norm of weights:  8.6566007969215\n",
            "Iteration#:  216 ; Loss:  7.368457037651691 ; l2 norm of gradient:  1.6140733464014352 ; l2 norm of weights:  8.656549612648641\n",
            "Iteration#:  220 ; Loss:  7.368368363179817 ; l2 norm of gradient:  1.6140647278819074 ; l2 norm of weights:  8.656498428771496\n",
            "Iteration#:  224 ; Loss:  7.368279686263624 ; l2 norm of gradient:  1.6140561093609047 ; l2 norm of weights:  8.65644724529006\n",
            "Iteration#:  228 ; Loss:  7.368191016792926 ; l2 norm of gradient:  1.6140474908384248 ; l2 norm of weights:  8.656396062204333\n",
            "Iteration#:  232 ; Loss:  7.368102345973128 ; l2 norm of gradient:  1.6140388723144643 ; l2 norm of weights:  8.656344879514315\n",
            "Iteration#:  236 ; Loss:  7.36801367602313 ; l2 norm of gradient:  1.614030253789023 ; l2 norm of weights:  8.656293697220006\n",
            "Iteration#:  240 ; Loss:  7.36792500666307 ; l2 norm of gradient:  1.6140216352620982 ; l2 norm of weights:  8.656242515321404\n",
            "Iteration#:  244 ; Loss:  7.367836337607841 ; l2 norm of gradient:  1.6140130167336888 ; l2 norm of weights:  8.65619133381851\n",
            "Iteration#:  248 ; Loss:  7.367747664320962 ; l2 norm of gradient:  1.6140043982037922 ; l2 norm of weights:  8.65614015271132\n",
            "Iteration#:  252 ; Loss:  7.367658995961969 ; l2 norm of gradient:  1.613995779672406 ; l2 norm of weights:  8.656088971999834\n",
            "Iteration#:  256 ; Loss:  7.367570324705033 ; l2 norm of gradient:  1.6139871611395296 ; l2 norm of weights:  8.656037791684053\n",
            "Iteration#:  260 ; Loss:  7.367481658696207 ; l2 norm of gradient:  1.613978542605161 ; l2 norm of weights:  8.655986611763977\n",
            "Iteration#:  264 ; Loss:  7.36739298964992 ; l2 norm of gradient:  1.6139699240692982 ; l2 norm of weights:  8.655935432239602\n",
            "Iteration#:  268 ; Loss:  7.367304324745168 ; l2 norm of gradient:  1.6139613055319393 ; l2 norm of weights:  8.65588425311093\n",
            "Iteration#:  272 ; Loss:  7.367215656801157 ; l2 norm of gradient:  1.6139526869930814 ; l2 norm of weights:  8.655833074377957\n",
            "Iteration#:  276 ; Loss:  7.367126992112252 ; l2 norm of gradient:  1.6139440684527242 ; l2 norm of weights:  8.655781896040686\n",
            "Iteration#:  280 ; Loss:  7.36703832620047 ; l2 norm of gradient:  1.6139354499108656 ; l2 norm of weights:  8.655730718099115\n",
            "Iteration#:  284 ; Loss:  7.366949664517966 ; l2 norm of gradient:  1.6139268313675028 ; l2 norm of weights:  8.65567954055324\n",
            "Iteration#:  288 ; Loss:  7.3668609929761715 ; l2 norm of gradient:  1.6139182128226348 ; l2 norm of weights:  8.655628363403066\n",
            "Iteration#:  292 ; Loss:  7.366772326941258 ; l2 norm of gradient:  1.6139095942762587 ; l2 norm of weights:  8.655577186648587\n",
            "Iteration#:  296 ; Loss:  7.366683661080016 ; l2 norm of gradient:  1.6139009757283733 ; l2 norm of weights:  8.655526010289805\n",
            "Iteration#:  300 ; Loss:  7.366595000599281 ; l2 norm of gradient:  1.6138923571789783 ; l2 norm of weights:  8.655474834326718\n",
            "Iteration#:  304 ; Loss:  7.366506342033425 ; l2 norm of gradient:  1.6138837386280684 ; l2 norm of weights:  8.655423658759327\n",
            "Iteration#:  308 ; Loss:  7.366417678637436 ; l2 norm of gradient:  1.6138751200756452 ; l2 norm of weights:  8.655372483587632\n",
            "Iteration#:  312 ; Loss:  7.366329016741499 ; l2 norm of gradient:  1.6138665015217044 ; l2 norm of weights:  8.655321308811626\n",
            "Iteration#:  316 ; Loss:  7.366240351866956 ; l2 norm of gradient:  1.6138578829662455 ; l2 norm of weights:  8.655270134431312\n",
            "Iteration#:  320 ; Loss:  7.366151692187968 ; l2 norm of gradient:  1.6138492644092663 ; l2 norm of weights:  8.655218960446692\n",
            "Iteration#:  324 ; Loss:  7.366063034521931 ; l2 norm of gradient:  1.613840645850764 ; l2 norm of weights:  8.655167786857763\n",
            "Iteration#:  328 ; Loss:  7.365974372242416 ; l2 norm of gradient:  1.6138320272907376 ; l2 norm of weights:  8.655116613664523\n",
            "Iteration#:  332 ; Loss:  7.365885713216398 ; l2 norm of gradient:  1.613823408729186 ; l2 norm of weights:  8.655065440866974\n",
            "Iteration#:  336 ; Loss:  7.36579705628201 ; l2 norm of gradient:  1.6138147901661057 ; l2 norm of weights:  8.65501426846511\n",
            "Iteration#:  340 ; Loss:  7.365708395667816 ; l2 norm of gradient:  1.6138061716014958 ; l2 norm of weights:  8.654963096458935\n",
            "Iteration#:  344 ; Loss:  7.365619738447121 ; l2 norm of gradient:  1.6137975530353548 ; l2 norm of weights:  8.65491192484845\n",
            "Iteration#:  348 ; Loss:  7.365531082321269 ; l2 norm of gradient:  1.6137889344676801 ; l2 norm of weights:  8.654860753633647\n",
            "Iteration#:  352 ; Loss:  7.365442427372316 ; l2 norm of gradient:  1.6137803158984692 ; l2 norm of weights:  8.654809582814533\n",
            "Iteration#:  356 ; Loss:  7.365353770516864 ; l2 norm of gradient:  1.6137716973277214 ; l2 norm of weights:  8.654758412391102\n",
            "Iteration#:  360 ; Loss:  7.365265118255024 ; l2 norm of gradient:  1.613763078755435 ; l2 norm of weights:  8.654707242363353\n",
            "Iteration#:  364 ; Loss:  7.365176460623911 ; l2 norm of gradient:  1.613754460181607 ; l2 norm of weights:  8.654656072731287\n",
            "Iteration#:  368 ; Loss:  7.365087803374781 ; l2 norm of gradient:  1.6137458416062367 ; l2 norm of weights:  8.654604903494906\n",
            "Iteration#:  372 ; Loss:  7.364999151074945 ; l2 norm of gradient:  1.6137372230293214 ; l2 norm of weights:  8.654553734654206\n",
            "Iteration#:  376 ; Loss:  7.364910501498615 ; l2 norm of gradient:  1.6137286044508594 ; l2 norm of weights:  8.654502566209185\n",
            "Iteration#:  380 ; Loss:  7.364821845823341 ; l2 norm of gradient:  1.613719985870849 ; l2 norm of weights:  8.654451398159845\n",
            "Iteration#:  384 ; Loss:  7.36473319299559 ; l2 norm of gradient:  1.613711367289288 ; l2 norm of weights:  8.654400230506184\n",
            "Iteration#:  388 ; Loss:  7.364644541762729 ; l2 norm of gradient:  1.613702748706175 ; l2 norm of weights:  8.6543490632482\n",
            "Iteration#:  392 ; Loss:  7.364555891847605 ; l2 norm of gradient:  1.6136941301215073 ; l2 norm of weights:  8.654297896385893\n",
            "Iteration#:  396 ; Loss:  7.364467242155794 ; l2 norm of gradient:  1.6136855115352846 ; l2 norm of weights:  8.654246729919263\n",
            "Iteration#:  400 ; Loss:  7.3643785898705865 ; l2 norm of gradient:  1.6136768929475034 ; l2 norm of weights:  8.65419556384831\n",
            "Iteration#:  404 ; Loss:  7.364289939566627 ; l2 norm of gradient:  1.6136682743581634 ; l2 norm of weights:  8.654144398173033\n",
            "Iteration#:  408 ; Loss:  7.3642012905401195 ; l2 norm of gradient:  1.6136596557672596 ; l2 norm of weights:  8.65409323289343\n",
            "Iteration#:  412 ; Loss:  7.364112638273696 ; l2 norm of gradient:  1.613651037174794 ; l2 norm of weights:  8.654042068009499\n",
            "Iteration#:  416 ; Loss:  7.364023992674358 ; l2 norm of gradient:  1.6136424185807627 ; l2 norm of weights:  8.65399090352124\n",
            "Iteration#:  420 ; Loss:  7.363935343282456 ; l2 norm of gradient:  1.6136337999851638 ; l2 norm of weights:  8.653939739428656\n",
            "Iteration#:  424 ; Loss:  7.363846696754828 ; l2 norm of gradient:  1.6136251813879958 ; l2 norm of weights:  8.65388857573174\n",
            "Iteration#:  428 ; Loss:  7.363758048520635 ; l2 norm of gradient:  1.6136165627892574 ; l2 norm of weights:  8.653837412430498\n",
            "Iteration#:  432 ; Loss:  7.363669405764269 ; l2 norm of gradient:  1.6136079441889444 ; l2 norm of weights:  8.653786249524924\n",
            "Iteration#:  436 ; Loss:  7.363580761996639 ; l2 norm of gradient:  1.613599325587059 ; l2 norm of weights:  8.653735087015017\n",
            "Iteration#:  440 ; Loss:  7.363492116167508 ; l2 norm of gradient:  1.6135907069835955 ; l2 norm of weights:  8.653683924900779\n",
            "Iteration#:  444 ; Loss:  7.36340347317941 ; l2 norm of gradient:  1.613582088378554 ; l2 norm of weights:  8.653632763182209\n",
            "Iteration#:  448 ; Loss:  7.363314822710428 ; l2 norm of gradient:  1.613573469771931 ; l2 norm of weights:  8.653581601859303\n",
            "Iteration#:  452 ; Loss:  7.363226185541134 ; l2 norm of gradient:  1.6135648511637266 ; l2 norm of weights:  8.653530440932066\n",
            "Iteration#:  456 ; Loss:  7.3631375383289885 ; l2 norm of gradient:  1.6135562325539374 ; l2 norm of weights:  8.653479280400491\n",
            "Iteration#:  460 ; Loss:  7.363048897093743 ; l2 norm of gradient:  1.6135476139425633 ; l2 norm of weights:  8.653428120264582\n",
            "Iteration#:  464 ; Loss:  7.3629602584385 ; l2 norm of gradient:  1.6135389953296002 ; l2 norm of weights:  8.653376960524337\n",
            "Iteration#:  468 ; Loss:  7.36287161500802 ; l2 norm of gradient:  1.613530376715047 ; l2 norm of weights:  8.653325801179754\n",
            "Iteration#:  472 ; Loss:  7.362782972084598 ; l2 norm of gradient:  1.6135217580989027 ; l2 norm of weights:  8.65327464223083\n",
            "Iteration#:  476 ; Loss:  7.362694334239088 ; l2 norm of gradient:  1.613513139481164 ; l2 norm of weights:  8.653223483677571\n",
            "Iteration#:  480 ; Loss:  7.362605690726749 ; l2 norm of gradient:  1.61350452086183 ; l2 norm of weights:  8.653172325519968\n",
            "Iteration#:  484 ; Loss:  7.362517054060233 ; l2 norm of gradient:  1.613495902240899 ; l2 norm of weights:  8.653121167758028\n",
            "Iteration#:  488 ; Loss:  7.362428412427089 ; l2 norm of gradient:  1.6134872836183685 ; l2 norm of weights:  8.653070010391746\n",
            "Iteration#:  492 ; Loss:  7.3623397740496035 ; l2 norm of gradient:  1.6134786649942368 ; l2 norm of weights:  8.65301885342112\n",
            "Iteration#:  496 ; Loss:  7.362251136681021 ; l2 norm of gradient:  1.6134700463685012 ; l2 norm of weights:  8.65296769684615\n",
            "Iteration#:  500 ; Loss:  7.362162499768285 ; l2 norm of gradient:  1.6134614277411616 ; l2 norm of weights:  8.65291654066684\n",
            "Iteration#:  504 ; Loss:  7.36207386002609 ; l2 norm of gradient:  1.6134528091122151 ; l2 norm of weights:  8.652865384883183\n",
            "Iteration#:  508 ; Loss:  7.361985227252478 ; l2 norm of gradient:  1.6134441904816588 ; l2 norm of weights:  8.652814229495181\n",
            "Iteration#:  512 ; Loss:  7.361896592209094 ; l2 norm of gradient:  1.6134355718494933 ; l2 norm of weights:  8.652763074502833\n",
            "Iteration#:  516 ; Loss:  7.361807954861394 ; l2 norm of gradient:  1.613426953215714 ; l2 norm of weights:  8.652711919906137\n",
            "Iteration#:  520 ; Loss:  7.361719321876697 ; l2 norm of gradient:  1.6134183345803208 ; l2 norm of weights:  8.652660765705097\n",
            "Iteration#:  524 ; Loss:  7.361630684549774 ; l2 norm of gradient:  1.6134097159433116 ; l2 norm of weights:  8.652609611899704\n",
            "Iteration#:  528 ; Loss:  7.361542051261862 ; l2 norm of gradient:  1.6134010973046828 ; l2 norm of weights:  8.652558458489963\n",
            "Iteration#:  532 ; Loss:  7.3614534175378274 ; l2 norm of gradient:  1.6133924786644351 ; l2 norm of weights:  8.652507305475874\n",
            "Iteration#:  536 ; Loss:  7.3613647844733165 ; l2 norm of gradient:  1.6133838600225643 ; l2 norm of weights:  8.65245615285743\n",
            "Iteration#:  540 ; Loss:  7.361276153052553 ; l2 norm of gradient:  1.6133752413790707 ; l2 norm of weights:  8.652405000634639\n",
            "Iteration#:  544 ; Loss:  7.361187523139565 ; l2 norm of gradient:  1.6133666227339507 ; l2 norm of weights:  8.652353848807495\n",
            "Iteration#:  548 ; Loss:  7.361098893288572 ; l2 norm of gradient:  1.6133580040872029 ; l2 norm of weights:  8.652302697375996\n",
            "Iteration#:  552 ; Loss:  7.3610102603249095 ; l2 norm of gradient:  1.6133493854388261 ; l2 norm of weights:  8.65225154634014\n",
            "Iteration#:  556 ; Loss:  7.360921630973214 ; l2 norm of gradient:  1.6133407667888167 ; l2 norm of weights:  8.652200395699934\n",
            "Iteration#:  560 ; Loss:  7.360832999274329 ; l2 norm of gradient:  1.6133321481371745 ; l2 norm of weights:  8.652149245455371\n",
            "Iteration#:  564 ; Loss:  7.360744375097767 ; l2 norm of gradient:  1.6133235294838968 ; l2 norm of weights:  8.652098095606453\n",
            "Iteration#:  568 ; Loss:  7.360655740588257 ; l2 norm of gradient:  1.6133149108289815 ; l2 norm of weights:  8.652046946153176\n",
            "Iteration#:  572 ; Loss:  7.360567118243184 ; l2 norm of gradient:  1.613306292172427 ; l2 norm of weights:  8.651995797095541\n",
            "Iteration#:  576 ; Loss:  7.360478488683086 ; l2 norm of gradient:  1.6132976735142321 ; l2 norm of weights:  8.651944648433547\n",
            "Iteration#:  580 ; Loss:  7.36038986225113 ; l2 norm of gradient:  1.6132890548543941 ; l2 norm of weights:  8.651893500167194\n",
            "Iteration#:  584 ; Loss:  7.360301231013457 ; l2 norm of gradient:  1.6132804361929116 ; l2 norm of weights:  8.65184235229648\n",
            "Iteration#:  588 ; Loss:  7.360212608104629 ; l2 norm of gradient:  1.6132718175297813 ; l2 norm of weights:  8.651791204821407\n",
            "Iteration#:  592 ; Loss:  7.360123978811947 ; l2 norm of gradient:  1.6132631988650028 ; l2 norm of weights:  8.651740057741971\n",
            "Iteration#:  596 ; Loss:  7.360035357654443 ; l2 norm of gradient:  1.6132545801985743 ; l2 norm of weights:  8.651688911058171\n",
            "Iteration#:  600 ; Loss:  7.359946731474821 ; l2 norm of gradient:  1.613245961530492 ; l2 norm of weights:  8.65163776477001\n",
            "Iteration#:  604 ; Loss:  7.359858109955253 ; l2 norm of gradient:  1.6132373428607565 ; l2 norm of weights:  8.651586618877483\n",
            "Iteration#:  608 ; Loss:  7.359769486148854 ; l2 norm of gradient:  1.6132287241893644 ; l2 norm of weights:  8.65153547338059\n",
            "Iteration#:  612 ; Loss:  7.359680859598816 ; l2 norm of gradient:  1.6132201055163138 ; l2 norm of weights:  8.651484328279334\n",
            "Iteration#:  616 ; Loss:  7.359592241154276 ; l2 norm of gradient:  1.6132114868416032 ; l2 norm of weights:  8.651433183573708\n",
            "Iteration#:  620 ; Loss:  7.359503619280533 ; l2 norm of gradient:  1.6132028681652306 ; l2 norm of weights:  8.651382039263718\n",
            "Iteration#:  624 ; Loss:  7.359414997352912 ; l2 norm of gradient:  1.613194249487194 ; l2 norm of weights:  8.651330895349359\n",
            "Iteration#:  628 ; Loss:  7.359326376578099 ; l2 norm of gradient:  1.6131856308074912 ; l2 norm of weights:  8.65127975183063\n",
            "Iteration#:  632 ; Loss:  7.359237756881146 ; l2 norm of gradient:  1.613177012126122 ; l2 norm of weights:  8.65122860870753\n",
            "Iteration#:  636 ; Loss:  7.359149137414824 ; l2 norm of gradient:  1.6131683934430823 ; l2 norm of weights:  8.651177465980062\n",
            "Iteration#:  640 ; Loss:  7.3590605175087695 ; l2 norm of gradient:  1.6131597747583712 ; l2 norm of weights:  8.651126323648223\n",
            "Iteration#:  644 ; Loss:  7.358971894822769 ; l2 norm of gradient:  1.613151156071986 ; l2 norm of weights:  8.65107518171201\n",
            "Iteration#:  648 ; Loss:  7.358883278199823 ; l2 norm of gradient:  1.613142537383926 ; l2 norm of weights:  8.651024040171423\n",
            "Iteration#:  652 ; Loss:  7.358794659090725 ; l2 norm of gradient:  1.6131339186941884 ; l2 norm of weights:  8.650972899026465\n",
            "Iteration#:  656 ; Loss:  7.358706045177065 ; l2 norm of gradient:  1.6131253000027719 ; l2 norm of weights:  8.650921758277132\n",
            "Iteration#:  660 ; Loss:  7.358617429032833 ; l2 norm of gradient:  1.6131166813096738 ; l2 norm of weights:  8.650870617923424\n",
            "Iteration#:  664 ; Loss:  7.358528809488442 ; l2 norm of gradient:  1.6131080626148921 ; l2 norm of weights:  8.650819477965337\n",
            "Iteration#:  668 ; Loss:  7.358440194098195 ; l2 norm of gradient:  1.6130994439184272 ; l2 norm of weights:  8.650768338402875\n",
            "Iteration#:  672 ; Loss:  7.358351580326406 ; l2 norm of gradient:  1.6130908252202738 ; l2 norm of weights:  8.650717199236036\n",
            "Iteration#:  676 ; Loss:  7.358262968340014 ; l2 norm of gradient:  1.613082206520433 ; l2 norm of weights:  8.650666060464818\n",
            "Iteration#:  680 ; Loss:  7.358174352697615 ; l2 norm of gradient:  1.6130735878189 ; l2 norm of weights:  8.65061492208922\n",
            "Iteration#:  684 ; Loss:  7.358085739116266 ; l2 norm of gradient:  1.6130649691156753 ; l2 norm of weights:  8.650563784109243\n",
            "Iteration#:  688 ; Loss:  7.357997122002558 ; l2 norm of gradient:  1.613056350410755 ; l2 norm of weights:  8.650512646524886\n",
            "Iteration#:  692 ; Loss:  7.357908509247039 ; l2 norm of gradient:  1.6130477317041396 ; l2 norm of weights:  8.650461509336147\n",
            "Iteration#:  696 ; Loss:  7.357819898787023 ; l2 norm of gradient:  1.6130391129958255 ; l2 norm of weights:  8.650410372543025\n",
            "Iteration#:  700 ; Loss:  7.357731286837843 ; l2 norm of gradient:  1.6130304942858102 ; l2 norm of weights:  8.65035923614552\n",
            "Iteration#:  704 ; Loss:  7.3576426802735915 ; l2 norm of gradient:  1.6130218755740926 ; l2 norm of weights:  8.650308100143631\n",
            "Iteration#:  708 ; Loss:  7.35755406629333 ; l2 norm of gradient:  1.6130132568606717 ; l2 norm of weights:  8.650256964537357\n",
            "Iteration#:  712 ; Loss:  7.357465455435602 ; l2 norm of gradient:  1.6130046381455445 ; l2 norm of weights:  8.650205829326698\n",
            "Iteration#:  716 ; Loss:  7.357376845959148 ; l2 norm of gradient:  1.6129960194287098 ; l2 norm of weights:  8.650154694511652\n",
            "Iteration#:  720 ; Loss:  7.357288238621674 ; l2 norm of gradient:  1.612987400710164 ; l2 norm of weights:  8.65010356009222\n",
            "Iteration#:  724 ; Loss:  7.35719963016648 ; l2 norm of gradient:  1.612978781989907 ; l2 norm of weights:  8.6500524260684\n",
            "Iteration#:  728 ; Loss:  7.35711102580095 ; l2 norm of gradient:  1.6129701632679363 ; l2 norm of weights:  8.65000129244019\n",
            "Iteration#:  732 ; Loss:  7.357022417471114 ; l2 norm of gradient:  1.6129615445442493 ; l2 norm of weights:  8.649950159207592\n",
            "Iteration#:  736 ; Loss:  7.356933809603471 ; l2 norm of gradient:  1.6129529258188449 ; l2 norm of weights:  8.649899026370601\n",
            "Iteration#:  740 ; Loss:  7.356845203203282 ; l2 norm of gradient:  1.6129443070917213 ; l2 norm of weights:  8.649847893929222\n",
            "Iteration#:  744 ; Loss:  7.35675659607023 ; l2 norm of gradient:  1.6129356883628767 ; l2 norm of weights:  8.64979676188345\n",
            "Iteration#:  748 ; Loss:  7.356667992711231 ; l2 norm of gradient:  1.6129270696323077 ; l2 norm of weights:  8.649745630233285\n",
            "Iteration#:  752 ; Loss:  7.356579388520508 ; l2 norm of gradient:  1.6129184509000134 ; l2 norm of weights:  8.649694498978729\n",
            "Iteration#:  756 ; Loss:  7.356490781724485 ; l2 norm of gradient:  1.6129098321659918 ; l2 norm of weights:  8.649643368119776\n",
            "Iteration#:  760 ; Loss:  7.3564021792866665 ; l2 norm of gradient:  1.6129012134302416 ; l2 norm of weights:  8.649592237656428\n",
            "Iteration#:  764 ; Loss:  7.356313575571466 ; l2 norm of gradient:  1.6128925946927597 ; l2 norm of weights:  8.649541107588686\n",
            "Iteration#:  768 ; Loss:  7.35622497244597 ; l2 norm of gradient:  1.6128839759535452 ; l2 norm of weights:  8.649489977916547\n",
            "Iteration#:  772 ; Loss:  7.356136373031108 ; l2 norm of gradient:  1.612875357212595 ; l2 norm of weights:  8.649438848640012\n",
            "Iteration#:  776 ; Loss:  7.356047767632839 ; l2 norm of gradient:  1.6128667384699087 ; l2 norm of weights:  8.649387719759076\n",
            "Iteration#:  780 ; Loss:  7.355959166935748 ; l2 norm of gradient:  1.6128581197254823 ; l2 norm of weights:  8.649336591273743\n",
            "Iteration#:  784 ; Loss:  7.355870564254614 ; l2 norm of gradient:  1.6128495009793167 ; l2 norm of weights:  8.64928546318401\n",
            "Iteration#:  788 ; Loss:  7.3557819654015955 ; l2 norm of gradient:  1.6128408822314075 ; l2 norm of weights:  8.649234335489876\n",
            "Iteration#:  792 ; Loss:  7.355693370066868 ; l2 norm of gradient:  1.6128322634817531 ; l2 norm of weights:  8.64918320819134\n",
            "Iteration#:  796 ; Loss:  7.355604766474647 ; l2 norm of gradient:  1.612823644730353 ; l2 norm of weights:  8.649132081288402\n",
            "Iteration#:  800 ; Loss:  7.355516168237456 ; l2 norm of gradient:  1.6128150259772043 ; l2 norm of weights:  8.649080954781065\n",
            "Iteration#:  804 ; Loss:  7.355427570252515 ; l2 norm of gradient:  1.612806407222305 ; l2 norm of weights:  8.64902982866932\n",
            "Iteration#:  808 ; Loss:  7.355338973561203 ; l2 norm of gradient:  1.6127977884656528 ; l2 norm of weights:  8.64897870295317\n",
            "Iteration#:  812 ; Loss:  7.3552503778687 ; l2 norm of gradient:  1.6127891697072472 ; l2 norm of weights:  8.648927577632618\n",
            "Iteration#:  816 ; Loss:  7.355161779141168 ; l2 norm of gradient:  1.6127805509470847 ; l2 norm of weights:  8.648876452707656\n",
            "Iteration#:  820 ; Loss:  7.35507318158695 ; l2 norm of gradient:  1.6127719321851641 ; l2 norm of weights:  8.64882532817829\n",
            "Iteration#:  824 ; Loss:  7.354984586845015 ; l2 norm of gradient:  1.6127633134214834 ; l2 norm of weights:  8.648774204044518\n",
            "Iteration#:  828 ; Loss:  7.354895993343834 ; l2 norm of gradient:  1.61275469465604 ; l2 norm of weights:  8.648723080306336\n",
            "Iteration#:  832 ; Loss:  7.354807396842088 ; l2 norm of gradient:  1.6127460758888335 ; l2 norm of weights:  8.648671956963742\n",
            "Iteration#:  836 ; Loss:  7.3547188027463 ; l2 norm of gradient:  1.6127374571198603 ; l2 norm of weights:  8.648620834016741\n",
            "Iteration#:  840 ; Loss:  7.354630208631388 ; l2 norm of gradient:  1.61272883834912 ; l2 norm of weights:  8.64856971146533\n",
            "Iteration#:  844 ; Loss:  7.3545416181322665 ; l2 norm of gradient:  1.612720219576609 ; l2 norm of weights:  8.648518589309505\n",
            "Iteration#:  848 ; Loss:  7.354453025797257 ; l2 norm of gradient:  1.6127116008023261 ; l2 norm of weights:  8.648467467549272\n",
            "Iteration#:  852 ; Loss:  7.354364433377883 ; l2 norm of gradient:  1.6127029820262702 ; l2 norm of weights:  8.648416346184622\n",
            "Iteration#:  856 ; Loss:  7.354275840160355 ; l2 norm of gradient:  1.6126943632484385 ; l2 norm of weights:  8.648365225215558\n",
            "Iteration#:  860 ; Loss:  7.354187249176309 ; l2 norm of gradient:  1.6126857444688287 ; l2 norm of weights:  8.648314104642079\n",
            "Iteration#:  864 ; Loss:  7.354098662052214 ; l2 norm of gradient:  1.6126771256874397 ; l2 norm of weights:  8.648262984464187\n",
            "Iteration#:  868 ; Loss:  7.3540100692219905 ; l2 norm of gradient:  1.6126685069042694 ; l2 norm of weights:  8.648211864681878\n",
            "Iteration#:  872 ; Loss:  7.353921482173249 ; l2 norm of gradient:  1.6126598881193148 ; l2 norm of weights:  8.64816074529515\n",
            "Iteration#:  876 ; Loss:  7.3538328898710335 ; l2 norm of gradient:  1.6126512693325752 ; l2 norm of weights:  8.648109626304008\n",
            "Iteration#:  880 ; Loss:  7.353744304695381 ; l2 norm of gradient:  1.6126426505440483 ; l2 norm of weights:  8.648058507708443\n",
            "Iteration#:  884 ; Loss:  7.353655719432463 ; l2 norm of gradient:  1.6126340317537315 ; l2 norm of weights:  8.648007389508463\n",
            "Iteration#:  888 ; Loss:  7.3535671282945945 ; l2 norm of gradient:  1.6126254129616246 ; l2 norm of weights:  8.647956271704057\n",
            "Iteration#:  892 ; Loss:  7.353478543484906 ; l2 norm of gradient:  1.6126167941677236 ; l2 norm of weights:  8.647905154295234\n",
            "Iteration#:  896 ; Loss:  7.35338995540051 ; l2 norm of gradient:  1.6126081753720272 ; l2 norm of weights:  8.64785403728199\n",
            "Iteration#:  900 ; Loss:  7.353301373238976 ; l2 norm of gradient:  1.6125995565745346 ; l2 norm of weights:  8.647802920664322\n",
            "Iteration#:  904 ; Loss:  7.3532127834718946 ; l2 norm of gradient:  1.6125909377752425 ; l2 norm of weights:  8.64775180444223\n",
            "Iteration#:  908 ; Loss:  7.353124200466592 ; l2 norm of gradient:  1.6125823189741495 ; l2 norm of weights:  8.647700688615714\n",
            "Iteration#:  912 ; Loss:  7.353035616889336 ; l2 norm of gradient:  1.6125737001712535 ; l2 norm of weights:  8.647649573184772\n",
            "Iteration#:  916 ; Loss:  7.352947034099823 ; l2 norm of gradient:  1.6125650813665526 ; l2 norm of weights:  8.647598458149407\n",
            "Iteration#:  920 ; Loss:  7.352858451166455 ; l2 norm of gradient:  1.6125564625600453 ; l2 norm of weights:  8.647547343509615\n",
            "Iteration#:  924 ; Loss:  7.3527698662942536 ; l2 norm of gradient:  1.6125478437517284 ; l2 norm of weights:  8.647496229265395\n",
            "Iteration#:  928 ; Loss:  7.352681286101534 ; l2 norm of gradient:  1.6125392249416006 ; l2 norm of weights:  8.647445115416746\n",
            "Iteration#:  932 ; Loss:  7.352592701511103 ; l2 norm of gradient:  1.6125306061296605 ; l2 norm of weights:  8.647394001963672\n",
            "Iteration#:  936 ; Loss:  7.3525041225446 ; l2 norm of gradient:  1.6125219873159051 ; l2 norm of weights:  8.647342888906163\n",
            "Iteration#:  940 ; Loss:  7.352415542362817 ; l2 norm of gradient:  1.612513368500334 ; l2 norm of weights:  8.647291776244227\n",
            "Iteration#:  944 ; Loss:  7.3523269585454845 ; l2 norm of gradient:  1.6125047496829445 ; l2 norm of weights:  8.647240663977858\n",
            "Iteration#:  948 ; Loss:  7.352238385627212 ; l2 norm of gradient:  1.6124961308637338 ; l2 norm of weights:  8.647189552107058\n",
            "Iteration#:  952 ; Loss:  7.352149802551705 ; l2 norm of gradient:  1.6124875120427005 ; l2 norm of weights:  8.647138440631824\n",
            "Iteration#:  956 ; Loss:  7.352061225635795 ; l2 norm of gradient:  1.612478893219843 ; l2 norm of weights:  8.647087329552157\n",
            "Iteration#:  960 ; Loss:  7.3519726479785765 ; l2 norm of gradient:  1.6124702743951582 ; l2 norm of weights:  8.647036218868056\n",
            "Iteration#:  964 ; Loss:  7.351884073740178 ; l2 norm of gradient:  1.612461655568646 ; l2 norm of weights:  8.64698510857952\n",
            "Iteration#:  968 ; Loss:  7.35179549186237 ; l2 norm of gradient:  1.6124530367403034 ; l2 norm of weights:  8.646933998686547\n",
            "Iteration#:  972 ; Loss:  7.351706916430199 ; l2 norm of gradient:  1.6124444179101283 ; l2 norm of weights:  8.646882889189138\n",
            "Iteration#:  976 ; Loss:  7.351618344051474 ; l2 norm of gradient:  1.6124357990781188 ; l2 norm of weights:  8.646831780087291\n",
            "Iteration#:  980 ; Loss:  7.351529767428103 ; l2 norm of gradient:  1.612427180244273 ; l2 norm of weights:  8.646780671381006\n",
            "Iteration#:  984 ; Loss:  7.351441189741661 ; l2 norm of gradient:  1.612418561408589 ; l2 norm of weights:  8.646729563070283\n",
            "Iteration#:  988 ; Loss:  7.351352616360584 ; l2 norm of gradient:  1.6124099425710656 ; l2 norm of weights:  8.646678455155119\n",
            "Iteration#:  992 ; Loss:  7.3512640432005485 ; l2 norm of gradient:  1.6124013237316988 ; l2 norm of weights:  8.646627347635512\n",
            "Iteration#:  996 ; Loss:  7.351175469078401 ; l2 norm of gradient:  1.6123927048904885 ; l2 norm of weights:  8.646576240511466\n",
            "Iteration#:  1000 ; Loss:  7.351086896145979 ; l2 norm of gradient:  1.6123840860474328 ; l2 norm of weights:  8.646525133782978\n",
            "Iteration#:  1004 ; Loss:  7.350998321548609 ; l2 norm of gradient:  1.612375467202529 ; l2 norm of weights:  8.646474027450045\n",
            "Iteration#:  1008 ; Loss:  7.35090975402181 ; l2 norm of gradient:  1.6123668483557738 ; l2 norm of weights:  8.64642292151267\n",
            "Iteration#:  1012 ; Loss:  7.350821179197345 ; l2 norm of gradient:  1.6123582295071677 ; l2 norm of weights:  8.64637181597085\n",
            "Iteration#:  1016 ; Loss:  7.350732606594917 ; l2 norm of gradient:  1.6123496106567081 ; l2 norm of weights:  8.646320710824584\n",
            "Iteration#:  1020 ; Loss:  7.3506440400437745 ; l2 norm of gradient:  1.6123409918043912 ; l2 norm of weights:  8.646269606073872\n",
            "Iteration#:  1024 ; Loss:  7.350555470356861 ; l2 norm of gradient:  1.6123323729502177 ; l2 norm of weights:  8.646218501718712\n",
            "Iteration#:  1028 ; Loss:  7.350466898720562 ; l2 norm of gradient:  1.612323754094184 ; l2 norm of weights:  8.646167397759106\n",
            "Iteration#:  1032 ; Loss:  7.3503783327692425 ; l2 norm of gradient:  1.612315135236288 ; l2 norm of weights:  8.646116294195052\n",
            "Iteration#:  1036 ; Loss:  7.350289762082594 ; l2 norm of gradient:  1.6123065163765287 ; l2 norm of weights:  8.646065191026544\n",
            "Iteration#:  1040 ; Loss:  7.350201197246193 ; l2 norm of gradient:  1.6122978975149043 ; l2 norm of weights:  8.64601408825359\n",
            "Iteration#:  1044 ; Loss:  7.3501126267340995 ; l2 norm of gradient:  1.6122892786514114 ; l2 norm of weights:  8.645962985876183\n",
            "Iteration#:  1048 ; Loss:  7.35002406103715 ; l2 norm of gradient:  1.6122806597860477 ; l2 norm of weights:  8.645911883894325\n",
            "Iteration#:  1052 ; Loss:  7.34993549955928 ; l2 norm of gradient:  1.6122720409188143 ; l2 norm of weights:  8.645860782308015\n",
            "Iteration#:  1056 ; Loss:  7.349846927101147 ; l2 norm of gradient:  1.6122634220497067 ; l2 norm of weights:  8.645809681117251\n",
            "Iteration#:  1060 ; Loss:  7.349758366078122 ; l2 norm of gradient:  1.6122548031787225 ; l2 norm of weights:  8.645758580322033\n",
            "Iteration#:  1064 ; Loss:  7.349669797722683 ; l2 norm of gradient:  1.612246184305862 ; l2 norm of weights:  8.64570747992236\n",
            "Iteration#:  1068 ; Loss:  7.3495812392876285 ; l2 norm of gradient:  1.6122375654311218 ; l2 norm of weights:  8.64565637991823\n",
            "Iteration#:  1072 ; Loss:  7.34949267366269 ; l2 norm of gradient:  1.6122289465544997 ; l2 norm of weights:  8.645605280309645\n",
            "Iteration#:  1076 ; Loss:  7.34940411239463 ; l2 norm of gradient:  1.6122203276759945 ; l2 norm of weights:  8.645554181096603\n",
            "Iteration#:  1080 ; Loss:  7.349315547825865 ; l2 norm of gradient:  1.612211708795603 ; l2 norm of weights:  8.645503082279104\n",
            "Iteration#:  1084 ; Loss:  7.349226982703891 ; l2 norm of gradient:  1.6122030899133244 ; l2 norm of weights:  8.645451983857145\n",
            "Iteration#:  1088 ; Loss:  7.349138422530732 ; l2 norm of gradient:  1.6121944710291565 ; l2 norm of weights:  8.645400885830725\n",
            "Iteration#:  1092 ; Loss:  7.349049862800215 ; l2 norm of gradient:  1.6121858521430967 ; l2 norm of weights:  8.645349788199846\n",
            "Iteration#:  1096 ; Loss:  7.348961300116943 ; l2 norm of gradient:  1.6121772332551445 ; l2 norm of weights:  8.645298690964504\n",
            "Iteration#:  1100 ; Loss:  7.348872743023236 ; l2 norm of gradient:  1.612168614365296 ; l2 norm of weights:  8.645247594124703\n",
            "Iteration#:  1104 ; Loss:  7.348784186431796 ; l2 norm of gradient:  1.6121599954735506 ; l2 norm of weights:  8.645196497680436\n",
            "Iteration#:  1108 ; Loss:  7.348695624834176 ; l2 norm of gradient:  1.612151376579906 ; l2 norm of weights:  8.645145401631709\n",
            "Iteration#:  1112 ; Loss:  7.34860706239235 ; l2 norm of gradient:  1.61214275768436 ; l2 norm of weights:  8.645094305978514\n",
            "Iteration#:  1116 ; Loss:  7.3485185086788425 ; l2 norm of gradient:  1.6121341387869106 ; l2 norm of weights:  8.645043210720855\n",
            "Iteration#:  1120 ; Loss:  7.348429950302791 ; l2 norm of gradient:  1.612125519887556 ; l2 norm of weights:  8.644992115858729\n",
            "Iteration#:  1124 ; Loss:  7.34834139483019 ; l2 norm of gradient:  1.6121169009862937 ; l2 norm of weights:  8.644941021392137\n",
            "Iteration#:  1128 ; Loss:  7.348252835713038 ; l2 norm of gradient:  1.6121082820831227 ; l2 norm of weights:  8.64488992732108\n",
            "Iteration#:  1132 ; Loss:  7.348164281537878 ; l2 norm of gradient:  1.6120996631780395 ; l2 norm of weights:  8.644838833645553\n",
            "Iteration#:  1136 ; Loss:  7.348075725864423 ; l2 norm of gradient:  1.6120910442710445 ; l2 norm of weights:  8.644787740365556\n",
            "Iteration#:  1140 ; Loss:  7.347987170478357 ; l2 norm of gradient:  1.6120824253621338 ; l2 norm of weights:  8.644736647481087\n",
            "Iteration#:  1144 ; Loss:  7.347898616622409 ; l2 norm of gradient:  1.6120738064513045 ; l2 norm of weights:  8.644685554992149\n",
            "Iteration#:  1148 ; Loss:  7.3478100632749035 ; l2 norm of gradient:  1.6120651875385583 ; l2 norm of weights:  8.644634462898743\n",
            "Iteration#:  1152 ; Loss:  7.347721510261467 ; l2 norm of gradient:  1.6120565686238895 ; l2 norm of weights:  8.644583371200861\n",
            "Iteration#:  1156 ; Loss:  7.3476329559846105 ; l2 norm of gradient:  1.612047949707298 ; l2 norm of weights:  8.644532279898506\n",
            "Iteration#:  1160 ; Loss:  7.347544403823225 ; l2 norm of gradient:  1.6120393307887806 ; l2 norm of weights:  8.644481188991678\n",
            "Iteration#:  1164 ; Loss:  7.347455854657017 ; l2 norm of gradient:  1.612030711868337 ; l2 norm of weights:  8.644430098480374\n",
            "Iteration#:  1168 ; Loss:  7.347367301405477 ; l2 norm of gradient:  1.6120220929459637 ; l2 norm of weights:  8.644379008364595\n",
            "Iteration#:  1172 ; Loss:  7.347278754280847 ; l2 norm of gradient:  1.6120134740216603 ; l2 norm of weights:  8.64432791864434\n",
            "Iteration#:  1176 ; Loss:  7.347190202336501 ; l2 norm of gradient:  1.6120048550954225 ; l2 norm of weights:  8.644276829319612\n",
            "Iteration#:  1180 ; Loss:  7.347101651727316 ; l2 norm of gradient:  1.61199623616725 ; l2 norm of weights:  8.6442257403904\n",
            "Iteration#:  1184 ; Loss:  7.347013104983056 ; l2 norm of gradient:  1.6119876172371408 ; l2 norm of weights:  8.644174651856712\n",
            "Iteration#:  1188 ; Loss:  7.346924555613262 ; l2 norm of gradient:  1.6119789983050918 ; l2 norm of weights:  8.644123563718544\n",
            "Iteration#:  1192 ; Loss:  7.346836007017526 ; l2 norm of gradient:  1.6119703793711022 ; l2 norm of weights:  8.644072475975896\n",
            "Iteration#:  1196 ; Loss:  7.346747461028517 ; l2 norm of gradient:  1.6119617604351697 ; l2 norm of weights:  8.644021388628769\n",
            "Iteration#:  1200 ; Loss:  7.34665891122037 ; l2 norm of gradient:  1.6119531414972916 ; l2 norm of weights:  8.643970301677157\n",
            "Iteration#:  1204 ; Loss:  7.346570362154999 ; l2 norm of gradient:  1.6119445225574673 ; l2 norm of weights:  8.643919215121064\n",
            "Iteration#:  1208 ; Loss:  7.346481820815331 ; l2 norm of gradient:  1.611935903615693 ; l2 norm of weights:  8.643868128960488\n",
            "Iteration#:  1212 ; Loss:  7.3463932741875375 ; l2 norm of gradient:  1.611927284671968 ; l2 norm of weights:  8.643817043195428\n",
            "Iteration#:  1216 ; Loss:  7.346304729260394 ; l2 norm of gradient:  1.6119186657262898 ; l2 norm of weights:  8.643765957825883\n",
            "Iteration#:  1220 ; Loss:  7.346216190484997 ; l2 norm of gradient:  1.611910046778656 ; l2 norm of weights:  8.64371487285185\n",
            "Iteration#:  1224 ; Loss:  7.3461276421565405 ; l2 norm of gradient:  1.6119014278290666 ; l2 norm of weights:  8.643663788273333\n",
            "Iteration#:  1228 ; Loss:  7.346039098487184 ; l2 norm of gradient:  1.6118928088775166 ; l2 norm of weights:  8.643612704090328\n",
            "Iteration#:  1232 ; Loss:  7.345950552212416 ; l2 norm of gradient:  1.6118841899240075 ; l2 norm of weights:  8.643561620302835\n",
            "Iteration#:  1236 ; Loss:  7.34586200898285 ; l2 norm of gradient:  1.6118755709685335 ; l2 norm of weights:  8.643510536910853\n",
            "Iteration#:  1240 ; Loss:  7.34577347556963 ; l2 norm of gradient:  1.6118669520110946 ; l2 norm of weights:  8.643459453914382\n",
            "Iteration#:  1244 ; Loss:  7.345684926284276 ; l2 norm of gradient:  1.611858333051689 ; l2 norm of weights:  8.643408371313418\n",
            "Iteration#:  1248 ; Loss:  7.345596389526583 ; l2 norm of gradient:  1.611849714090315 ; l2 norm of weights:  8.643357289107966\n",
            "Iteration#:  1252 ; Loss:  7.345507849368863 ; l2 norm of gradient:  1.6118410951269693 ; l2 norm of weights:  8.64330620729802\n",
            "Iteration#:  1256 ; Loss:  7.345419309519513 ; l2 norm of gradient:  1.6118324761616505 ; l2 norm of weights:  8.64325512588358\n",
            "Iteration#:  1260 ; Loss:  7.345330771485691 ; l2 norm of gradient:  1.6118238571943568 ; l2 norm of weights:  8.64320404486465\n",
            "Iteration#:  1264 ; Loss:  7.345242231010818 ; l2 norm of gradient:  1.6118152382250857 ; l2 norm of weights:  8.643152964241223\n",
            "Iteration#:  1268 ; Loss:  7.345153691725956 ; l2 norm of gradient:  1.6118066192538356 ; l2 norm of weights:  8.6431018840133\n",
            "Iteration#:  1272 ; Loss:  7.34506515341694 ; l2 norm of gradient:  1.611798000280605 ; l2 norm of weights:  8.643050804180882\n",
            "Iteration#:  1276 ; Loss:  7.344976621597812 ; l2 norm of gradient:  1.6117893813053914 ; l2 norm of weights:  8.64299972474397\n",
            "Iteration#:  1280 ; Loss:  7.344888084572702 ; l2 norm of gradient:  1.6117807623281912 ; l2 norm of weights:  8.642948645702557\n",
            "Iteration#:  1284 ; Loss:  7.344799548982573 ; l2 norm of gradient:  1.6117721433490055 ; l2 norm of weights:  8.642897567056647\n",
            "Iteration#:  1288 ; Loss:  7.344711010561818 ; l2 norm of gradient:  1.6117635243678294 ; l2 norm of weights:  8.642846488806237\n",
            "Iteration#:  1292 ; Loss:  7.344622474724165 ; l2 norm of gradient:  1.6117549053846632 ; l2 norm of weights:  8.642795410951328\n",
            "Iteration#:  1296 ; Loss:  7.344533938173946 ; l2 norm of gradient:  1.6117462863995033 ; l2 norm of weights:  8.642744333491915\n",
            "Iteration#:  1300 ; Loss:  7.344445409287388 ; l2 norm of gradient:  1.6117376674123487 ; l2 norm of weights:  8.642693256428004\n",
            "Iteration#:  1304 ; Loss:  7.344356872198992 ; l2 norm of gradient:  1.6117290484231965 ; l2 norm of weights:  8.64264217975959\n",
            "Iteration#:  1308 ; Loss:  7.344268340359722 ; l2 norm of gradient:  1.611720429432045 ; l2 norm of weights:  8.642591103486673\n",
            "Iteration#:  1312 ; Loss:  7.3441798088797405 ; l2 norm of gradient:  1.611711810438893 ; l2 norm of weights:  8.642540027609252\n",
            "Iteration#:  1316 ; Loss:  7.344091278840947 ; l2 norm of gradient:  1.6117031914437374 ; l2 norm of weights:  8.642488952127326\n",
            "Iteration#:  1320 ; Loss:  7.3440027475100935 ; l2 norm of gradient:  1.611694572446576 ; l2 norm of weights:  8.642437877040894\n",
            "Iteration#:  1324 ; Loss:  7.3439142138727 ; l2 norm of gradient:  1.6116859534474088 ; l2 norm of weights:  8.642386802349955\n",
            "Iteration#:  1328 ; Loss:  7.343825684279553 ; l2 norm of gradient:  1.6116773344462314 ; l2 norm of weights:  8.64233572805451\n",
            "Iteration#:  1332 ; Loss:  7.343737152376324 ; l2 norm of gradient:  1.611668715443043 ; l2 norm of weights:  8.64228465415456\n",
            "Iteration#:  1336 ; Loss:  7.343648625145817 ; l2 norm of gradient:  1.6116600964378414 ; l2 norm of weights:  8.642233580650098\n",
            "Iteration#:  1340 ; Loss:  7.343560094641823 ; l2 norm of gradient:  1.6116514774306236 ; l2 norm of weights:  8.642182507541127\n",
            "Iteration#:  1344 ; Loss:  7.343471566001007 ; l2 norm of gradient:  1.611642858421389 ; l2 norm of weights:  8.642131434827647\n",
            "Iteration#:  1348 ; Loss:  7.343383042088638 ; l2 norm of gradient:  1.611634239410136 ; l2 norm of weights:  8.642080362509654\n",
            "Iteration#:  1352 ; Loss:  7.343294514496712 ; l2 norm of gradient:  1.6116256203968606 ; l2 norm of weights:  8.642029290587152\n",
            "Iteration#:  1356 ; Loss:  7.343205986054859 ; l2 norm of gradient:  1.611617001381562 ; l2 norm of weights:  8.641978219060135\n",
            "Iteration#:  1360 ; Loss:  7.343117462660576 ; l2 norm of gradient:  1.6116083823642389 ; l2 norm of weights:  8.641927147928604\n",
            "Iteration#:  1364 ; Loss:  7.343028933923117 ; l2 norm of gradient:  1.6115997633448873 ; l2 norm of weights:  8.64187607719256\n",
            "Iteration#:  1368 ; Loss:  7.3429404110690335 ; l2 norm of gradient:  1.611591144323506 ; l2 norm of weights:  8.641825006852\n",
            "Iteration#:  1372 ; Loss:  7.342851883680634 ; l2 norm of gradient:  1.6115825253000948 ; l2 norm of weights:  8.641773936906926\n",
            "Iteration#:  1376 ; Loss:  7.342763360539854 ; l2 norm of gradient:  1.611573906274649 ; l2 norm of weights:  8.641722867357336\n",
            "Iteration#:  1380 ; Loss:  7.342674834622332 ; l2 norm of gradient:  1.6115652872471677 ; l2 norm of weights:  8.641671798203229\n",
            "Iteration#:  1384 ; Loss:  7.342586309656839 ; l2 norm of gradient:  1.6115566682176494 ; l2 norm of weights:  8.6416207294446\n",
            "Iteration#:  1388 ; Loss:  7.342497790294756 ; l2 norm of gradient:  1.611548049186092 ; l2 norm of weights:  8.641569661081453\n",
            "Iteration#:  1392 ; Loss:  7.342409268671021 ; l2 norm of gradient:  1.6115394301524917 ; l2 norm of weights:  8.641518593113787\n",
            "Iteration#:  1396 ; Loss:  7.342320747924813 ; l2 norm of gradient:  1.6115308111168487 ; l2 norm of weights:  8.641467525541604\n",
            "Iteration#:  1400 ; Loss:  7.342232223023467 ; l2 norm of gradient:  1.61152219207916 ; l2 norm of weights:  8.641416458364894\n",
            "Iteration#:  1404 ; Loss:  7.342143706045397 ; l2 norm of gradient:  1.6115135730394232 ; l2 norm of weights:  8.641365391583665\n",
            "Iteration#:  1408 ; Loss:  7.3420551869123045 ; l2 norm of gradient:  1.6115049539976376 ; l2 norm of weights:  8.641314325197913\n",
            "Iteration#:  1412 ; Loss:  7.3419666669722545 ; l2 norm of gradient:  1.6114963349537998 ; l2 norm of weights:  8.641263259207639\n",
            "Iteration#:  1416 ; Loss:  7.341878145954773 ; l2 norm of gradient:  1.6114877159079088 ; l2 norm of weights:  8.641212193612837\n",
            "Iteration#:  1420 ; Loss:  7.341789624976917 ; l2 norm of gradient:  1.6114790968599608 ; l2 norm of weights:  8.641161128413511\n",
            "Iteration#:  1424 ; Loss:  7.341701106391776 ; l2 norm of gradient:  1.6114704778099562 ; l2 norm of weights:  8.641110063609661\n",
            "Iteration#:  1428 ; Loss:  7.341612589323013 ; l2 norm of gradient:  1.611461858757891 ; l2 norm of weights:  8.64105899920128\n",
            "Iteration#:  1432 ; Loss:  7.341524073695332 ; l2 norm of gradient:  1.6114532397037646 ; l2 norm of weights:  8.641007935188373\n",
            "Iteration#:  1436 ; Loss:  7.341435556373062 ; l2 norm of gradient:  1.611444620647574 ; l2 norm of weights:  8.640956871570939\n",
            "Iteration#:  1440 ; Loss:  7.341347040454108 ; l2 norm of gradient:  1.6114360015893174 ; l2 norm of weights:  8.640905808348977\n",
            "Iteration#:  1444 ; Loss:  7.3412585257531795 ; l2 norm of gradient:  1.6114273825289933 ; l2 norm of weights:  8.640854745522482\n",
            "Iteration#:  1448 ; Loss:  7.341170013055011 ; l2 norm of gradient:  1.6114187634665993 ; l2 norm of weights:  8.640803683091457\n",
            "Iteration#:  1452 ; Loss:  7.34108149626656 ; l2 norm of gradient:  1.6114101444021327 ; l2 norm of weights:  8.640752621055901\n",
            "Iteration#:  1456 ; Loss:  7.340992984968343 ; l2 norm of gradient:  1.6114015253355924 ; l2 norm of weights:  8.640701559415811\n",
            "Iteration#:  1460 ; Loss:  7.340904469900716 ; l2 norm of gradient:  1.6113929062669765 ; l2 norm of weights:  8.64065049817119\n",
            "Iteration#:  1464 ; Loss:  7.340815957172443 ; l2 norm of gradient:  1.6113842871962818 ; l2 norm of weights:  8.640599437322033\n",
            "Iteration#:  1468 ; Loss:  7.340727445647159 ; l2 norm of gradient:  1.6113756681235072 ; l2 norm of weights:  8.640548376868344\n",
            "Iteration#:  1472 ; Loss:  7.340638932850442 ; l2 norm of gradient:  1.6113670490486498 ; l2 norm of weights:  8.640497316810116\n",
            "Iteration#:  1476 ; Loss:  7.340550420719216 ; l2 norm of gradient:  1.6113584299717088 ; l2 norm of weights:  8.640446257147355\n",
            "Iteration#:  1480 ; Loss:  7.340461910034856 ; l2 norm of gradient:  1.6113498108926818 ; l2 norm of weights:  8.640395197880055\n",
            "Iteration#:  1484 ; Loss:  7.340373400364551 ; l2 norm of gradient:  1.6113411918115657 ; l2 norm of weights:  8.640344139008217\n",
            "Iteration#:  1488 ; Loss:  7.340284893240595 ; l2 norm of gradient:  1.61133257272836 ; l2 norm of weights:  8.640293080531842\n",
            "Iteration#:  1492 ; Loss:  7.340196384516721 ; l2 norm of gradient:  1.611323953643062 ; l2 norm of weights:  8.640242022450925\n",
            "Iteration#:  1496 ; Loss:  7.340107870966303 ; l2 norm of gradient:  1.611315334555669 ; l2 norm of weights:  8.64019096476547\n",
            "Iteration#:  1500 ; Loss:  7.340019364693558 ; l2 norm of gradient:  1.6113067154661802 ; l2 norm of weights:  8.640139907475472\n",
            "Iteration#:  1504 ; Loss:  7.339930857893845 ; l2 norm of gradient:  1.6112980963745924 ; l2 norm of weights:  8.640088850580934\n",
            "Iteration#:  1508 ; Loss:  7.339842349520536 ; l2 norm of gradient:  1.6112894772809039 ; l2 norm of weights:  8.640037794081854\n",
            "Iteration#:  1512 ; Loss:  7.339753842997649 ; l2 norm of gradient:  1.6112808581851128 ; l2 norm of weights:  8.639986737978226\n",
            "Iteration#:  1516 ; Loss:  7.3396653388154425 ; l2 norm of gradient:  1.6112722390872174 ; l2 norm of weights:  8.639935682270057\n",
            "Iteration#:  1520 ; Loss:  7.339576831606983 ; l2 norm of gradient:  1.6112636199872157 ; l2 norm of weights:  8.639884626957345\n",
            "Iteration#:  1524 ; Loss:  7.3394883251639 ; l2 norm of gradient:  1.6112550008851059 ; l2 norm of weights:  8.639833572040082\n",
            "Iteration#:  1528 ; Loss:  7.339399817774561 ; l2 norm of gradient:  1.6112463817808835 ; l2 norm of weights:  8.639782517518276\n",
            "Iteration#:  1532 ; Loss:  7.339311318226714 ; l2 norm of gradient:  1.6112377626745498 ; l2 norm of weights:  8.639731463391922\n",
            "Iteration#:  1536 ; Loss:  7.33922281386298 ; l2 norm of gradient:  1.6112291435661008 ; l2 norm of weights:  8.639680409661018\n",
            "Iteration#:  1540 ; Loss:  7.339134312882129 ; l2 norm of gradient:  1.6112205244555333 ; l2 norm of weights:  8.639629356325567\n",
            "Iteration#:  1544 ; Loss:  7.339045809013365 ; l2 norm of gradient:  1.6112119053428489 ; l2 norm of weights:  8.639578303385566\n",
            "Iteration#:  1548 ; Loss:  7.338957306059818 ; l2 norm of gradient:  1.6112032862280434 ; l2 norm of weights:  8.639527250841015\n",
            "Iteration#:  1552 ; Loss:  7.338868809490588 ; l2 norm of gradient:  1.6111946671111141 ; l2 norm of weights:  8.639476198691911\n",
            "Iteration#:  1556 ; Loss:  7.338780303387668 ; l2 norm of gradient:  1.61118604799206 ; l2 norm of weights:  8.639425146938255\n",
            "Iteration#:  1560 ; Loss:  7.3386918039205105 ; l2 norm of gradient:  1.6111774288708787 ; l2 norm of weights:  8.639374095580047\n",
            "Iteration#:  1564 ; Loss:  7.3386033044074415 ; l2 norm of gradient:  1.6111688097475687 ; l2 norm of weights:  8.639323044617283\n",
            "Iteration#:  1568 ; Loss:  7.338514805824577 ; l2 norm of gradient:  1.6111601906221265 ; l2 norm of weights:  8.639271994049968\n",
            "Iteration#:  1572 ; Loss:  7.338426305641384 ; l2 norm of gradient:  1.6111515714945517 ; l2 norm of weights:  8.639220943878097\n",
            "Iteration#:  1576 ; Loss:  7.338337808974783 ; l2 norm of gradient:  1.611142952364841 ; l2 norm of weights:  8.63916989410167\n",
            "Iteration#:  1580 ; Loss:  7.338249307424187 ; l2 norm of gradient:  1.611134333232993 ; l2 norm of weights:  8.639118844720684\n",
            "Iteration#:  1584 ; Loss:  7.3381608128624585 ; l2 norm of gradient:  1.6111257140990058 ; l2 norm of weights:  8.63906779573514\n",
            "Iteration#:  1588 ; Loss:  7.33807231476891 ; l2 norm of gradient:  1.6111170949628777 ; l2 norm of weights:  8.639016747145039\n",
            "Iteration#:  1592 ; Loss:  7.337983818564237 ; l2 norm of gradient:  1.6111084758246048 ; l2 norm of weights:  8.638965698950381\n",
            "Iteration#:  1596 ; Loss:  7.337895322842806 ; l2 norm of gradient:  1.6110998566841874 ; l2 norm of weights:  8.638914651151161\n",
            "Iteration#:  1600 ; Loss:  7.337806827965317 ; l2 norm of gradient:  1.6110912375416209 ; l2 norm of weights:  8.638863603747378\n",
            "Iteration#:  1604 ; Loss:  7.337718332637319 ; l2 norm of gradient:  1.611082618396906 ; l2 norm of weights:  8.638812556739033\n",
            "Iteration#:  1608 ; Loss:  7.337629836546743 ; l2 norm of gradient:  1.611073999250039 ; l2 norm of weights:  8.638761510126129\n",
            "Iteration#:  1612 ; Loss:  7.337541344125597 ; l2 norm of gradient:  1.6110653801010173 ; l2 norm of weights:  8.63871046390866\n",
            "Iteration#:  1616 ; Loss:  7.33745285010273 ; l2 norm of gradient:  1.6110567609498412 ; l2 norm of weights:  8.638659418086629\n",
            "Iteration#:  1620 ; Loss:  7.337364362096849 ; l2 norm of gradient:  1.6110481417965057 ; l2 norm of weights:  8.638608372660029\n",
            "Iteration#:  1624 ; Loss:  7.337275863743692 ; l2 norm of gradient:  1.611039522641011 ; l2 norm of weights:  8.638557327628867\n",
            "Iteration#:  1628 ; Loss:  7.337187378016105 ; l2 norm of gradient:  1.611030903483354 ; l2 norm of weights:  8.638506282993138\n",
            "Iteration#:  1632 ; Loss:  7.33709888176345 ; l2 norm of gradient:  1.6110222843235324 ; l2 norm of weights:  8.638455238752838\n",
            "Iteration#:  1636 ; Loss:  7.33701039293398 ; l2 norm of gradient:  1.6110136651615454 ; l2 norm of weights:  8.638404194907976\n",
            "Iteration#:  1640 ; Loss:  7.336921900363112 ; l2 norm of gradient:  1.6110050459973904 ; l2 norm of weights:  8.63835315145854\n",
            "Iteration#:  1644 ; Loss:  7.336833412010007 ; l2 norm of gradient:  1.6109964268310635 ; l2 norm of weights:  8.638302108404538\n",
            "Iteration#:  1648 ; Loss:  7.3367449229982435 ; l2 norm of gradient:  1.6109878076625648 ; l2 norm of weights:  8.638251065745964\n",
            "Iteration#:  1652 ; Loss:  7.336656434951951 ; l2 norm of gradient:  1.6109791884918918 ; l2 norm of weights:  8.63820002348282\n",
            "Iteration#:  1656 ; Loss:  7.336567946903511 ; l2 norm of gradient:  1.6109705693190428 ; l2 norm of weights:  8.638148981615103\n",
            "Iteration#:  1660 ; Loss:  7.336479459214973 ; l2 norm of gradient:  1.610961950144014 ; l2 norm of weights:  8.638097940142814\n",
            "Iteration#:  1664 ; Loss:  7.336390970002865 ; l2 norm of gradient:  1.610953330966806 ; l2 norm of weights:  8.638046899065952\n",
            "Iteration#:  1668 ; Loss:  7.3363024851282495 ; l2 norm of gradient:  1.610944711787414 ; l2 norm of weights:  8.637995858384514\n",
            "Iteration#:  1672 ; Loss:  7.336214004009208 ; l2 norm of gradient:  1.6109360926058376 ; l2 norm of weights:  8.637944818098502\n",
            "Iteration#:  1676 ; Loss:  7.3361255138971675 ; l2 norm of gradient:  1.6109274734220742 ; l2 norm of weights:  8.637893778207912\n",
            "Iteration#:  1680 ; Loss:  7.336037033072171 ; l2 norm of gradient:  1.6109188542361226 ; l2 norm of weights:  8.637842738712747\n",
            "Iteration#:  1684 ; Loss:  7.335948547358671 ; l2 norm of gradient:  1.610910235047979 ; l2 norm of weights:  8.637791699613004\n",
            "Iteration#:  1688 ; Loss:  7.335860064807225 ; l2 norm of gradient:  1.6109016158576435 ; l2 norm of weights:  8.637740660908683\n",
            "Iteration#:  1692 ; Loss:  7.335771579004103 ; l2 norm of gradient:  1.6108929966651115 ; l2 norm of weights:  8.637689622599783\n",
            "Iteration#:  1696 ; Loss:  7.3356830968475615 ; l2 norm of gradient:  1.6108843774703834 ; l2 norm of weights:  8.637638584686304\n",
            "Iteration#:  1700 ; Loss:  7.335594615808833 ; l2 norm of gradient:  1.6108757582734547 ; l2 norm of weights:  8.637587547168243\n",
            "Iteration#:  1704 ; Loss:  7.335506132399809 ; l2 norm of gradient:  1.6108671390743254 ; l2 norm of weights:  8.637536510045601\n",
            "Iteration#:  1708 ; Loss:  7.335417651698275 ; l2 norm of gradient:  1.6108585198729926 ; l2 norm of weights:  8.637485473318376\n",
            "Iteration#:  1712 ; Loss:  7.3353291703841546 ; l2 norm of gradient:  1.6108499006694548 ; l2 norm of weights:  8.637434436986569\n",
            "Iteration#:  1716 ; Loss:  7.335240694789654 ; l2 norm of gradient:  1.6108412814637088 ; l2 norm of weights:  8.637383401050178\n",
            "Iteration#:  1720 ; Loss:  7.335152211948264 ; l2 norm of gradient:  1.610832662255753 ; l2 norm of weights:  8.637332365509202\n",
            "Iteration#:  1724 ; Loss:  7.335063731681839 ; l2 norm of gradient:  1.6108240430455856 ; l2 norm of weights:  8.63728133036364\n",
            "Iteration#:  1728 ; Loss:  7.334975252945933 ; l2 norm of gradient:  1.6108154238332044 ; l2 norm of weights:  8.637230295613493\n",
            "Iteration#:  1732 ; Loss:  7.334886769047365 ; l2 norm of gradient:  1.6108068046186073 ; l2 norm of weights:  8.637179261258758\n",
            "Iteration#:  1736 ; Loss:  7.33479829497894 ; l2 norm of gradient:  1.6107981854017928 ; l2 norm of weights:  8.637128227299437\n",
            "Iteration#:  1740 ; Loss:  7.334709820605746 ; l2 norm of gradient:  1.6107895661827574 ; l2 norm of weights:  8.637077193735527\n",
            "Iteration#:  1744 ; Loss:  7.334621337963732 ; l2 norm of gradient:  1.6107809469615002 ; l2 norm of weights:  8.637026160567025\n",
            "Iteration#:  1748 ; Loss:  7.334532864751019 ; l2 norm of gradient:  1.6107723277380182 ; l2 norm of weights:  8.636975127793935\n",
            "Iteration#:  1752 ; Loss:  7.33444439124054 ; l2 norm of gradient:  1.6107637085123105 ; l2 norm of weights:  8.636924095416255\n",
            "Iteration#:  1756 ; Loss:  7.334355917814456 ; l2 norm of gradient:  1.6107550892843738 ; l2 norm of weights:  8.63687306343398\n",
            "Iteration#:  1760 ; Loss:  7.334267440169384 ; l2 norm of gradient:  1.6107464700542071 ; l2 norm of weights:  8.636822031847114\n",
            "Iteration#:  1764 ; Loss:  7.334178968315096 ; l2 norm of gradient:  1.6107378508218075 ; l2 norm of weights:  8.636771000655655\n",
            "Iteration#:  1768 ; Loss:  7.334090492815255 ; l2 norm of gradient:  1.6107292315871744 ; l2 norm of weights:  8.6367199698596\n",
            "Iteration#:  1772 ; Loss:  7.334002019696474 ; l2 norm of gradient:  1.6107206123503037 ; l2 norm of weights:  8.636668939458954\n",
            "Iteration#:  1776 ; Loss:  7.33391354861517 ; l2 norm of gradient:  1.6107119931111942 ; l2 norm of weights:  8.636617909453708\n",
            "Iteration#:  1780 ; Loss:  7.333825074140324 ; l2 norm of gradient:  1.6107033738698444 ; l2 norm of weights:  8.636566879843869\n",
            "Iteration#:  1784 ; Loss:  7.333736601625915 ; l2 norm of gradient:  1.6106947546262504 ; l2 norm of weights:  8.63651585062943\n",
            "Iteration#:  1788 ; Loss:  7.333648129213103 ; l2 norm of gradient:  1.6106861353804127 ; l2 norm of weights:  8.636464821810394\n",
            "Iteration#:  1792 ; Loss:  7.333559661013803 ; l2 norm of gradient:  1.6106775161323268 ; l2 norm of weights:  8.636413793386762\n",
            "Iteration#:  1796 ; Loss:  7.333471189894414 ; l2 norm of gradient:  1.6106688968819922 ; l2 norm of weights:  8.636362765358525\n",
            "Iteration#:  1800 ; Loss:  7.333382718626094 ; l2 norm of gradient:  1.6106602776294068 ; l2 norm of weights:  8.636311737725691\n",
            "Iteration#:  1804 ; Loss:  7.333294253463218 ; l2 norm of gradient:  1.610651658374566 ; l2 norm of weights:  8.636260710488255\n",
            "Iteration#:  1808 ; Loss:  7.333205782885251 ; l2 norm of gradient:  1.6106430391174715 ; l2 norm of weights:  8.636209683646216\n",
            "Iteration#:  1812 ; Loss:  7.333117315774414 ; l2 norm of gradient:  1.610634419858119 ; l2 norm of weights:  8.636158657199575\n",
            "Iteration#:  1816 ; Loss:  7.333028849397847 ; l2 norm of gradient:  1.610625800596507 ; l2 norm of weights:  8.636107631148331\n",
            "Iteration#:  1820 ; Loss:  7.33294038018173 ; l2 norm of gradient:  1.6106171813326329 ; l2 norm of weights:  8.636056605492481\n",
            "Iteration#:  1824 ; Loss:  7.3328519132608445 ; l2 norm of gradient:  1.6106085620664954 ; l2 norm of weights:  8.636005580232025\n",
            "Iteration#:  1828 ; Loss:  7.332763449465844 ; l2 norm of gradient:  1.6105999427980917 ; l2 norm of weights:  8.635954555366965\n",
            "Iteration#:  1832 ; Loss:  7.332674984771253 ; l2 norm of gradient:  1.6105913235274196 ; l2 norm of weights:  8.635903530897298\n",
            "Iteration#:  1836 ; Loss:  7.332586520620003 ; l2 norm of gradient:  1.6105827042544774 ; l2 norm of weights:  8.635852506823023\n",
            "Iteration#:  1840 ; Loss:  7.332498058124421 ; l2 norm of gradient:  1.6105740849792631 ; l2 norm of weights:  8.635801483144139\n",
            "Iteration#:  1844 ; Loss:  7.332409592371846 ; l2 norm of gradient:  1.610565465701775 ; l2 norm of weights:  8.635750459860645\n",
            "Iteration#:  1848 ; Loss:  7.3323211290490065 ; l2 norm of gradient:  1.6105568464220097 ; l2 norm of weights:  8.635699436972544\n",
            "Iteration#:  1852 ; Loss:  7.332232665065815 ; l2 norm of gradient:  1.6105482271399658 ; l2 norm of weights:  8.63564841447983\n",
            "Iteration#:  1856 ; Loss:  7.332144208105682 ; l2 norm of gradient:  1.6105396078556418 ; l2 norm of weights:  8.635597392382504\n",
            "Iteration#:  1860 ; Loss:  7.332055740594628 ; l2 norm of gradient:  1.6105309885690349 ; l2 norm of weights:  8.635546370680569\n",
            "Iteration#:  1864 ; Loss:  7.331967284040928 ; l2 norm of gradient:  1.6105223692801431 ; l2 norm of weights:  8.635495349374018\n",
            "Iteration#:  1868 ; Loss:  7.331878822325651 ; l2 norm of gradient:  1.610513749988964 ; l2 norm of weights:  8.635444328462853\n",
            "Iteration#:  1872 ; Loss:  7.331790363953517 ; l2 norm of gradient:  1.6105051306954976 ; l2 norm of weights:  8.635393307947073\n",
            "Iteration#:  1876 ; Loss:  7.33170190112285 ; l2 norm of gradient:  1.6104965113997383 ; l2 norm of weights:  8.63534228782668\n",
            "Iteration#:  1880 ; Loss:  7.331613443836941 ; l2 norm of gradient:  1.6104878921016865 ; l2 norm of weights:  8.635291268101668\n",
            "Iteration#:  1884 ; Loss:  7.331524990238105 ; l2 norm of gradient:  1.610479272801339 ; l2 norm of weights:  8.635240248772039\n",
            "Iteration#:  1888 ; Loss:  7.331436524717478 ; l2 norm of gradient:  1.6104706534986946 ; l2 norm of weights:  8.635189229837794\n",
            "Iteration#:  1892 ; Loss:  7.331348065376089 ; l2 norm of gradient:  1.61046203419375 ; l2 norm of weights:  8.635138211298928\n",
            "Iteration#:  1896 ; Loss:  7.331259612200725 ; l2 norm of gradient:  1.610453414886504 ; l2 norm of weights:  8.635087193155442\n",
            "Iteration#:  1900 ; Loss:  7.331171153062055 ; l2 norm of gradient:  1.6104447955769547 ; l2 norm of weights:  8.635036175407338\n",
            "Iteration#:  1904 ; Loss:  7.331082697618465 ; l2 norm of gradient:  1.6104361762650987 ; l2 norm of weights:  8.634985158054613\n",
            "Iteration#:  1908 ; Loss:  7.330994242851512 ; l2 norm of gradient:  1.6104275569509354 ; l2 norm of weights:  8.634934141097263\n",
            "Iteration#:  1912 ; Loss:  7.330905790448118 ; l2 norm of gradient:  1.6104189376344624 ; l2 norm of weights:  8.634883124535293\n",
            "Iteration#:  1916 ; Loss:  7.330817332717632 ; l2 norm of gradient:  1.6104103183156766 ; l2 norm of weights:  8.6348321083687\n",
            "Iteration#:  1920 ; Loss:  7.330728880222287 ; l2 norm of gradient:  1.6104016989945766 ; l2 norm of weights:  8.63478109259748\n",
            "Iteration#:  1924 ; Loss:  7.330640422377716 ; l2 norm of gradient:  1.6103930796711603 ; l2 norm of weights:  8.634730077221636\n",
            "Iteration#:  1928 ; Loss:  7.330551969367901 ; l2 norm of gradient:  1.6103844603454254 ; l2 norm of weights:  8.634679062241169\n",
            "Iteration#:  1932 ; Loss:  7.330463517992981 ; l2 norm of gradient:  1.6103758410173705 ; l2 norm of weights:  8.634628047656072\n",
            "Iteration#:  1936 ; Loss:  7.330375069901463 ; l2 norm of gradient:  1.6103672216869918 ; l2 norm of weights:  8.634577033466348\n",
            "Iteration#:  1940 ; Loss:  7.330286616268738 ; l2 norm of gradient:  1.6103586023542888 ; l2 norm of weights:  8.634526019671998\n",
            "Iteration#:  1944 ; Loss:  7.330198166502488 ; l2 norm of gradient:  1.6103499830192591 ; l2 norm of weights:  8.634475006273016\n",
            "Iteration#:  1948 ; Loss:  7.330109718030584 ; l2 norm of gradient:  1.6103413636819004 ; l2 norm of weights:  8.634423993269406\n",
            "Iteration#:  1952 ; Loss:  7.3300212676529775 ; l2 norm of gradient:  1.6103327443422104 ; l2 norm of weights:  8.634372980661164\n",
            "Iteration#:  1956 ; Loss:  7.329932824465702 ; l2 norm of gradient:  1.6103241250001876 ; l2 norm of weights:  8.634321968448294\n",
            "Iteration#:  1960 ; Loss:  7.329844368923329 ; l2 norm of gradient:  1.6103155056558285 ; l2 norm of weights:  8.634270956630788\n",
            "Iteration#:  1964 ; Loss:  7.329755923341897 ; l2 norm of gradient:  1.6103068863091325 ; l2 norm of weights:  8.63421994520865\n",
            "Iteration#:  1968 ; Loss:  7.329667472776934 ; l2 norm of gradient:  1.6102982669600967 ; l2 norm of weights:  8.634168934181877\n",
            "Iteration#:  1972 ; Loss:  7.32957902740243 ; l2 norm of gradient:  1.6102896476087194 ; l2 norm of weights:  8.634117923550471\n",
            "Iteration#:  1976 ; Loss:  7.329490580521965 ; l2 norm of gradient:  1.610281028254997 ; l2 norm of weights:  8.63406691331443\n",
            "Iteration#:  1980 ; Loss:  7.329402131997127 ; l2 norm of gradient:  1.61027240889893 ; l2 norm of weights:  8.634015903473754\n",
            "Iteration#:  1984 ; Loss:  7.329313685819235 ; l2 norm of gradient:  1.610263789540515 ; l2 norm of weights:  8.633964894028438\n",
            "Iteration#:  1988 ; Loss:  7.329225241652672 ; l2 norm of gradient:  1.6102551701797496 ; l2 norm of weights:  8.633913884978485\n",
            "Iteration#:  1992 ; Loss:  7.329136797335211 ; l2 norm of gradient:  1.610246550816632 ; l2 norm of weights:  8.633862876323894\n",
            "Iteration#:  1996 ; Loss:  7.329048352677564 ; l2 norm of gradient:  1.6102379314511592 ; l2 norm of weights:  8.633811868064665\n",
            "Iteration#:  2000 ; Loss:  7.328959909441744 ; l2 norm of gradient:  1.6102293120833306 ; l2 norm of weights:  8.633760860200795\n",
            "Iteration#:  2004 ; Loss:  7.328871466134018 ; l2 norm of gradient:  1.6102206927131433 ; l2 norm of weights:  8.633709852732284\n",
            "Iteration#:  2008 ; Loss:  7.3287830263797495 ; l2 norm of gradient:  1.6102120733405956 ; l2 norm of weights:  8.63365884565913\n",
            "Iteration#:  2012 ; Loss:  7.328694583822076 ; l2 norm of gradient:  1.6102034539656833 ; l2 norm of weights:  8.633607838981334\n",
            "Iteration#:  2016 ; Loss:  7.3286061415484305 ; l2 norm of gradient:  1.6101948345884074 ; l2 norm of weights:  8.633556832698895\n",
            "Iteration#:  2020 ; Loss:  7.328517699603866 ; l2 norm of gradient:  1.6101862152087643 ; l2 norm of weights:  8.633505826811811\n",
            "Iteration#:  2024 ; Loss:  7.328429262005962 ; l2 norm of gradient:  1.6101775958267515 ; l2 norm of weights:  8.633454821320084\n",
            "Iteration#:  2028 ; Loss:  7.328340817788618 ; l2 norm of gradient:  1.610168976442367 ; l2 norm of weights:  8.63340381622371\n",
            "Iteration#:  2032 ; Loss:  7.328252378940151 ; l2 norm of gradient:  1.61016035705561 ; l2 norm of weights:  8.633352811522691\n",
            "Iteration#:  2036 ; Loss:  7.328163942468323 ; l2 norm of gradient:  1.6101517376664765 ; l2 norm of weights:  8.633301807217023\n",
            "Iteration#:  2040 ; Loss:  7.328075503734077 ; l2 norm of gradient:  1.6101431182749653 ; l2 norm of weights:  8.633250803306709\n",
            "Iteration#:  2044 ; Loss:  7.327987060567044 ; l2 norm of gradient:  1.610134498881074 ; l2 norm of weights:  8.633199799791743\n",
            "Iteration#:  2048 ; Loss:  7.3278986261034245 ; l2 norm of gradient:  1.610125879484801 ; l2 norm of weights:  8.63314879667213\n",
            "Iteration#:  2052 ; Loss:  7.3278101892355 ; l2 norm of gradient:  1.6101172600861442 ; l2 norm of weights:  8.633097793947865\n",
            "Iteration#:  2056 ; Loss:  7.327721751701812 ; l2 norm of gradient:  1.6101086406851006 ; l2 norm of weights:  8.63304679161895\n",
            "Iteration#:  2060 ; Loss:  7.327633316542016 ; l2 norm of gradient:  1.6101000212816676 ; l2 norm of weights:  8.632995789685381\n",
            "Iteration#:  2064 ; Loss:  7.327544883554946 ; l2 norm of gradient:  1.610091401875846 ; l2 norm of weights:  8.632944788147162\n",
            "Iteration#:  2068 ; Loss:  7.327456444106037 ; l2 norm of gradient:  1.61008278246763 ; l2 norm of weights:  8.632893787004287\n",
            "Iteration#:  2072 ; Loss:  7.327368012759347 ; l2 norm of gradient:  1.6100741630570206 ; l2 norm of weights:  8.632842786256758\n",
            "Iteration#:  2076 ; Loss:  7.327279580458166 ; l2 norm of gradient:  1.6100655436440134 ; l2 norm of weights:  8.632791785904574\n",
            "Iteration#:  2080 ; Loss:  7.32719114435691 ; l2 norm of gradient:  1.6100569242286076 ; l2 norm of weights:  8.632740785947735\n",
            "Iteration#:  2084 ; Loss:  7.327102715256135 ; l2 norm of gradient:  1.6100483048108005 ; l2 norm of weights:  8.632689786386237\n",
            "Iteration#:  2088 ; Loss:  7.327014282408371 ; l2 norm of gradient:  1.6100396853905898 ; l2 norm of weights:  8.632638787220083\n",
            "Iteration#:  2092 ; Loss:  7.326925849199576 ; l2 norm of gradient:  1.6100310659679733 ; l2 norm of weights:  8.63258778844927\n",
            "Iteration#:  2096 ; Loss:  7.326837418181436 ; l2 norm of gradient:  1.6100224465429493 ; l2 norm of weights:  8.6325367900738\n",
            "Iteration#:  2100 ; Loss:  7.326748984615992 ; l2 norm of gradient:  1.6100138271155164 ; l2 norm of weights:  8.632485792093666\n",
            "Iteration#:  2104 ; Loss:  7.3266605572859875 ; l2 norm of gradient:  1.6100052076856701 ; l2 norm of weights:  8.632434794508875\n",
            "Iteration#:  2108 ; Loss:  7.326572131421192 ; l2 norm of gradient:  1.6099965882534109 ; l2 norm of weights:  8.63238379731942\n",
            "Iteration#:  2112 ; Loss:  7.326483696178759 ; l2 norm of gradient:  1.6099879688187348 ; l2 norm of weights:  8.632332800525303\n",
            "Iteration#:  2116 ; Loss:  7.326395269785855 ; l2 norm of gradient:  1.6099793493816408 ; l2 norm of weights:  8.632281804126523\n",
            "Iteration#:  2120 ; Loss:  7.326306841509666 ; l2 norm of gradient:  1.609970729942126 ; l2 norm of weights:  8.632230808123081\n",
            "Iteration#:  2124 ; Loss:  7.326218413097618 ; l2 norm of gradient:  1.6099621105001893 ; l2 norm of weights:  8.632179812514972\n",
            "Iteration#:  2128 ; Loss:  7.3261299861794935 ; l2 norm of gradient:  1.6099534910558277 ; l2 norm of weights:  8.632128817302197\n",
            "Iteration#:  2132 ; Loss:  7.326041560806145 ; l2 norm of gradient:  1.6099448716090392 ; l2 norm of weights:  8.632077822484758\n",
            "Iteration#:  2136 ; Loss:  7.3259531350947915 ; l2 norm of gradient:  1.609936252159821 ; l2 norm of weights:  8.632026828062651\n",
            "Iteration#:  2140 ; Loss:  7.32586471112867 ; l2 norm of gradient:  1.6099276327081724 ; l2 norm of weights:  8.631975834035876\n",
            "Iteration#:  2144 ; Loss:  7.3257762867377645 ; l2 norm of gradient:  1.6099190132540901 ; l2 norm of weights:  8.63192484040443\n",
            "Iteration#:  2148 ; Loss:  7.325687859845331 ; l2 norm of gradient:  1.6099103937975716 ; l2 norm of weights:  8.631873847168318\n",
            "Iteration#:  2152 ; Loss:  7.325599432516514 ; l2 norm of gradient:  1.6099017743386168 ; l2 norm of weights:  8.631822854327535\n",
            "Iteration#:  2156 ; Loss:  7.325511011774351 ; l2 norm of gradient:  1.6098931548772215 ; l2 norm of weights:  8.63177186188208\n",
            "Iteration#:  2160 ; Loss:  7.325422588494769 ; l2 norm of gradient:  1.6098845354133848 ; l2 norm of weights:  8.631720869831952\n",
            "Iteration#:  2164 ; Loss:  7.32533416454142 ; l2 norm of gradient:  1.6098759159471037 ; l2 norm of weights:  8.631669878177153\n",
            "Iteration#:  2168 ; Loss:  7.325245747224169 ; l2 norm of gradient:  1.6098672964783773 ; l2 norm of weights:  8.631618886917682\n",
            "Iteration#:  2172 ; Loss:  7.325157323549806 ; l2 norm of gradient:  1.6098586770072014 ; l2 norm of weights:  8.631567896053534\n",
            "Iteration#:  2176 ; Loss:  7.325068902316746 ; l2 norm of gradient:  1.609850057533575 ; l2 norm of weights:  8.63151690558471\n",
            "Iteration#:  2180 ; Loss:  7.324980480571368 ; l2 norm of gradient:  1.6098414380574964 ; l2 norm of weights:  8.631465915511212\n",
            "Iteration#:  2184 ; Loss:  7.324892057902232 ; l2 norm of gradient:  1.609832818578963 ; l2 norm of weights:  8.631414925833036\n",
            "Iteration#:  2188 ; Loss:  7.324803642570458 ; l2 norm of gradient:  1.6098241990979734 ; l2 norm of weights:  8.631363936550184\n",
            "Iteration#:  2192 ; Loss:  7.324715223781418 ; l2 norm of gradient:  1.609815579614523 ; l2 norm of weights:  8.631312947662654\n",
            "Iteration#:  2196 ; Loss:  7.324626805887952 ; l2 norm of gradient:  1.6098069601286125 ; l2 norm of weights:  8.631261959170445\n",
            "Iteration#:  2200 ; Loss:  7.324538387613314 ; l2 norm of gradient:  1.6097983406402392 ; l2 norm of weights:  8.631210971073555\n",
            "Iteration#:  2204 ; Loss:  7.324449973499011 ; l2 norm of gradient:  1.6097897211493986 ; l2 norm of weights:  8.631159983371985\n",
            "Iteration#:  2208 ; Loss:  7.324361553358511 ; l2 norm of gradient:  1.609781101656092 ; l2 norm of weights:  8.631108996065732\n",
            "Iteration#:  2212 ; Loss:  7.3242731326341755 ; l2 norm of gradient:  1.6097724821603145 ; l2 norm of weights:  8.631058009154799\n",
            "Iteration#:  2216 ; Loss:  7.324184720206906 ; l2 norm of gradient:  1.6097638626620656 ; l2 norm of weights:  8.631007022639183\n",
            "Iteration#:  2220 ; Loss:  7.3240963067512075 ; l2 norm of gradient:  1.6097552431613416 ; l2 norm of weights:  8.63095603651888\n",
            "Iteration#:  2224 ; Loss:  7.3240078922035545 ; l2 norm of gradient:  1.6097466236581413 ; l2 norm of weights:  8.630905050793896\n",
            "Iteration#:  2228 ; Loss:  7.323919472534728 ; l2 norm of gradient:  1.6097380041524638 ; l2 norm of weights:  8.630854065464227\n",
            "Iteration#:  2232 ; Loss:  7.323831063179819 ; l2 norm of gradient:  1.6097293846443042 ; l2 norm of weights:  8.63080308052987\n",
            "Iteration#:  2236 ; Loss:  7.323742650290711 ; l2 norm of gradient:  1.609720765133663 ; l2 norm of weights:  8.630752095990825\n",
            "Iteration#:  2240 ; Loss:  7.323654239071571 ; l2 norm of gradient:  1.609712145620537 ; l2 norm of weights:  8.630701111847094\n",
            "Iteration#:  2244 ; Loss:  7.323565822977942 ; l2 norm of gradient:  1.6097035261049226 ; l2 norm of weights:  8.630650128098674\n",
            "Iteration#:  2248 ; Loss:  7.323477417506979 ; l2 norm of gradient:  1.60969490658682 ; l2 norm of weights:  8.630599144745563\n",
            "Iteration#:  2252 ; Loss:  7.323389005440518 ; l2 norm of gradient:  1.6096862870662256 ; l2 norm of weights:  8.630548161787765\n",
            "Iteration#:  2256 ; Loss:  7.323300594729503 ; l2 norm of gradient:  1.609677667543138 ; l2 norm of weights:  8.630497179225275\n",
            "Iteration#:  2260 ; Loss:  7.323212181486136 ; l2 norm of gradient:  1.6096690480175533 ; l2 norm of weights:  8.630446197058092\n",
            "Iteration#:  2264 ; Loss:  7.323123774325667 ; l2 norm of gradient:  1.6096604284894716 ; l2 norm of weights:  8.630395215286217\n",
            "Iteration#:  2268 ; Loss:  7.3230353633213845 ; l2 norm of gradient:  1.6096518089588896 ; l2 norm of weights:  8.63034423390965\n",
            "Iteration#:  2272 ; Loss:  7.322946959900699 ; l2 norm of gradient:  1.6096431894258059 ; l2 norm of weights:  8.630293252928386\n",
            "Iteration#:  2276 ; Loss:  7.322858552260408 ; l2 norm of gradient:  1.609634569890218 ; l2 norm of weights:  8.630242272342429\n",
            "Iteration#:  2280 ; Loss:  7.322770141042151 ; l2 norm of gradient:  1.609625950352122 ; l2 norm of weights:  8.630191292151776\n",
            "Iteration#:  2284 ; Loss:  7.322681733749461 ; l2 norm of gradient:  1.609617330811519 ; l2 norm of weights:  8.630140312356428\n",
            "Iteration#:  2288 ; Loss:  7.3225933293849055 ; l2 norm of gradient:  1.609608711268404 ; l2 norm of weights:  8.63008933295638\n",
            "Iteration#:  2292 ; Loss:  7.3225049237283955 ; l2 norm of gradient:  1.609600091722776 ; l2 norm of weights:  8.630038353951635\n",
            "Iteration#:  2296 ; Loss:  7.322416519515977 ; l2 norm of gradient:  1.6095914721746332 ; l2 norm of weights:  8.629987375342193\n",
            "Iteration#:  2300 ; Loss:  7.322328115816368 ; l2 norm of gradient:  1.6095828526239728 ; l2 norm of weights:  8.629936397128047\n",
            "Iteration#:  2304 ; Loss:  7.3222397102712815 ; l2 norm of gradient:  1.609574233070792 ; l2 norm of weights:  8.629885419309202\n",
            "Iteration#:  2308 ; Loss:  7.322151305731181 ; l2 norm of gradient:  1.60956561351509 ; l2 norm of weights:  8.629834441885656\n",
            "Iteration#:  2312 ; Loss:  7.322062899296358 ; l2 norm of gradient:  1.6095569939568646 ; l2 norm of weights:  8.62978346485741\n",
            "Iteration#:  2316 ; Loss:  7.321974500923735 ; l2 norm of gradient:  1.6095483743961123 ; l2 norm of weights:  8.629732488224457\n",
            "Iteration#:  2320 ; Loss:  7.32188610198325 ; l2 norm of gradient:  1.6095397548328325 ; l2 norm of weights:  8.629681511986806\n",
            "Iteration#:  2324 ; Loss:  7.321797695455922 ; l2 norm of gradient:  1.6095311352670214 ; l2 norm of weights:  8.629630536144447\n",
            "Iteration#:  2328 ; Loss:  7.32170929298698 ; l2 norm of gradient:  1.6095225156986788 ; l2 norm of weights:  8.629579560697383\n",
            "Iteration#:  2332 ; Loss:  7.321620893231746 ; l2 norm of gradient:  1.6095138961278002 ; l2 norm of weights:  8.629528585645613\n",
            "Iteration#:  2336 ; Loss:  7.321532492730571 ; l2 norm of gradient:  1.6095052765543847 ; l2 norm of weights:  8.629477610989136\n",
            "Iteration#:  2340 ; Loss:  7.321444091485057 ; l2 norm of gradient:  1.6094966569784313 ; l2 norm of weights:  8.62942663672795\n",
            "Iteration#:  2344 ; Loss:  7.321355692342973 ; l2 norm of gradient:  1.609488037399935 ; l2 norm of weights:  8.62937566286206\n",
            "Iteration#:  2348 ; Loss:  7.3212672951023965 ; l2 norm of gradient:  1.609479417818896 ; l2 norm of weights:  8.629324689391456\n",
            "Iteration#:  2352 ; Loss:  7.321178899335083 ; l2 norm of gradient:  1.6094707982353107 ; l2 norm of weights:  8.629273716316145\n",
            "Iteration#:  2356 ; Loss:  7.321090501160663 ; l2 norm of gradient:  1.6094621786491783 ; l2 norm of weights:  8.62922274363612\n",
            "Iteration#:  2360 ; Loss:  7.3210021057987245 ; l2 norm of gradient:  1.6094535590604957 ; l2 norm of weights:  8.629171771351388\n",
            "Iteration#:  2364 ; Loss:  7.3209137082688995 ; l2 norm of gradient:  1.6094449394692607 ; l2 norm of weights:  8.62912079946194\n",
            "Iteration#:  2368 ; Loss:  7.320825309532577 ; l2 norm of gradient:  1.609436319875471 ; l2 norm of weights:  8.62906982796778\n",
            "Iteration#:  2372 ; Loss:  7.320736912182298 ; l2 norm of gradient:  1.6094277002791257 ; l2 norm of weights:  8.629018856868903\n",
            "Iteration#:  2376 ; Loss:  7.320648518166752 ; l2 norm of gradient:  1.609419080680221 ; l2 norm of weights:  8.628967886165315\n",
            "Iteration#:  2380 ; Loss:  7.320560125979041 ; l2 norm of gradient:  1.6094104610787554 ; l2 norm of weights:  8.628916915857012\n",
            "Iteration#:  2384 ; Loss:  7.320471729801492 ; l2 norm of gradient:  1.6094018414747266 ; l2 norm of weights:  8.628865945943991\n",
            "Iteration#:  2388 ; Loss:  7.320383339328261 ; l2 norm of gradient:  1.6093932218681326 ; l2 norm of weights:  8.628814976426252\n",
            "Iteration#:  2392 ; Loss:  7.320294947034748 ; l2 norm of gradient:  1.6093846022589702 ; l2 norm of weights:  8.628764007303795\n",
            "Iteration#:  2396 ; Loss:  7.320206553491365 ; l2 norm of gradient:  1.6093759826472387 ; l2 norm of weights:  8.628713038576619\n",
            "Iteration#:  2400 ; Loss:  7.320118157402817 ; l2 norm of gradient:  1.609367363032936 ; l2 norm of weights:  8.628662070244724\n",
            "Iteration#:  2404 ; Loss:  7.320029769680359 ; l2 norm of gradient:  1.6093587434160586 ; l2 norm of weights:  8.628611102308112\n",
            "Iteration#:  2408 ; Loss:  7.319941377084854 ; l2 norm of gradient:  1.6093501237966055 ; l2 norm of weights:  8.628560134766774\n",
            "Iteration#:  2412 ; Loss:  7.319852988193338 ; l2 norm of gradient:  1.6093415041745738 ; l2 norm of weights:  8.628509167620717\n",
            "Iteration#:  2416 ; Loss:  7.319764597790186 ; l2 norm of gradient:  1.609332884549961 ; l2 norm of weights:  8.628458200869932\n",
            "Iteration#:  2420 ; Loss:  7.3196762081726785 ; l2 norm of gradient:  1.609324264922765 ; l2 norm of weights:  8.62840723451443\n",
            "Iteration#:  2424 ; Loss:  7.319587818977865 ; l2 norm of gradient:  1.609315645292985 ; l2 norm of weights:  8.6283562685542\n",
            "Iteration#:  2428 ; Loss:  7.319499432251061 ; l2 norm of gradient:  1.6093070256606177 ; l2 norm of weights:  8.628305302989245\n",
            "Iteration#:  2432 ; Loss:  7.319411045099844 ; l2 norm of gradient:  1.6092984060256603 ; l2 norm of weights:  8.628254337819566\n",
            "Iteration#:  2436 ; Loss:  7.319322658062132 ; l2 norm of gradient:  1.6092897863881115 ; l2 norm of weights:  8.62820337304516\n",
            "Iteration#:  2440 ; Loss:  7.319234270421537 ; l2 norm of gradient:  1.6092811667479698 ; l2 norm of weights:  8.628152408666027\n",
            "Iteration#:  2444 ; Loss:  7.319145882755339 ; l2 norm of gradient:  1.6092725471052316 ; l2 norm of weights:  8.628101444682164\n",
            "Iteration#:  2448 ; Loss:  7.319057498309245 ; l2 norm of gradient:  1.609263927459895 ; l2 norm of weights:  8.628050481093572\n",
            "Iteration#:  2452 ; Loss:  7.318969114630238 ; l2 norm of gradient:  1.6092553078119585 ; l2 norm of weights:  8.62799951790025\n",
            "Iteration#:  2456 ; Loss:  7.318880728214041 ; l2 norm of gradient:  1.609246688161419 ; l2 norm of weights:  8.627948555102197\n",
            "Iteration#:  2460 ; Loss:  7.318792344592923 ; l2 norm of gradient:  1.6092380685082743 ; l2 norm of weights:  8.627897592699416\n",
            "Iteration#:  2464 ; Loss:  7.318703958190265 ; l2 norm of gradient:  1.6092294488525236 ; l2 norm of weights:  8.627846630691899\n",
            "Iteration#:  2468 ; Loss:  7.318615574416554 ; l2 norm of gradient:  1.609220829194164 ; l2 norm of weights:  8.62779566907965\n",
            "Iteration#:  2472 ; Loss:  7.3185271943523444 ; l2 norm of gradient:  1.6092122095331918 ; l2 norm of weights:  8.627744707862666\n",
            "Iteration#:  2476 ; Loss:  7.318438809343115 ; l2 norm of gradient:  1.6092035898696073 ; l2 norm of weights:  8.627693747040949\n",
            "Iteration#:  2480 ; Loss:  7.318350428667298 ; l2 norm of gradient:  1.609194970203406 ; l2 norm of weights:  8.627642786614496\n",
            "Iteration#:  2484 ; Loss:  7.318262047871835 ; l2 norm of gradient:  1.609186350534588 ; l2 norm of weights:  8.627591826583306\n",
            "Iteration#:  2488 ; Loss:  7.3181736676719655 ; l2 norm of gradient:  1.609177730863149 ; l2 norm of weights:  8.62754086694738\n",
            "Iteration#:  2492 ; Loss:  7.318085285121036 ; l2 norm of gradient:  1.6091691111890873 ; l2 norm of weights:  8.627489907706714\n",
            "Iteration#:  2496 ; Loss:  7.317996909964807 ; l2 norm of gradient:  1.6091604915124023 ; l2 norm of weights:  8.627438948861311\n",
            "Iteration#:  2500 ; Loss:  7.317908530035044 ; l2 norm of gradient:  1.6091518718330895 ; l2 norm of weights:  8.627387990411167\n",
            "Iteration#:  2504 ; Loss:  7.317820151003122 ; l2 norm of gradient:  1.609143252151148 ; l2 norm of weights:  8.627337032356285\n",
            "Iteration#:  2508 ; Loss:  7.317731772356687 ; l2 norm of gradient:  1.6091346324665763 ; l2 norm of weights:  8.627286074696661\n",
            "Iteration#:  2512 ; Loss:  7.3176433969562655 ; l2 norm of gradient:  1.6091260127793707 ; l2 norm of weights:  8.627235117432296\n",
            "Iteration#:  2516 ; Loss:  7.317555021203654 ; l2 norm of gradient:  1.609117393089529 ; l2 norm of weights:  8.627184160563186\n",
            "Iteration#:  2520 ; Loss:  7.317466644399908 ; l2 norm of gradient:  1.6091087733970495 ; l2 norm of weights:  8.627133204089333\n",
            "Iteration#:  2524 ; Loss:  7.317378268089193 ; l2 norm of gradient:  1.6091001537019298 ; l2 norm of weights:  8.627082248010737\n",
            "Iteration#:  2528 ; Loss:  7.317289893283677 ; l2 norm of gradient:  1.6090915340041694 ; l2 norm of weights:  8.627031292327397\n",
            "Iteration#:  2532 ; Loss:  7.317201518724453 ; l2 norm of gradient:  1.6090829143037633 ; l2 norm of weights:  8.626980337039312\n",
            "Iteration#:  2536 ; Loss:  7.3171131428325396 ; l2 norm of gradient:  1.6090742946007108 ; l2 norm of weights:  8.626929382146477\n",
            "Iteration#:  2540 ; Loss:  7.317024766088501 ; l2 norm of gradient:  1.6090656748950105 ; l2 norm of weights:  8.626878427648895\n",
            "Iteration#:  2544 ; Loss:  7.316936392756691 ; l2 norm of gradient:  1.6090570551866579 ; l2 norm of weights:  8.626827473546566\n",
            "Iteration#:  2548 ; Loss:  7.316848023991163 ; l2 norm of gradient:  1.6090484354756527 ; l2 norm of weights:  8.626776519839488\n",
            "Iteration#:  2552 ; Loss:  7.3167596553680685 ; l2 norm of gradient:  1.6090398157619918 ; l2 norm of weights:  8.626725566527659\n",
            "Iteration#:  2556 ; Loss:  7.316671277543433 ; l2 norm of gradient:  1.6090311960456736 ; l2 norm of weights:  8.62667461361108\n",
            "Iteration#:  2560 ; Loss:  7.316582909005472 ; l2 norm of gradient:  1.6090225763266948 ; l2 norm of weights:  8.626623661089749\n",
            "Iteration#:  2564 ; Loss:  7.316494535682236 ; l2 norm of gradient:  1.609013956605055 ; l2 norm of weights:  8.626572708963668\n",
            "Iteration#:  2568 ; Loss:  7.316406165521055 ; l2 norm of gradient:  1.6090053368807504 ; l2 norm of weights:  8.626521757232833\n",
            "Iteration#:  2572 ; Loss:  7.316317798091529 ; l2 norm of gradient:  1.608996717153778 ; l2 norm of weights:  8.626470805897243\n",
            "Iteration#:  2576 ; Loss:  7.316229430747668 ; l2 norm of gradient:  1.6089880974241395 ; l2 norm of weights:  8.6264198549569\n",
            "Iteration#:  2580 ; Loss:  7.316141060394374 ; l2 norm of gradient:  1.6089794776918283 ; l2 norm of weights:  8.626368904411802\n",
            "Iteration#:  2584 ; Loss:  7.316052696726304 ; l2 norm of gradient:  1.6089708579568447 ; l2 norm of weights:  8.626317954261944\n",
            "Iteration#:  2588 ; Loss:  7.315964322536078 ; l2 norm of gradient:  1.6089622382191855 ; l2 norm of weights:  8.626267004507334\n",
            "Iteration#:  2592 ; Loss:  7.31587595579622 ; l2 norm of gradient:  1.6089536184788484 ; l2 norm of weights:  8.626216055147962\n",
            "Iteration#:  2596 ; Loss:  7.315787587743282 ; l2 norm of gradient:  1.6089449987358322 ; l2 norm of weights:  8.626165106183834\n",
            "Iteration#:  2600 ; Loss:  7.315699222838251 ; l2 norm of gradient:  1.6089363789901325 ; l2 norm of weights:  8.626114157614946\n",
            "Iteration#:  2604 ; Loss:  7.315610859151164 ; l2 norm of gradient:  1.6089277592417504 ; l2 norm of weights:  8.626063209441298\n",
            "Iteration#:  2608 ; Loss:  7.3155224941891435 ; l2 norm of gradient:  1.6089191394906808 ; l2 norm of weights:  8.62601226166289\n",
            "Iteration#:  2612 ; Loss:  7.315434130112488 ; l2 norm of gradient:  1.6089105197369231 ; l2 norm of weights:  8.625961314279719\n",
            "Iteration#:  2616 ; Loss:  7.315345761606115 ; l2 norm of gradient:  1.608901899980474 ; l2 norm of weights:  8.625910367291784\n",
            "Iteration#:  2620 ; Loss:  7.315257402373078 ; l2 norm of gradient:  1.608893280221332 ; l2 norm of weights:  8.625859420699088\n",
            "Iteration#:  2624 ; Loss:  7.315169038152285 ; l2 norm of gradient:  1.6088846604594944 ; l2 norm of weights:  8.625808474501627\n",
            "Iteration#:  2628 ; Loss:  7.315080673793074 ; l2 norm of gradient:  1.6088760406949603 ; l2 norm of weights:  8.625757528699399\n",
            "Iteration#:  2632 ; Loss:  7.314992314573333 ; l2 norm of gradient:  1.6088674209277252 ; l2 norm of weights:  8.625706583292407\n",
            "Iteration#:  2636 ; Loss:  7.31490395240952 ; l2 norm of gradient:  1.6088588011577876 ; l2 norm of weights:  8.625655638280652\n",
            "Iteration#:  2640 ; Loss:  7.314815589116632 ; l2 norm of gradient:  1.6088501813851472 ; l2 norm of weights:  8.625604693664124\n",
            "Iteration#:  2644 ; Loss:  7.3147272316938405 ; l2 norm of gradient:  1.6088415616097995 ; l2 norm of weights:  8.625553749442833\n",
            "Iteration#:  2648 ; Loss:  7.314638873109358 ; l2 norm of gradient:  1.6088329418317437 ; l2 norm of weights:  8.625502805616769\n",
            "Iteration#:  2652 ; Loss:  7.314550511656689 ; l2 norm of gradient:  1.6088243220509761 ; l2 norm of weights:  8.625451862185937\n",
            "Iteration#:  2656 ; Loss:  7.314462156519984 ; l2 norm of gradient:  1.608815702267496 ; l2 norm of weights:  8.625400919150334\n",
            "Iteration#:  2660 ; Loss:  7.314373798221604 ; l2 norm of gradient:  1.6088070824813003 ; l2 norm of weights:  8.625349976509959\n",
            "Iteration#:  2664 ; Loss:  7.314285437537196 ; l2 norm of gradient:  1.608798462692387 ; l2 norm of weights:  8.625299034264813\n",
            "Iteration#:  2668 ; Loss:  7.314197082373287 ; l2 norm of gradient:  1.6087898429007532 ; l2 norm of weights:  8.625248092414893\n",
            "Iteration#:  2672 ; Loss:  7.314108724901009 ; l2 norm of gradient:  1.6087812231063976 ; l2 norm of weights:  8.6251971509602\n",
            "Iteration#:  2676 ; Loss:  7.314020368562323 ; l2 norm of gradient:  1.6087726033093186 ; l2 norm of weights:  8.625146209900734\n",
            "Iteration#:  2680 ; Loss:  7.313932010001517 ; l2 norm of gradient:  1.6087639835095129 ; l2 norm of weights:  8.62509526923649\n",
            "Iteration#:  2684 ; Loss:  7.313843656877175 ; l2 norm of gradient:  1.6087553637069778 ; l2 norm of weights:  8.625044328967471\n",
            "Iteration#:  2688 ; Loss:  7.313755301908889 ; l2 norm of gradient:  1.608746743901712 ; l2 norm of weights:  8.624993389093678\n",
            "Iteration#:  2692 ; Loss:  7.313666948303636 ; l2 norm of gradient:  1.6087381240937122 ; l2 norm of weights:  8.624942449615105\n",
            "Iteration#:  2696 ; Loss:  7.31357859195509 ; l2 norm of gradient:  1.608729504282978 ; l2 norm of weights:  8.624891510531754\n",
            "Iteration#:  2700 ; Loss:  7.3134902410197205 ; l2 norm of gradient:  1.6087208844695058 ; l2 norm of weights:  8.62484057184362\n",
            "Iteration#:  2704 ; Loss:  7.3134018881374105 ; l2 norm of gradient:  1.6087122646532934 ; l2 norm of weights:  8.62478963355071\n",
            "Iteration#:  2708 ; Loss:  7.313313535395431 ; l2 norm of gradient:  1.6087036448343384 ; l2 norm of weights:  8.624738695653019\n",
            "Iteration#:  2712 ; Loss:  7.313225184174673 ; l2 norm of gradient:  1.6086950250126397 ; l2 norm of weights:  8.624687758150547\n",
            "Iteration#:  2716 ; Loss:  7.313136830618985 ; l2 norm of gradient:  1.6086864051881942 ; l2 norm of weights:  8.624636821043289\n",
            "Iteration#:  2720 ; Loss:  7.313048484340873 ; l2 norm of gradient:  1.6086777853609995 ; l2 norm of weights:  8.62458588433125\n",
            "Iteration#:  2724 ; Loss:  7.312960135347064 ; l2 norm of gradient:  1.6086691655310537 ; l2 norm of weights:  8.624534948014427\n",
            "Iteration#:  2728 ; Loss:  7.3128717853073955 ; l2 norm of gradient:  1.6086605456983538 ; l2 norm of weights:  8.624484012092818\n",
            "Iteration#:  2732 ; Loss:  7.312783436859531 ; l2 norm of gradient:  1.6086519258628995 ; l2 norm of weights:  8.624433076566424\n",
            "Iteration#:  2736 ; Loss:  7.312695088926713 ; l2 norm of gradient:  1.608643306024687 ; l2 norm of weights:  8.624382141435245\n",
            "Iteration#:  2740 ; Loss:  7.312606741194619 ; l2 norm of gradient:  1.6086346861837149 ; l2 norm of weights:  8.624331206699276\n",
            "Iteration#:  2744 ; Loss:  7.312518388193071 ; l2 norm of gradient:  1.6086260663399796 ; l2 norm of weights:  8.624280272358522\n",
            "Iteration#:  2748 ; Loss:  7.312430042832471 ; l2 norm of gradient:  1.6086174464934802 ; l2 norm of weights:  8.624229338412977\n",
            "Iteration#:  2752 ; Loss:  7.312341697601699 ; l2 norm of gradient:  1.6086088266442125 ; l2 norm of weights:  8.624178404862644\n",
            "Iteration#:  2756 ; Loss:  7.312253354085193 ; l2 norm of gradient:  1.6086002067921763 ; l2 norm of weights:  8.624127471707519\n",
            "Iteration#:  2760 ; Loss:  7.312165007656857 ; l2 norm of gradient:  1.6085915869373695 ; l2 norm of weights:  8.624076538947602\n",
            "Iteration#:  2764 ; Loss:  7.3120766597278655 ; l2 norm of gradient:  1.608582967079789 ; l2 norm of weights:  8.624025606582897\n",
            "Iteration#:  2768 ; Loss:  7.3119883186765104 ; l2 norm of gradient:  1.6085743472194327 ; l2 norm of weights:  8.623974674613395\n",
            "Iteration#:  2772 ; Loss:  7.311899976065707 ; l2 norm of gradient:  1.6085657273562979 ; l2 norm of weights:  8.6239237430391\n",
            "Iteration#:  2776 ; Loss:  7.311811625891728 ; l2 norm of gradient:  1.608557107490383 ; l2 norm of weights:  8.623872811860013\n",
            "Iteration#:  2780 ; Loss:  7.31172328665279 ; l2 norm of gradient:  1.6085484876216858 ; l2 norm of weights:  8.623821881076127\n",
            "Iteration#:  2784 ; Loss:  7.3116349443505175 ; l2 norm of gradient:  1.608539867750203 ; l2 norm of weights:  8.623770950687447\n",
            "Iteration#:  2788 ; Loss:  7.3115466030512355 ; l2 norm of gradient:  1.6085312478759335 ; l2 norm of weights:  8.623720020693971\n",
            "Iteration#:  2792 ; Loss:  7.311458262700246 ; l2 norm of gradient:  1.6085226279988747 ; l2 norm of weights:  8.623669091095698\n",
            "Iteration#:  2796 ; Loss:  7.311369923693816 ; l2 norm of gradient:  1.6085140081190246 ; l2 norm of weights:  8.623618161892624\n",
            "Iteration#:  2800 ; Loss:  7.311281585279726 ; l2 norm of gradient:  1.6085053882363796 ; l2 norm of weights:  8.623567233084753\n",
            "Iteration#:  2804 ; Loss:  7.311193242405143 ; l2 norm of gradient:  1.6084967683509397 ; l2 norm of weights:  8.623516304672082\n",
            "Iteration#:  2808 ; Loss:  7.311104903943834 ; l2 norm of gradient:  1.6084881484627014 ; l2 norm of weights:  8.623465376654607\n",
            "Iteration#:  2812 ; Loss:  7.311016565723644 ; l2 norm of gradient:  1.608479528571662 ; l2 norm of weights:  8.623414449032333\n",
            "Iteration#:  2816 ; Loss:  7.310928225017342 ; l2 norm of gradient:  1.6084709086778193 ; l2 norm of weights:  8.623363521805256\n",
            "Iteration#:  2820 ; Loss:  7.3108398880941365 ; l2 norm of gradient:  1.6084622887811721 ; l2 norm of weights:  8.623312594973378\n",
            "Iteration#:  2824 ; Loss:  7.310751554398874 ; l2 norm of gradient:  1.608453668881717 ; l2 norm of weights:  8.623261668536692\n",
            "Iteration#:  2828 ; Loss:  7.310663213601323 ; l2 norm of gradient:  1.6084450489794537 ; l2 norm of weights:  8.623210742495205\n",
            "Iteration#:  2832 ; Loss:  7.310574878686789 ; l2 norm of gradient:  1.6084364290743767 ; l2 norm of weights:  8.623159816848911\n",
            "Iteration#:  2836 ; Loss:  7.3104865430836385 ; l2 norm of gradient:  1.6084278091664863 ; l2 norm of weights:  8.623108891597811\n",
            "Iteration#:  2840 ; Loss:  7.3103982071133835 ; l2 norm of gradient:  1.60841918925578 ; l2 norm of weights:  8.623057966741904\n",
            "Iteration#:  2844 ; Loss:  7.31030987098635 ; l2 norm of gradient:  1.608410569342253 ; l2 norm of weights:  8.623007042281188\n",
            "Iteration#:  2848 ; Loss:  7.310221538632996 ; l2 norm of gradient:  1.6084019494259079 ; l2 norm of weights:  8.622956118215663\n",
            "Iteration#:  2852 ; Loss:  7.31013320428684 ; l2 norm of gradient:  1.6083933295067372 ; l2 norm of weights:  8.622905194545329\n",
            "Iteration#:  2856 ; Loss:  7.3100448712993975 ; l2 norm of gradient:  1.6083847095847414 ; l2 norm of weights:  8.622854271270185\n",
            "Iteration#:  2860 ; Loss:  7.309956539781801 ; l2 norm of gradient:  1.608376089659919 ; l2 norm of weights:  8.62280334839023\n",
            "Iteration#:  2864 ; Loss:  7.309868207934961 ; l2 norm of gradient:  1.6083674697322663 ; l2 norm of weights:  8.622752425905464\n",
            "Iteration#:  2868 ; Loss:  7.30977987676479 ; l2 norm of gradient:  1.6083588498017807 ; l2 norm of weights:  8.622701503815883\n",
            "Iteration#:  2872 ; Loss:  7.309691542100671 ; l2 norm of gradient:  1.6083502298684615 ; l2 norm of weights:  8.622650582121489\n",
            "Iteration#:  2876 ; Loss:  7.30960321305024 ; l2 norm of gradient:  1.608341609932305 ; l2 norm of weights:  8.622599660822281\n",
            "Iteration#:  2880 ; Loss:  7.30951488255981 ; l2 norm of gradient:  1.608332989993309 ; l2 norm of weights:  8.622548739918257\n",
            "Iteration#:  2884 ; Loss:  7.309426554464862 ; l2 norm of gradient:  1.6083243700514718 ; l2 norm of weights:  8.622497819409418\n",
            "Iteration#:  2888 ; Loss:  7.309338229589792 ; l2 norm of gradient:  1.6083157501067915 ; l2 norm of weights:  8.622446899295763\n",
            "Iteration#:  2892 ; Loss:  7.309249902425725 ; l2 norm of gradient:  1.6083071301592657 ; l2 norm of weights:  8.62239597957729\n",
            "Iteration#:  2896 ; Loss:  7.309161572730812 ; l2 norm of gradient:  1.608298510208891 ; l2 norm of weights:  8.622345060253998\n",
            "Iteration#:  2900 ; Loss:  7.30907324629477 ; l2 norm of gradient:  1.6082898902556655 ; l2 norm of weights:  8.622294141325888\n",
            "Iteration#:  2904 ; Loss:  7.308984920369753 ; l2 norm of gradient:  1.608281270299588 ; l2 norm of weights:  8.622243222792957\n",
            "Iteration#:  2908 ; Loss:  7.308896594124959 ; l2 norm of gradient:  1.608272650340655 ; l2 norm of weights:  8.622192304655206\n",
            "Iteration#:  2912 ; Loss:  7.308808269741798 ; l2 norm of gradient:  1.6082640303788658 ; l2 norm of weights:  8.622141386912633\n",
            "Iteration#:  2916 ; Loss:  7.308719942347408 ; l2 norm of gradient:  1.6082554104142157 ; l2 norm of weights:  8.622090469565238\n",
            "Iteration#:  2920 ; Loss:  7.308631617591466 ; l2 norm of gradient:  1.6082467904467053 ; l2 norm of weights:  8.62203955261302\n",
            "Iteration#:  2924 ; Loss:  7.308543291826561 ; l2 norm of gradient:  1.60823817047633 ; l2 norm of weights:  8.621988636055976\n",
            "Iteration#:  2928 ; Loss:  7.308454970545121 ; l2 norm of gradient:  1.6082295505030888 ; l2 norm of weights:  8.62193771989411\n",
            "Iteration#:  2932 ; Loss:  7.308366646957694 ; l2 norm of gradient:  1.6082209305269783 ; l2 norm of weights:  8.621886804127417\n",
            "Iteration#:  2936 ; Loss:  7.308278322872946 ; l2 norm of gradient:  1.608212310547997 ; l2 norm of weights:  8.621835888755896\n",
            "Iteration#:  2940 ; Loss:  7.30819000026416 ; l2 norm of gradient:  1.6082036905661428 ; l2 norm of weights:  8.621784973779553\n",
            "Iteration#:  2944 ; Loss:  7.308101679649675 ; l2 norm of gradient:  1.6081950705814132 ; l2 norm of weights:  8.621734059198378\n",
            "Iteration#:  2948 ; Loss:  7.308013359323971 ; l2 norm of gradient:  1.6081864505938064 ; l2 norm of weights:  8.621683145012375\n",
            "Iteration#:  2952 ; Loss:  7.307925033764941 ; l2 norm of gradient:  1.6081778306033183 ; l2 norm of weights:  8.621632231221543\n",
            "Iteration#:  2956 ; Loss:  7.307836715892739 ; l2 norm of gradient:  1.6081692106099494 ; l2 norm of weights:  8.621581317825882\n",
            "Iteration#:  2960 ; Loss:  7.30774839958749 ; l2 norm of gradient:  1.6081605906136958 ; l2 norm of weights:  8.621530404825387\n",
            "Iteration#:  2964 ; Loss:  7.3076600789858634 ; l2 norm of gradient:  1.6081519706145544 ; l2 norm of weights:  8.621479492220065\n",
            "Iteration#:  2968 ; Loss:  7.307571759153829 ; l2 norm of gradient:  1.6081433506125244 ; l2 norm of weights:  8.621428580009905\n",
            "Iteration#:  2972 ; Loss:  7.307483440753541 ; l2 norm of gradient:  1.6081347306076035 ; l2 norm of weights:  8.621377668194913\n",
            "Iteration#:  2976 ; Loss:  7.3073951255447085 ; l2 norm of gradient:  1.608126110599788 ; l2 norm of weights:  8.621326756775087\n",
            "Iteration#:  2980 ; Loss:  7.307306806274655 ; l2 norm of gradient:  1.6081174905890767 ; l2 norm of weights:  8.621275845750427\n",
            "Iteration#:  2984 ; Loss:  7.30721849054229 ; l2 norm of gradient:  1.608108870575467 ; l2 norm of weights:  8.62122493512093\n",
            "Iteration#:  2988 ; Loss:  7.307130179243305 ; l2 norm of gradient:  1.6081002505589577 ; l2 norm of weights:  8.621174024886596\n",
            "Iteration#:  2992 ; Loss:  7.307041859644121 ; l2 norm of gradient:  1.6080916305395443 ; l2 norm of weights:  8.621123115047425\n",
            "Iteration#:  2996 ; Loss:  7.306953546148529 ; l2 norm of gradient:  1.6080830105172266 ; l2 norm of weights:  8.621072205603417\n",
            "Iteration#:  3000 ; Loss:  7.306865234668071 ; l2 norm of gradient:  1.6080743904920018 ; l2 norm of weights:  8.62102129655457\n",
            "Iteration#:  3004 ; Loss:  7.306776916800066 ; l2 norm of gradient:  1.6080657704638663 ; l2 norm of weights:  8.620970387900883\n",
            "Iteration#:  3008 ; Loss:  7.306688605377098 ; l2 norm of gradient:  1.6080571504328196 ; l2 norm of weights:  8.620919479642353\n",
            "Iteration#:  3012 ; Loss:  7.306600291878185 ; l2 norm of gradient:  1.6080485303988583 ; l2 norm of weights:  8.620868571778983\n",
            "Iteration#:  3016 ; Loss:  7.306511978264343 ; l2 norm of gradient:  1.608039910361981 ; l2 norm of weights:  8.62081766431077\n",
            "Iteration#:  3020 ; Loss:  7.306423667620036 ; l2 norm of gradient:  1.6080312903221838 ; l2 norm of weights:  8.620766757237716\n",
            "Iteration#:  3024 ; Loss:  7.306335357338547 ; l2 norm of gradient:  1.6080226702794664 ; l2 norm of weights:  8.620715850559817\n",
            "Iteration#:  3028 ; Loss:  7.3062470468606335 ; l2 norm of gradient:  1.608014050233825 ; l2 norm of weights:  8.62066494427707\n",
            "Iteration#:  3032 ; Loss:  7.30615873507187 ; l2 norm of gradient:  1.608005430185258 ; l2 norm of weights:  8.620614038389482\n",
            "Iteration#:  3036 ; Loss:  7.306070427656744 ; l2 norm of gradient:  1.6079968101337636 ; l2 norm of weights:  8.620563132897045\n",
            "Iteration#:  3040 ; Loss:  7.305982119140884 ; l2 norm of gradient:  1.6079881900793376 ; l2 norm of weights:  8.620512227799765\n",
            "Iteration#:  3044 ; Loss:  7.305893814735368 ; l2 norm of gradient:  1.6079795700219808 ; l2 norm of weights:  8.620461323097635\n",
            "Iteration#:  3048 ; Loss:  7.305805504421894 ; l2 norm of gradient:  1.6079709499616872 ; l2 norm of weights:  8.620410418790653\n",
            "Iteration#:  3052 ; Loss:  7.305717196061321 ; l2 norm of gradient:  1.6079623298984578 ; l2 norm of weights:  8.620359514878825\n",
            "Iteration#:  3056 ; Loss:  7.305628888325051 ; l2 norm of gradient:  1.6079537098322876 ; l2 norm of weights:  8.620308611362146\n",
            "Iteration#:  3060 ; Loss:  7.305540585378656 ; l2 norm of gradient:  1.6079450897631764 ; l2 norm of weights:  8.620257708240615\n",
            "Iteration#:  3064 ; Loss:  7.30545227779276 ; l2 norm of gradient:  1.6079364696911211 ; l2 norm of weights:  8.620206805514234\n",
            "Iteration#:  3068 ; Loss:  7.305363966905746 ; l2 norm of gradient:  1.607927849616119 ; l2 norm of weights:  8.620155903182999\n",
            "Iteration#:  3072 ; Loss:  7.3052756600775215 ; l2 norm of gradient:  1.6079192295381686 ; l2 norm of weights:  8.62010500124691\n",
            "Iteration#:  3076 ; Loss:  7.305187358283035 ; l2 norm of gradient:  1.6079106094572677 ; l2 norm of weights:  8.62005409970597\n",
            "Iteration#:  3080 ; Loss:  7.305099053936529 ; l2 norm of gradient:  1.6079019893734119 ; l2 norm of weights:  8.620003198560173\n",
            "Iteration#:  3084 ; Loss:  7.305010753684586 ; l2 norm of gradient:  1.6078933692866022 ; l2 norm of weights:  8.619952297809519\n",
            "Iteration#:  3088 ; Loss:  7.30492244907448 ; l2 norm of gradient:  1.607884749196834 ; l2 norm of weights:  8.61990139745401\n",
            "Iteration#:  3092 ; Loss:  7.304834146939168 ; l2 norm of gradient:  1.6078761291041057 ; l2 norm of weights:  8.619850497493644\n",
            "Iteration#:  3096 ; Loss:  7.304745842705211 ; l2 norm of gradient:  1.607867509008414 ; l2 norm of weights:  8.619799597928418\n",
            "Iteration#:  3100 ; Loss:  7.304657543652213 ; l2 norm of gradient:  1.6078588889097578 ; l2 norm of weights:  8.619748698758332\n",
            "Iteration#:  3104 ; Loss:  7.30456924081609 ; l2 norm of gradient:  1.607850268808135 ; l2 norm of weights:  8.619697799983388\n",
            "Iteration#:  3108 ; Loss:  7.3044809402025885 ; l2 norm of gradient:  1.607841648703543 ; l2 norm of weights:  8.619646901603586\n",
            "Iteration#:  3112 ; Loss:  7.3043926425169365 ; l2 norm of gradient:  1.6078330285959785 ; l2 norm of weights:  8.619596003618918\n",
            "Iteration#:  3116 ; Loss:  7.304304342211438 ; l2 norm of gradient:  1.6078244084854403 ; l2 norm of weights:  8.61954510602939\n",
            "Iteration#:  3120 ; Loss:  7.304216047660038 ; l2 norm of gradient:  1.607815788371925 ; l2 norm of weights:  8.619494208834999\n",
            "Iteration#:  3124 ; Loss:  7.304127746352691 ; l2 norm of gradient:  1.6078071682554322 ; l2 norm of weights:  8.619443312035743\n",
            "Iteration#:  3128 ; Loss:  7.304039450231094 ; l2 norm of gradient:  1.6077985481359578 ; l2 norm of weights:  8.619392415631623\n",
            "Iteration#:  3132 ; Loss:  7.303951153860137 ; l2 norm of gradient:  1.6077899280134997 ; l2 norm of weights:  8.619341519622639\n",
            "Iteration#:  3136 ; Loss:  7.303862852790706 ; l2 norm of gradient:  1.6077813078880565 ; l2 norm of weights:  8.619290624008787\n",
            "Iteration#:  3140 ; Loss:  7.303774554404942 ; l2 norm of gradient:  1.6077726877596252 ; l2 norm of weights:  8.619239728790069\n",
            "Iteration#:  3144 ; Loss:  7.303686258503217 ; l2 norm of gradient:  1.6077640676282041 ; l2 norm of weights:  8.619188833966483\n",
            "Iteration#:  3148 ; Loss:  7.303597965626835 ; l2 norm of gradient:  1.6077554474937905 ; l2 norm of weights:  8.619137939538028\n",
            "Iteration#:  3152 ; Loss:  7.3035096745912504 ; l2 norm of gradient:  1.607746827356381 ; l2 norm of weights:  8.619087045504704\n",
            "Iteration#:  3156 ; Loss:  7.303421380020627 ; l2 norm of gradient:  1.607738207215975 ; l2 norm of weights:  8.619036151866508\n",
            "Iteration#:  3160 ; Loss:  7.3033330867073545 ; l2 norm of gradient:  1.6077295870725694 ; l2 norm of weights:  8.618985258623443\n",
            "Iteration#:  3164 ; Loss:  7.303244791908515 ; l2 norm of gradient:  1.6077209669261623 ; l2 norm of weights:  8.618934365775505\n",
            "Iteration#:  3168 ; Loss:  7.303156503463196 ; l2 norm of gradient:  1.6077123467767511 ; l2 norm of weights:  8.618883473322695\n",
            "Iteration#:  3172 ; Loss:  7.303068208463998 ; l2 norm of gradient:  1.607703726624333 ; l2 norm of weights:  8.618832581265012\n",
            "Iteration#:  3176 ; Loss:  7.30297991635868 ; l2 norm of gradient:  1.6076951064689071 ; l2 norm of weights:  8.618781689602455\n",
            "Iteration#:  3180 ; Loss:  7.302891627312105 ; l2 norm of gradient:  1.6076864863104692 ; l2 norm of weights:  8.618730798335022\n",
            "Iteration#:  3184 ; Loss:  7.302803338856173 ; l2 norm of gradient:  1.6076778661490179 ; l2 norm of weights:  8.618679907462713\n",
            "Iteration#:  3188 ; Loss:  7.302715050004691 ; l2 norm of gradient:  1.6076692459845505 ; l2 norm of weights:  8.61862901698553\n",
            "Iteration#:  3192 ; Loss:  7.302626753960849 ; l2 norm of gradient:  1.6076606258170665 ; l2 norm of weights:  8.618578126903468\n",
            "Iteration#:  3196 ; Loss:  7.302538466853809 ; l2 norm of gradient:  1.6076520056465606 ; l2 norm of weights:  8.618527237216528\n",
            "Iteration#:  3200 ; Loss:  7.302450176891312 ; l2 norm of gradient:  1.6076433854730328 ; l2 norm of weights:  8.618476347924709\n",
            "Iteration#:  3204 ; Loss:  7.302361886879078 ; l2 norm of gradient:  1.6076347652964798 ; l2 norm of weights:  8.618425459028009\n",
            "Iteration#:  3208 ; Loss:  7.302273599785035 ; l2 norm of gradient:  1.6076261451168996 ; l2 norm of weights:  8.618374570526429\n",
            "Iteration#:  3212 ; Loss:  7.302185315856215 ; l2 norm of gradient:  1.6076175249342899 ; l2 norm of weights:  8.618323682419968\n",
            "Iteration#:  3216 ; Loss:  7.3020970289422795 ; l2 norm of gradient:  1.6076089047486475 ; l2 norm of weights:  8.618272794708624\n",
            "Iteration#:  3220 ; Loss:  7.302008747106115 ; l2 norm of gradient:  1.6076002845599706 ; l2 norm of weights:  8.618221907392398\n",
            "Iteration#:  3224 ; Loss:  7.301920460593417 ; l2 norm of gradient:  1.6075916643682586 ; l2 norm of weights:  8.618171020471287\n",
            "Iteration#:  3228 ; Loss:  7.30183216894704 ; l2 norm of gradient:  1.607583044173507 ; l2 norm of weights:  8.618120133945293\n",
            "Iteration#:  3232 ; Loss:  7.3017438872166185 ; l2 norm of gradient:  1.607574423975713 ; l2 norm of weights:  8.618069247814413\n",
            "Iteration#:  3236 ; Loss:  7.301655603093287 ; l2 norm of gradient:  1.6075658037748763 ; l2 norm of weights:  8.618018362078647\n",
            "Iteration#:  3240 ; Loss:  7.3015673194879405 ; l2 norm of gradient:  1.6075571835709936 ; l2 norm of weights:  8.617967476737993\n",
            "Iteration#:  3244 ; Loss:  7.301479038586784 ; l2 norm of gradient:  1.6075485633640623 ; l2 norm of weights:  8.617916591792453\n",
            "Iteration#:  3248 ; Loss:  7.301390756605457 ; l2 norm of gradient:  1.6075399431540807 ; l2 norm of weights:  8.617865707242023\n",
            "Iteration#:  3252 ; Loss:  7.301302479197076 ; l2 norm of gradient:  1.6075313229410457 ; l2 norm of weights:  8.617814823086702\n",
            "Iteration#:  3256 ; Loss:  7.301214198545366 ; l2 norm of gradient:  1.6075227027249552 ; l2 norm of weights:  8.617763939326494\n",
            "Iteration#:  3260 ; Loss:  7.301125912839313 ; l2 norm of gradient:  1.6075140825058092 ; l2 norm of weights:  8.617713055961394\n",
            "Iteration#:  3264 ; Loss:  7.301037630916344 ; l2 norm of gradient:  1.6075054622836016 ; l2 norm of weights:  8.617662172991402\n",
            "Iteration#:  3268 ; Loss:  7.300949353946075 ; l2 norm of gradient:  1.607496842058331 ; l2 norm of weights:  8.617611290416518\n",
            "Iteration#:  3272 ; Loss:  7.300861074776957 ; l2 norm of gradient:  1.6074882218299962 ; l2 norm of weights:  8.61756040823674\n",
            "Iteration#:  3276 ; Loss:  7.300772793395927 ; l2 norm of gradient:  1.6074796015985953 ; l2 norm of weights:  8.617509526452068\n",
            "Iteration#:  3280 ; Loss:  7.30068451958085 ; l2 norm of gradient:  1.6074709813641246 ; l2 norm of weights:  8.6174586450625\n",
            "Iteration#:  3284 ; Loss:  7.300596240965902 ; l2 norm of gradient:  1.607462361126582 ; l2 norm of weights:  8.617407764068039\n",
            "Iteration#:  3288 ; Loss:  7.300507967191461 ; l2 norm of gradient:  1.6074537408859655 ; l2 norm of weights:  8.617356883468679\n",
            "Iteration#:  3292 ; Loss:  7.300419686587841 ; l2 norm of gradient:  1.6074451206422735 ; l2 norm of weights:  8.617306003264423\n",
            "Iteration#:  3296 ; Loss:  7.300331410315184 ; l2 norm of gradient:  1.6074365003955013 ; l2 norm of weights:  8.617255123455267\n",
            "Iteration#:  3300 ; Loss:  7.300243138213176 ; l2 norm of gradient:  1.6074278801456492 ; l2 norm of weights:  8.617204244041213\n",
            "Iteration#:  3304 ; Loss:  7.300154858396027 ; l2 norm of gradient:  1.6074192598927135 ; l2 norm of weights:  8.61715336502226\n",
            "Iteration#:  3308 ; Loss:  7.3000665866108445 ; l2 norm of gradient:  1.6074106396366925 ; l2 norm of weights:  8.617102486398405\n",
            "Iteration#:  3312 ; Loss:  7.299978313575569 ; l2 norm of gradient:  1.6074020193775826 ; l2 norm of weights:  8.61705160816965\n",
            "Iteration#:  3316 ; Loss:  7.299890039168276 ; l2 norm of gradient:  1.6073933991153826 ; l2 norm of weights:  8.617000730335993\n",
            "Iteration#:  3320 ; Loss:  7.2998017626064975 ; l2 norm of gradient:  1.6073847788500906 ; l2 norm of weights:  8.616949852897433\n",
            "Iteration#:  3324 ; Loss:  7.2997134955774765 ; l2 norm of gradient:  1.6073761585817032 ; l2 norm of weights:  8.616898975853966\n",
            "Iteration#:  3328 ; Loss:  7.2996252207580925 ; l2 norm of gradient:  1.6073675383102184 ; l2 norm of weights:  8.6168480992056\n",
            "Iteration#:  3332 ; Loss:  7.299536946431807 ; l2 norm of gradient:  1.6073589180356338 ; l2 norm of weights:  8.616797222952325\n",
            "Iteration#:  3336 ; Loss:  7.299448678607403 ; l2 norm of gradient:  1.6073502977579468 ; l2 norm of weights:  8.616746347094146\n",
            "Iteration#:  3340 ; Loss:  7.299360410809504 ; l2 norm of gradient:  1.6073416774771554 ; l2 norm of weights:  8.616695471631056\n",
            "Iteration#:  3344 ; Loss:  7.299272140078626 ; l2 norm of gradient:  1.6073330571932583 ; l2 norm of weights:  8.616644596563063\n",
            "Iteration#:  3348 ; Loss:  7.299183869108224 ; l2 norm of gradient:  1.6073244369062505 ; l2 norm of weights:  8.616593721890158\n",
            "Iteration#:  3352 ; Loss:  7.299095600674476 ; l2 norm of gradient:  1.6073158166161317 ; l2 norm of weights:  8.616542847612344\n",
            "Iteration#:  3356 ; Loss:  7.299007334231986 ; l2 norm of gradient:  1.6073071963229 ; l2 norm of weights:  8.616491973729621\n",
            "Iteration#:  3360 ; Loss:  7.29891906397744 ; l2 norm of gradient:  1.6072985760265508 ; l2 norm of weights:  8.616441100241987\n",
            "Iteration#:  3364 ; Loss:  7.298830800852588 ; l2 norm of gradient:  1.6072899557270834 ; l2 norm of weights:  8.61639022714944\n",
            "Iteration#:  3368 ; Loss:  7.2987425323516995 ; l2 norm of gradient:  1.6072813354244955 ; l2 norm of weights:  8.616339354451982\n",
            "Iteration#:  3372 ; Loss:  7.298654267263433 ; l2 norm of gradient:  1.607272715118784 ; l2 norm of weights:  8.61628848214961\n",
            "Iteration#:  3376 ; Loss:  7.298566001337527 ; l2 norm of gradient:  1.6072640948099468 ; l2 norm of weights:  8.616237610242324\n",
            "Iteration#:  3380 ; Loss:  7.298477737105433 ; l2 norm of gradient:  1.6072554744979823 ; l2 norm of weights:  8.616186738730123\n",
            "Iteration#:  3384 ; Loss:  7.298389471428607 ; l2 norm of gradient:  1.6072468541828866 ; l2 norm of weights:  8.616135867613007\n",
            "Iteration#:  3388 ; Loss:  7.298301204728226 ; l2 norm of gradient:  1.6072382338646587 ; l2 norm of weights:  8.616084996890972\n",
            "Iteration#:  3392 ; Loss:  7.298212943542941 ; l2 norm of gradient:  1.6072296135432957 ; l2 norm of weights:  8.616034126564022\n",
            "Iteration#:  3396 ; Loss:  7.2981246784524405 ; l2 norm of gradient:  1.607220993218795 ; l2 norm of weights:  8.615983256632154\n",
            "Iteration#:  3400 ; Loss:  7.298036417788545 ; l2 norm of gradient:  1.6072123728911558 ; l2 norm of weights:  8.615932387095365\n",
            "Iteration#:  3404 ; Loss:  7.297948153131345 ; l2 norm of gradient:  1.6072037525603726 ; l2 norm of weights:  8.615881517953657\n",
            "Iteration#:  3408 ; Loss:  7.29785989262664 ; l2 norm of gradient:  1.6071951322264466 ; l2 norm of weights:  8.615830649207028\n",
            "Iteration#:  3412 ; Loss:  7.297771632538552 ; l2 norm of gradient:  1.607186511889373 ; l2 norm of weights:  8.61577978085548\n",
            "Iteration#:  3416 ; Loss:  7.297683372476545 ; l2 norm of gradient:  1.60717789154915 ; l2 norm of weights:  8.615728912899007\n",
            "Iteration#:  3420 ; Loss:  7.297595110865574 ; l2 norm of gradient:  1.6071692712057755 ; l2 norm of weights:  8.615678045337614\n",
            "Iteration#:  3424 ; Loss:  7.297506850546556 ; l2 norm of gradient:  1.6071606508592482 ; l2 norm of weights:  8.615627178171296\n",
            "Iteration#:  3428 ; Loss:  7.2974185923695245 ; l2 norm of gradient:  1.6071520305095632 ; l2 norm of weights:  8.615576311400051\n",
            "Iteration#:  3432 ; Loss:  7.297330334817888 ; l2 norm of gradient:  1.607143410156719 ; l2 norm of weights:  8.615525445023884\n",
            "Iteration#:  3436 ; Loss:  7.297242076674271 ; l2 norm of gradient:  1.6071347898007164 ; l2 norm of weights:  8.615474579042788\n",
            "Iteration#:  3440 ; Loss:  7.297153818724572 ; l2 norm of gradient:  1.6071261694415482 ; l2 norm of weights:  8.61542371345677\n",
            "Iteration#:  3444 ; Loss:  7.297065563287381 ; l2 norm of gradient:  1.6071175490792147 ; l2 norm of weights:  8.615372848265821\n",
            "Iteration#:  3448 ; Loss:  7.296977305020176 ; l2 norm of gradient:  1.6071089287137132 ; l2 norm of weights:  8.615321983469943\n",
            "Iteration#:  3452 ; Loss:  7.296889050085964 ; l2 norm of gradient:  1.607100308345042 ; l2 norm of weights:  8.615271119069135\n",
            "Iteration#:  3456 ; Loss:  7.296800793954924 ; l2 norm of gradient:  1.6070916879731971 ; l2 norm of weights:  8.615220255063399\n",
            "Iteration#:  3460 ; Loss:  7.296712539121213 ; l2 norm of gradient:  1.6070830675981773 ; l2 norm of weights:  8.61516939145273\n",
            "Iteration#:  3464 ; Loss:  7.296624284067823 ; l2 norm of gradient:  1.6070744472199794 ; l2 norm of weights:  8.615118528237131\n",
            "Iteration#:  3468 ; Loss:  7.296536031351123 ; l2 norm of gradient:  1.6070658268386022 ; l2 norm of weights:  8.615067665416598\n",
            "Iteration#:  3472 ; Loss:  7.296447777903666 ; l2 norm of gradient:  1.6070572064540425 ; l2 norm of weights:  8.615016802991132\n",
            "Iteration#:  3476 ; Loss:  7.296359524483437 ; l2 norm of gradient:  1.6070485860662973 ; l2 norm of weights:  8.614965940960731\n",
            "Iteration#:  3480 ; Loss:  7.296271273935684 ; l2 norm of gradient:  1.6070399656753667 ; l2 norm of weights:  8.614915079325398\n",
            "Iteration#:  3484 ; Loss:  7.296183022327925 ; l2 norm of gradient:  1.6070313452812455 ; l2 norm of weights:  8.614864218085126\n",
            "Iteration#:  3488 ; Loss:  7.296094768027517 ; l2 norm of gradient:  1.6070227248839326 ; l2 norm of weights:  8.61481335723992\n",
            "Iteration#:  3492 ; Loss:  7.296006522006257 ; l2 norm of gradient:  1.6070141044834256 ; l2 norm of weights:  8.614762496789776\n",
            "Iteration#:  3496 ; Loss:  7.295918272742813 ; l2 norm of gradient:  1.607005484079722 ; l2 norm of weights:  8.614711636734693\n",
            "Iteration#:  3500 ; Loss:  7.295830021916553 ; l2 norm of gradient:  1.6069968636728191 ; l2 norm of weights:  8.614660777074672\n",
            "Iteration#:  3504 ; Loss:  7.295741772469431 ; l2 norm of gradient:  1.6069882432627154 ; l2 norm of weights:  8.614609917809709\n",
            "Iteration#:  3508 ; Loss:  7.295653518601029 ; l2 norm of gradient:  1.6069796228494078 ; l2 norm of weights:  8.614559058939808\n",
            "Iteration#:  3512 ; Loss:  7.2955652696770725 ; l2 norm of gradient:  1.6069710024328943 ; l2 norm of weights:  8.614508200464964\n",
            "Iteration#:  3516 ; Loss:  7.295477028137663 ; l2 norm of gradient:  1.6069623820131715 ; l2 norm of weights:  8.61445734238518\n",
            "Iteration#:  3520 ; Loss:  7.295388782007537 ; l2 norm of gradient:  1.6069537615902392 ; l2 norm of weights:  8.614406484700451\n",
            "Iteration#:  3524 ; Loss:  7.2953005369573924 ; l2 norm of gradient:  1.6069451411640927 ; l2 norm of weights:  8.614355627410779\n",
            "Iteration#:  3528 ; Loss:  7.295212287994847 ; l2 norm of gradient:  1.6069365207347313 ; l2 norm of weights:  8.614304770516162\n",
            "Iteration#:  3532 ; Loss:  7.295124039629645 ; l2 norm of gradient:  1.606927900302151 ; l2 norm of weights:  8.6142539140166\n",
            "Iteration#:  3536 ; Loss:  7.295035797031602 ; l2 norm of gradient:  1.6069192798663514 ; l2 norm of weights:  8.614203057912095\n",
            "Iteration#:  3540 ; Loss:  7.294947551728352 ; l2 norm of gradient:  1.6069106594273277 ; l2 norm of weights:  8.614152202202638\n",
            "Iteration#:  3544 ; Loss:  7.294859309391748 ; l2 norm of gradient:  1.6069020389850803 ; l2 norm of weights:  8.614101346888235\n",
            "Iteration#:  3548 ; Loss:  7.294771067085936 ; l2 norm of gradient:  1.6068934185396047 ; l2 norm of weights:  8.614050491968882\n",
            "Iteration#:  3552 ; Loss:  7.294682826207738 ; l2 norm of gradient:  1.6068847980908985 ; l2 norm of weights:  8.613999637444582\n",
            "Iteration#:  3556 ; Loss:  7.294594581541234 ; l2 norm of gradient:  1.6068761776389606 ; l2 norm of weights:  8.613948783315331\n",
            "Iteration#:  3560 ; Loss:  7.294506338530869 ; l2 norm of gradient:  1.6068675571837887 ; l2 norm of weights:  8.613897929581132\n",
            "Iteration#:  3564 ; Loss:  7.294418097054894 ; l2 norm of gradient:  1.6068589367253787 ; l2 norm of weights:  8.613847076241976\n",
            "Iteration#:  3568 ; Loss:  7.2943298557505605 ; l2 norm of gradient:  1.60685031626373 ; l2 norm of weights:  8.61379622329787\n",
            "Iteration#:  3572 ; Loss:  7.2942416138315025 ; l2 norm of gradient:  1.6068416957988387 ; l2 norm of weights:  8.613745370748811\n",
            "Iteration#:  3576 ; Loss:  7.294153372094378 ; l2 norm of gradient:  1.6068330753307036 ; l2 norm of weights:  8.613694518594798\n",
            "Iteration#:  3580 ; Loss:  7.294065140915418 ; l2 norm of gradient:  1.6068244548593225 ; l2 norm of weights:  8.613643666835829\n",
            "Iteration#:  3584 ; Loss:  7.293976899231091 ; l2 norm of gradient:  1.6068158343846917 ; l2 norm of weights:  8.613592815471904\n",
            "Iteration#:  3588 ; Loss:  7.293888657565244 ; l2 norm of gradient:  1.6068072139068088 ; l2 norm of weights:  8.613541964503023\n",
            "Iteration#:  3592 ; Loss:  7.29380041847249 ; l2 norm of gradient:  1.606798593425673 ; l2 norm of weights:  8.613491113929186\n",
            "Iteration#:  3596 ; Loss:  7.293712186143309 ; l2 norm of gradient:  1.6067899729412807 ; l2 norm of weights:  8.61344026375039\n",
            "Iteration#:  3600 ; Loss:  7.293623949410336 ; l2 norm of gradient:  1.6067813524536299 ; l2 norm of weights:  8.613389413966635\n",
            "Iteration#:  3604 ; Loss:  7.293535707682954 ; l2 norm of gradient:  1.606772731962718 ; l2 norm of weights:  8.61333856457792\n",
            "Iteration#:  3608 ; Loss:  7.293447476534512 ; l2 norm of gradient:  1.6067641114685425 ; l2 norm of weights:  8.613287715584242\n",
            "Iteration#:  3612 ; Loss:  7.2933592411917765 ; l2 norm of gradient:  1.606755490971101 ; l2 norm of weights:  8.613236866985606\n",
            "Iteration#:  3616 ; Loss:  7.2932710006679535 ; l2 norm of gradient:  1.6067468704703927 ; l2 norm of weights:  8.613186018782006\n",
            "Iteration#:  3620 ; Loss:  7.293182770151922 ; l2 norm of gradient:  1.6067382499664122 ; l2 norm of weights:  8.613135170973443\n",
            "Iteration#:  3624 ; Loss:  7.29309453816396 ; l2 norm of gradient:  1.6067296294591595 ; l2 norm of weights:  8.613084323559917\n",
            "Iteration#:  3628 ; Loss:  7.293006306764816 ; l2 norm of gradient:  1.6067210089486317 ; l2 norm of weights:  8.613033476541428\n",
            "Iteration#:  3632 ; Loss:  7.292918070322597 ; l2 norm of gradient:  1.6067123884348253 ; l2 norm of weights:  8.612982629917973\n",
            "Iteration#:  3636 ; Loss:  7.292829842930802 ; l2 norm of gradient:  1.6067037679177392 ; l2 norm of weights:  8.61293178368955\n",
            "Iteration#:  3640 ; Loss:  7.292741609716035 ; l2 norm of gradient:  1.6066951473973705 ; l2 norm of weights:  8.612880937856161\n",
            "Iteration#:  3644 ; Loss:  7.292653378675367 ; l2 norm of gradient:  1.6066865268737172 ; l2 norm of weights:  8.612830092417804\n",
            "Iteration#:  3648 ; Loss:  7.292565149860226 ; l2 norm of gradient:  1.6066779063467758 ; l2 norm of weights:  8.612779247374482\n",
            "Iteration#:  3652 ; Loss:  7.292476919732009 ; l2 norm of gradient:  1.6066692858165441 ; l2 norm of weights:  8.612728402726185\n",
            "Iteration#:  3656 ; Loss:  7.292388692643252 ; l2 norm of gradient:  1.6066606652830218 ; l2 norm of weights:  8.61267755847292\n",
            "Iteration#:  3660 ; Loss:  7.292300462332877 ; l2 norm of gradient:  1.6066520447462027 ; l2 norm of weights:  8.612626714614684\n",
            "Iteration#:  3664 ; Loss:  7.2922122326428305 ; l2 norm of gradient:  1.6066434242060887 ; l2 norm of weights:  8.612575871151476\n",
            "Iteration#:  3668 ; Loss:  7.292124006458675 ; l2 norm of gradient:  1.606634803662675 ; l2 norm of weights:  8.612525028083297\n",
            "Iteration#:  3672 ; Loss:  7.292035776799373 ; l2 norm of gradient:  1.6066261831159594 ; l2 norm of weights:  8.612474185410141\n",
            "Iteration#:  3676 ; Loss:  7.291947550894015 ; l2 norm of gradient:  1.6066175625659382 ; l2 norm of weights:  8.612423343132015\n",
            "Iteration#:  3680 ; Loss:  7.291859321206045 ; l2 norm of gradient:  1.6066089420126122 ; l2 norm of weights:  8.612372501248911\n",
            "Iteration#:  3684 ; Loss:  7.2917710964563875 ; l2 norm of gradient:  1.6066003214559754 ; l2 norm of weights:  8.612321659760834\n",
            "Iteration#:  3688 ; Loss:  7.291682870123939 ; l2 norm of gradient:  1.6065917008960282 ; l2 norm of weights:  8.612270818667778\n",
            "Iteration#:  3692 ; Loss:  7.291594644483501 ; l2 norm of gradient:  1.6065830803327668 ; l2 norm of weights:  8.612219977969746\n",
            "Iteration#:  3696 ; Loss:  7.291506418107284 ; l2 norm of gradient:  1.6065744597661893 ; l2 norm of weights:  8.612169137666735\n",
            "Iteration#:  3700 ; Loss:  7.2914181968601435 ; l2 norm of gradient:  1.606565839196293 ; l2 norm of weights:  8.612118297758744\n",
            "Iteration#:  3704 ; Loss:  7.291329970429029 ; l2 norm of gradient:  1.606557218623075 ; l2 norm of weights:  8.612067458245775\n",
            "Iteration#:  3708 ; Loss:  7.2912417501588935 ; l2 norm of gradient:  1.6065485980465344 ; l2 norm of weights:  8.612016619127827\n",
            "Iteration#:  3712 ; Loss:  7.291153527049671 ; l2 norm of gradient:  1.6065399774666673 ; l2 norm of weights:  8.611965780404896\n",
            "Iteration#:  3716 ; Loss:  7.291065309065425 ; l2 norm of gradient:  1.6065313568834716 ; l2 norm of weights:  8.61191494207698\n",
            "Iteration#:  3720 ; Loss:  7.290977086854857 ; l2 norm of gradient:  1.6065227362969456 ; l2 norm of weights:  8.611864104144084\n",
            "Iteration#:  3724 ; Loss:  7.290888865112285 ; l2 norm of gradient:  1.6065141157070866 ; l2 norm of weights:  8.611813266606205\n",
            "Iteration#:  3728 ; Loss:  7.290800648870176 ; l2 norm of gradient:  1.6065054951138915 ; l2 norm of weights:  8.61176242946334\n",
            "Iteration#:  3732 ; Loss:  7.29071242585884 ; l2 norm of gradient:  1.6064968745173585 ; l2 norm of weights:  8.611711592715489\n",
            "Iteration#:  3736 ; Loss:  7.290624210096895 ; l2 norm of gradient:  1.606488253917485 ; l2 norm of weights:  8.611660756362655\n",
            "Iteration#:  3740 ; Loss:  7.2905359929240685 ; l2 norm of gradient:  1.6064796333142684 ; l2 norm of weights:  8.61160992040483\n",
            "Iteration#:  3744 ; Loss:  7.290447771582243 ; l2 norm of gradient:  1.6064710127077066 ; l2 norm of weights:  8.611559084842018\n",
            "Iteration#:  3748 ; Loss:  7.290359555185823 ; l2 norm of gradient:  1.6064623920977974 ; l2 norm of weights:  8.611508249674218\n",
            "Iteration#:  3752 ; Loss:  7.290271337869081 ; l2 norm of gradient:  1.606453771484538 ; l2 norm of weights:  8.611457414901428\n",
            "Iteration#:  3756 ; Loss:  7.290183120349341 ; l2 norm of gradient:  1.6064451508679256 ; l2 norm of weights:  8.611406580523651\n",
            "Iteration#:  3760 ; Loss:  7.290094905837461 ; l2 norm of gradient:  1.606436530247959 ; l2 norm of weights:  8.611355746540879\n",
            "Iteration#:  3764 ; Loss:  7.290006687416276 ; l2 norm of gradient:  1.6064279096246339 ; l2 norm of weights:  8.611304912953116\n",
            "Iteration#:  3768 ; Loss:  7.289918475143239 ; l2 norm of gradient:  1.6064192889979494 ; l2 norm of weights:  8.611254079760363\n",
            "Iteration#:  3772 ; Loss:  7.289830260434671 ; l2 norm of gradient:  1.606410668367903 ; l2 norm of weights:  8.611203246962614\n",
            "Iteration#:  3776 ; Loss:  7.2897420471853565 ; l2 norm of gradient:  1.6064020477344918 ; l2 norm of weights:  8.611152414559871\n",
            "Iteration#:  3780 ; Loss:  7.289653832430994 ; l2 norm of gradient:  1.6063934270977136 ; l2 norm of weights:  8.611101582552132\n",
            "Iteration#:  3784 ; Loss:  7.289565620994458 ; l2 norm of gradient:  1.606384806457565 ; l2 norm of weights:  8.611050750939402\n",
            "Iteration#:  3788 ; Loss:  7.289477406064199 ; l2 norm of gradient:  1.6063761858140446 ; l2 norm of weights:  8.610999919721671\n",
            "Iteration#:  3792 ; Loss:  7.289389195295027 ; l2 norm of gradient:  1.6063675651671505 ; l2 norm of weights:  8.610949088898943\n",
            "Iteration#:  3796 ; Loss:  7.289300987237947 ; l2 norm of gradient:  1.6063589445168787 ; l2 norm of weights:  8.610898258471218\n",
            "Iteration#:  3800 ; Loss:  7.289212771770274 ; l2 norm of gradient:  1.6063503238632288 ; l2 norm of weights:  8.610847428438495\n",
            "Iteration#:  3804 ; Loss:  7.289124562143206 ; l2 norm of gradient:  1.606341703206197 ; l2 norm of weights:  8.610796598800771\n",
            "Iteration#:  3808 ; Loss:  7.289036357998889 ; l2 norm of gradient:  1.6063330825457796 ; l2 norm of weights:  8.610745769558045\n",
            "Iteration#:  3812 ; Loss:  7.2889481437663 ; l2 norm of gradient:  1.6063244618819779 ; l2 norm of weights:  8.610694940710317\n",
            "Iteration#:  3816 ; Loss:  7.2888599371768095 ; l2 norm of gradient:  1.6063158412147862 ; l2 norm of weights:  8.61064411225759\n",
            "Iteration#:  3820 ; Loss:  7.288771726829181 ; l2 norm of gradient:  1.6063072205442022 ; l2 norm of weights:  8.610593284199858\n",
            "Iteration#:  3824 ; Loss:  7.28868352123243 ; l2 norm of gradient:  1.6062985998702242 ; l2 norm of weights:  8.610542456537123\n",
            "Iteration#:  3828 ; Loss:  7.288595308142787 ; l2 norm of gradient:  1.6062899791928509 ; l2 norm of weights:  8.610491629269383\n",
            "Iteration#:  3832 ; Loss:  7.288507106377663 ; l2 norm of gradient:  1.606281358512078 ; l2 norm of weights:  8.610440802396639\n",
            "Iteration#:  3836 ; Loss:  7.288418898902256 ; l2 norm of gradient:  1.6062727378279056 ; l2 norm of weights:  8.610389975918887\n",
            "Iteration#:  3840 ; Loss:  7.288330692451753 ; l2 norm of gradient:  1.606264117140328 ; l2 norm of weights:  8.610339149836129\n",
            "Iteration#:  3844 ; Loss:  7.288242487508719 ; l2 norm of gradient:  1.606255496449344 ; l2 norm of weights:  8.610288324148362\n",
            "Iteration#:  3848 ; Loss:  7.288154284330561 ; l2 norm of gradient:  1.6062468757549526 ; l2 norm of weights:  8.610237498855584\n",
            "Iteration#:  3852 ; Loss:  7.288066077635592 ; l2 norm of gradient:  1.6062382550571497 ; l2 norm of weights:  8.6101866739578\n",
            "Iteration#:  3856 ; Loss:  7.28797787659432 ; l2 norm of gradient:  1.6062296343559337 ; l2 norm of weights:  8.610135849455006\n",
            "Iteration#:  3860 ; Loss:  7.2878896773898365 ; l2 norm of gradient:  1.6062210136513015 ; l2 norm of weights:  8.6100850253472\n",
            "Iteration#:  3864 ; Loss:  7.287801475952462 ; l2 norm of gradient:  1.6062123929432515 ; l2 norm of weights:  8.610034201634383\n",
            "Iteration#:  3868 ; Loss:  7.287713270671384 ; l2 norm of gradient:  1.6062037722317808 ; l2 norm of weights:  8.609983378316553\n",
            "Iteration#:  3872 ; Loss:  7.287625070899296 ; l2 norm of gradient:  1.6061951515168864 ; l2 norm of weights:  8.609932555393708\n",
            "Iteration#:  3876 ; Loss:  7.287536865661318 ; l2 norm of gradient:  1.6061865307985665 ; l2 norm of weights:  8.609881732865851\n",
            "Iteration#:  3880 ; Loss:  7.287448666776152 ; l2 norm of gradient:  1.6061779100768185 ; l2 norm of weights:  8.609830910732978\n",
            "Iteration#:  3884 ; Loss:  7.287360468863044 ; l2 norm of gradient:  1.6061692893516406 ; l2 norm of weights:  8.609780088995088\n",
            "Iteration#:  3888 ; Loss:  7.2872722697459285 ; l2 norm of gradient:  1.6061606686230292 ; l2 norm of weights:  8.609729267652183\n",
            "Iteration#:  3892 ; Loss:  7.2871840680895925 ; l2 norm of gradient:  1.6061520478909825 ; l2 norm of weights:  8.60967844670426\n",
            "Iteration#:  3896 ; Loss:  7.28709587362432 ; l2 norm of gradient:  1.6061434271554973 ; l2 norm of weights:  8.609627626151317\n",
            "Iteration#:  3900 ; Loss:  7.287007669678818 ; l2 norm of gradient:  1.6061348064165728 ; l2 norm of weights:  8.609576805993358\n",
            "Iteration#:  3904 ; Loss:  7.286919472785511 ; l2 norm of gradient:  1.6061261856742055 ; l2 norm of weights:  8.609525986230377\n",
            "Iteration#:  3908 ; Loss:  7.2868312772188055 ; l2 norm of gradient:  1.6061175649283925 ; l2 norm of weights:  8.609475166862376\n",
            "Iteration#:  3912 ; Loss:  7.286743083219605 ; l2 norm of gradient:  1.6061089441791314 ; l2 norm of weights:  8.609424347889355\n",
            "Iteration#:  3916 ; Loss:  7.286654885259965 ; l2 norm of gradient:  1.6061003234264208 ; l2 norm of weights:  8.609373529311311\n",
            "Iteration#:  3920 ; Loss:  7.286566693687788 ; l2 norm of gradient:  1.6060917026702575 ; l2 norm of weights:  8.609322711128243\n",
            "Iteration#:  3924 ; Loss:  7.2864784984224285 ; l2 norm of gradient:  1.6060830819106398 ; l2 norm of weights:  8.609271893340152\n",
            "Iteration#:  3928 ; Loss:  7.286390304199696 ; l2 norm of gradient:  1.606074461147564 ; l2 norm of weights:  8.609221075947035\n",
            "Iteration#:  3932 ; Loss:  7.286302107340003 ; l2 norm of gradient:  1.6060658403810284 ; l2 norm of weights:  8.609170258948893\n",
            "Iteration#:  3936 ; Loss:  7.286213915352313 ; l2 norm of gradient:  1.6060572196110299 ; l2 norm of weights:  8.609119442345726\n",
            "Iteration#:  3940 ; Loss:  7.286125723738127 ; l2 norm of gradient:  1.606048598837567 ; l2 norm of weights:  8.609068626137532\n",
            "Iteration#:  3944 ; Loss:  7.286037530376816 ; l2 norm of gradient:  1.606039978060637 ; l2 norm of weights:  8.609017810324309\n",
            "Iteration#:  3948 ; Loss:  7.285949343299571 ; l2 norm of gradient:  1.6060313572802374 ; l2 norm of weights:  8.608966994906059\n",
            "Iteration#:  3952 ; Loss:  7.285861148422509 ; l2 norm of gradient:  1.606022736496365 ; l2 norm of weights:  8.608916179882778\n",
            "Iteration#:  3956 ; Loss:  7.285772958574601 ; l2 norm of gradient:  1.606014115709018 ; l2 norm of weights:  8.608865365254466\n",
            "Iteration#:  3960 ; Loss:  7.2856847659255335 ; l2 norm of gradient:  1.6060054949181946 ; l2 norm of weights:  8.608814551021124\n",
            "Iteration#:  3964 ; Loss:  7.285596573508062 ; l2 norm of gradient:  1.605996874123891 ; l2 norm of weights:  8.608763737182748\n",
            "Iteration#:  3968 ; Loss:  7.28550838678437 ; l2 norm of gradient:  1.6059882533261047 ; l2 norm of weights:  8.608712923739342\n",
            "Iteration#:  3972 ; Loss:  7.285420198706953 ; l2 norm of gradient:  1.6059796325248352 ; l2 norm of weights:  8.6086621106909\n",
            "Iteration#:  3976 ; Loss:  7.285332010100931 ; l2 norm of gradient:  1.6059710117200774 ; l2 norm of weights:  8.608611298037426\n",
            "Iteration#:  3980 ; Loss:  7.285243821042018 ; l2 norm of gradient:  1.6059623909118317 ; l2 norm of weights:  8.608560485778918\n",
            "Iteration#:  3984 ; Loss:  7.285155633768095 ; l2 norm of gradient:  1.6059537701000923 ; l2 norm of weights:  8.60850967391537\n",
            "Iteration#:  3988 ; Loss:  7.285067450904275 ; l2 norm of gradient:  1.6059451492848593 ; l2 norm of weights:  8.608458862446788\n",
            "Iteration#:  3992 ; Loss:  7.284979265976916 ; l2 norm of gradient:  1.6059365284661296 ; l2 norm of weights:  8.60840805137317\n",
            "Iteration#:  3996 ; Loss:  7.2848910754564535 ; l2 norm of gradient:  1.6059279076439006 ; l2 norm of weights:  8.608357240694511\n",
            "Iteration#:  4000 ; Loss:  7.284802889600651 ; l2 norm of gradient:  1.6059192868181698 ; l2 norm of weights:  8.608306430410813\n",
            "Iteration#:  4004 ; Loss:  7.284714708319074 ; l2 norm of gradient:  1.6059106659889337 ; l2 norm of weights:  8.608255620522076\n",
            "Iteration#:  4008 ; Loss:  7.284626526496492 ; l2 norm of gradient:  1.605902045156192 ; l2 norm of weights:  8.6082048110283\n",
            "Iteration#:  4012 ; Loss:  7.284538345078451 ; l2 norm of gradient:  1.6058934243199414 ; l2 norm of weights:  8.608154001929478\n",
            "Iteration#:  4016 ; Loss:  7.284450157322166 ; l2 norm of gradient:  1.6058848034801774 ; l2 norm of weights:  8.608103193225617\n",
            "Iteration#:  4020 ; Loss:  7.28436197557408 ; l2 norm of gradient:  1.6058761826369012 ; l2 norm of weights:  8.608052384916713\n",
            "Iteration#:  4024 ; Loss:  7.2842737958999635 ; l2 norm of gradient:  1.6058675617901075 ; l2 norm of weights:  8.608001577002764\n",
            "Iteration#:  4028 ; Loss:  7.284185612888171 ; l2 norm of gradient:  1.6058589409397943 ; l2 norm of weights:  8.607950769483768\n",
            "Iteration#:  4032 ; Loss:  7.284097431966249 ; l2 norm of gradient:  1.6058503200859593 ; l2 norm of weights:  8.607899962359731\n",
            "Iteration#:  4036 ; Loss:  7.28400924942303 ; l2 norm of gradient:  1.6058416992286013 ; l2 norm of weights:  8.607849155630644\n",
            "Iteration#:  4040 ; Loss:  7.283921071904635 ; l2 norm of gradient:  1.6058330783677162 ; l2 norm of weights:  8.607798349296512\n",
            "Iteration#:  4044 ; Loss:  7.283832898455758 ; l2 norm of gradient:  1.6058244575033018 ; l2 norm of weights:  8.60774754335733\n",
            "Iteration#:  4048 ; Loss:  7.283744718133191 ; l2 norm of gradient:  1.6058158366353563 ; l2 norm of weights:  8.6076967378131\n",
            "Iteration#:  4052 ; Loss:  7.283656539118006 ; l2 norm of gradient:  1.6058072157638779 ; l2 norm of weights:  8.607645932663823\n",
            "Iteration#:  4056 ; Loss:  7.283568359444006 ; l2 norm of gradient:  1.6057985948888625 ; l2 norm of weights:  8.607595127909493\n",
            "Iteration#:  4060 ; Loss:  7.283480182961091 ; l2 norm of gradient:  1.6057899740103072 ; l2 norm of weights:  8.607544323550114\n",
            "Iteration#:  4064 ; Loss:  7.283392006547729 ; l2 norm of gradient:  1.6057813531282115 ; l2 norm of weights:  8.60749351958568\n",
            "Iteration#:  4068 ; Loss:  7.2833038312129545 ; l2 norm of gradient:  1.6057727322425703 ; l2 norm of weights:  8.607442716016195\n",
            "Iteration#:  4072 ; Loss:  7.283215657762437 ; l2 norm of gradient:  1.6057641113533843 ; l2 norm of weights:  8.607391912841656\n",
            "Iteration#:  4076 ; Loss:  7.2831274812060105 ; l2 norm of gradient:  1.6057554904606486 ; l2 norm of weights:  8.607341110062064\n",
            "Iteration#:  4080 ; Loss:  7.283039303804058 ; l2 norm of gradient:  1.6057468695643626 ; l2 norm of weights:  8.607290307677415\n",
            "Iteration#:  4084 ; Loss:  7.282951132470073 ; l2 norm of gradient:  1.6057382486645224 ; l2 norm of weights:  8.60723950568771\n",
            "Iteration#:  4088 ; Loss:  7.282862959302829 ; l2 norm of gradient:  1.6057296277611255 ; l2 norm of weights:  8.60718870409295\n",
            "Iteration#:  4092 ; Loss:  7.282774784543703 ; l2 norm of gradient:  1.60572100685417 ; l2 norm of weights:  8.607137902893133\n",
            "Iteration#:  4096 ; Loss:  7.282686610920534 ; l2 norm of gradient:  1.605712385943653 ; l2 norm of weights:  8.607087102088256\n",
            "Iteration#:  4100 ; Loss:  7.282598439574167 ; l2 norm of gradient:  1.605703765029572 ; l2 norm of weights:  8.60703630167832\n",
            "Iteration#:  4104 ; Loss:  7.282510269844522 ; l2 norm of gradient:  1.6056951441119258 ; l2 norm of weights:  8.606985501663324\n",
            "Iteration#:  4108 ; Loss:  7.282422099101351 ; l2 norm of gradient:  1.6056865231907098 ; l2 norm of weights:  8.606934702043267\n",
            "Iteration#:  4112 ; Loss:  7.282333926113599 ; l2 norm of gradient:  1.605677902265923 ; l2 norm of weights:  8.606883902818149\n",
            "Iteration#:  4116 ; Loss:  7.282245755589107 ; l2 norm of gradient:  1.6056692813375626 ; l2 norm of weights:  8.606833103987968\n",
            "Iteration#:  4120 ; Loss:  7.282157587835719 ; l2 norm of gradient:  1.605660660405626 ; l2 norm of weights:  8.606782305552725\n",
            "Iteration#:  4124 ; Loss:  7.28206941869675 ; l2 norm of gradient:  1.6056520394701108 ; l2 norm of weights:  8.606731507512416\n",
            "Iteration#:  4128 ; Loss:  7.281981250085723 ; l2 norm of gradient:  1.6056434185310144 ; l2 norm of weights:  8.606680709867044\n",
            "Iteration#:  4132 ; Loss:  7.281893087076455 ; l2 norm of gradient:  1.6056347975883336 ; l2 norm of weights:  8.606629912616606\n",
            "Iteration#:  4136 ; Loss:  7.281804913160663 ; l2 norm of gradient:  1.6056261766420672 ; l2 norm of weights:  8.606579115761102\n",
            "Iteration#:  4140 ; Loss:  7.28171674905144 ; l2 norm of gradient:  1.6056175556922119 ; l2 norm of weights:  8.60652831930053\n",
            "Iteration#:  4144 ; Loss:  7.2816285825962295 ; l2 norm of gradient:  1.605608934738765 ; l2 norm of weights:  8.60647752323489\n",
            "Iteration#:  4148 ; Loss:  7.2815404172926055 ; l2 norm of gradient:  1.6056003137817254 ; l2 norm of weights:  8.60642672756418\n",
            "Iteration#:  4152 ; Loss:  7.28145225293097 ; l2 norm of gradient:  1.605591692821089 ; l2 norm of weights:  8.606375932288403\n",
            "Iteration#:  4156 ; Loss:  7.281364090211533 ; l2 norm of gradient:  1.605583071856854 ; l2 norm of weights:  8.606325137407552\n",
            "Iteration#:  4160 ; Loss:  7.281275926400927 ; l2 norm of gradient:  1.6055744508890175 ; l2 norm of weights:  8.606274342921633\n",
            "Iteration#:  4164 ; Loss:  7.281187762200933 ; l2 norm of gradient:  1.605565829917578 ; l2 norm of weights:  8.606223548830641\n",
            "Iteration#:  4168 ; Loss:  7.281099595136315 ; l2 norm of gradient:  1.6055572089425325 ; l2 norm of weights:  8.606172755134578\n",
            "Iteration#:  4172 ; Loss:  7.281011436821353 ; l2 norm of gradient:  1.6055485879638776 ; l2 norm of weights:  8.60612196183344\n",
            "Iteration#:  4176 ; Loss:  7.280923274297451 ; l2 norm of gradient:  1.6055399669816122 ; l2 norm of weights:  8.606071168927226\n",
            "Iteration#:  4180 ; Loss:  7.280835110665392 ; l2 norm of gradient:  1.6055313459957326 ; l2 norm of weights:  8.606020376415938\n",
            "Iteration#:  4184 ; Loss:  7.280746953011873 ; l2 norm of gradient:  1.605522725006237 ; l2 norm of weights:  8.605969584299574\n",
            "Iteration#:  4188 ; Loss:  7.28065879089225 ; l2 norm of gradient:  1.6055141040131222 ; l2 norm of weights:  8.605918792578134\n",
            "Iteration#:  4192 ; Loss:  7.280570629773921 ; l2 norm of gradient:  1.6055054830163864 ; l2 norm of weights:  8.605868001251615\n",
            "Iteration#:  4196 ; Loss:  7.280482466804474 ; l2 norm of gradient:  1.6054968620160277 ; l2 norm of weights:  8.605817210320017\n",
            "Iteration#:  4200 ; Loss:  7.280394309391772 ; l2 norm of gradient:  1.6054882410120428 ; l2 norm of weights:  8.60576641978334\n",
            "Iteration#:  4204 ; Loss:  7.280306150329052 ; l2 norm of gradient:  1.6054796200044281 ; l2 norm of weights:  8.605715629641585\n",
            "Iteration#:  4208 ; Loss:  7.280217993863028 ; l2 norm of gradient:  1.6054709989931832 ; l2 norm of weights:  8.605664839894748\n",
            "Iteration#:  4212 ; Loss:  7.280129839830314 ; l2 norm of gradient:  1.6054623779783042 ; l2 norm of weights:  8.60561405054283\n",
            "Iteration#:  4216 ; Loss:  7.2800416812254625 ; l2 norm of gradient:  1.6054537569597889 ; l2 norm of weights:  8.605563261585829\n",
            "Iteration#:  4220 ; Loss:  7.279953522111605 ; l2 norm of gradient:  1.6054451359376343 ; l2 norm of weights:  8.605512473023744\n",
            "Iteration#:  4224 ; Loss:  7.279865364541375 ; l2 norm of gradient:  1.605436514911839 ; l2 norm of weights:  8.605461684856575\n",
            "Iteration#:  4228 ; Loss:  7.279777213960298 ; l2 norm of gradient:  1.6054278938824005 ; l2 norm of weights:  8.605410897084322\n",
            "Iteration#:  4232 ; Loss:  7.279689056397938 ; l2 norm of gradient:  1.6054192728493155 ; l2 norm of weights:  8.605360109706984\n",
            "Iteration#:  4236 ; Loss:  7.279600898979387 ; l2 norm of gradient:  1.6054106518125808 ; l2 norm of weights:  8.605309322724557\n",
            "Iteration#:  4240 ; Loss:  7.279512750363275 ; l2 norm of gradient:  1.6054020307721957 ; l2 norm of weights:  8.605258536137045\n",
            "Iteration#:  4244 ; Loss:  7.279424596945754 ; l2 norm of gradient:  1.6053934097281561 ; l2 norm of weights:  8.605207749944443\n",
            "Iteration#:  4248 ; Loss:  7.279336445527479 ; l2 norm of gradient:  1.605384788680461 ; l2 norm of weights:  8.605156964146754\n",
            "Iteration#:  4252 ; Loss:  7.279248289417994 ; l2 norm of gradient:  1.6053761676291058 ; l2 norm of weights:  8.605106178743974\n",
            "Iteration#:  4256 ; Loss:  7.279160136308211 ; l2 norm of gradient:  1.6053675465740904 ; l2 norm of weights:  8.605055393736103\n",
            "Iteration#:  4260 ; Loss:  7.279071984157065 ; l2 norm of gradient:  1.6053589255154104 ; l2 norm of weights:  8.605004609123142\n",
            "Iteration#:  4264 ; Loss:  7.278983836874703 ; l2 norm of gradient:  1.605350304453064 ; l2 norm of weights:  8.604953824905088\n",
            "Iteration#:  4268 ; Loss:  7.278895686866397 ; l2 norm of gradient:  1.605341683387049 ; l2 norm of weights:  8.604903041081942\n",
            "Iteration#:  4272 ; Loss:  7.278807537492241 ; l2 norm of gradient:  1.605333062317363 ; l2 norm of weights:  8.604852257653702\n",
            "Iteration#:  4276 ; Loss:  7.278719387325724 ; l2 norm of gradient:  1.6053244412440024 ; l2 norm of weights:  8.604801474620366\n",
            "Iteration#:  4280 ; Loss:  7.278631242846912 ; l2 norm of gradient:  1.605315820166965 ; l2 norm of weights:  8.604750691981938\n",
            "Iteration#:  4284 ; Loss:  7.278543089881263 ; l2 norm of gradient:  1.60530719908625 ; l2 norm of weights:  8.604699909738411\n",
            "Iteration#:  4288 ; Loss:  7.278454942672378 ; l2 norm of gradient:  1.6052985780018516 ; l2 norm of weights:  8.604649127889788\n",
            "Iteration#:  4292 ; Loss:  7.278366799706948 ; l2 norm of gradient:  1.6052899569137695 ; l2 norm of weights:  8.604598346436068\n",
            "Iteration#:  4296 ; Loss:  7.278278652105467 ; l2 norm of gradient:  1.6052813358220015 ; l2 norm of weights:  8.60454756537725\n",
            "Iteration#:  4300 ; Loss:  7.2781905063623 ; l2 norm of gradient:  1.6052727147265453 ; l2 norm of weights:  8.604496784713328\n",
            "Iteration#:  4304 ; Loss:  7.2781023574127515 ; l2 norm of gradient:  1.6052640936273965 ; l2 norm of weights:  8.60444600444431\n",
            "Iteration#:  4308 ; Loss:  7.278014214642014 ; l2 norm of gradient:  1.6052554725245523 ; l2 norm of weights:  8.60439522457019\n",
            "Iteration#:  4312 ; Loss:  7.277926068100265 ; l2 norm of gradient:  1.6052468514180125 ; l2 norm of weights:  8.604344445090968\n",
            "Iteration#:  4316 ; Loss:  7.277837924285482 ; l2 norm of gradient:  1.6052382303077735 ; l2 norm of weights:  8.604293666006644\n",
            "Iteration#:  4320 ; Loss:  7.277749786149748 ; l2 norm of gradient:  1.605229609193832 ; l2 norm of weights:  8.604242887317216\n",
            "Iteration#:  4324 ; Loss:  7.277661638995895 ; l2 norm of gradient:  1.6052209880761876 ; l2 norm of weights:  8.604192109022685\n",
            "Iteration#:  4328 ; Loss:  7.277573497465674 ; l2 norm of gradient:  1.605212366954836 ; l2 norm of weights:  8.60414133112305\n",
            "Iteration#:  4332 ; Loss:  7.27748535724105 ; l2 norm of gradient:  1.6052037458297745 ; l2 norm of weights:  8.604090553618306\n",
            "Iteration#:  4336 ; Loss:  7.277397212517094 ; l2 norm of gradient:  1.6051951247010015 ; l2 norm of weights:  8.604039776508456\n",
            "Iteration#:  4340 ; Loss:  7.277309070342147 ; l2 norm of gradient:  1.6051865035685144 ; l2 norm of weights:  8.6039889997935\n",
            "Iteration#:  4344 ; Loss:  7.27722093051322 ; l2 norm of gradient:  1.605177882432309 ; l2 norm of weights:  8.603938223473437\n",
            "Iteration#:  4348 ; Loss:  7.277132788294114 ; l2 norm of gradient:  1.6051692612923854 ; l2 norm of weights:  8.603887447548264\n",
            "Iteration#:  4352 ; Loss:  7.277044653053128 ; l2 norm of gradient:  1.6051606401487393 ; l2 norm of weights:  8.60383667201798\n",
            "Iteration#:  4356 ; Loss:  7.276956513625426 ; l2 norm of gradient:  1.6051520190013688 ; l2 norm of weights:  8.603785896882586\n",
            "Iteration#:  4360 ; Loss:  7.2768683731814 ; l2 norm of gradient:  1.6051433978502705 ; l2 norm of weights:  8.603735122142082\n",
            "Iteration#:  4364 ; Loss:  7.27678024186392 ; l2 norm of gradient:  1.6051347766954434 ; l2 norm of weights:  8.603684347796465\n",
            "Iteration#:  4368 ; Loss:  7.276692101017942 ; l2 norm of gradient:  1.6051261555368845 ; l2 norm of weights:  8.603633573845734\n",
            "Iteration#:  4372 ; Loss:  7.276603961950254 ; l2 norm of gradient:  1.605117534374591 ; l2 norm of weights:  8.603582800289889\n",
            "Iteration#:  4376 ; Loss:  7.276515826851531 ; l2 norm of gradient:  1.6051089132085585 ; l2 norm of weights:  8.60353202712893\n",
            "Iteration#:  4380 ; Loss:  7.2764276907816186 ; l2 norm of gradient:  1.605100292038787 ; l2 norm of weights:  8.603481254362855\n",
            "Iteration#:  4384 ; Loss:  7.276339556146416 ; l2 norm of gradient:  1.6050916708652745 ; l2 norm of weights:  8.603430481991664\n",
            "Iteration#:  4388 ; Loss:  7.276251423831405 ; l2 norm of gradient:  1.605083049688015 ; l2 norm of weights:  8.603379710015357\n",
            "Iteration#:  4392 ; Loss:  7.276163289973859 ; l2 norm of gradient:  1.6050744285070095 ; l2 norm of weights:  8.603328938433933\n",
            "Iteration#:  4396 ; Loss:  7.27607515637809 ; l2 norm of gradient:  1.6050658073222535 ; l2 norm of weights:  8.603278167247389\n",
            "Iteration#:  4400 ; Loss:  7.275987022537439 ; l2 norm of gradient:  1.6050571861337455 ; l2 norm of weights:  8.603227396455724\n",
            "Iteration#:  4404 ; Loss:  7.275898889993996 ; l2 norm of gradient:  1.605048564941482 ; l2 norm of weights:  8.603176626058941\n",
            "Iteration#:  4408 ; Loss:  7.2758107549924205 ; l2 norm of gradient:  1.6050399437454614 ; l2 norm of weights:  8.603125856057037\n",
            "Iteration#:  4412 ; Loss:  7.27572262813834 ; l2 norm of gradient:  1.6050313225456798 ; l2 norm of weights:  8.60307508645001\n",
            "Iteration#:  4416 ; Loss:  7.275634493384521 ; l2 norm of gradient:  1.6050227013421359 ; l2 norm of weights:  8.60302431723786\n",
            "Iteration#:  4420 ; Loss:  7.27554636470877 ; l2 norm of gradient:  1.6050140801348274 ; l2 norm of weights:  8.602973548420588\n",
            "Iteration#:  4424 ; Loss:  7.275458232709923 ; l2 norm of gradient:  1.60500545892375 ; l2 norm of weights:  8.60292277999819\n",
            "Iteration#:  4428 ; Loss:  7.2753701043284735 ; l2 norm of gradient:  1.6049968377089028 ; l2 norm of weights:  8.602872011970668\n",
            "Iteration#:  4432 ; Loss:  7.27528197513492 ; l2 norm of gradient:  1.604988216490283 ; l2 norm of weights:  8.602821244338019\n",
            "Iteration#:  4436 ; Loss:  7.275193847569323 ; l2 norm of gradient:  1.6049795952678874 ; l2 norm of weights:  8.602770477100245\n",
            "Iteration#:  4440 ; Loss:  7.275105720312315 ; l2 norm of gradient:  1.604970974041713 ; l2 norm of weights:  8.602719710257343\n",
            "Iteration#:  4444 ; Loss:  7.275017595544728 ; l2 norm of gradient:  1.6049623528117596 ; l2 norm of weights:  8.602668943809313\n",
            "Iteration#:  4448 ; Loss:  7.274929467106015 ; l2 norm of gradient:  1.6049537315780222 ; l2 norm of weights:  8.602618177756154\n",
            "Iteration#:  4452 ; Loss:  7.274841341577157 ; l2 norm of gradient:  1.6049451103404992 ; l2 norm of weights:  8.602567412097864\n",
            "Iteration#:  4456 ; Loss:  7.274753211787621 ; l2 norm of gradient:  1.6049364890991886 ; l2 norm of weights:  8.602516646834443\n",
            "Iteration#:  4460 ; Loss:  7.274665088811204 ; l2 norm of gradient:  1.604927867854086 ; l2 norm of weights:  8.602465881965891\n",
            "Iteration#:  4464 ; Loss:  7.274576965544692 ; l2 norm of gradient:  1.604919246605191 ; l2 norm of weights:  8.602415117492207\n",
            "Iteration#:  4468 ; Loss:  7.274488840513133 ; l2 norm of gradient:  1.6049106253524998 ; l2 norm of weights:  8.602364353413392\n",
            "Iteration#:  4472 ; Loss:  7.274400717197885 ; l2 norm of gradient:  1.6049020040960105 ; l2 norm of weights:  8.602313589729441\n",
            "Iteration#:  4476 ; Loss:  7.274312590950245 ; l2 norm of gradient:  1.6048933828357201 ; l2 norm of weights:  8.602262826440356\n",
            "Iteration#:  4480 ; Loss:  7.274224468918785 ; l2 norm of gradient:  1.6048847615716255 ; l2 norm of weights:  8.602212063546133\n",
            "Iteration#:  4484 ; Loss:  7.274136348174004 ; l2 norm of gradient:  1.6048761403037255 ; l2 norm of weights:  8.602161301046776\n",
            "Iteration#:  4488 ; Loss:  7.274048229339054 ; l2 norm of gradient:  1.6048675190320165 ; l2 norm of weights:  8.60211053894228\n",
            "Iteration#:  4492 ; Loss:  7.2739601032720165 ; l2 norm of gradient:  1.6048588977564961 ; l2 norm of weights:  8.602059777232649\n",
            "Iteration#:  4496 ; Loss:  7.273871986754074 ; l2 norm of gradient:  1.6048502764771624 ; l2 norm of weights:  8.602009015917877\n",
            "Iteration#:  4500 ; Loss:  7.2737838639898795 ; l2 norm of gradient:  1.604841655194012 ; l2 norm of weights:  8.601958254997967\n",
            "Iteration#:  4504 ; Loss:  7.2736957407562866 ; l2 norm of gradient:  1.6048330339070425 ; l2 norm of weights:  8.601907494472917\n",
            "Iteration#:  4508 ; Loss:  7.273607623614545 ; l2 norm of gradient:  1.6048244126162516 ; l2 norm of weights:  8.601856734342723\n",
            "Iteration#:  4512 ; Loss:  7.273519499294942 ; l2 norm of gradient:  1.6048157913216363 ; l2 norm of weights:  8.60180597460739\n",
            "Iteration#:  4516 ; Loss:  7.273431387565297 ; l2 norm of gradient:  1.6048071700231947 ; l2 norm of weights:  8.60175521526691\n",
            "Iteration#:  4520 ; Loss:  7.273343269083387 ; l2 norm of gradient:  1.6047985487209242 ; l2 norm of weights:  8.601704456321292\n",
            "Iteration#:  4524 ; Loss:  7.273255150260534 ; l2 norm of gradient:  1.6047899274148218 ; l2 norm of weights:  8.601653697770525\n",
            "Iteration#:  4528 ; Loss:  7.273167032454992 ; l2 norm of gradient:  1.6047813061048848 ; l2 norm of weights:  8.601602939614617\n",
            "Iteration#:  4532 ; Loss:  7.273078920949384 ; l2 norm of gradient:  1.604772684791111 ; l2 norm of weights:  8.60155218185356\n",
            "Iteration#:  4536 ; Loss:  7.272990806854732 ; l2 norm of gradient:  1.6047640634734979 ; l2 norm of weights:  8.601501424487356\n",
            "Iteration#:  4540 ; Loss:  7.272902686115124 ; l2 norm of gradient:  1.6047554421520414 ; l2 norm of weights:  8.601450667516005\n",
            "Iteration#:  4544 ; Loss:  7.272814572831949 ; l2 norm of gradient:  1.6047468208267417 ; l2 norm of weights:  8.601399910939506\n",
            "Iteration#:  4548 ; Loss:  7.272726462142819 ; l2 norm of gradient:  1.6047381994975947 ; l2 norm of weights:  8.601349154757859\n",
            "Iteration#:  4552 ; Loss:  7.272638349081885 ; l2 norm of gradient:  1.6047295781645974 ; l2 norm of weights:  8.60129839897106\n",
            "Iteration#:  4556 ; Loss:  7.272550236858994 ; l2 norm of gradient:  1.6047209568277467 ; l2 norm of weights:  8.60124764357911\n",
            "Iteration#:  4560 ; Loss:  7.272462121937313 ; l2 norm of gradient:  1.604712335487043 ; l2 norm of weights:  8.60119688858201\n",
            "Iteration#:  4564 ; Loss:  7.2723740119850575 ; l2 norm of gradient:  1.6047037141424811 ; l2 norm of weights:  8.601146133979757\n",
            "Iteration#:  4568 ; Loss:  7.27228589645831 ; l2 norm of gradient:  1.6046950927940582 ; l2 norm of weights:  8.601095379772351\n",
            "Iteration#:  4572 ; Loss:  7.272197785599651 ; l2 norm of gradient:  1.6046864714417741 ; l2 norm of weights:  8.60104462595979\n",
            "Iteration#:  4576 ; Loss:  7.272109675259781 ; l2 norm of gradient:  1.6046778500856238 ; l2 norm of weights:  8.600993872542075\n",
            "Iteration#:  4580 ; Loss:  7.272021567972109 ; l2 norm of gradient:  1.604669228725606 ; l2 norm of weights:  8.600943119519203\n",
            "Iteration#:  4584 ; Loss:  7.271933463429976 ; l2 norm of gradient:  1.604660607361718 ; l2 norm of weights:  8.600892366891175\n",
            "Iteration#:  4588 ; Loss:  7.271845351651442 ; l2 norm of gradient:  1.6046519859939568 ; l2 norm of weights:  8.600841614657991\n",
            "Iteration#:  4592 ; Loss:  7.271757244567766 ; l2 norm of gradient:  1.6046433646223197 ; l2 norm of weights:  8.600790862819649\n",
            "Iteration#:  4596 ; Loss:  7.271669135347442 ; l2 norm of gradient:  1.6046347432468047 ; l2 norm of weights:  8.600740111376147\n",
            "Iteration#:  4600 ; Loss:  7.271581030735748 ; l2 norm of gradient:  1.6046261218674096 ; l2 norm of weights:  8.600689360327486\n",
            "Iteration#:  4604 ; Loss:  7.271492918425368 ; l2 norm of gradient:  1.6046175004841297 ; l2 norm of weights:  8.600638609673664\n",
            "Iteration#:  4608 ; Loss:  7.271404812455476 ; l2 norm of gradient:  1.6046088790969648 ; l2 norm of weights:  8.60058785941468\n",
            "Iteration#:  4612 ; Loss:  7.27131670944598 ; l2 norm of gradient:  1.6046002577059113 ; l2 norm of weights:  8.600537109550537\n",
            "Iteration#:  4616 ; Loss:  7.271228601726893 ; l2 norm of gradient:  1.6045916363109665 ; l2 norm of weights:  8.600486360081229\n",
            "Iteration#:  4620 ; Loss:  7.27114050092883 ; l2 norm of gradient:  1.6045830149121285 ; l2 norm of weights:  8.600435611006757\n",
            "Iteration#:  4624 ; Loss:  7.271052394749857 ; l2 norm of gradient:  1.6045743935093932 ; l2 norm of weights:  8.60038486232712\n",
            "Iteration#:  4628 ; Loss:  7.270964284943311 ; l2 norm of gradient:  1.6045657721027602 ; l2 norm of weights:  8.600334114042319\n",
            "Iteration#:  4632 ; Loss:  7.270876183882185 ; l2 norm of gradient:  1.6045571506922252 ; l2 norm of weights:  8.60028336615235\n",
            "Iteration#:  4636 ; Loss:  7.270788087399311 ; l2 norm of gradient:  1.6045485292777861 ; l2 norm of weights:  8.600232618657216\n",
            "Iteration#:  4640 ; Loss:  7.270699984595584 ; l2 norm of gradient:  1.6045399078594407 ; l2 norm of weights:  8.600181871556915\n",
            "Iteration#:  4644 ; Loss:  7.270611881290581 ; l2 norm of gradient:  1.6045312864371857 ; l2 norm of weights:  8.600131124851444\n",
            "Iteration#:  4648 ; Loss:  7.270523778784543 ; l2 norm of gradient:  1.6045226650110194 ; l2 norm of weights:  8.600080378540802\n",
            "Iteration#:  4652 ; Loss:  7.270435680987298 ; l2 norm of gradient:  1.604514043580938 ; l2 norm of weights:  8.600029632624992\n",
            "Iteration#:  4656 ; Loss:  7.2703475816882985 ; l2 norm of gradient:  1.6045054221469406 ; l2 norm of weights:  8.599978887104012\n",
            "Iteration#:  4660 ; Loss:  7.270259481711584 ; l2 norm of gradient:  1.6044968007090226 ; l2 norm of weights:  8.59992814197786\n",
            "Iteration#:  4664 ; Loss:  7.270171380087062 ; l2 norm of gradient:  1.6044881792671821 ; l2 norm of weights:  8.599877397246532\n",
            "Iteration#:  4668 ; Loss:  7.270083281493079 ; l2 norm of gradient:  1.604479557821418 ; l2 norm of weights:  8.599826652910034\n",
            "Iteration#:  4672 ; Loss:  7.2699951851937685 ; l2 norm of gradient:  1.6044709363717264 ; l2 norm of weights:  8.599775908968361\n",
            "Iteration#:  4676 ; Loss:  7.26990708999247 ; l2 norm of gradient:  1.6044623149181043 ; l2 norm of weights:  8.599725165421514\n",
            "Iteration#:  4680 ; Loss:  7.2698189925965035 ; l2 norm of gradient:  1.6044536934605498 ; l2 norm of weights:  8.599674422269489\n",
            "Iteration#:  4684 ; Loss:  7.26973089666429 ; l2 norm of gradient:  1.60444507199906 ; l2 norm of weights:  8.59962367951229\n",
            "Iteration#:  4688 ; Loss:  7.2696428012075 ; l2 norm of gradient:  1.6044364505336322 ; l2 norm of weights:  8.59957293714991\n",
            "Iteration#:  4692 ; Loss:  7.269554700861679 ; l2 norm of gradient:  1.6044278290642637 ; l2 norm of weights:  8.599522195182354\n",
            "Iteration#:  4696 ; Loss:  7.26946660973473 ; l2 norm of gradient:  1.6044192075909531 ; l2 norm of weights:  8.599471453609619\n",
            "Iteration#:  4700 ; Loss:  7.269378515256241 ; l2 norm of gradient:  1.6044105861136961 ; l2 norm of weights:  8.599420712431705\n",
            "Iteration#:  4704 ; Loss:  7.269290421205104 ; l2 norm of gradient:  1.6044019646324914 ; l2 norm of weights:  8.59936997164861\n",
            "Iteration#:  4708 ; Loss:  7.269202325981038 ; l2 norm of gradient:  1.6043933431473365 ; l2 norm of weights:  8.599319231260333\n",
            "Iteration#:  4712 ; Loss:  7.269114232466398 ; l2 norm of gradient:  1.6043847216582268 ; l2 norm of weights:  8.599268491266875\n",
            "Iteration#:  4716 ; Loss:  7.269026141278882 ; l2 norm of gradient:  1.604376100165162 ; l2 norm of weights:  8.599217751668233\n",
            "Iteration#:  4720 ; Loss:  7.268938050450805 ; l2 norm of gradient:  1.6043674786681381 ; l2 norm of weights:  8.599167012464408\n",
            "Iteration#:  4724 ; Loss:  7.268849961789878 ; l2 norm of gradient:  1.6043588571671532 ; l2 norm of weights:  8.599116273655397\n",
            "Iteration#:  4728 ; Loss:  7.268761870599263 ; l2 norm of gradient:  1.6043502356622044 ; l2 norm of weights:  8.599065535241202\n",
            "Iteration#:  4732 ; Loss:  7.268673778585859 ; l2 norm of gradient:  1.604341614153289 ; l2 norm of weights:  8.599014797221818\n",
            "Iteration#:  4736 ; Loss:  7.268585687942443 ; l2 norm of gradient:  1.6043329926404042 ; l2 norm of weights:  8.598964059597249\n",
            "Iteration#:  4740 ; Loss:  7.268497602559674 ; l2 norm of gradient:  1.6043243711235486 ; l2 norm of weights:  8.598913322367492\n",
            "Iteration#:  4744 ; Loss:  7.268409513008482 ; l2 norm of gradient:  1.6043157496027187 ; l2 norm of weights:  8.598862585532546\n",
            "Iteration#:  4748 ; Loss:  7.268321427318012 ; l2 norm of gradient:  1.6043071280779118 ; l2 norm of weights:  8.598811849092412\n",
            "Iteration#:  4752 ; Loss:  7.268233341587916 ; l2 norm of gradient:  1.604298506549124 ; l2 norm of weights:  8.598761113047086\n",
            "Iteration#:  4756 ; Loss:  7.268145253227795 ; l2 norm of gradient:  1.604289885016356 ; l2 norm of weights:  8.598710377396568\n",
            "Iteration#:  4760 ; Loss:  7.268057165806672 ; l2 norm of gradient:  1.6042812634796022 ; l2 norm of weights:  8.598659642140861\n",
            "Iteration#:  4764 ; Loss:  7.2679690751097485 ; l2 norm of gradient:  1.604272641938861 ; l2 norm of weights:  8.59860890727996\n",
            "Iteration#:  4768 ; Loss:  7.267880989635919 ; l2 norm of gradient:  1.60426402039413 ; l2 norm of weights:  8.598558172813865\n",
            "Iteration#:  4772 ; Loss:  7.267792908331434 ; l2 norm of gradient:  1.6042553988454058 ; l2 norm of weights:  8.598507438742576\n",
            "Iteration#:  4776 ; Loss:  7.267704819712423 ; l2 norm of gradient:  1.6042467772926872 ; l2 norm of weights:  8.598456705066091\n",
            "Iteration#:  4780 ; Loss:  7.267616738041709 ; l2 norm of gradient:  1.6042381557359702 ; l2 norm of weights:  8.59840597178441\n",
            "Iteration#:  4784 ; Loss:  7.26752865481777 ; l2 norm of gradient:  1.604229534175253 ; l2 norm of weights:  8.598355238897534\n",
            "Iteration#:  4788 ; Loss:  7.267440572910905 ; l2 norm of gradient:  1.6042209126105333 ; l2 norm of weights:  8.59830450640546\n",
            "Iteration#:  4792 ; Loss:  7.267352489896405 ; l2 norm of gradient:  1.6042122910418075 ; l2 norm of weights:  8.598253774308187\n",
            "Iteration#:  4796 ; Loss:  7.267264407849822 ; l2 norm of gradient:  1.6042036694690724 ; l2 norm of weights:  8.598203042605714\n",
            "Iteration#:  4800 ; Loss:  7.267176326655246 ; l2 norm of gradient:  1.6041950478923281 ; l2 norm of weights:  8.598152311298042\n",
            "Iteration#:  4804 ; Loss:  7.267088246425687 ; l2 norm of gradient:  1.6041864263115697 ; l2 norm of weights:  8.59810158038517\n",
            "Iteration#:  4808 ; Loss:  7.267000164333414 ; l2 norm of gradient:  1.604177804726794 ; l2 norm of weights:  8.598050849867098\n",
            "Iteration#:  4812 ; Loss:  7.266912088812393 ; l2 norm of gradient:  1.6041691831380003 ; l2 norm of weights:  8.59800011974382\n",
            "Iteration#:  4816 ; Loss:  7.266824007973417 ; l2 norm of gradient:  1.6041605615451846 ; l2 norm of weights:  8.59794939001534\n",
            "Iteration#:  4820 ; Loss:  7.266735924770771 ; l2 norm of gradient:  1.6041519399483457 ; l2 norm of weights:  8.597898660681656\n",
            "Iteration#:  4824 ; Loss:  7.266647850127061 ; l2 norm of gradient:  1.6041433183474796 ; l2 norm of weights:  8.597847931742768\n",
            "Iteration#:  4828 ; Loss:  7.2665597702162135 ; l2 norm of gradient:  1.604134696742584 ; l2 norm of weights:  8.597797203198676\n",
            "Iteration#:  4832 ; Loss:  7.266471696108715 ; l2 norm of gradient:  1.6041260751336566 ; l2 norm of weights:  8.597746475049373\n",
            "Iteration#:  4836 ; Loss:  7.266383621240358 ; l2 norm of gradient:  1.604117453520695 ; l2 norm of weights:  8.597695747294868\n",
            "Iteration#:  4840 ; Loss:  7.266295542001365 ; l2 norm of gradient:  1.6041088319036951 ; l2 norm of weights:  8.59764501993515\n",
            "Iteration#:  4844 ; Loss:  7.266207467404014 ; l2 norm of gradient:  1.604100210282656 ; l2 norm of weights:  8.597594292970227\n",
            "Iteration#:  4848 ; Loss:  7.2661193915708555 ; l2 norm of gradient:  1.6040915886575744 ; l2 norm of weights:  8.597543566400093\n",
            "Iteration#:  4852 ; Loss:  7.266031319296515 ; l2 norm of gradient:  1.6040829670284475 ; l2 norm of weights:  8.597492840224747\n",
            "Iteration#:  4856 ; Loss:  7.265943243746527 ; l2 norm of gradient:  1.6040743453952735 ; l2 norm of weights:  8.597442114444192\n",
            "Iteration#:  4860 ; Loss:  7.2658551688702415 ; l2 norm of gradient:  1.604065723758048 ; l2 norm of weights:  8.597391389058423\n",
            "Iteration#:  4864 ; Loss:  7.265767096048922 ; l2 norm of gradient:  1.6040571021167704 ; l2 norm of weights:  8.597340664067444\n",
            "Iteration#:  4868 ; Loss:  7.265679026039331 ; l2 norm of gradient:  1.6040484804714368 ; l2 norm of weights:  8.59728993947125\n",
            "Iteration#:  4872 ; Loss:  7.265590953825168 ; l2 norm of gradient:  1.6040398588220444 ; l2 norm of weights:  8.597239215269841\n",
            "Iteration#:  4876 ; Loss:  7.265502880563869 ; l2 norm of gradient:  1.6040312371685925 ; l2 norm of weights:  8.597188491463216\n",
            "Iteration#:  4880 ; Loss:  7.265414806345701 ; l2 norm of gradient:  1.6040226155110755 ; l2 norm of weights:  8.597137768051377\n",
            "Iteration#:  4884 ; Loss:  7.26532673832579 ; l2 norm of gradient:  1.6040139938494928 ; l2 norm of weights:  8.59708704503432\n",
            "Iteration#:  4888 ; Loss:  7.265238666665391 ; l2 norm of gradient:  1.6040053721838414 ; l2 norm of weights:  8.597036322412045\n",
            "Iteration#:  4892 ; Loss:  7.265150599165018 ; l2 norm of gradient:  1.6039967505141184 ; l2 norm of weights:  8.596985600184551\n",
            "Iteration#:  4896 ; Loss:  7.2650625299256255 ; l2 norm of gradient:  1.6039881288403208 ; l2 norm of weights:  8.596934878351838\n",
            "Iteration#:  4900 ; Loss:  7.264974460029941 ; l2 norm of gradient:  1.603979507162447 ; l2 norm of weights:  8.596884156913907\n",
            "Iteration#:  4904 ; Loss:  7.264886394646736 ; l2 norm of gradient:  1.6039708854804935 ; l2 norm of weights:  8.596833435870751\n",
            "Iteration#:  4908 ; Loss:  7.264798326265963 ; l2 norm of gradient:  1.6039622637944575 ; l2 norm of weights:  8.596782715222377\n",
            "Iteration#:  4912 ; Loss:  7.264710256368148 ; l2 norm of gradient:  1.6039536421043372 ; l2 norm of weights:  8.59673199496878\n",
            "Iteration#:  4916 ; Loss:  7.264622195650615 ; l2 norm of gradient:  1.6039450204101295 ; l2 norm of weights:  8.596681275109958\n",
            "Iteration#:  4920 ; Loss:  7.264534125714592 ; l2 norm of gradient:  1.6039363987118316 ; l2 norm of weights:  8.596630555645913\n",
            "Iteration#:  4924 ; Loss:  7.264446064890812 ; l2 norm of gradient:  1.6039277770094416 ; l2 norm of weights:  8.596579836576643\n",
            "Iteration#:  4928 ; Loss:  7.264357999464415 ; l2 norm of gradient:  1.603919155302955 ; l2 norm of weights:  8.596529117902147\n",
            "Iteration#:  4932 ; Loss:  7.264269936463176 ; l2 norm of gradient:  1.6039105335923718 ; l2 norm of weights:  8.596478399622425\n",
            "Iteration#:  4936 ; Loss:  7.264181872000612 ; l2 norm of gradient:  1.6039019118776876 ; l2 norm of weights:  8.596427681737474\n",
            "Iteration#:  4940 ; Loss:  7.2640938070587495 ; l2 norm of gradient:  1.6038932901588996 ; l2 norm of weights:  8.596376964247295\n",
            "Iteration#:  4944 ; Loss:  7.264005745979312 ; l2 norm of gradient:  1.6038846684360064 ; l2 norm of weights:  8.59632624715189\n",
            "Iteration#:  4948 ; Loss:  7.263917681628021 ; l2 norm of gradient:  1.6038760467090043 ; l2 norm of weights:  8.596275530451251\n",
            "Iteration#:  4952 ; Loss:  7.2638296223033025 ; l2 norm of gradient:  1.6038674249778906 ; l2 norm of weights:  8.596224814145385\n",
            "Iteration#:  4956 ; Loss:  7.263741564011072 ; l2 norm of gradient:  1.6038588032426635 ; l2 norm of weights:  8.596174098234286\n",
            "Iteration#:  4960 ; Loss:  7.26365350056731 ; l2 norm of gradient:  1.60385018150332 ; l2 norm of weights:  8.596123382717957\n",
            "Iteration#:  4964 ; Loss:  7.263565441625461 ; l2 norm of gradient:  1.6038415597598568 ; l2 norm of weights:  8.596072667596392\n",
            "Iteration#:  4968 ; Loss:  7.26347737886782 ; l2 norm of gradient:  1.6038329380122711 ; l2 norm of weights:  8.596021952869593\n",
            "Iteration#:  4972 ; Loss:  7.263389317852384 ; l2 norm of gradient:  1.603824316260562 ; l2 norm of weights:  8.59597123853756\n",
            "Iteration#:  4976 ; Loss:  7.263301263154217 ; l2 norm of gradient:  1.6038156945047262 ; l2 norm of weights:  8.595920524600293\n",
            "Iteration#:  4980 ; Loss:  7.263213206238618 ; l2 norm of gradient:  1.6038070727447593 ; l2 norm of weights:  8.59586981105779\n",
            "Iteration#:  4984 ; Loss:  7.263125146777615 ; l2 norm of gradient:  1.6037984509806604 ; l2 norm of weights:  8.595819097910049\n",
            "Iteration#:  4988 ; Loss:  7.263037089960495 ; l2 norm of gradient:  1.6037898292124264 ; l2 norm of weights:  8.59576838515707\n",
            "Iteration#:  4992 ; Loss:  7.262949031002357 ; l2 norm of gradient:  1.603781207440054 ; l2 norm of weights:  8.595717672798852\n",
            "Iteration#:  4996 ; Loss:  7.2628609798599335 ; l2 norm of gradient:  1.603772585663542 ; l2 norm of weights:  8.595666960835397\n",
            "Iteration#:  5000 ; Loss:  7.2627729212799 ; l2 norm of gradient:  1.603763963882886 ; l2 norm of weights:  8.595616249266698\n",
            "Iteration#:  5004 ; Loss:  7.262684870136642 ; l2 norm of gradient:  1.6037553420980852 ; l2 norm of weights:  8.59556553809276\n",
            "Iteration#:  5008 ; Loss:  7.262596816512925 ; l2 norm of gradient:  1.603746720309135 ; l2 norm of weights:  8.595514827313579\n",
            "Iteration#:  5012 ; Loss:  7.2625087571994875 ; l2 norm of gradient:  1.6037380985160334 ; l2 norm of weights:  8.595464116929156\n",
            "Iteration#:  5016 ; Loss:  7.262420705473033 ; l2 norm of gradient:  1.6037294767187795 ; l2 norm of weights:  8.595413406939489\n",
            "Iteration#:  5020 ; Loss:  7.262332652242414 ; l2 norm of gradient:  1.603720854917368 ; l2 norm of weights:  8.595362697344578\n",
            "Iteration#:  5024 ; Loss:  7.262244596916967 ; l2 norm of gradient:  1.6037122331117972 ; l2 norm of weights:  8.595311988144424\n",
            "Iteration#:  5028 ; Loss:  7.262156549010147 ; l2 norm of gradient:  1.603703611302065 ; l2 norm of weights:  8.59526127933902\n",
            "Iteration#:  5032 ; Loss:  7.262068496321858 ; l2 norm of gradient:  1.603694989488169 ; l2 norm of weights:  8.595210570928373\n",
            "Iteration#:  5036 ; Loss:  7.261980444666973 ; l2 norm of gradient:  1.6036863676701045 ; l2 norm of weights:  8.595159862912476\n",
            "Iteration#:  5040 ; Loss:  7.261892397969751 ; l2 norm of gradient:  1.6036777458478708 ; l2 norm of weights:  8.59510915529133\n",
            "Iteration#:  5044 ; Loss:  7.261804347791511 ; l2 norm of gradient:  1.6036691240214633 ; l2 norm of weights:  8.595058448064936\n",
            "Iteration#:  5048 ; Loss:  7.2617162969777205 ; l2 norm of gradient:  1.6036605021908825 ; l2 norm of weights:  8.595007741233292\n",
            "Iteration#:  5052 ; Loss:  7.261628251197569 ; l2 norm of gradient:  1.603651880356123 ; l2 norm of weights:  8.594957034796398\n",
            "Iteration#:  5056 ; Loss:  7.261540202621795 ; l2 norm of gradient:  1.6036432585171838 ; l2 norm of weights:  8.594906328754254\n",
            "Iteration#:  5060 ; Loss:  7.261452155230039 ; l2 norm of gradient:  1.6036346366740604 ; l2 norm of weights:  8.594855623106856\n",
            "Iteration#:  5064 ; Loss:  7.261364107764394 ; l2 norm of gradient:  1.6036260148267523 ; l2 norm of weights:  8.594804917854203\n",
            "Iteration#:  5068 ; Loss:  7.2612760568636405 ; l2 norm of gradient:  1.603617392975255 ; l2 norm of weights:  8.594754212996298\n",
            "Iteration#:  5072 ; Loss:  7.2611880135560405 ; l2 norm of gradient:  1.603608771119565 ; l2 norm of weights:  8.594703508533138\n",
            "Iteration#:  5076 ; Loss:  7.261099969081558 ; l2 norm of gradient:  1.603600149259683 ; l2 norm of weights:  8.594652804464722\n",
            "Iteration#:  5080 ; Loss:  7.261011921607145 ; l2 norm of gradient:  1.6035915273956034 ; l2 norm of weights:  8.59460210079105\n",
            "Iteration#:  5084 ; Loss:  7.2609238803109974 ; l2 norm of gradient:  1.6035829055273256 ; l2 norm of weights:  8.59455139751212\n",
            "Iteration#:  5088 ; Loss:  7.260835831877506 ; l2 norm of gradient:  1.6035742836548446 ; l2 norm of weights:  8.59450069462793\n",
            "Iteration#:  5092 ; Loss:  7.26074779094829 ; l2 norm of gradient:  1.6035656617781595 ; l2 norm of weights:  8.594449992138486\n",
            "Iteration#:  5096 ; Loss:  7.260659750168707 ; l2 norm of gradient:  1.6035570398972674 ; l2 norm of weights:  8.59439929004378\n",
            "Iteration#:  5100 ; Loss:  7.260571704127544 ; l2 norm of gradient:  1.6035484180121644 ; l2 norm of weights:  8.594348588343813\n",
            "Iteration#:  5104 ; Loss:  7.260483664192489 ; l2 norm of gradient:  1.6035397961228504 ; l2 norm of weights:  8.594297887038586\n",
            "Iteration#:  5108 ; Loss:  7.260395623375887 ; l2 norm of gradient:  1.6035311742293195 ; l2 norm of weights:  8.594247186128094\n",
            "Iteration#:  5112 ; Loss:  7.260307579651347 ; l2 norm of gradient:  1.603522552331571 ; l2 norm of weights:  8.59419648561234\n",
            "Iteration#:  5116 ; Loss:  7.260219541986132 ; l2 norm of gradient:  1.6035139304296016 ; l2 norm of weights:  8.594145785491325\n",
            "Iteration#:  5120 ; Loss:  7.260131502025995 ; l2 norm of gradient:  1.6035053085234088 ; l2 norm of weights:  8.594095085765042\n",
            "Iteration#:  5124 ; Loss:  7.260043465302822 ; l2 norm of gradient:  1.6034966866129896 ; l2 norm of weights:  8.594044386433497\n",
            "Iteration#:  5128 ; Loss:  7.259955426693659 ; l2 norm of gradient:  1.6034880646983418 ; l2 norm of weights:  8.593993687496683\n",
            "Iteration#:  5132 ; Loss:  7.259867387245409 ; l2 norm of gradient:  1.6034794427794627 ; l2 norm of weights:  8.593942988954606\n",
            "Iteration#:  5136 ; Loss:  7.259779350261526 ; l2 norm of gradient:  1.6034708208563497 ; l2 norm of weights:  8.593892290807258\n",
            "Iteration#:  5140 ; Loss:  7.259691311929934 ; l2 norm of gradient:  1.6034621989289992 ; l2 norm of weights:  8.593841593054643\n",
            "Iteration#:  5144 ; Loss:  7.259603275658637 ; l2 norm of gradient:  1.6034535769974094 ; l2 norm of weights:  8.593790895696758\n",
            "Iteration#:  5148 ; Loss:  7.25951523955274 ; l2 norm of gradient:  1.6034449550615768 ; l2 norm of weights:  8.5937401987336\n",
            "Iteration#:  5152 ; Loss:  7.259427199116085 ; l2 norm of gradient:  1.6034363331214998 ; l2 norm of weights:  8.593689502165175\n",
            "Iteration#:  5156 ; Loss:  7.25933916835478 ; l2 norm of gradient:  1.6034277111771758 ; l2 norm of weights:  8.593638805991478\n",
            "Iteration#:  5160 ; Loss:  7.259251134263018 ; l2 norm of gradient:  1.6034190892286004 ; l2 norm of weights:  8.593588110212506\n",
            "Iteration#:  5164 ; Loss:  7.259163099444895 ; l2 norm of gradient:  1.6034104672757727 ; l2 norm of weights:  8.593537414828264\n",
            "Iteration#:  5168 ; Loss:  7.259075067339928 ; l2 norm of gradient:  1.6034018453186887 ; l2 norm of weights:  8.593486719838747\n",
            "Iteration#:  5172 ; Loss:  7.258987032717455 ; l2 norm of gradient:  1.603393223357346 ; l2 norm of weights:  8.593436025243953\n",
            "Iteration#:  5176 ; Loss:  7.258898999401433 ; l2 norm of gradient:  1.6033846013917423 ; l2 norm of weights:  8.593385331043885\n",
            "Iteration#:  5180 ; Loss:  7.25881097060557 ; l2 norm of gradient:  1.6033759794218745 ; l2 norm of weights:  8.59333463723854\n",
            "Iteration#:  5184 ; Loss:  7.258722936367839 ; l2 norm of gradient:  1.603367357447741 ; l2 norm of weights:  8.593283943827917\n",
            "Iteration#:  5188 ; Loss:  7.258634906533599 ; l2 norm of gradient:  1.6033587354693386 ; l2 norm of weights:  8.593233250812016\n",
            "Iteration#:  5192 ; Loss:  7.258546876803136 ; l2 norm of gradient:  1.603350113486663 ; l2 norm of weights:  8.593182558190835\n",
            "Iteration#:  5196 ; Loss:  7.258458841850027 ; l2 norm of gradient:  1.6033414914997126 ; l2 norm of weights:  8.593131865964375\n",
            "Iteration#:  5200 ; Loss:  7.258370811214379 ; l2 norm of gradient:  1.6033328695084854 ; l2 norm of weights:  8.593081174132635\n",
            "Iteration#:  5204 ; Loss:  7.258282788160733 ; l2 norm of gradient:  1.6033242475129785 ; l2 norm of weights:  8.593030482695614\n",
            "Iteration#:  5208 ; Loss:  7.258194755690246 ; l2 norm of gradient:  1.6033156255131877 ; l2 norm of weights:  8.59297979165331\n",
            "Iteration#:  5212 ; Loss:  7.258106729365774 ; l2 norm of gradient:  1.6033070035091128 ; l2 norm of weights:  8.592929101005721\n",
            "Iteration#:  5216 ; Loss:  7.258018704218809 ; l2 norm of gradient:  1.6032983815007487 ; l2 norm of weights:  8.592878410752851\n",
            "Iteration#:  5220 ; Loss:  7.257930673706861 ; l2 norm of gradient:  1.6032897594880946 ; l2 norm of weights:  8.592827720894695\n",
            "Iteration#:  5224 ; Loss:  7.257842651273565 ; l2 norm of gradient:  1.6032811374711462 ; l2 norm of weights:  8.592777031431254\n",
            "Iteration#:  5228 ; Loss:  7.257754626410077 ; l2 norm of gradient:  1.6032725154499015 ; l2 norm of weights:  8.592726342362527\n",
            "Iteration#:  5232 ; Loss:  7.2576665944488985 ; l2 norm of gradient:  1.603263893424358 ; l2 norm of weights:  8.592675653688511\n",
            "Iteration#:  5236 ; Loss:  7.2575785744752 ; l2 norm of gradient:  1.6032552713945116 ; l2 norm of weights:  8.59262496540921\n",
            "Iteration#:  5240 ; Loss:  7.257490551300005 ; l2 norm of gradient:  1.6032466493603619 ; l2 norm of weights:  8.592574277524617\n",
            "Iteration#:  5244 ; Loss:  7.257402525946917 ; l2 norm of gradient:  1.6032380273219056 ; l2 norm of weights:  8.592523590034736\n",
            "Iteration#:  5248 ; Loss:  7.257314506860276 ; l2 norm of gradient:  1.6032294052791383 ; l2 norm of weights:  8.592472902939564\n",
            "Iteration#:  5252 ; Loss:  7.257226479945321 ; l2 norm of gradient:  1.6032207832320582 ; l2 norm of weights:  8.592422216239104\n",
            "Iteration#:  5256 ; Loss:  7.2571384558725125 ; l2 norm of gradient:  1.6032121611806645 ; l2 norm of weights:  8.592371529933347\n",
            "Iteration#:  5260 ; Loss:  7.25705043696372 ; l2 norm of gradient:  1.6032035391249513 ; l2 norm of weights:  8.5923208440223\n",
            "Iteration#:  5264 ; Loss:  7.256962415205627 ; l2 norm of gradient:  1.6031949170649176 ; l2 norm of weights:  8.592270158505958\n",
            "Iteration#:  5268 ; Loss:  7.2568743923974175 ; l2 norm of gradient:  1.6031862950005606 ; l2 norm of weights:  8.592219473384324\n",
            "Iteration#:  5272 ; Loss:  7.256786376298679 ; l2 norm of gradient:  1.6031776729318779 ; l2 norm of weights:  8.592168788657393\n",
            "Iteration#:  5276 ; Loss:  7.256698351387199 ; l2 norm of gradient:  1.603169050858865 ; l2 norm of weights:  8.592118104325168\n",
            "Iteration#:  5280 ; Loss:  7.25661033463634 ; l2 norm of gradient:  1.6031604287815209 ; l2 norm of weights:  8.592067420387645\n",
            "Iteration#:  5284 ; Loss:  7.256522317198607 ; l2 norm of gradient:  1.6031518066998434 ; l2 norm of weights:  8.592016736844824\n",
            "Iteration#:  5288 ; Loss:  7.256434296267732 ; l2 norm of gradient:  1.603143184613828 ; l2 norm of weights:  8.591966053696703\n",
            "Iteration#:  5292 ; Loss:  7.256346284539912 ; l2 norm of gradient:  1.6031345625234723 ; l2 norm of weights:  8.591915370943285\n",
            "Iteration#:  5296 ; Loss:  7.2562582633801656 ; l2 norm of gradient:  1.6031259404287743 ; l2 norm of weights:  8.591864688584566\n",
            "Iteration#:  5300 ; Loss:  7.25617024819933 ; l2 norm of gradient:  1.6031173183297325 ; l2 norm of weights:  8.591814006620547\n",
            "Iteration#:  5304 ; Loss:  7.256082234383056 ; l2 norm of gradient:  1.6031086962263414 ; l2 norm of weights:  8.591763325051227\n",
            "Iteration#:  5308 ; Loss:  7.255994215040477 ; l2 norm of gradient:  1.6031000741185997 ; l2 norm of weights:  8.591712643876603\n",
            "Iteration#:  5312 ; Loss:  7.2559062019098555 ; l2 norm of gradient:  1.6030914520065052 ; l2 norm of weights:  8.591661963096676\n",
            "Iteration#:  5316 ; Loss:  7.255818189478093 ; l2 norm of gradient:  1.6030828298900548 ; l2 norm of weights:  8.591611282711446\n",
            "Iteration#:  5320 ; Loss:  7.255730173508682 ; l2 norm of gradient:  1.603074207769244 ; l2 norm of weights:  8.591560602720909\n",
            "Iteration#:  5324 ; Loss:  7.255642159186241 ; l2 norm of gradient:  1.6030655856440723 ; l2 norm of weights:  8.591509923125068\n",
            "Iteration#:  5328 ; Loss:  7.2555541470554665 ; l2 norm of gradient:  1.6030569635145366 ; l2 norm of weights:  8.59145924392392\n",
            "Iteration#:  5332 ; Loss:  7.255466135644504 ; l2 norm of gradient:  1.6030483413806338 ; l2 norm of weights:  8.591408565117463\n",
            "Iteration#:  5336 ; Loss:  7.255378122701774 ; l2 norm of gradient:  1.603039719242361 ; l2 norm of weights:  8.591357886705701\n",
            "Iteration#:  5340 ; Loss:  7.255290112665113 ; l2 norm of gradient:  1.6030310970997153 ; l2 norm of weights:  8.591307208688628\n",
            "Iteration#:  5344 ; Loss:  7.255202100401274 ; l2 norm of gradient:  1.6030224749526947 ; l2 norm of weights:  8.591256531066248\n",
            "Iteration#:  5348 ; Loss:  7.255114088780127 ; l2 norm of gradient:  1.6030138528012954 ; l2 norm of weights:  8.591205853838554\n",
            "Iteration#:  5352 ; Loss:  7.25502607875719 ; l2 norm of gradient:  1.6030052306455171 ; l2 norm of weights:  8.59115517700555\n",
            "Iteration#:  5356 ; Loss:  7.2549380697374914 ; l2 norm of gradient:  1.6029966084853542 ; l2 norm of weights:  8.591104500567237\n",
            "Iteration#:  5360 ; Loss:  7.2548500601632195 ; l2 norm of gradient:  1.6029879863208047 ; l2 norm of weights:  8.591053824523607\n",
            "Iteration#:  5364 ; Loss:  7.254762053322898 ; l2 norm of gradient:  1.602979364151867 ; l2 norm of weights:  8.591003148874663\n",
            "Iteration#:  5368 ; Loss:  7.254674045678303 ; l2 norm of gradient:  1.6029707419785366 ; l2 norm of weights:  8.590952473620405\n",
            "Iteration#:  5372 ; Loss:  7.254586038406414 ; l2 norm of gradient:  1.602962119800813 ; l2 norm of weights:  8.590901798760834\n",
            "Iteration#:  5376 ; Loss:  7.254498033307998 ; l2 norm of gradient:  1.6029534976186912 ; l2 norm of weights:  8.590851124295943\n",
            "Iteration#:  5380 ; Loss:  7.2544100266201825 ; l2 norm of gradient:  1.6029448754321707 ; l2 norm of weights:  8.590800450225737\n",
            "Iteration#:  5384 ; Loss:  7.2543220235894506 ; l2 norm of gradient:  1.6029362532412466 ; l2 norm of weights:  8.590749776550215\n",
            "Iteration#:  5388 ; Loss:  7.254234017764334 ; l2 norm of gradient:  1.602927631045917 ; l2 norm of weights:  8.59069910326937\n",
            "Iteration#:  5392 ; Loss:  7.254146010587641 ; l2 norm of gradient:  1.6029190088461795 ; l2 norm of weights:  8.590648430383208\n",
            "Iteration#:  5396 ; Loss:  7.254058008113898 ; l2 norm of gradient:  1.6029103866420311 ; l2 norm of weights:  8.590597757891727\n",
            "Iteration#:  5400 ; Loss:  7.253970004702175 ; l2 norm of gradient:  1.6029017644334678 ; l2 norm of weights:  8.590547085794924\n",
            "Iteration#:  5404 ; Loss:  7.253882002240799 ; l2 norm of gradient:  1.6028931422204906 ; l2 norm of weights:  8.590496414092799\n",
            "Iteration#:  5408 ; Loss:  7.2537940019415 ; l2 norm of gradient:  1.6028845200030928 ; l2 norm of weights:  8.59044574278535\n",
            "Iteration#:  5412 ; Loss:  7.253705997913551 ; l2 norm of gradient:  1.6028758977812738 ; l2 norm of weights:  8.590395071872578\n",
            "Iteration#:  5416 ; Loss:  7.253617993634624 ; l2 norm of gradient:  1.6028672755550295 ; l2 norm of weights:  8.590344401354482\n",
            "Iteration#:  5420 ; Loss:  7.253529992224681 ; l2 norm of gradient:  1.6028586533243572 ; l2 norm of weights:  8.590293731231062\n",
            "Iteration#:  5424 ; Loss:  7.253441991741359 ; l2 norm of gradient:  1.6028500310892557 ; l2 norm of weights:  8.590243061502317\n",
            "Iteration#:  5428 ; Loss:  7.2533539906481215 ; l2 norm of gradient:  1.6028414088497214 ; l2 norm of weights:  8.590192392168243\n",
            "Iteration#:  5432 ; Loss:  7.25326599349529 ; l2 norm of gradient:  1.602832786605751 ; l2 norm of weights:  8.590141723228841\n",
            "Iteration#:  5436 ; Loss:  7.253177993980403 ; l2 norm of gradient:  1.6028241643573435 ; l2 norm of weights:  8.590091054684112\n",
            "Iteration#:  5440 ; Loss:  7.253089995145711 ; l2 norm of gradient:  1.6028155421044932 ; l2 norm of weights:  8.590040386534055\n",
            "Iteration#:  5444 ; Loss:  7.253001991277125 ; l2 norm of gradient:  1.6028069198472004 ; l2 norm of weights:  8.589989718778666\n",
            "Iteration#:  5448 ; Loss:  7.25291399970759 ; l2 norm of gradient:  1.602798297585459 ; l2 norm of weights:  8.589939051417947\n",
            "Iteration#:  5452 ; Loss:  7.2528260023273 ; l2 norm of gradient:  1.6027896753192699 ; l2 norm of weights:  8.589888384451895\n",
            "Iteration#:  5456 ; Loss:  7.252738007614472 ; l2 norm of gradient:  1.6027810530486282 ; l2 norm of weights:  8.589837717880512\n",
            "Iteration#:  5460 ; Loss:  7.252650013583203 ; l2 norm of gradient:  1.6027724307735312 ; l2 norm of weights:  8.589787051703796\n",
            "Iteration#:  5464 ; Loss:  7.252562018622662 ; l2 norm of gradient:  1.6027638084939773 ; l2 norm of weights:  8.589736385921746\n",
            "Iteration#:  5468 ; Loss:  7.2524740236989675 ; l2 norm of gradient:  1.6027551862099623 ; l2 norm of weights:  8.589685720534362\n",
            "Iteration#:  5472 ; Loss:  7.252386028412882 ; l2 norm of gradient:  1.6027465639214842 ; l2 norm of weights:  8.58963505554164\n",
            "Iteration#:  5476 ; Loss:  7.25229802955166 ; l2 norm of gradient:  1.6027379416285406 ; l2 norm of weights:  8.589584390943582\n",
            "Iteration#:  5480 ; Loss:  7.2522100408060055 ; l2 norm of gradient:  1.6027293193311272 ; l2 norm of weights:  8.589533726740187\n",
            "Iteration#:  5484 ; Loss:  7.252122050851339 ; l2 norm of gradient:  1.6027206970292422 ; l2 norm of weights:  8.589483062931455\n",
            "Iteration#:  5488 ; Loss:  7.2520340555718015 ; l2 norm of gradient:  1.6027120747228845 ; l2 norm of weights:  8.589432399517385\n",
            "Iteration#:  5492 ; Loss:  7.25194606342969 ; l2 norm of gradient:  1.602703452412049 ; l2 norm of weights:  8.589381736497971\n",
            "Iteration#:  5496 ; Loss:  7.251858073992932 ; l2 norm of gradient:  1.6026948300967339 ; l2 norm of weights:  8.589331073873222\n",
            "Iteration#:  5500 ; Loss:  7.251770085600894 ; l2 norm of gradient:  1.6026862077769357 ; l2 norm of weights:  8.589280411643129\n",
            "Iteration#:  5504 ; Loss:  7.251682091387432 ; l2 norm of gradient:  1.6026775854526523 ; l2 norm of weights:  8.589229749807695\n",
            "Iteration#:  5508 ; Loss:  7.251594105712391 ; l2 norm of gradient:  1.6026689631238809 ; l2 norm of weights:  8.589179088366915\n",
            "Iteration#:  5512 ; Loss:  7.2515061172452615 ; l2 norm of gradient:  1.6026603407906193 ; l2 norm of weights:  8.589128427320794\n",
            "Iteration#:  5516 ; Loss:  7.251418125558905 ; l2 norm of gradient:  1.6026517184528632 ; l2 norm of weights:  8.589077766669327\n",
            "Iteration#:  5520 ; Loss:  7.251330137861711 ; l2 norm of gradient:  1.6026430961106106 ; l2 norm of weights:  8.589027106412518\n",
            "Iteration#:  5524 ; Loss:  7.25124215032641 ; l2 norm of gradient:  1.6026344737638598 ; l2 norm of weights:  8.58897644655036\n",
            "Iteration#:  5528 ; Loss:  7.25115416257362 ; l2 norm of gradient:  1.6026258514126062 ; l2 norm of weights:  8.588925787082855\n",
            "Iteration#:  5532 ; Loss:  7.251066175244075 ; l2 norm of gradient:  1.6026172290568483 ; l2 norm of weights:  8.588875128010002\n",
            "Iteration#:  5536 ; Loss:  7.250978187253816 ; l2 norm of gradient:  1.6026086066965826 ; l2 norm of weights:  8.5888244693318\n",
            "Iteration#:  5540 ; Loss:  7.250890205646586 ; l2 norm of gradient:  1.6025999843318068 ; l2 norm of weights:  8.588773811048252\n",
            "Iteration#:  5544 ; Loss:  7.250802221925852 ; l2 norm of gradient:  1.6025913619625174 ; l2 norm of weights:  8.58872315315935\n",
            "Iteration#:  5548 ; Loss:  7.250714236629461 ; l2 norm of gradient:  1.6025827395887131 ; l2 norm of weights:  8.588672495665099\n",
            "Iteration#:  5552 ; Loss:  7.2506262521127525 ; l2 norm of gradient:  1.6025741172103893 ; l2 norm of weights:  8.588621838565496\n",
            "Iteration#:  5556 ; Loss:  7.250538268056413 ; l2 norm of gradient:  1.602565494827545 ; l2 norm of weights:  8.588571181860539\n",
            "Iteration#:  5560 ; Loss:  7.250450287318595 ; l2 norm of gradient:  1.6025568724401762 ; l2 norm of weights:  8.588520525550232\n",
            "Iteration#:  5564 ; Loss:  7.250362310132907 ; l2 norm of gradient:  1.6025482500482808 ; l2 norm of weights:  8.588469869634567\n",
            "Iteration#:  5568 ; Loss:  7.250274322535628 ; l2 norm of gradient:  1.6025396276518553 ; l2 norm of weights:  8.58841921411355\n",
            "Iteration#:  5572 ; Loss:  7.250186344061845 ; l2 norm of gradient:  1.6025310052508972 ; l2 norm of weights:  8.588368558987176\n",
            "Iteration#:  5576 ; Loss:  7.250098361000667 ; l2 norm of gradient:  1.6025223828454043 ; l2 norm of weights:  8.588317904255442\n",
            "Iteration#:  5580 ; Loss:  7.250010380164694 ; l2 norm of gradient:  1.6025137604353723 ; l2 norm of weights:  8.588267249918355\n",
            "Iteration#:  5584 ; Loss:  7.249922398838889 ; l2 norm of gradient:  1.6025051380208002 ; l2 norm of weights:  8.588216595975908\n",
            "Iteration#:  5588 ; Loss:  7.249834420954703 ; l2 norm of gradient:  1.6024965156016848 ; l2 norm of weights:  8.5881659424281\n",
            "Iteration#:  5592 ; Loss:  7.24974644228531 ; l2 norm of gradient:  1.6024878931780222 ; l2 norm of weights:  8.588115289274935\n",
            "Iteration#:  5596 ; Loss:  7.249658464183479 ; l2 norm of gradient:  1.6024792707498101 ; l2 norm of weights:  8.588064636516409\n",
            "Iteration#:  5600 ; Loss:  7.249570488411456 ; l2 norm of gradient:  1.602470648317047 ; l2 norm of weights:  8.58801398415252\n",
            "Iteration#:  5604 ; Loss:  7.249482509599267 ; l2 norm of gradient:  1.6024620258797284 ; l2 norm of weights:  8.58796333218327\n",
            "Iteration#:  5608 ; Loss:  7.249394534777487 ; l2 norm of gradient:  1.6024534034378535 ; l2 norm of weights:  8.587912680608659\n",
            "Iteration#:  5612 ; Loss:  7.2493065600355 ; l2 norm of gradient:  1.6024447809914173 ; l2 norm of weights:  8.587862029428681\n",
            "Iteration#:  5616 ; Loss:  7.249218587723339 ; l2 norm of gradient:  1.6024361585404177 ; l2 norm of weights:  8.587811378643337\n",
            "Iteration#:  5620 ; Loss:  7.249130610838114 ; l2 norm of gradient:  1.6024275360848521 ; l2 norm of weights:  8.58776072825263\n",
            "Iteration#:  5624 ; Loss:  7.249042635624988 ; l2 norm of gradient:  1.6024189136247182 ; l2 norm of weights:  8.587710078256556\n",
            "Iteration#:  5628 ; Loss:  7.2489546605023705 ; l2 norm of gradient:  1.6024102911600127 ; l2 norm of weights:  8.587659428655115\n",
            "Iteration#:  5632 ; Loss:  7.248866683059541 ; l2 norm of gradient:  1.602401668690733 ; l2 norm of weights:  8.587608779448304\n",
            "Iteration#:  5636 ; Loss:  7.248778711383869 ; l2 norm of gradient:  1.6023930462168758 ; l2 norm of weights:  8.587558130636127\n",
            "Iteration#:  5640 ; Loss:  7.248690740762623 ; l2 norm of gradient:  1.602384423738439 ; l2 norm of weights:  8.587507482218577\n",
            "Iteration#:  5644 ; Loss:  7.248602767543093 ; l2 norm of gradient:  1.6023758012554192 ; l2 norm of weights:  8.58745683419566\n",
            "Iteration#:  5648 ; Loss:  7.248514796788099 ; l2 norm of gradient:  1.6023671787678138 ; l2 norm of weights:  8.58740618656737\n",
            "Iteration#:  5652 ; Loss:  7.2484268210468095 ; l2 norm of gradient:  1.6023585562756208 ; l2 norm of weights:  8.587355539333707\n",
            "Iteration#:  5656 ; Loss:  7.248338854422562 ; l2 norm of gradient:  1.6023499337788352 ; l2 norm of weights:  8.587304892494673\n",
            "Iteration#:  5660 ; Loss:  7.248250886662207 ; l2 norm of gradient:  1.602341311277458 ; l2 norm of weights:  8.587254246050266\n",
            "Iteration#:  5664 ; Loss:  7.248162912938138 ; l2 norm of gradient:  1.6023326887714817 ; l2 norm of weights:  8.587203600000484\n",
            "Iteration#:  5668 ; Loss:  7.2480749474562165 ; l2 norm of gradient:  1.602324066260908 ; l2 norm of weights:  8.587152954345326\n",
            "Iteration#:  5672 ; Loss:  7.247986977878128 ; l2 norm of gradient:  1.6023154437457303 ; l2 norm of weights:  8.587102309084791\n",
            "Iteration#:  5676 ; Loss:  7.247899013462819 ; l2 norm of gradient:  1.6023068212259477 ; l2 norm of weights:  8.58705166421888\n",
            "Iteration#:  5680 ; Loss:  7.247811043080855 ; l2 norm of gradient:  1.6022981987015583 ; l2 norm of weights:  8.587001019747591\n",
            "Iteration#:  5684 ; Loss:  7.247723074860046 ; l2 norm of gradient:  1.6022895761725575 ; l2 norm of weights:  8.586950375670924\n",
            "Iteration#:  5688 ; Loss:  7.247635114271556 ; l2 norm of gradient:  1.6022809536389437 ; l2 norm of weights:  8.58689973198888\n",
            "Iteration#:  5692 ; Loss:  7.24754714798523 ; l2 norm of gradient:  1.6022723311007119 ; l2 norm of weights:  8.58684908870145\n",
            "Iteration#:  5696 ; Loss:  7.24745918227587 ; l2 norm of gradient:  1.6022637085578624 ; l2 norm of weights:  8.586798445808641\n",
            "Iteration#:  5700 ; Loss:  7.247371214201404 ; l2 norm of gradient:  1.6022550860103908 ; l2 norm of weights:  8.586747803310454\n",
            "Iteration#:  5704 ; Loss:  7.247283252919669 ; l2 norm of gradient:  1.602246463458294 ; l2 norm of weights:  8.58669716120688\n",
            "Iteration#:  5708 ; Loss:  7.247195291601077 ; l2 norm of gradient:  1.6022378409015698 ; l2 norm of weights:  8.586646519497926\n",
            "Iteration#:  5712 ; Loss:  7.247107324225691 ; l2 norm of gradient:  1.602229218340215 ; l2 norm of weights:  8.586595878183585\n",
            "Iteration#:  5716 ; Loss:  7.247019361723748 ; l2 norm of gradient:  1.6022205957742273 ; l2 norm of weights:  8.586545237263861\n",
            "Iteration#:  5720 ; Loss:  7.246931396680186 ; l2 norm of gradient:  1.6022119732036035 ; l2 norm of weights:  8.58649459673875\n",
            "Iteration#:  5724 ; Loss:  7.246843437212505 ; l2 norm of gradient:  1.6022033506283406 ; l2 norm of weights:  8.586443956608253\n",
            "Iteration#:  5728 ; Loss:  7.246755479751092 ; l2 norm of gradient:  1.602194728048437 ; l2 norm of weights:  8.58639331687237\n",
            "Iteration#:  5732 ; Loss:  7.246667516424596 ; l2 norm of gradient:  1.6021861054638875 ; l2 norm of weights:  8.586342677531096\n",
            "Iteration#:  5736 ; Loss:  7.246579559367194 ; l2 norm of gradient:  1.6021774828746902 ; l2 norm of weights:  8.586292038584434\n",
            "Iteration#:  5740 ; Loss:  7.246491598917314 ; l2 norm of gradient:  1.6021688602808444 ; l2 norm of weights:  8.586241400032382\n",
            "Iteration#:  5744 ; Loss:  7.24640364372199 ; l2 norm of gradient:  1.6021602376823463 ; l2 norm of weights:  8.58619076187494\n",
            "Iteration#:  5748 ; Loss:  7.246315681690666 ; l2 norm of gradient:  1.6021516150791906 ; l2 norm of weights:  8.586140124112104\n",
            "Iteration#:  5752 ; Loss:  7.246227723431291 ; l2 norm of gradient:  1.6021429924713775 ; l2 norm of weights:  8.586089486743878\n",
            "Iteration#:  5756 ; Loss:  7.246139764783535 ; l2 norm of gradient:  1.6021343698589028 ; l2 norm of weights:  8.586038849770258\n",
            "Iteration#:  5760 ; Loss:  7.246051808004999 ; l2 norm of gradient:  1.6021257472417636 ; l2 norm of weights:  8.585988213191245\n",
            "Iteration#:  5764 ; Loss:  7.245963848821335 ; l2 norm of gradient:  1.6021171246199581 ; l2 norm of weights:  8.585937577006836\n",
            "Iteration#:  5768 ; Loss:  7.245875899112386 ; l2 norm of gradient:  1.6021085019934818 ; l2 norm of weights:  8.585886941217034\n",
            "Iteration#:  5772 ; Loss:  7.245787941727404 ; l2 norm of gradient:  1.6020998793623331 ; l2 norm of weights:  8.585836305821832\n",
            "Iteration#:  5776 ; Loss:  7.245699986786118 ; l2 norm of gradient:  1.6020912567265093 ; l2 norm of weights:  8.585785670821236\n",
            "Iteration#:  5780 ; Loss:  7.245612033513508 ; l2 norm of gradient:  1.6020826340860068 ; l2 norm of weights:  8.58573503621524\n",
            "Iteration#:  5784 ; Loss:  7.2455240762461175 ; l2 norm of gradient:  1.6020740114408236 ; l2 norm of weights:  8.585684402003846\n",
            "Iteration#:  5788 ; Loss:  7.2454361229865665 ; l2 norm of gradient:  1.6020653887909553 ; l2 norm of weights:  8.585633768187053\n",
            "Iteration#:  5792 ; Loss:  7.245348171973324 ; l2 norm of gradient:  1.6020567661364016 ; l2 norm of weights:  8.585583134764859\n",
            "Iteration#:  5796 ; Loss:  7.245260219657498 ; l2 norm of gradient:  1.6020481434771578 ; l2 norm of weights:  8.585532501737262\n",
            "Iteration#:  5800 ; Loss:  7.245172267834706 ; l2 norm of gradient:  1.6020395208132208 ; l2 norm of weights:  8.585481869104266\n",
            "Iteration#:  5804 ; Loss:  7.245084320917479 ; l2 norm of gradient:  1.6020308981445892 ; l2 norm of weights:  8.585431236865865\n",
            "Iteration#:  5808 ; Loss:  7.244996367993052 ; l2 norm of gradient:  1.60202227547126 ; l2 norm of weights:  8.585380605022062\n",
            "Iteration#:  5812 ; Loss:  7.24490841671717 ; l2 norm of gradient:  1.6020136527932285 ; l2 norm of weights:  8.585329973572854\n",
            "Iteration#:  5816 ; Loss:  7.244820468537304 ; l2 norm of gradient:  1.6020050301104947 ; l2 norm of weights:  8.58527934251824\n",
            "Iteration#:  5820 ; Loss:  7.244732517975937 ; l2 norm of gradient:  1.6019964074230533 ; l2 norm of weights:  8.585228711858221\n",
            "Iteration#:  5824 ; Loss:  7.244644572935888 ; l2 norm of gradient:  1.6019877847309028 ; l2 norm of weights:  8.585178081592792\n",
            "Iteration#:  5828 ; Loss:  7.244556620088224 ; l2 norm of gradient:  1.6019791620340398 ; l2 norm of weights:  8.58512745172196\n",
            "Iteration#:  5832 ; Loss:  7.24446867552111 ; l2 norm of gradient:  1.6019705393324617 ; l2 norm of weights:  8.585076822245718\n",
            "Iteration#:  5836 ; Loss:  7.244380730464203 ; l2 norm of gradient:  1.6019619166261656 ; l2 norm of weights:  8.585026193164065\n",
            "Iteration#:  5840 ; Loss:  7.244292781900098 ; l2 norm of gradient:  1.6019532939151484 ; l2 norm of weights:  8.584975564477002\n",
            "Iteration#:  5844 ; Loss:  7.244204834267347 ; l2 norm of gradient:  1.6019446711994072 ; l2 norm of weights:  8.584924936184528\n",
            "Iteration#:  5848 ; Loss:  7.244116890478129 ; l2 norm of gradient:  1.6019360484789407 ; l2 norm of weights:  8.584874308286645\n",
            "Iteration#:  5852 ; Loss:  7.244028946265748 ; l2 norm of gradient:  1.6019274257537441 ; l2 norm of weights:  8.584823680783346\n",
            "Iteration#:  5856 ; Loss:  7.24394100190993 ; l2 norm of gradient:  1.6019188030238156 ; l2 norm of weights:  8.584773053674637\n",
            "Iteration#:  5860 ; Loss:  7.243853054732693 ; l2 norm of gradient:  1.6019101802891516 ; l2 norm of weights:  8.58472242696051\n",
            "Iteration#:  5864 ; Loss:  7.243765113236979 ; l2 norm of gradient:  1.6019015575497508 ; l2 norm of weights:  8.58467180064097\n",
            "Iteration#:  5868 ; Loss:  7.243677168896584 ; l2 norm of gradient:  1.6018929348056081 ; l2 norm of weights:  8.584621174716014\n",
            "Iteration#:  5872 ; Loss:  7.243589225477242 ; l2 norm of gradient:  1.6018843120567228 ; l2 norm of weights:  8.584570549185642\n",
            "Iteration#:  5876 ; Loss:  7.243501282389694 ; l2 norm of gradient:  1.6018756893030908 ; l2 norm of weights:  8.584519924049852\n",
            "Iteration#:  5880 ; Loss:  7.243413347273284 ; l2 norm of gradient:  1.6018670665447097 ; l2 norm of weights:  8.584469299308644\n",
            "Iteration#:  5884 ; Loss:  7.243325405233884 ; l2 norm of gradient:  1.6018584437815757 ; l2 norm of weights:  8.584418674962015\n",
            "Iteration#:  5888 ; Loss:  7.2432374609296355 ; l2 norm of gradient:  1.6018498210136873 ; l2 norm of weights:  8.58436805100997\n",
            "Iteration#:  5892 ; Loss:  7.243149525348301 ; l2 norm of gradient:  1.6018411982410408 ; l2 norm of weights:  8.5843174274525\n",
            "Iteration#:  5896 ; Loss:  7.243061586168305 ; l2 norm of gradient:  1.601832575463634 ; l2 norm of weights:  8.584266804289612\n",
            "Iteration#:  5900 ; Loss:  7.242973646243367 ; l2 norm of gradient:  1.601823952681464 ; l2 norm of weights:  8.5842161815213\n",
            "Iteration#:  5904 ; Loss:  7.242885711205158 ; l2 norm of gradient:  1.601815329894527 ; l2 norm of weights:  8.584165559147566\n",
            "Iteration#:  5908 ; Loss:  7.242797769787948 ; l2 norm of gradient:  1.6018067071028208 ; l2 norm of weights:  8.584114937168406\n",
            "Iteration#:  5912 ; Loss:  7.242709835810347 ; l2 norm of gradient:  1.601798084306343 ; l2 norm of weights:  8.584064315583822\n",
            "Iteration#:  5916 ; Loss:  7.242621896607605 ; l2 norm of gradient:  1.6017894615050896 ; l2 norm of weights:  8.584013694393814\n",
            "Iteration#:  5920 ; Loss:  7.242533962057075 ; l2 norm of gradient:  1.60178083869906 ; l2 norm of weights:  8.58396307359838\n",
            "Iteration#:  5924 ; Loss:  7.242446025513654 ; l2 norm of gradient:  1.6017722158882475 ; l2 norm of weights:  8.583912453197517\n",
            "Iteration#:  5928 ; Loss:  7.242358089034008 ; l2 norm of gradient:  1.6017635930726533 ; l2 norm of weights:  8.583861833191225\n",
            "Iteration#:  5932 ; Loss:  7.242270154466007 ; l2 norm of gradient:  1.6017549702522715 ; l2 norm of weights:  8.583811213579505\n",
            "Iteration#:  5936 ; Loss:  7.242182218524661 ; l2 norm of gradient:  1.6017463474271012 ; l2 norm of weights:  8.583760594362356\n",
            "Iteration#:  5940 ; Loss:  7.242094287149968 ; l2 norm of gradient:  1.6017377245971387 ; l2 norm of weights:  8.583709975539778\n",
            "Iteration#:  5944 ; Loss:  7.24200635544498 ; l2 norm of gradient:  1.6017291017623818 ; l2 norm of weights:  8.583659357111767\n",
            "Iteration#:  5948 ; Loss:  7.241918421724229 ; l2 norm of gradient:  1.601720478922826 ; l2 norm of weights:  8.583608739078324\n",
            "Iteration#:  5952 ; Loss:  7.241830488519202 ; l2 norm of gradient:  1.6017118560784707 ; l2 norm of weights:  8.583558121439449\n",
            "Iteration#:  5956 ; Loss:  7.241742557714261 ; l2 norm of gradient:  1.6017032332293102 ; l2 norm of weights:  8.58350750419514\n",
            "Iteration#:  5960 ; Loss:  7.2416546265556665 ; l2 norm of gradient:  1.6016946103753449 ; l2 norm of weights:  8.583456887345394\n",
            "Iteration#:  5964 ; Loss:  7.241566696403575 ; l2 norm of gradient:  1.6016859875165705 ; l2 norm of weights:  8.583406270890217\n",
            "Iteration#:  5968 ; Loss:  7.241478767058098 ; l2 norm of gradient:  1.6016773646529827 ; l2 norm of weights:  8.583355654829603\n",
            "Iteration#:  5972 ; Loss:  7.241390835588275 ; l2 norm of gradient:  1.6016687417845803 ; l2 norm of weights:  8.58330503916355\n",
            "Iteration#:  5976 ; Loss:  7.241302910213575 ; l2 norm of gradient:  1.6016601189113615 ; l2 norm of weights:  8.58325442389206\n",
            "Iteration#:  5980 ; Loss:  7.241214981098213 ; l2 norm of gradient:  1.6016514960333201 ; l2 norm of weights:  8.583203809015131\n",
            "Iteration#:  5984 ; Loss:  7.2411270528763465 ; l2 norm of gradient:  1.6016428731504564 ; l2 norm of weights:  8.583153194532763\n",
            "Iteration#:  5988 ; Loss:  7.241039121803989 ; l2 norm of gradient:  1.6016342502627654 ; l2 norm of weights:  8.583102580444955\n",
            "Iteration#:  5992 ; Loss:  7.240951198410068 ; l2 norm of gradient:  1.6016256273702447 ; l2 norm of weights:  8.583051966751707\n",
            "Iteration#:  5996 ; Loss:  7.240863275541493 ; l2 norm of gradient:  1.6016170044728928 ; l2 norm of weights:  8.583001353453017\n",
            "Iteration#:  6000 ; Loss:  7.240775344474189 ; l2 norm of gradient:  1.6016083815707056 ; l2 norm of weights:  8.582950740548885\n",
            "Iteration#:  6004 ; Loss:  7.240687417616636 ; l2 norm of gradient:  1.6015997586636799 ; l2 norm of weights:  8.582900128039308\n",
            "Iteration#:  6008 ; Loss:  7.24059949369722 ; l2 norm of gradient:  1.6015911357518138 ; l2 norm of weights:  8.582849515924288\n",
            "Iteration#:  6012 ; Loss:  7.240511571845806 ; l2 norm of gradient:  1.6015825128351044 ; l2 norm of weights:  8.582798904203822\n",
            "Iteration#:  6016 ; Loss:  7.240423648562562 ; l2 norm of gradient:  1.601573889913548 ; l2 norm of weights:  8.58274829287791\n",
            "Iteration#:  6020 ; Loss:  7.240335722984897 ; l2 norm of gradient:  1.6015652669871416 ; l2 norm of weights:  8.582697681946552\n",
            "Iteration#:  6024 ; Loss:  7.240247804709838 ; l2 norm of gradient:  1.6015566440558837 ; l2 norm of weights:  8.582647071409745\n",
            "Iteration#:  6028 ; Loss:  7.240159879254409 ; l2 norm of gradient:  1.6015480211197701 ; l2 norm of weights:  8.58259646126749\n",
            "Iteration#:  6032 ; Loss:  7.2400719538789975 ; l2 norm of gradient:  1.601539398178798 ; l2 norm of weights:  8.58254585151979\n",
            "Iteration#:  6036 ; Loss:  7.239984034646442 ; l2 norm of gradient:  1.6015307752329664 ; l2 norm of weights:  8.582495242166637\n",
            "Iteration#:  6040 ; Loss:  7.239896119935249 ; l2 norm of gradient:  1.60152215228227 ; l2 norm of weights:  8.582444633208032\n",
            "Iteration#:  6044 ; Loss:  7.239808198464836 ; l2 norm of gradient:  1.6015135293267069 ; l2 norm of weights:  8.582394024643978\n",
            "Iteration#:  6048 ; Loss:  7.239720279723534 ; l2 norm of gradient:  1.6015049063662736 ; l2 norm of weights:  8.58234341647447\n",
            "Iteration#:  6052 ; Loss:  7.239632360169519 ; l2 norm of gradient:  1.6014962834009687 ; l2 norm of weights:  8.582292808699508\n",
            "Iteration#:  6056 ; Loss:  7.2395444414774754 ; l2 norm of gradient:  1.6014876604307875 ; l2 norm of weights:  8.582242201319094\n",
            "Iteration#:  6060 ; Loss:  7.239456523954145 ; l2 norm of gradient:  1.6014790374557286 ; l2 norm of weights:  8.582191594333224\n",
            "Iteration#:  6064 ; Loss:  7.239368608116422 ; l2 norm of gradient:  1.6014704144757888 ; l2 norm of weights:  8.5821409877419\n",
            "Iteration#:  6068 ; Loss:  7.2392806870675255 ; l2 norm of gradient:  1.6014617914909643 ; l2 norm of weights:  8.58209038154512\n",
            "Iteration#:  6072 ; Loss:  7.23919277147019 ; l2 norm of gradient:  1.6014531685012527 ; l2 norm of weights:  8.582039775742883\n",
            "Iteration#:  6076 ; Loss:  7.239104856575603 ; l2 norm of gradient:  1.6014445455066522 ; l2 norm of weights:  8.581989170335184\n",
            "Iteration#:  6080 ; Loss:  7.239016938599315 ; l2 norm of gradient:  1.601435922507159 ; l2 norm of weights:  8.581938565322028\n",
            "Iteration#:  6084 ; Loss:  7.238929026804389 ; l2 norm of gradient:  1.6014272995027687 ; l2 norm of weights:  8.581887960703414\n",
            "Iteration#:  6088 ; Loss:  7.238841108477789 ; l2 norm of gradient:  1.6014186764934806 ; l2 norm of weights:  8.581837356479337\n",
            "Iteration#:  6092 ; Loss:  7.2387531952145645 ; l2 norm of gradient:  1.601410053479292 ; l2 norm of weights:  8.581786752649801\n",
            "Iteration#:  6096 ; Loss:  7.238665284849997 ; l2 norm of gradient:  1.6014014304601982 ; l2 norm of weights:  8.581736149214802\n",
            "Iteration#:  6100 ; Loss:  7.238577370645728 ; l2 norm of gradient:  1.6013928074361972 ; l2 norm of weights:  8.581685546174342\n",
            "Iteration#:  6104 ; Loss:  7.238489459601668 ; l2 norm of gradient:  1.6013841844072867 ; l2 norm of weights:  8.581634943528416\n",
            "Iteration#:  6108 ; Loss:  7.238401549422795 ; l2 norm of gradient:  1.6013755613734628 ; l2 norm of weights:  8.581584341277024\n",
            "Iteration#:  6112 ; Loss:  7.238313636053357 ; l2 norm of gradient:  1.6013669383347227 ; l2 norm of weights:  8.58153373942017\n",
            "Iteration#:  6116 ; Loss:  7.238225728638833 ; l2 norm of gradient:  1.6013583152910642 ; l2 norm of weights:  8.581483137957848\n",
            "Iteration#:  6120 ; Loss:  7.238137813904271 ; l2 norm of gradient:  1.601349692242484 ; l2 norm of weights:  8.58143253689006\n",
            "Iteration#:  6124 ; Loss:  7.238049904499665 ; l2 norm of gradient:  1.6013410691889793 ; l2 norm of weights:  8.581381936216804\n",
            "Iteration#:  6128 ; Loss:  7.237961998381078 ; l2 norm of gradient:  1.601332446130546 ; l2 norm of weights:  8.58133133593808\n",
            "Iteration#:  6132 ; Loss:  7.237874091005562 ; l2 norm of gradient:  1.6013238230671836 ; l2 norm of weights:  8.581280736053884\n",
            "Iteration#:  6136 ; Loss:  7.23778617872313 ; l2 norm of gradient:  1.601315199998887 ; l2 norm of weights:  8.581230136564221\n",
            "Iteration#:  6140 ; Loss:  7.237698271723451 ; l2 norm of gradient:  1.6013065769256551 ; l2 norm of weights:  8.581179537469085\n",
            "Iteration#:  6144 ; Loss:  7.237610364119504 ; l2 norm of gradient:  1.6012979538474845 ; l2 norm of weights:  8.58112893876848\n",
            "Iteration#:  6148 ; Loss:  7.237522458282941 ; l2 norm of gradient:  1.6012893307643707 ; l2 norm of weights:  8.5810783404624\n",
            "Iteration#:  6152 ; Loss:  7.23743455413348 ; l2 norm of gradient:  1.6012807076763116 ; l2 norm of weights:  8.581027742550846\n",
            "Iteration#:  6156 ; Loss:  7.237346649520884 ; l2 norm of gradient:  1.6012720845833055 ; l2 norm of weights:  8.580977145033817\n",
            "Iteration#:  6160 ; Loss:  7.237258743132532 ; l2 norm of gradient:  1.6012634614853483 ; l2 norm of weights:  8.580926547911314\n",
            "Iteration#:  6164 ; Loss:  7.2371708406465265 ; l2 norm of gradient:  1.6012548383824385 ; l2 norm of weights:  8.580875951183339\n",
            "Iteration#:  6168 ; Loss:  7.2370829378274735 ; l2 norm of gradient:  1.6012462152745714 ; l2 norm of weights:  8.58082535484988\n",
            "Iteration#:  6172 ; Loss:  7.236995030820272 ; l2 norm of gradient:  1.6012375921617454 ; l2 norm of weights:  8.58077475891095\n",
            "Iteration#:  6176 ; Loss:  7.236907126139908 ; l2 norm of gradient:  1.601228969043956 ; l2 norm of weights:  8.580724163366538\n",
            "Iteration#:  6180 ; Loss:  7.236819227947684 ; l2 norm of gradient:  1.6012203459212018 ; l2 norm of weights:  8.580673568216648\n",
            "Iteration#:  6184 ; Loss:  7.236731323821019 ; l2 norm of gradient:  1.6012117227934797 ; l2 norm of weights:  8.58062297346128\n",
            "Iteration#:  6188 ; Loss:  7.236643421466577 ; l2 norm of gradient:  1.601203099660786 ; l2 norm of weights:  8.580572379100428\n",
            "Iteration#:  6192 ; Loss:  7.2365555222735 ; l2 norm of gradient:  1.6011944765231174 ; l2 norm of weights:  8.580521785134097\n",
            "Iteration#:  6196 ; Loss:  7.236467619164072 ; l2 norm of gradient:  1.6011858533804733 ; l2 norm of weights:  8.58047119156228\n",
            "Iteration#:  6200 ; Loss:  7.236379724856183 ; l2 norm of gradient:  1.601177230232849 ; l2 norm of weights:  8.580420598384983\n",
            "Iteration#:  6204 ; Loss:  7.236291823278211 ; l2 norm of gradient:  1.6011686070802418 ; l2 norm of weights:  8.580370005602202\n",
            "Iteration#:  6208 ; Loss:  7.2362039255649435 ; l2 norm of gradient:  1.6011599839226482 ; l2 norm of weights:  8.580319413213935\n",
            "Iteration#:  6212 ; Loss:  7.236116027414365 ; l2 norm of gradient:  1.6011513607600676 ; l2 norm of weights:  8.580268821220182\n",
            "Iteration#:  6216 ; Loss:  7.236028127188795 ; l2 norm of gradient:  1.6011427375924938 ; l2 norm of weights:  8.580218229620943\n",
            "Iteration#:  6220 ; Loss:  7.235940229529602 ; l2 norm of gradient:  1.6011341144199258 ; l2 norm of weights:  8.580167638416219\n",
            "Iteration#:  6224 ; Loss:  7.235852334980958 ; l2 norm of gradient:  1.6011254912423614 ; l2 norm of weights:  8.580117047606002\n",
            "Iteration#:  6228 ; Loss:  7.235764437057803 ; l2 norm of gradient:  1.6011168680597958 ; l2 norm of weights:  8.5800664571903\n",
            "Iteration#:  6232 ; Loss:  7.235676542951534 ; l2 norm of gradient:  1.6011082448722274 ; l2 norm of weights:  8.580015867169108\n",
            "Iteration#:  6236 ; Loss:  7.2355886487099355 ; l2 norm of gradient:  1.6010996216796531 ; l2 norm of weights:  8.579965277542422\n",
            "Iteration#:  6240 ; Loss:  7.235500753089713 ; l2 norm of gradient:  1.6010909984820696 ; l2 norm of weights:  8.57991468831025\n",
            "Iteration#:  6244 ; Loss:  7.235412858827489 ; l2 norm of gradient:  1.6010823752794736 ; l2 norm of weights:  8.579864099472582\n",
            "Iteration#:  6248 ; Loss:  7.235324964143923 ; l2 norm of gradient:  1.6010737520718625 ; l2 norm of weights:  8.579813511029423\n",
            "Iteration#:  6252 ; Loss:  7.235237073265117 ; l2 norm of gradient:  1.6010651288592337 ; l2 norm of weights:  8.57976292298077\n",
            "Iteration#:  6256 ; Loss:  7.235149178747978 ; l2 norm of gradient:  1.601056505641584 ; l2 norm of weights:  8.57971233532662\n",
            "Iteration#:  6260 ; Loss:  7.235061289211378 ; l2 norm of gradient:  1.6010478824189107 ; l2 norm of weights:  8.579661748066977\n",
            "Iteration#:  6264 ; Loss:  7.234973395224419 ; l2 norm of gradient:  1.6010392591912113 ; l2 norm of weights:  8.57961116120184\n",
            "Iteration#:  6268 ; Loss:  7.234885502226472 ; l2 norm of gradient:  1.6010306359584814 ; l2 norm of weights:  8.579560574731204\n",
            "Iteration#:  6272 ; Loss:  7.234797609318586 ; l2 norm of gradient:  1.6010220127207195 ; l2 norm of weights:  8.579509988655067\n",
            "Iteration#:  6276 ; Loss:  7.234709718829349 ; l2 norm of gradient:  1.6010133894779224 ; l2 norm of weights:  8.579459402973434\n",
            "Iteration#:  6280 ; Loss:  7.2346218322622615 ; l2 norm of gradient:  1.6010047662300861 ; l2 norm of weights:  8.579408817686303\n",
            "Iteration#:  6284 ; Loss:  7.234533944046584 ; l2 norm of gradient:  1.6009961429772086 ; l2 norm of weights:  8.57935823279367\n",
            "Iteration#:  6288 ; Loss:  7.234446053826165 ; l2 norm of gradient:  1.6009875197192875 ; l2 norm of weights:  8.579307648295536\n",
            "Iteration#:  6292 ; Loss:  7.2343581690597585 ; l2 norm of gradient:  1.600978896456318 ; l2 norm of weights:  8.5792570641919\n",
            "Iteration#:  6296 ; Loss:  7.234270282092793 ; l2 norm of gradient:  1.6009702731882998 ; l2 norm of weights:  8.579206480482762\n",
            "Iteration#:  6300 ; Loss:  7.234182394319291 ; l2 norm of gradient:  1.600961649915228 ; l2 norm of weights:  8.579155897168121\n",
            "Iteration#:  6304 ; Loss:  7.234094509075264 ; l2 norm of gradient:  1.6009530266371 ; l2 norm of weights:  8.579105314247974\n",
            "Iteration#:  6308 ; Loss:  7.234006623668633 ; l2 norm of gradient:  1.6009444033539126 ; l2 norm of weights:  8.579054731722321\n",
            "Iteration#:  6312 ; Loss:  7.233918736268736 ; l2 norm of gradient:  1.6009357800656645 ; l2 norm of weights:  8.579004149591166\n",
            "Iteration#:  6316 ; Loss:  7.233830853896493 ; l2 norm of gradient:  1.6009271567723504 ; l2 norm of weights:  8.578953567854503\n",
            "Iteration#:  6320 ; Loss:  7.23374296945401 ; l2 norm of gradient:  1.6009185334739682 ; l2 norm of weights:  8.578902986512329\n",
            "Iteration#:  6324 ; Loss:  7.2336550839884115 ; l2 norm of gradient:  1.600909910170516 ; l2 norm of weights:  8.57885240556465\n",
            "Iteration#:  6328 ; Loss:  7.233567200549833 ; l2 norm of gradient:  1.6009012868619907 ; l2 norm of weights:  8.57880182501146\n",
            "Iteration#:  6332 ; Loss:  7.233479320707536 ; l2 norm of gradient:  1.6008926635483873 ; l2 norm of weights:  8.57875124485276\n",
            "Iteration#:  6336 ; Loss:  7.233391438775861 ; l2 norm of gradient:  1.6008840402297058 ; l2 norm of weights:  8.578700665088551\n",
            "Iteration#:  6340 ; Loss:  7.233303553601424 ; l2 norm of gradient:  1.6008754169059403 ; l2 norm of weights:  8.578650085718829\n",
            "Iteration#:  6344 ; Loss:  7.233215674466828 ; l2 norm of gradient:  1.6008667935770897 ; l2 norm of weights:  8.578599506743595\n",
            "Iteration#:  6348 ; Loss:  7.233127791260132 ; l2 norm of gradient:  1.6008581702431515 ; l2 norm of weights:  8.578548928162848\n",
            "Iteration#:  6352 ; Loss:  7.23303991286465 ; l2 norm of gradient:  1.6008495469041217 ; l2 norm of weights:  8.578498349976586\n",
            "Iteration#:  6356 ; Loss:  7.232952034297231 ; l2 norm of gradient:  1.6008409235599965 ; l2 norm of weights:  8.57844777218481\n",
            "Iteration#:  6360 ; Loss:  7.232864154292478 ; l2 norm of gradient:  1.6008323002107752 ; l2 norm of weights:  8.578397194787518\n",
            "Iteration#:  6364 ; Loss:  7.232776275842582 ; l2 norm of gradient:  1.6008236768564528 ; l2 norm of weights:  8.578346617784709\n",
            "Iteration#:  6368 ; Loss:  7.232688398634269 ; l2 norm of gradient:  1.6008150534970282 ; l2 norm of weights:  8.578296041176383\n",
            "Iteration#:  6372 ; Loss:  7.2326005187773585 ; l2 norm of gradient:  1.6008064301324958 ; l2 norm of weights:  8.578245464962539\n",
            "Iteration#:  6376 ; Loss:  7.232512643875252 ; l2 norm of gradient:  1.6007978067628565 ; l2 norm of weights:  8.578194889143175\n",
            "Iteration#:  6380 ; Loss:  7.232424768840923 ; l2 norm of gradient:  1.6007891833881034 ; l2 norm of weights:  8.578144313718292\n",
            "Iteration#:  6384 ; Loss:  7.232336890069758 ; l2 norm of gradient:  1.6007805600082368 ; l2 norm of weights:  8.578093738687889\n",
            "Iteration#:  6388 ; Loss:  7.232249014172123 ; l2 norm of gradient:  1.6007719366232505 ; l2 norm of weights:  8.578043164051964\n",
            "Iteration#:  6392 ; Loss:  7.2321611418063405 ; l2 norm of gradient:  1.6007633132331431 ; l2 norm of weights:  8.577992589810515\n",
            "Iteration#:  6396 ; Loss:  7.232073265583788 ; l2 norm of gradient:  1.600754689837913 ; l2 norm of weights:  8.577942015963547\n",
            "Iteration#:  6400 ; Loss:  7.231985396196622 ; l2 norm of gradient:  1.6007460664375561 ; l2 norm of weights:  8.577891442511051\n",
            "Iteration#:  6404 ; Loss:  7.231897518696623 ; l2 norm of gradient:  1.6007374430320687 ; l2 norm of weights:  8.577840869453032\n",
            "Iteration#:  6408 ; Loss:  7.2318096457756305 ; l2 norm of gradient:  1.6007288196214489 ; l2 norm of weights:  8.577790296789487\n",
            "Iteration#:  6412 ; Loss:  7.231721771995639 ; l2 norm of gradient:  1.6007201962056932 ; l2 norm of weights:  8.577739724520418\n",
            "Iteration#:  6416 ; Loss:  7.231633903283133 ; l2 norm of gradient:  1.6007115727847991 ; l2 norm of weights:  8.57768915264582\n",
            "Iteration#:  6420 ; Loss:  7.231546029158059 ; l2 norm of gradient:  1.6007029493587626 ; l2 norm of weights:  8.577638581165695\n",
            "Iteration#:  6424 ; Loss:  7.231458159690698 ; l2 norm of gradient:  1.6006943259275823 ; l2 norm of weights:  8.577588010080042\n",
            "Iteration#:  6428 ; Loss:  7.2313702899731815 ; l2 norm of gradient:  1.6006857024912533 ; l2 norm of weights:  8.577537439388857\n",
            "Iteration#:  6432 ; Loss:  7.231282415718795 ; l2 norm of gradient:  1.6006770790497749 ; l2 norm of weights:  8.577486869092144\n",
            "Iteration#:  6436 ; Loss:  7.231194548862218 ; l2 norm of gradient:  1.6006684556031419 ; l2 norm of weights:  8.577436299189898\n",
            "Iteration#:  6440 ; Loss:  7.23110667852424 ; l2 norm of gradient:  1.6006598321513523 ; l2 norm of weights:  8.577385729682122\n",
            "Iteration#:  6444 ; Loss:  7.231018814973779 ; l2 norm of gradient:  1.6006512086944045 ; l2 norm of weights:  8.57733516056881\n",
            "Iteration#:  6448 ; Loss:  7.230930946065423 ; l2 norm of gradient:  1.6006425852322927 ; l2 norm of weights:  8.577284591849967\n",
            "Iteration#:  6452 ; Loss:  7.230843078876747 ; l2 norm of gradient:  1.6006339617650163 ; l2 norm of weights:  8.577234023525588\n",
            "Iteration#:  6456 ; Loss:  7.23075521183403 ; l2 norm of gradient:  1.6006253382925715 ; l2 norm of weights:  8.577183455595675\n",
            "Iteration#:  6460 ; Loss:  7.230667346335338 ; l2 norm of gradient:  1.6006167148149546 ; l2 norm of weights:  8.577132888060227\n",
            "Iteration#:  6464 ; Loss:  7.2305794812247335 ; l2 norm of gradient:  1.6006080913321639 ; l2 norm of weights:  8.57708232091924\n",
            "Iteration#:  6468 ; Loss:  7.230491615739646 ; l2 norm of gradient:  1.6005994678441955 ; l2 norm of weights:  8.577031754172715\n",
            "Iteration#:  6472 ; Loss:  7.23040374593781 ; l2 norm of gradient:  1.6005908443510473 ; l2 norm of weights:  8.576981187820653\n",
            "Iteration#:  6476 ; Loss:  7.230315884872548 ; l2 norm of gradient:  1.600582220852715 ; l2 norm of weights:  8.576930621863053\n",
            "Iteration#:  6480 ; Loss:  7.230228023228914 ; l2 norm of gradient:  1.6005735973491972 ; l2 norm of weights:  8.576880056299911\n",
            "Iteration#:  6484 ; Loss:  7.2301401615662355 ; l2 norm of gradient:  1.60056497384049 ; l2 norm of weights:  8.576829491131226\n",
            "Iteration#:  6488 ; Loss:  7.230052298191435 ; l2 norm of gradient:  1.6005563503265898 ; l2 norm of weights:  8.576778926357003\n",
            "Iteration#:  6492 ; Loss:  7.229964434680953 ; l2 norm of gradient:  1.6005477268074948 ; l2 norm of weights:  8.576728361977237\n",
            "Iteration#:  6496 ; Loss:  7.229876575855835 ; l2 norm of gradient:  1.600539103283202 ; l2 norm of weights:  8.576677797991925\n",
            "Iteration#:  6500 ; Loss:  7.22978871171223 ; l2 norm of gradient:  1.6005304797537077 ; l2 norm of weights:  8.57662723440107\n",
            "Iteration#:  6504 ; Loss:  7.22970085157573 ; l2 norm of gradient:  1.6005218562190087 ; l2 norm of weights:  8.576576671204672\n",
            "Iteration#:  6508 ; Loss:  7.229612993897329 ; l2 norm of gradient:  1.6005132326791034 ; l2 norm of weights:  8.576526108402726\n",
            "Iteration#:  6512 ; Loss:  7.22952513419431 ; l2 norm of gradient:  1.6005046091339872 ; l2 norm of weights:  8.576475545995235\n",
            "Iteration#:  6516 ; Loss:  7.229437273579665 ; l2 norm of gradient:  1.6004959855836582 ; l2 norm of weights:  8.576424983982193\n",
            "Iteration#:  6520 ; Loss:  7.229349416640586 ; l2 norm of gradient:  1.6004873620281124 ; l2 norm of weights:  8.576374422363607\n",
            "Iteration#:  6524 ; Loss:  7.229261559330106 ; l2 norm of gradient:  1.6004787384673482 ; l2 norm of weights:  8.57632386113947\n",
            "Iteration#:  6528 ; Loss:  7.229173701651508 ; l2 norm of gradient:  1.6004701149013616 ; l2 norm of weights:  8.576273300309783\n",
            "Iteration#:  6532 ; Loss:  7.229085848302406 ; l2 norm of gradient:  1.6004614913301505 ; l2 norm of weights:  8.576222739874547\n",
            "Iteration#:  6536 ; Loss:  7.228997991826636 ; l2 norm of gradient:  1.6004528677537107 ; l2 norm of weights:  8.576172179833756\n",
            "Iteration#:  6540 ; Loss:  7.22891013290554 ; l2 norm of gradient:  1.60044424417204 ; l2 norm of weights:  8.576121620187417\n",
            "Iteration#:  6544 ; Loss:  7.228822276854613 ; l2 norm of gradient:  1.6004356205851353 ; l2 norm of weights:  8.57607106093552\n",
            "Iteration#:  6548 ; Loss:  7.228734422165575 ; l2 norm of gradient:  1.600426996992992 ; l2 norm of weights:  8.576020502078073\n",
            "Iteration#:  6552 ; Loss:  7.228646569548221 ; l2 norm of gradient:  1.6004183733956105 ; l2 norm of weights:  8.57596994361507\n",
            "Iteration#:  6556 ; Loss:  7.228558717237555 ; l2 norm of gradient:  1.6004097497929852 ; l2 norm of weights:  8.57591938554651\n",
            "Iteration#:  6560 ; Loss:  7.228470863284097 ; l2 norm of gradient:  1.600401126185114 ; l2 norm of weights:  8.575868827872394\n",
            "Iteration#:  6564 ; Loss:  7.228383006892927 ; l2 norm of gradient:  1.6003925025719934 ; l2 norm of weights:  8.575818270592723\n",
            "Iteration#:  6568 ; Loss:  7.228295157597415 ; l2 norm of gradient:  1.6003838789536213 ; l2 norm of weights:  8.575767713707492\n",
            "Iteration#:  6572 ; Loss:  7.2282073077539035 ; l2 norm of gradient:  1.6003752553299933 ; l2 norm of weights:  8.575717157216703\n",
            "Iteration#:  6576 ; Loss:  7.228119453639326 ; l2 norm of gradient:  1.6003666317011078 ; l2 norm of weights:  8.575666601120352\n",
            "Iteration#:  6580 ; Loss:  7.2280316057580585 ; l2 norm of gradient:  1.6003580080669608 ; l2 norm of weights:  8.575616045418442\n",
            "Iteration#:  6584 ; Loss:  7.227943755934348 ; l2 norm of gradient:  1.6003493844275503 ; l2 norm of weights:  8.575565490110971\n",
            "Iteration#:  6588 ; Loss:  7.22785590284159 ; l2 norm of gradient:  1.6003407607828726 ; l2 norm of weights:  8.575514935197942\n",
            "Iteration#:  6592 ; Loss:  7.227768058880331 ; l2 norm of gradient:  1.6003321371329249 ; l2 norm of weights:  8.575464380679344\n",
            "Iteration#:  6596 ; Loss:  7.22768020653891 ; l2 norm of gradient:  1.6003235134777036 ; l2 norm of weights:  8.575413826555184\n",
            "Iteration#:  6600 ; Loss:  7.227592359759322 ; l2 norm of gradient:  1.6003148898172055 ; l2 norm of weights:  8.57536327282546\n",
            "Iteration#:  6604 ; Loss:  7.227504512750176 ; l2 norm of gradient:  1.6003062661514293 ; l2 norm of weights:  8.575312719490173\n",
            "Iteration#:  6608 ; Loss:  7.2274166664356 ; l2 norm of gradient:  1.6002976424803712 ; l2 norm of weights:  8.575262166549315\n",
            "Iteration#:  6612 ; Loss:  7.227328820526736 ; l2 norm of gradient:  1.6002890188040277 ; l2 norm of weights:  8.575211614002894\n",
            "Iteration#:  6616 ; Loss:  7.227240973802585 ; l2 norm of gradient:  1.6002803951223958 ; l2 norm of weights:  8.575161061850904\n",
            "Iteration#:  6620 ; Loss:  7.227153130582957 ; l2 norm of gradient:  1.600271771435473 ; l2 norm of weights:  8.575110510093346\n",
            "Iteration#:  6624 ; Loss:  7.22706528434993 ; l2 norm of gradient:  1.6002631477432554 ; l2 norm of weights:  8.575059958730217\n",
            "Iteration#:  6628 ; Loss:  7.226977440433929 ; l2 norm of gradient:  1.6002545240457418 ; l2 norm of weights:  8.575009407761518\n",
            "Iteration#:  6632 ; Loss:  7.226889599041709 ; l2 norm of gradient:  1.6002459003429272 ; l2 norm of weights:  8.57495885718725\n",
            "Iteration#:  6636 ; Loss:  7.226801751976956 ; l2 norm of gradient:  1.6002372766348087 ; l2 norm of weights:  8.574908307007409\n",
            "Iteration#:  6640 ; Loss:  7.226713910250331 ; l2 norm of gradient:  1.600228652921385 ; l2 norm of weights:  8.574857757221995\n",
            "Iteration#:  6644 ; Loss:  7.22662607066335 ; l2 norm of gradient:  1.6002200292026516 ; l2 norm of weights:  8.574807207831006\n",
            "Iteration#:  6648 ; Loss:  7.2265382296203695 ; l2 norm of gradient:  1.6002114054786067 ; l2 norm of weights:  8.574756658834445\n",
            "Iteration#:  6652 ; Loss:  7.226450386567147 ; l2 norm of gradient:  1.6002027817492455 ; l2 norm of weights:  8.57470611023231\n",
            "Iteration#:  6656 ; Loss:  7.226362545799841 ; l2 norm of gradient:  1.6001941580145662 ; l2 norm of weights:  8.574655562024594\n",
            "Iteration#:  6660 ; Loss:  7.22627470214562 ; l2 norm of gradient:  1.6001855342745652 ; l2 norm of weights:  8.574605014211304\n",
            "Iteration#:  6664 ; Loss:  7.2261868623178 ; l2 norm of gradient:  1.6001769105292412 ; l2 norm of weights:  8.574554466792437\n",
            "Iteration#:  6668 ; Loss:  7.2260990242674366 ; l2 norm of gradient:  1.6001682867785894 ; l2 norm of weights:  8.574503919767992\n",
            "Iteration#:  6672 ; Loss:  7.226011182702276 ; l2 norm of gradient:  1.6001596630226056 ; l2 norm of weights:  8.574453373137967\n",
            "Iteration#:  6676 ; Loss:  7.22592334529025 ; l2 norm of gradient:  1.6001510392612892 ; l2 norm of weights:  8.574402826902363\n",
            "Iteration#:  6680 ; Loss:  7.2258355098445515 ; l2 norm of gradient:  1.6001424154946378 ; l2 norm of weights:  8.574352281061175\n",
            "Iteration#:  6684 ; Loss:  7.225747674003234 ; l2 norm of gradient:  1.6001337917226455 ; l2 norm of weights:  8.574301735614409\n",
            "Iteration#:  6688 ; Loss:  7.225659835335274 ; l2 norm of gradient:  1.6001251679453106 ; l2 norm of weights:  8.574251190562059\n",
            "Iteration#:  6692 ; Loss:  7.225572000195477 ; l2 norm of gradient:  1.6001165441626306 ; l2 norm of weights:  8.574200645904124\n",
            "Iteration#:  6696 ; Loss:  7.225484163743591 ; l2 norm of gradient:  1.6001079203746027 ; l2 norm of weights:  8.574150101640607\n",
            "Iteration#:  6700 ; Loss:  7.225396332246086 ; l2 norm of gradient:  1.6000992965812226 ; l2 norm of weights:  8.574099557771506\n",
            "Iteration#:  6704 ; Loss:  7.225308494946303 ; l2 norm of gradient:  1.6000906727824873 ; l2 norm of weights:  8.574049014296818\n",
            "Iteration#:  6708 ; Loss:  7.225220659970003 ; l2 norm of gradient:  1.600082048978395 ; l2 norm of weights:  8.573998471216543\n",
            "Iteration#:  6712 ; Loss:  7.225132830139154 ; l2 norm of gradient:  1.6000734251689426 ; l2 norm of weights:  8.573947928530682\n",
            "Iteration#:  6716 ; Loss:  7.2250450022377635 ; l2 norm of gradient:  1.6000648013541265 ; l2 norm of weights:  8.573897386239231\n",
            "Iteration#:  6720 ; Loss:  7.224957163915803 ; l2 norm of gradient:  1.600056177533942 ; l2 norm of weights:  8.573846844342192\n",
            "Iteration#:  6724 ; Loss:  7.224869333876737 ; l2 norm of gradient:  1.6000475537083885 ; l2 norm of weights:  8.573796302839563\n",
            "Iteration#:  6728 ; Loss:  7.224781502217262 ; l2 norm of gradient:  1.6000389298774622 ; l2 norm of weights:  8.573745761731344\n",
            "Iteration#:  6732 ; Loss:  7.22469366915484 ; l2 norm of gradient:  1.60003030604116 ; l2 norm of weights:  8.573695221017534\n",
            "Iteration#:  6736 ; Loss:  7.224605839868879 ; l2 norm of gradient:  1.6000216821994795 ; l2 norm of weights:  8.573644680698129\n",
            "Iteration#:  6740 ; Loss:  7.224518008561072 ; l2 norm of gradient:  1.6000130583524164 ; l2 norm of weights:  8.573594140773134\n",
            "Iteration#:  6744 ; Loss:  7.224430179105946 ; l2 norm of gradient:  1.600004434499969 ; l2 norm of weights:  8.573543601242543\n",
            "Iteration#:  6748 ; Loss:  7.224342351390616 ; l2 norm of gradient:  1.5999958106421335 ; l2 norm of weights:  8.573493062106358\n",
            "Iteration#:  6752 ; Loss:  7.2242545253005215 ; l2 norm of gradient:  1.5999871867789062 ; l2 norm of weights:  8.573442523364578\n",
            "Iteration#:  6756 ; Loss:  7.2241666976937156 ; l2 norm of gradient:  1.5999785629102854 ; l2 norm of weights:  8.573391985017201\n",
            "Iteration#:  6760 ; Loss:  7.224078870537994 ; l2 norm of gradient:  1.599969939036267 ; l2 norm of weights:  8.573341447064228\n",
            "Iteration#:  6764 ; Loss:  7.223991043259019 ; l2 norm of gradient:  1.5999613151568486 ; l2 norm of weights:  8.573290909505657\n",
            "Iteration#:  6768 ; Loss:  7.223903214821684 ; l2 norm of gradient:  1.5999526912720272 ; l2 norm of weights:  8.573240372341486\n",
            "Iteration#:  6772 ; Loss:  7.2238153893461945 ; l2 norm of gradient:  1.599944067381799 ; l2 norm of weights:  8.573189835571716\n",
            "Iteration#:  6776 ; Loss:  7.223727564825262 ; l2 norm of gradient:  1.5999354434861617 ; l2 norm of weights:  8.573139299196345\n",
            "Iteration#:  6780 ; Loss:  7.2236397413610085 ; l2 norm of gradient:  1.5999268195851126 ; l2 norm of weights:  8.573088763215374\n",
            "Iteration#:  6784 ; Loss:  7.223551916583757 ; l2 norm of gradient:  1.5999181956786477 ; l2 norm of weights:  8.5730382276288\n",
            "Iteration#:  6788 ; Loss:  7.2234640894050814 ; l2 norm of gradient:  1.5999095717667642 ; l2 norm of weights:  8.572987692436623\n",
            "Iteration#:  6792 ; Loss:  7.223376271772238 ; l2 norm of gradient:  1.5999009478494588 ; l2 norm of weights:  8.572937157638844\n",
            "Iteration#:  6796 ; Loss:  7.223288450860235 ; l2 norm of gradient:  1.5998923239267293 ; l2 norm of weights:  8.572886623235458\n",
            "Iteration#:  6800 ; Loss:  7.22320062501686 ; l2 norm of gradient:  1.599883699998573 ; l2 norm of weights:  8.572836089226469\n",
            "Iteration#:  6804 ; Loss:  7.223112804652681 ; l2 norm of gradient:  1.5998750760649842 ; l2 norm of weights:  8.572785555611874\n",
            "Iteration#:  6808 ; Loss:  7.223024983223302 ; l2 norm of gradient:  1.599866452125963 ; l2 norm of weights:  8.57273502239167\n",
            "Iteration#:  6812 ; Loss:  7.222937164719918 ; l2 norm of gradient:  1.5998578281815043 ; l2 norm of weights:  8.57268448956586\n",
            "Iteration#:  6816 ; Loss:  7.222849342254182 ; l2 norm of gradient:  1.5998492042316057 ; l2 norm of weights:  8.572633957134443\n",
            "Iteration#:  6820 ; Loss:  7.222761523183094 ; l2 norm of gradient:  1.5998405802762643 ; l2 norm of weights:  8.572583425097413\n",
            "Iteration#:  6824 ; Loss:  7.22267370308408 ; l2 norm of gradient:  1.5998319563154777 ; l2 norm of weights:  8.572532893454774\n",
            "Iteration#:  6828 ; Loss:  7.222585884153689 ; l2 norm of gradient:  1.5998233323492417 ; l2 norm of weights:  8.572482362206523\n",
            "Iteration#:  6832 ; Loss:  7.222498066015726 ; l2 norm of gradient:  1.5998147083775531 ; l2 norm of weights:  8.572431831352663\n",
            "Iteration#:  6836 ; Loss:  7.222410252428434 ; l2 norm of gradient:  1.5998060844004103 ; l2 norm of weights:  8.572381300893188\n",
            "Iteration#:  6840 ; Loss:  7.222322433927831 ; l2 norm of gradient:  1.5997974604178087 ; l2 norm of weights:  8.572330770828101\n",
            "Iteration#:  6844 ; Loss:  7.222234619484322 ; l2 norm of gradient:  1.599788836429745 ; l2 norm of weights:  8.572280241157399\n",
            "Iteration#:  6848 ; Loss:  7.22214680458718 ; l2 norm of gradient:  1.5997802124362177 ; l2 norm of weights:  8.57222971188108\n",
            "Iteration#:  6852 ; Loss:  7.222058985458396 ; l2 norm of gradient:  1.5997715884372226 ; l2 norm of weights:  8.572179182999148\n",
            "Iteration#:  6856 ; Loss:  7.221971172828312 ; l2 norm of gradient:  1.5997629644327582 ; l2 norm of weights:  8.5721286545116\n",
            "Iteration#:  6860 ; Loss:  7.221883357041321 ; l2 norm of gradient:  1.599754340422819 ; l2 norm of weights:  8.572078126418432\n",
            "Iteration#:  6864 ; Loss:  7.2217955432442675 ; l2 norm of gradient:  1.5997457164074038 ; l2 norm of weights:  8.572027598719648\n",
            "Iteration#:  6868 ; Loss:  7.221707731791584 ; l2 norm of gradient:  1.599737092386509 ; l2 norm of weights:  8.571977071415242\n",
            "Iteration#:  6872 ; Loss:  7.221619918620286 ; l2 norm of gradient:  1.5997284683601312 ; l2 norm of weights:  8.571926544505219\n",
            "Iteration#:  6876 ; Loss:  7.221532101190885 ; l2 norm of gradient:  1.5997198443282679 ; l2 norm of weights:  8.571876017989574\n",
            "Iteration#:  6880 ; Loss:  7.221444294406661 ; l2 norm of gradient:  1.5997112202909154 ; l2 norm of weights:  8.571825491868307\n",
            "Iteration#:  6884 ; Loss:  7.221356481802669 ; l2 norm of gradient:  1.5997025962480704 ; l2 norm of weights:  8.571774966141417\n",
            "Iteration#:  6888 ; Loss:  7.221268671530053 ; l2 norm of gradient:  1.5996939721997305 ; l2 norm of weights:  8.571724440808905\n",
            "Iteration#:  6892 ; Loss:  7.221180860026427 ; l2 norm of gradient:  1.5996853481458944 ; l2 norm of weights:  8.571673915870768\n",
            "Iteration#:  6896 ; Loss:  7.2210930500099035 ; l2 norm of gradient:  1.5996767240865553 ; l2 norm of weights:  8.571623391327005\n",
            "Iteration#:  6900 ; Loss:  7.2210052419961865 ; l2 norm of gradient:  1.5996681000217123 ; l2 norm of weights:  8.57157286717762\n",
            "Iteration#:  6904 ; Loss:  7.220917433669513 ; l2 norm of gradient:  1.5996594759513623 ; l2 norm of weights:  8.571522343422604\n",
            "Iteration#:  6908 ; Loss:  7.2208296250984745 ; l2 norm of gradient:  1.5996508518755013 ; l2 norm of weights:  8.571471820061966\n",
            "Iteration#:  6912 ; Loss:  7.220741818106122 ; l2 norm of gradient:  1.5996422277941276 ; l2 norm of weights:  8.571421297095696\n",
            "Iteration#:  6916 ; Loss:  7.220654012259873 ; l2 norm of gradient:  1.5996336037072363 ; l2 norm of weights:  8.571370774523796\n",
            "Iteration#:  6920 ; Loss:  7.2205662051711945 ; l2 norm of gradient:  1.599624979614826 ; l2 norm of weights:  8.57132025234627\n",
            "Iteration#:  6924 ; Loss:  7.220478399289252 ; l2 norm of gradient:  1.5996163555168927 ; l2 norm of weights:  8.571269730563111\n",
            "Iteration#:  6928 ; Loss:  7.22039059419355 ; l2 norm of gradient:  1.599607731413434 ; l2 norm of weights:  8.571219209174322\n",
            "Iteration#:  6932 ; Loss:  7.220302785269805 ; l2 norm of gradient:  1.5995991073044458 ; l2 norm of weights:  8.571168688179899\n",
            "Iteration#:  6936 ; Loss:  7.220214982999211 ; l2 norm of gradient:  1.5995904831899268 ; l2 norm of weights:  8.571118167579844\n",
            "Iteration#:  6940 ; Loss:  7.220127181578613 ; l2 norm of gradient:  1.599581859069872 ; l2 norm of weights:  8.571067647374155\n",
            "Iteration#:  6944 ; Loss:  7.220039379287982 ; l2 norm of gradient:  1.5995732349442784 ; l2 norm of weights:  8.57101712756283\n",
            "Iteration#:  6948 ; Loss:  7.219951574236479 ; l2 norm of gradient:  1.599564610813144 ; l2 norm of weights:  8.570966608145874\n",
            "Iteration#:  6952 ; Loss:  7.219863774263745 ; l2 norm of gradient:  1.5995559866764657 ; l2 norm of weights:  8.570916089123278\n",
            "Iteration#:  6956 ; Loss:  7.219775973716576 ; l2 norm of gradient:  1.5995473625342396 ; l2 norm of weights:  8.570865570495048\n",
            "Iteration#:  6960 ; Loss:  7.219688171064325 ; l2 norm of gradient:  1.599538738386463 ; l2 norm of weights:  8.570815052261176\n",
            "Iteration#:  6964 ; Loss:  7.219600367599911 ; l2 norm of gradient:  1.5995301142331328 ; l2 norm of weights:  8.570764534421667\n",
            "Iteration#:  6968 ; Loss:  7.2195125691359054 ; l2 norm of gradient:  1.5995214900742458 ; l2 norm of weights:  8.570714016976519\n",
            "Iteration#:  6972 ; Loss:  7.219424771500733 ; l2 norm of gradient:  1.599512865909799 ; l2 norm of weights:  8.570663499925729\n",
            "Iteration#:  6976 ; Loss:  7.219336969429425 ; l2 norm of gradient:  1.5995042417397909 ; l2 norm of weights:  8.5706129832693\n",
            "Iteration#:  6980 ; Loss:  7.219249169294861 ; l2 norm of gradient:  1.5994956175642143 ; l2 norm of weights:  8.570562467007226\n",
            "Iteration#:  6984 ; Loss:  7.219161376403713 ; l2 norm of gradient:  1.5994869933830702 ; l2 norm of weights:  8.570511951139512\n",
            "Iteration#:  6988 ; Loss:  7.219073575805799 ; l2 norm of gradient:  1.5994783691963534 ; l2 norm of weights:  8.570461435666154\n",
            "Iteration#:  6992 ; Loss:  7.218985781041772 ; l2 norm of gradient:  1.5994697450040616 ; l2 norm of weights:  8.57041092058715\n",
            "Iteration#:  6996 ; Loss:  7.218897981841483 ; l2 norm of gradient:  1.5994611208061906 ; l2 norm of weights:  8.570360405902504\n",
            "Iteration#:  7000 ; Loss:  7.218810189599775 ; l2 norm of gradient:  1.5994524966027397 ; l2 norm of weights:  8.570309891612208\n",
            "Iteration#:  7004 ; Loss:  7.21872239519476 ; l2 norm of gradient:  1.5994438723937026 ; l2 norm of weights:  8.570259377716267\n",
            "Iteration#:  7008 ; Loss:  7.218634598931148 ; l2 norm of gradient:  1.5994352481790786 ; l2 norm of weights:  8.570208864214681\n",
            "Iteration#:  7012 ; Loss:  7.2185468042227825 ; l2 norm of gradient:  1.5994266239588641 ; l2 norm of weights:  8.570158351107441\n",
            "Iteration#:  7016 ; Loss:  7.2184590066923775 ; l2 norm of gradient:  1.5994179997330558 ; l2 norm of weights:  8.570107838394554\n",
            "Iteration#:  7020 ; Loss:  7.218371213670448 ; l2 norm of gradient:  1.5994093755016494 ; l2 norm of weights:  8.570057326076018\n",
            "Iteration#:  7024 ; Loss:  7.218283422930161 ; l2 norm of gradient:  1.5994007512646442 ; l2 norm of weights:  8.570006814151832\n",
            "Iteration#:  7028 ; Loss:  7.218195629160763 ; l2 norm of gradient:  1.5993921270220357 ; l2 norm of weights:  8.56995630262199\n",
            "Iteration#:  7032 ; Loss:  7.218107834872699 ; l2 norm of gradient:  1.5993835027738201 ; l2 norm of weights:  8.569905791486498\n",
            "Iteration#:  7036 ; Loss:  7.218020041317265 ; l2 norm of gradient:  1.599374878519995 ; l2 norm of weights:  8.569855280745353\n",
            "Iteration#:  7040 ; Loss:  7.217932254492036 ; l2 norm of gradient:  1.5993662542605582 ; l2 norm of weights:  8.569804770398553\n",
            "Iteration#:  7044 ; Loss:  7.2178444617711675 ; l2 norm of gradient:  1.599357629995506 ; l2 norm of weights:  8.569754260446098\n",
            "Iteration#:  7048 ; Loss:  7.217756672427795 ; l2 norm of gradient:  1.5993490057248345 ; l2 norm of weights:  8.569703750887987\n",
            "Iteration#:  7052 ; Loss:  7.217668886405575 ; l2 norm of gradient:  1.5993403814485414 ; l2 norm of weights:  8.56965324172422\n",
            "Iteration#:  7056 ; Loss:  7.217581095599831 ; l2 norm of gradient:  1.5993317571666226 ; l2 norm of weights:  8.569602732954793\n",
            "Iteration#:  7060 ; Loss:  7.217493309455428 ; l2 norm of gradient:  1.599323132879077 ; l2 norm of weights:  8.569552224579711\n",
            "Iteration#:  7064 ; Loss:  7.21740552394561 ; l2 norm of gradient:  1.5993145085858989 ; l2 norm of weights:  8.56950171659897\n",
            "Iteration#:  7068 ; Loss:  7.217317734061645 ; l2 norm of gradient:  1.599305884287088 ; l2 norm of weights:  8.569451209012566\n",
            "Iteration#:  7072 ; Loss:  7.217229949598659 ; l2 norm of gradient:  1.599297259982639 ; l2 norm of weights:  8.569400701820504\n",
            "Iteration#:  7076 ; Loss:  7.217142161491852 ; l2 norm of gradient:  1.599288635672549 ; l2 norm of weights:  8.569350195022778\n",
            "Iteration#:  7080 ; Loss:  7.217054375501586 ; l2 norm of gradient:  1.5992800113568157 ; l2 norm of weights:  8.569299688619392\n",
            "Iteration#:  7084 ; Loss:  7.216966593842763 ; l2 norm of gradient:  1.5992713870354354 ; l2 norm of weights:  8.56924918261034\n",
            "Iteration#:  7088 ; Loss:  7.216878806929821 ; l2 norm of gradient:  1.5992627627084062 ; l2 norm of weights:  8.569198676995626\n",
            "Iteration#:  7092 ; Loss:  7.2167910194269975 ; l2 norm of gradient:  1.5992541383757226 ; l2 norm of weights:  8.569148171775247\n",
            "Iteration#:  7096 ; Loss:  7.216703235557285 ; l2 norm of gradient:  1.5992455140373838 ; l2 norm of weights:  8.569097666949203\n",
            "Iteration#:  7100 ; Loss:  7.216615453620224 ; l2 norm of gradient:  1.5992368896933848 ; l2 norm of weights:  8.56904716251749\n",
            "Iteration#:  7104 ; Loss:  7.216527673115043 ; l2 norm of gradient:  1.599228265343724 ; l2 norm of weights:  8.568996658480112\n",
            "Iteration#:  7108 ; Loss:  7.216439889263668 ; l2 norm of gradient:  1.5992196409883994 ; l2 norm of weights:  8.568946154837063\n",
            "Iteration#:  7112 ; Loss:  7.21635210872318 ; l2 norm of gradient:  1.5992110166274038 ; l2 norm of weights:  8.568895651588347\n",
            "Iteration#:  7116 ; Loss:  7.2162643256636 ; l2 norm of gradient:  1.5992023922607375 ; l2 norm of weights:  8.568845148733962\n",
            "Iteration#:  7120 ; Loss:  7.21617654819025 ; l2 norm of gradient:  1.599193767888396 ; l2 norm of weights:  8.568794646273904\n",
            "Iteration#:  7124 ; Loss:  7.21608876529864 ; l2 norm of gradient:  1.599185143510377 ; l2 norm of weights:  8.568744144208178\n",
            "Iteration#:  7128 ; Loss:  7.216000984523907 ; l2 norm of gradient:  1.5991765191266758 ; l2 norm of weights:  8.568693642536779\n",
            "Iteration#:  7132 ; Loss:  7.215913208212329 ; l2 norm of gradient:  1.599167894737291 ; l2 norm of weights:  8.568643141259704\n",
            "Iteration#:  7136 ; Loss:  7.215825429609018 ; l2 norm of gradient:  1.5991592703422188 ; l2 norm of weights:  8.568592640376957\n",
            "Iteration#:  7140 ; Loss:  7.215737653678433 ; l2 norm of gradient:  1.5991506459414564 ; l2 norm of weights:  8.568542139888537\n",
            "Iteration#:  7144 ; Loss:  7.215649875832719 ; l2 norm of gradient:  1.5991420215349996 ; l2 norm of weights:  8.56849163979444\n",
            "Iteration#:  7148 ; Loss:  7.215562099688692 ; l2 norm of gradient:  1.5991333971228463 ; l2 norm of weights:  8.568441140094667\n",
            "Iteration#:  7152 ; Loss:  7.215474322243427 ; l2 norm of gradient:  1.599124772704993 ; l2 norm of weights:  8.568390640789216\n",
            "Iteration#:  7156 ; Loss:  7.2153865451308 ; l2 norm of gradient:  1.5991161482814369 ; l2 norm of weights:  8.568340141878087\n",
            "Iteration#:  7160 ; Loss:  7.215298774294176 ; l2 norm of gradient:  1.5991075238521744 ; l2 norm of weights:  8.568289643361279\n",
            "Iteration#:  7164 ; Loss:  7.215210994776632 ; l2 norm of gradient:  1.5990988994172028 ; l2 norm of weights:  8.568239145238792\n",
            "Iteration#:  7168 ; Loss:  7.215123217944838 ; l2 norm of gradient:  1.5990902749765186 ; l2 norm of weights:  8.568188647510626\n",
            "Iteration#:  7172 ; Loss:  7.215035450691373 ; l2 norm of gradient:  1.5990816505301186 ; l2 norm of weights:  8.568138150176777\n",
            "Iteration#:  7176 ; Loss:  7.214947673974855 ; l2 norm of gradient:  1.5990730260780004 ; l2 norm of weights:  8.56808765323725\n",
            "Iteration#:  7180 ; Loss:  7.214859903714215 ; l2 norm of gradient:  1.5990644016201596 ; l2 norm of weights:  8.568037156692034\n",
            "Iteration#:  7184 ; Loss:  7.214772127805155 ; l2 norm of gradient:  1.5990557771565934 ; l2 norm of weights:  8.567986660541138\n",
            "Iteration#:  7188 ; Loss:  7.214684356732045 ; l2 norm of gradient:  1.5990471526873 ; l2 norm of weights:  8.567936164784555\n",
            "Iteration#:  7192 ; Loss:  7.214596586804469 ; l2 norm of gradient:  1.5990385282122759 ; l2 norm of weights:  8.56788566942229\n",
            "Iteration#:  7196 ; Loss:  7.2145088181623915 ; l2 norm of gradient:  1.5990299037315154 ; l2 norm of weights:  8.567835174454336\n",
            "Iteration#:  7200 ; Loss:  7.214421047426477 ; l2 norm of gradient:  1.5990212792450176 ; l2 norm of weights:  8.567784679880697\n",
            "Iteration#:  7204 ; Loss:  7.214333276380456 ; l2 norm of gradient:  1.5990126547527803 ; l2 norm of weights:  8.56773418570137\n",
            "Iteration#:  7208 ; Loss:  7.214245509823687 ; l2 norm of gradient:  1.599004030254798 ; l2 norm of weights:  8.567683691916354\n",
            "Iteration#:  7212 ; Loss:  7.2141577385312985 ; l2 norm of gradient:  1.5989954057510698 ; l2 norm of weights:  8.567633198525648\n",
            "Iteration#:  7216 ; Loss:  7.214069971771052 ; l2 norm of gradient:  1.5989867812415903 ; l2 norm of weights:  8.567582705529254\n",
            "Iteration#:  7220 ; Loss:  7.213982203134044 ; l2 norm of gradient:  1.598978156726358 ; l2 norm of weights:  8.56753221292717\n",
            "Iteration#:  7224 ; Loss:  7.213894433560046 ; l2 norm of gradient:  1.5989695322053683 ; l2 norm of weights:  8.56748172071939\n",
            "Iteration#:  7228 ; Loss:  7.213806666455854 ; l2 norm of gradient:  1.59896090767862 ; l2 norm of weights:  8.56743122890592\n",
            "Iteration#:  7232 ; Loss:  7.2137189031565825 ; l2 norm of gradient:  1.5989522831461083 ; l2 norm of weights:  8.567380737486754\n",
            "Iteration#:  7236 ; Loss:  7.213631136988043 ; l2 norm of gradient:  1.5989436586078305 ; l2 norm of weights:  8.567330246461896\n",
            "Iteration#:  7240 ; Loss:  7.213543372416106 ; l2 norm of gradient:  1.5989350340637833 ; l2 norm of weights:  8.567279755831343\n",
            "Iteration#:  7244 ; Loss:  7.213455608753203 ; l2 norm of gradient:  1.5989264095139644 ; l2 norm of weights:  8.567229265595094\n",
            "Iteration#:  7248 ; Loss:  7.213367845476352 ; l2 norm of gradient:  1.5989177849583702 ; l2 norm of weights:  8.567178775753147\n",
            "Iteration#:  7252 ; Loss:  7.213280081724613 ; l2 norm of gradient:  1.5989091603969967 ; l2 norm of weights:  8.567128286305502\n",
            "Iteration#:  7256 ; Loss:  7.213192317939827 ; l2 norm of gradient:  1.598900535829842 ; l2 norm of weights:  8.56707779725216\n",
            "Iteration#:  7260 ; Loss:  7.213104557556967 ; l2 norm of gradient:  1.5988919112569027 ; l2 norm of weights:  8.56702730859312\n",
            "Iteration#:  7264 ; Loss:  7.21301679154126 ; l2 norm of gradient:  1.598883286678175 ; l2 norm of weights:  8.566976820328376\n",
            "Iteration#:  7268 ; Loss:  7.2129290369481645 ; l2 norm of gradient:  1.5988746620936556 ; l2 norm of weights:  8.566926332457937\n",
            "Iteration#:  7272 ; Loss:  7.2128412733317235 ; l2 norm of gradient:  1.5988660375033412 ; l2 norm of weights:  8.566875844981793\n",
            "Iteration#:  7276 ; Loss:  7.212753516760051 ; l2 norm of gradient:  1.5988574129072306 ; l2 norm of weights:  8.566825357899946\n",
            "Iteration#:  7280 ; Loss:  7.2126657541176495 ; l2 norm of gradient:  1.5988487883053182 ; l2 norm of weights:  8.566774871212397\n",
            "Iteration#:  7284 ; Loss:  7.212577996687527 ; l2 norm of gradient:  1.5988401636976015 ; l2 norm of weights:  8.566724384919143\n",
            "Iteration#:  7288 ; Loss:  7.212490238940028 ; l2 norm of gradient:  1.5988315390840793 ; l2 norm of weights:  8.566673899020184\n",
            "Iteration#:  7292 ; Loss:  7.212402480170315 ; l2 norm of gradient:  1.5988229144647463 ; l2 norm of weights:  8.566623413515522\n",
            "Iteration#:  7296 ; Loss:  7.212314719810256 ; l2 norm of gradient:  1.5988142898395992 ; l2 norm of weights:  8.566572928405149\n",
            "Iteration#:  7300 ; Loss:  7.212226962819319 ; l2 norm of gradient:  1.5988056652086364 ; l2 norm of weights:  8.566522443689072\n",
            "Iteration#:  7304 ; Loss:  7.212139199590753 ; l2 norm of gradient:  1.5987970405718532 ; l2 norm of weights:  8.566471959367284\n",
            "Iteration#:  7308 ; Loss:  7.212051447155688 ; l2 norm of gradient:  1.598788415929247 ; l2 norm of weights:  8.566421475439792\n",
            "Iteration#:  7312 ; Loss:  7.211963694020739 ; l2 norm of gradient:  1.5987797912808148 ; l2 norm of weights:  8.566370991906586\n",
            "Iteration#:  7316 ; Loss:  7.211875938695162 ; l2 norm of gradient:  1.5987711666265525 ; l2 norm of weights:  8.56632050876767\n",
            "Iteration#:  7320 ; Loss:  7.2117881829489185 ; l2 norm of gradient:  1.5987625419664593 ; l2 norm of weights:  8.566270026023043\n",
            "Iteration#:  7324 ; Loss:  7.2117004306811126 ; l2 norm of gradient:  1.5987539173005292 ; l2 norm of weights:  8.566219543672702\n",
            "Iteration#:  7328 ; Loss:  7.211612673096845 ; l2 norm of gradient:  1.5987452926287615 ; l2 norm of weights:  8.56616906171665\n",
            "Iteration#:  7332 ; Loss:  7.211524920385978 ; l2 norm of gradient:  1.5987366679511508 ; l2 norm of weights:  8.566118580154884\n",
            "Iteration#:  7336 ; Loss:  7.211437171296072 ; l2 norm of gradient:  1.598728043267695 ; l2 norm of weights:  8.566068098987403\n",
            "Iteration#:  7340 ; Loss:  7.2113494204139705 ; l2 norm of gradient:  1.5987194185783915 ; l2 norm of weights:  8.566017618214207\n",
            "Iteration#:  7344 ; Loss:  7.2112616655610875 ; l2 norm of gradient:  1.5987107938832366 ; l2 norm of weights:  8.565967137835294\n",
            "Iteration#:  7348 ; Loss:  7.2111739164434425 ; l2 norm of gradient:  1.5987021691822259 ; l2 norm of weights:  8.565916657850662\n",
            "Iteration#:  7352 ; Loss:  7.2110861693349015 ; l2 norm of gradient:  1.5986935444753585 ; l2 norm of weights:  8.565866178260315\n",
            "Iteration#:  7356 ; Loss:  7.210998416838459 ; l2 norm of gradient:  1.5986849197626287 ; l2 norm of weights:  8.565815699064247\n",
            "Iteration#:  7360 ; Loss:  7.2109106648651435 ; l2 norm of gradient:  1.598676295044035 ; l2 norm of weights:  8.56576522026246\n",
            "Iteration#:  7364 ; Loss:  7.210822920056492 ; l2 norm of gradient:  1.5986676703195746 ; l2 norm of weights:  8.565714741854954\n",
            "Iteration#:  7368 ; Loss:  7.210735168916793 ; l2 norm of gradient:  1.5986590455892429 ; l2 norm of weights:  8.565664263841725\n",
            "Iteration#:  7372 ; Loss:  7.2106474179714155 ; l2 norm of gradient:  1.598650420853038 ; l2 norm of weights:  8.565613786222775\n",
            "Iteration#:  7376 ; Loss:  7.210559670878451 ; l2 norm of gradient:  1.5986417961109547 ; l2 norm of weights:  8.565563308998101\n",
            "Iteration#:  7380 ; Loss:  7.2104719271084505 ; l2 norm of gradient:  1.5986331713629929 ; l2 norm of weights:  8.565512832167705\n",
            "Iteration#:  7384 ; Loss:  7.210384181115492 ; l2 norm of gradient:  1.598624546609147 ; l2 norm of weights:  8.565462355731583\n",
            "Iteration#:  7388 ; Loss:  7.210296434784033 ; l2 norm of gradient:  1.598615921849415 ; l2 norm of weights:  8.565411879689735\n",
            "Iteration#:  7392 ; Loss:  7.210208691902743 ; l2 norm of gradient:  1.598607297083792 ; l2 norm of weights:  8.565361404042163\n",
            "Iteration#:  7396 ; Loss:  7.210120942241453 ; l2 norm of gradient:  1.5985986723122765 ; l2 norm of weights:  8.565310928788863\n",
            "Iteration#:  7400 ; Loss:  7.210033200118973 ; l2 norm of gradient:  1.5985900475348649 ; l2 norm of weights:  8.565260453929834\n",
            "Iteration#:  7404 ; Loss:  7.209945457669999 ; l2 norm of gradient:  1.5985814227515542 ; l2 norm of weights:  8.565209979465077\n",
            "Iteration#:  7408 ; Loss:  7.209857713680191 ; l2 norm of gradient:  1.5985727979623405 ; l2 norm of weights:  8.565159505394591\n",
            "Iteration#:  7412 ; Loss:  7.209769970848875 ; l2 norm of gradient:  1.5985641731672215 ; l2 norm of weights:  8.565109031718375\n",
            "Iteration#:  7416 ; Loss:  7.209682227615177 ; l2 norm of gradient:  1.5985555483661937 ; l2 norm of weights:  8.565058558436426\n",
            "Iteration#:  7420 ; Loss:  7.209594490303353 ; l2 norm of gradient:  1.5985469235592538 ; l2 norm of weights:  8.56500808554875\n",
            "Iteration#:  7424 ; Loss:  7.209506746533411 ; l2 norm of gradient:  1.5985382987463967 ; l2 norm of weights:  8.564957613055336\n",
            "Iteration#:  7428 ; Loss:  7.2094190053419975 ; l2 norm of gradient:  1.5985296739276227 ; l2 norm of weights:  8.564907140956192\n",
            "Iteration#:  7432 ; Loss:  7.2093312654144555 ; l2 norm of gradient:  1.5985210491029265 ; l2 norm of weights:  8.564856669251311\n",
            "Iteration#:  7436 ; Loss:  7.209243528775209 ; l2 norm of gradient:  1.598512424272306 ; l2 norm of weights:  8.564806197940696\n",
            "Iteration#:  7440 ; Loss:  7.20915578535665 ; l2 norm of gradient:  1.5985037994357565 ; l2 norm of weights:  8.564755727024346\n",
            "Iteration#:  7444 ; Loss:  7.209068048006044 ; l2 norm of gradient:  1.5984951745932765 ; l2 norm of weights:  8.564705256502258\n",
            "Iteration#:  7448 ; Loss:  7.2089803075682894 ; l2 norm of gradient:  1.598486549744862 ; l2 norm of weights:  8.564654786374431\n",
            "Iteration#:  7452 ; Loss:  7.208892575963236 ; l2 norm of gradient:  1.598477924890509 ; l2 norm of weights:  8.564604316640867\n",
            "Iteration#:  7456 ; Loss:  7.208804834906987 ; l2 norm of gradient:  1.5984693000302146 ; l2 norm of weights:  8.564553847301566\n",
            "Iteration#:  7460 ; Loss:  7.208717099961993 ; l2 norm of gradient:  1.5984606751639765 ; l2 norm of weights:  8.564503378356523\n",
            "Iteration#:  7464 ; Loss:  7.208629361633394 ; l2 norm of gradient:  1.598452050291791 ; l2 norm of weights:  8.564452909805738\n",
            "Iteration#:  7468 ; Loss:  7.208541625585104 ; l2 norm of gradient:  1.5984434254136553 ; l2 norm of weights:  8.564402441649213\n",
            "Iteration#:  7472 ; Loss:  7.208453892612704 ; l2 norm of gradient:  1.5984348005295652 ; l2 norm of weights:  8.564351973886943\n",
            "Iteration#:  7476 ; Loss:  7.208366161248128 ; l2 norm of gradient:  1.5984261756395188 ; l2 norm of weights:  8.564301506518934\n",
            "Iteration#:  7480 ; Loss:  7.208278422460742 ; l2 norm of gradient:  1.5984175507435112 ; l2 norm of weights:  8.564251039545178\n",
            "Iteration#:  7484 ; Loss:  7.208190692310306 ; l2 norm of gradient:  1.59840892584154 ; l2 norm of weights:  8.564200572965676\n",
            "Iteration#:  7488 ; Loss:  7.208102962148443 ; l2 norm of gradient:  1.5984003009336032 ; l2 norm of weights:  8.564150106780428\n",
            "Iteration#:  7492 ; Loss:  7.208015228052817 ; l2 norm of gradient:  1.5983916760196957 ; l2 norm of weights:  8.564099640989436\n",
            "Iteration#:  7496 ; Loss:  7.207927494015912 ; l2 norm of gradient:  1.598383051099815 ; l2 norm of weights:  8.564049175592697\n",
            "Iteration#:  7500 ; Loss:  7.207839764791658 ; l2 norm of gradient:  1.5983744261739588 ; l2 norm of weights:  8.563998710590207\n",
            "Iteration#:  7504 ; Loss:  7.207752031965153 ; l2 norm of gradient:  1.5983658012421216 ; l2 norm of weights:  8.563948245981969\n",
            "Iteration#:  7508 ; Loss:  7.207664302775001 ; l2 norm of gradient:  1.5983571763043027 ; l2 norm of weights:  8.56389778176798\n",
            "Iteration#:  7512 ; Loss:  7.207576570764496 ; l2 norm of gradient:  1.5983485513604985 ; l2 norm of weights:  8.563847317948241\n",
            "Iteration#:  7516 ; Loss:  7.207488843343447 ; l2 norm of gradient:  1.5983399264107034 ; l2 norm of weights:  8.563796854522753\n",
            "Iteration#:  7520 ; Loss:  7.207401113575031 ; l2 norm of gradient:  1.598331301454916 ; l2 norm of weights:  8.563746391491508\n",
            "Iteration#:  7524 ; Loss:  7.207313387922284 ; l2 norm of gradient:  1.5983226764931335 ; l2 norm of weights:  8.563695928854514\n",
            "Iteration#:  7528 ; Loss:  7.207225662004703 ; l2 norm of gradient:  1.5983140515253522 ; l2 norm of weights:  8.563645466611762\n",
            "Iteration#:  7532 ; Loss:  7.207137932745695 ; l2 norm of gradient:  1.5983054265515697 ; l2 norm of weights:  8.563595004763258\n",
            "Iteration#:  7536 ; Loss:  7.207050209866011 ; l2 norm of gradient:  1.5982968015717802 ; l2 norm of weights:  8.563544543308998\n",
            "Iteration#:  7540 ; Loss:  7.2069624785394355 ; l2 norm of gradient:  1.598288176585983 ; l2 norm of weights:  8.563494082248981\n",
            "Iteration#:  7544 ; Loss:  7.206874752171544 ; l2 norm of gradient:  1.5982795515941735 ; l2 norm of weights:  8.563443621583206\n",
            "Iteration#:  7548 ; Loss:  7.206787029404712 ; l2 norm of gradient:  1.598270926596349 ; l2 norm of weights:  8.563393161311673\n",
            "Iteration#:  7552 ; Loss:  7.206699307723186 ; l2 norm of gradient:  1.5982623015925064 ; l2 norm of weights:  8.563342701434385\n",
            "Iteration#:  7556 ; Loss:  7.20661158096696 ; l2 norm of gradient:  1.598253676582642 ; l2 norm of weights:  8.563292241951332\n",
            "Iteration#:  7560 ; Loss:  7.206523858039059 ; l2 norm of gradient:  1.5982450515667532 ; l2 norm of weights:  8.56324178286252\n",
            "Iteration#:  7564 ; Loss:  7.206436133579196 ; l2 norm of gradient:  1.5982364265448357 ; l2 norm of weights:  8.56319132416795\n",
            "Iteration#:  7568 ; Loss:  7.20634841031996 ; l2 norm of gradient:  1.598227801516888 ; l2 norm of weights:  8.563140865867613\n",
            "Iteration#:  7572 ; Loss:  7.206260688279452 ; l2 norm of gradient:  1.5982191764829057 ; l2 norm of weights:  8.563090407961516\n",
            "Iteration#:  7576 ; Loss:  7.206172971152911 ; l2 norm of gradient:  1.5982105514428857 ; l2 norm of weights:  8.563039950449653\n",
            "Iteration#:  7580 ; Loss:  7.206085248937805 ; l2 norm of gradient:  1.5982019263968246 ; l2 norm of weights:  8.562989493332026\n",
            "Iteration#:  7584 ; Loss:  7.205997528217044 ; l2 norm of gradient:  1.598193301344719 ; l2 norm of weights:  8.562939036608634\n",
            "Iteration#:  7588 ; Loss:  7.205909806849261 ; l2 norm of gradient:  1.5981846762865668 ; l2 norm of weights:  8.562888580279479\n",
            "Iteration#:  7592 ; Loss:  7.205822087759696 ; l2 norm of gradient:  1.5981760512223628 ; l2 norm of weights:  8.562838124344553\n",
            "Iteration#:  7596 ; Loss:  7.205734373228513 ; l2 norm of gradient:  1.598167426152106 ; l2 norm of weights:  8.56278766880386\n",
            "Iteration#:  7600 ; Loss:  7.205646653653941 ; l2 norm of gradient:  1.5981588010757917 ; l2 norm of weights:  8.562737213657396\n",
            "Iteration#:  7604 ; Loss:  7.205558934096956 ; l2 norm of gradient:  1.598150175993417 ; l2 norm of weights:  8.562686758905164\n",
            "Iteration#:  7608 ; Loss:  7.20547121924792 ; l2 norm of gradient:  1.5981415509049786 ; l2 norm of weights:  8.562636304547164\n",
            "Iteration#:  7612 ; Loss:  7.205383498943817 ; l2 norm of gradient:  1.598132925810473 ; l2 norm of weights:  8.56258585058339\n",
            "Iteration#:  7616 ; Loss:  7.2052957764704075 ; l2 norm of gradient:  1.5981243007098977 ; l2 norm of weights:  8.562535397013844\n",
            "Iteration#:  7620 ; Loss:  7.205208064308456 ; l2 norm of gradient:  1.5981156756032493 ; l2 norm of weights:  8.562484943838529\n",
            "Iteration#:  7624 ; Loss:  7.20512034860392 ; l2 norm of gradient:  1.598107050490524 ; l2 norm of weights:  8.562434491057434\n",
            "Iteration#:  7628 ; Loss:  7.205032638586635 ; l2 norm of gradient:  1.5980984253717192 ; l2 norm of weights:  8.562384038670569\n",
            "Iteration#:  7632 ; Loss:  7.204944918573765 ; l2 norm of gradient:  1.5980898002468304 ; l2 norm of weights:  8.562333586677926\n",
            "Iteration#:  7636 ; Loss:  7.204857205373455 ; l2 norm of gradient:  1.5980811751158557 ; l2 norm of weights:  8.562283135079511\n",
            "Iteration#:  7640 ; Loss:  7.204769497087776 ; l2 norm of gradient:  1.5980725499787913 ; l2 norm of weights:  8.562232683875315\n",
            "Iteration#:  7644 ; Loss:  7.204681785916417 ; l2 norm of gradient:  1.5980639248356343 ; l2 norm of weights:  8.562182233065345\n",
            "Iteration#:  7648 ; Loss:  7.204594072159592 ; l2 norm of gradient:  1.5980552996863817 ; l2 norm of weights:  8.562131782649594\n",
            "Iteration#:  7652 ; Loss:  7.204506359014736 ; l2 norm of gradient:  1.5980466745310293 ; l2 norm of weights:  8.562081332628063\n",
            "Iteration#:  7656 ; Loss:  7.204418646904068 ; l2 norm of gradient:  1.5980380493695734 ; l2 norm of weights:  8.562030883000752\n",
            "Iteration#:  7660 ; Loss:  7.204330939005561 ; l2 norm of gradient:  1.5980294242020128 ; l2 norm of weights:  8.56198043376766\n",
            "Iteration#:  7664 ; Loss:  7.204243227864763 ; l2 norm of gradient:  1.5980207990283426 ; l2 norm of weights:  8.561929984928787\n",
            "Iteration#:  7668 ; Loss:  7.204155520140049 ; l2 norm of gradient:  1.5980121738485598 ; l2 norm of weights:  8.561879536484131\n",
            "Iteration#:  7672 ; Loss:  7.204067808736312 ; l2 norm of gradient:  1.5980035486626607 ; l2 norm of weights:  8.561829088433692\n",
            "Iteration#:  7676 ; Loss:  7.20398010304705 ; l2 norm of gradient:  1.5979949234706448 ; l2 norm of weights:  8.561778640777467\n",
            "Iteration#:  7680 ; Loss:  7.203892396060189 ; l2 norm of gradient:  1.5979862982725055 ; l2 norm of weights:  8.561728193515458\n",
            "Iteration#:  7684 ; Loss:  7.203804687612179 ; l2 norm of gradient:  1.5979776730682405 ; l2 norm of weights:  8.561677746647664\n",
            "Iteration#:  7688 ; Loss:  7.2037169810255985 ; l2 norm of gradient:  1.597969047857847 ; l2 norm of weights:  8.561627300174084\n",
            "Iteration#:  7692 ; Loss:  7.203629271874679 ; l2 norm of gradient:  1.5979604226413207 ; l2 norm of weights:  8.561576854094714\n",
            "Iteration#:  7696 ; Loss:  7.2035415695181015 ; l2 norm of gradient:  1.5979517974186608 ; l2 norm of weights:  8.561526408409556\n",
            "Iteration#:  7700 ; Loss:  7.20345386095441 ; l2 norm of gradient:  1.5979431721898607 ; l2 norm of weights:  8.561475963118609\n",
            "Iteration#:  7704 ; Loss:  7.20336615797437 ; l2 norm of gradient:  1.59793454695492 ; l2 norm of weights:  8.561425518221872\n",
            "Iteration#:  7708 ; Loss:  7.203278453875238 ; l2 norm of gradient:  1.5979259217138335 ; l2 norm of weights:  8.561375073719343\n",
            "Iteration#:  7712 ; Loss:  7.203190750757374 ; l2 norm of gradient:  1.597917296466599 ; l2 norm of weights:  8.561324629611025\n",
            "Iteration#:  7716 ; Loss:  7.203103046136843 ; l2 norm of gradient:  1.5979086712132136 ; l2 norm of weights:  8.561274185896913\n",
            "Iteration#:  7720 ; Loss:  7.203015346044316 ; l2 norm of gradient:  1.597900045953672 ; l2 norm of weights:  8.561223742577008\n",
            "Iteration#:  7724 ; Loss:  7.202927640471829 ; l2 norm of gradient:  1.5978914206879729 ; l2 norm of weights:  8.56117329965131\n",
            "Iteration#:  7728 ; Loss:  7.202839942338063 ; l2 norm of gradient:  1.597882795416112 ; l2 norm of weights:  8.561122857119814\n",
            "Iteration#:  7732 ; Loss:  7.20275223769988 ; l2 norm of gradient:  1.5978741701380867 ; l2 norm of weights:  8.561072414982526\n",
            "Iteration#:  7736 ; Loss:  7.20266453614378 ; l2 norm of gradient:  1.5978655448538936 ; l2 norm of weights:  8.561021973239438\n",
            "Iteration#:  7740 ; Loss:  7.202576836831566 ; l2 norm of gradient:  1.5978569195635297 ; l2 norm of weights:  8.560971531890553\n",
            "Iteration#:  7744 ; Loss:  7.202489139082986 ; l2 norm of gradient:  1.5978482942669903 ; l2 norm of weights:  8.56092109093587\n",
            "Iteration#:  7748 ; Loss:  7.202401437518291 ; l2 norm of gradient:  1.597839668964273 ; l2 norm of weights:  8.56087065037539\n",
            "Iteration#:  7752 ; Loss:  7.202313740361536 ; l2 norm of gradient:  1.5978310436553753 ; l2 norm of weights:  8.560820210209108\n",
            "Iteration#:  7756 ; Loss:  7.202226040458119 ; l2 norm of gradient:  1.5978224183402938 ; l2 norm of weights:  8.560769770437027\n",
            "Iteration#:  7760 ; Loss:  7.2021383469359375 ; l2 norm of gradient:  1.597813793019023 ; l2 norm of weights:  8.560719331059143\n",
            "Iteration#:  7764 ; Loss:  7.2020506499312535 ; l2 norm of gradient:  1.5978051676915626 ; l2 norm of weights:  8.560668892075457\n",
            "Iteration#:  7768 ; Loss:  7.20196294820786 ; l2 norm of gradient:  1.597796542357907 ; l2 norm of weights:  8.560618453485967\n",
            "Iteration#:  7772 ; Loss:  7.20187525585234 ; l2 norm of gradient:  1.5977879170180547 ; l2 norm of weights:  8.560568015290674\n",
            "Iteration#:  7776 ; Loss:  7.201787557834956 ; l2 norm of gradient:  1.597779291672001 ; l2 norm of weights:  8.560517577489575\n",
            "Iteration#:  7780 ; Loss:  7.201699866586416 ; l2 norm of gradient:  1.597770666319744 ; l2 norm of weights:  8.560467140082674\n",
            "Iteration#:  7784 ; Loss:  7.201612172376015 ; l2 norm of gradient:  1.597762040961279 ; l2 norm of weights:  8.560416703069963\n",
            "Iteration#:  7788 ; Loss:  7.20152448009368 ; l2 norm of gradient:  1.5977534155966038 ; l2 norm of weights:  8.560366266451446\n",
            "Iteration#:  7792 ; Loss:  7.201436783375863 ; l2 norm of gradient:  1.5977447902257145 ; l2 norm of weights:  8.56031583022712\n",
            "Iteration#:  7796 ; Loss:  7.201349088560422 ; l2 norm of gradient:  1.597736164848608 ; l2 norm of weights:  8.560265394396987\n",
            "Iteration#:  7800 ; Loss:  7.201261399291136 ; l2 norm of gradient:  1.5977275394652803 ; l2 norm of weights:  8.56021495896104\n",
            "Iteration#:  7804 ; Loss:  7.2011737119035075 ; l2 norm of gradient:  1.597718914075729 ; l2 norm of weights:  8.560164523919287\n",
            "Iteration#:  7808 ; Loss:  7.201086016385305 ; l2 norm of gradient:  1.5977102886799515 ; l2 norm of weights:  8.560114089271721\n",
            "Iteration#:  7812 ; Loss:  7.200998325042487 ; l2 norm of gradient:  1.5977016632779428 ; l2 norm of weights:  8.560063655018345\n",
            "Iteration#:  7816 ; Loss:  7.200910633669315 ; l2 norm of gradient:  1.5976930378697 ; l2 norm of weights:  8.560013221159155\n",
            "Iteration#:  7820 ; Loss:  7.200822947603328 ; l2 norm of gradient:  1.5976844124552219 ; l2 norm of weights:  8.559962787694149\n",
            "Iteration#:  7824 ; Loss:  7.2007352570244105 ; l2 norm of gradient:  1.5976757870345015 ; l2 norm of weights:  8.559912354623329\n",
            "Iteration#:  7828 ; Loss:  7.200647566194314 ; l2 norm of gradient:  1.597667161607539 ; l2 norm of weights:  8.559861921946693\n",
            "Iteration#:  7832 ; Loss:  7.200559879086815 ; l2 norm of gradient:  1.5976585361743294 ; l2 norm of weights:  8.559811489664241\n",
            "Iteration#:  7836 ; Loss:  7.2004721850083495 ; l2 norm of gradient:  1.5976499107348685 ; l2 norm of weights:  8.559761057775974\n",
            "Iteration#:  7840 ; Loss:  7.200384504047349 ; l2 norm of gradient:  1.5976412852891548 ; l2 norm of weights:  8.559710626281888\n",
            "Iteration#:  7844 ; Loss:  7.200296816522018 ; l2 norm of gradient:  1.597632659837184 ; l2 norm of weights:  8.559660195181982\n",
            "Iteration#:  7848 ; Loss:  7.200209131338598 ; l2 norm of gradient:  1.5976240343789532 ; l2 norm of weights:  8.559609764476255\n",
            "Iteration#:  7852 ; Loss:  7.200121443041396 ; l2 norm of gradient:  1.5976154089144592 ; l2 norm of weights:  8.559559334164712\n",
            "Iteration#:  7856 ; Loss:  7.200033762663994 ; l2 norm of gradient:  1.5976067834436982 ; l2 norm of weights:  8.559508904247345\n",
            "Iteration#:  7860 ; Loss:  7.199946072800488 ; l2 norm of gradient:  1.5975981579666676 ; l2 norm of weights:  8.559458474724154\n",
            "Iteration#:  7864 ; Loss:  7.199858394161978 ; l2 norm of gradient:  1.5975895324833627 ; l2 norm of weights:  8.559408045595145\n",
            "Iteration#:  7868 ; Loss:  7.199770712559525 ; l2 norm of gradient:  1.5975809069937823 ; l2 norm of weights:  8.559357616860309\n",
            "Iteration#:  7872 ; Loss:  7.19968302104944 ; l2 norm of gradient:  1.5975722814979214 ; l2 norm of weights:  8.55930718851965\n",
            "Iteration#:  7876 ; Loss:  7.199595340806215 ; l2 norm of gradient:  1.5975636559957778 ; l2 norm of weights:  8.559256760573165\n",
            "Iteration#:  7880 ; Loss:  7.19950766268753 ; l2 norm of gradient:  1.597555030487347 ; l2 norm of weights:  8.559206333020855\n",
            "Iteration#:  7884 ; Loss:  7.199419979395937 ; l2 norm of gradient:  1.5975464049726265 ; l2 norm of weights:  8.559155905862717\n",
            "Iteration#:  7888 ; Loss:  7.199332297508867 ; l2 norm of gradient:  1.5975377794516121 ; l2 norm of weights:  8.559105479098752\n",
            "Iteration#:  7892 ; Loss:  7.199244621881148 ; l2 norm of gradient:  1.597529153924302 ; l2 norm of weights:  8.559055052728958\n",
            "Iteration#:  7896 ; Loss:  7.199156938482016 ; l2 norm of gradient:  1.5975205283906921 ; l2 norm of weights:  8.559004626753332\n",
            "Iteration#:  7900 ; Loss:  7.19906926128751 ; l2 norm of gradient:  1.597511902850779 ; l2 norm of weights:  8.558954201171877\n",
            "Iteration#:  7904 ; Loss:  7.198981577157056 ; l2 norm of gradient:  1.5975032773045592 ; l2 norm of weights:  8.558903775984595\n",
            "Iteration#:  7908 ; Loss:  7.198893900913877 ; l2 norm of gradient:  1.5974946517520292 ; l2 norm of weights:  8.558853351191477\n",
            "Iteration#:  7912 ; Loss:  7.198806226143398 ; l2 norm of gradient:  1.5974860261931871 ; l2 norm of weights:  8.558802926792529\n",
            "Iteration#:  7916 ; Loss:  7.198718545603857 ; l2 norm of gradient:  1.5974774006280277 ; l2 norm of weights:  8.558752502787746\n",
            "Iteration#:  7920 ; Loss:  7.198630871648849 ; l2 norm of gradient:  1.5974687750565495 ; l2 norm of weights:  8.558702079177127\n",
            "Iteration#:  7924 ; Loss:  7.198543194638406 ; l2 norm of gradient:  1.5974601494787468 ; l2 norm of weights:  8.558651655960675\n",
            "Iteration#:  7928 ; Loss:  7.198455520631617 ; l2 norm of gradient:  1.5974515238946188 ; l2 norm of weights:  8.558601233138388\n",
            "Iteration#:  7932 ; Loss:  7.198367843613811 ; l2 norm of gradient:  1.59744289830416 ; l2 norm of weights:  8.55855081071026\n",
            "Iteration#:  7936 ; Loss:  7.1982801693008165 ; l2 norm of gradient:  1.5974342727073696 ; l2 norm of weights:  8.558500388676299\n",
            "Iteration#:  7940 ; Loss:  7.198192491744905 ; l2 norm of gradient:  1.5974256471042416 ; l2 norm of weights:  8.558449967036497\n",
            "Iteration#:  7944 ; Loss:  7.198104821445503 ; l2 norm of gradient:  1.5974170214947747 ; l2 norm of weights:  8.558399545790856\n",
            "Iteration#:  7948 ; Loss:  7.198017144926748 ; l2 norm of gradient:  1.597408395878964 ; l2 norm of weights:  8.558349124939374\n",
            "Iteration#:  7952 ; Loss:  7.197929471048136 ; l2 norm of gradient:  1.5973997702568072 ; l2 norm of weights:  8.558298704482054\n",
            "Iteration#:  7956 ; Loss:  7.197841803336162 ; l2 norm of gradient:  1.597391144628301 ; l2 norm of weights:  8.55824828441889\n",
            "Iteration#:  7960 ; Loss:  7.197754128648217 ; l2 norm of gradient:  1.5973825189934416 ; l2 norm of weights:  8.558197864749884\n",
            "Iteration#:  7964 ; Loss:  7.1976664621024025 ; l2 norm of gradient:  1.597373893352226 ; l2 norm of weights:  8.558147445475035\n",
            "Iteration#:  7968 ; Loss:  7.197578787496825 ; l2 norm of gradient:  1.5973652677046513 ; l2 norm of weights:  8.558097026594341\n",
            "Iteration#:  7972 ; Loss:  7.197491117590195 ; l2 norm of gradient:  1.597356642050712 ; l2 norm of weights:  8.558046608107803\n",
            "Iteration#:  7976 ; Loss:  7.1974034518269185 ; l2 norm of gradient:  1.597348016390407 ; l2 norm of weights:  8.557996190015418\n",
            "Iteration#:  7980 ; Loss:  7.197315782025909 ; l2 norm of gradient:  1.5973393907237328 ; l2 norm of weights:  8.557945772317186\n",
            "Iteration#:  7984 ; Loss:  7.197228109539296 ; l2 norm of gradient:  1.5973307650506845 ; l2 norm of weights:  8.557895355013109\n",
            "Iteration#:  7988 ; Loss:  7.197140443327079 ; l2 norm of gradient:  1.5973221393712604 ; l2 norm of weights:  8.557844938103184\n",
            "Iteration#:  7992 ; Loss:  7.1970527771360935 ; l2 norm of gradient:  1.5973135136854575 ; l2 norm of weights:  8.557794521587406\n",
            "Iteration#:  7996 ; Loss:  7.196965113376628 ; l2 norm of gradient:  1.5973048879932703 ; l2 norm of weights:  8.557744105465781\n",
            "Iteration#:  8000 ; Loss:  7.196877442633538 ; l2 norm of gradient:  1.5972962622946962 ; l2 norm of weights:  8.557693689738302\n",
            "Iteration#:  8004 ; Loss:  7.196789778208807 ; l2 norm of gradient:  1.5972876365897344 ; l2 norm of weights:  8.557643274404976\n",
            "Iteration#:  8008 ; Loss:  7.196702112580192 ; l2 norm of gradient:  1.5972790108783779 ; l2 norm of weights:  8.557592859465796\n",
            "Iteration#:  8012 ; Loss:  7.196614445429066 ; l2 norm of gradient:  1.5972703851606256 ; l2 norm of weights:  8.557542444920761\n",
            "Iteration#:  8016 ; Loss:  7.196526785425759 ; l2 norm of gradient:  1.5972617594364733 ; l2 norm of weights:  8.557492030769872\n",
            "Iteration#:  8020 ; Loss:  7.196439119916919 ; l2 norm of gradient:  1.5972531337059175 ; l2 norm of weights:  8.557441617013131\n",
            "Iteration#:  8024 ; Loss:  7.196351455577174 ; l2 norm of gradient:  1.5972445079689561 ; l2 norm of weights:  8.557391203650532\n",
            "Iteration#:  8028 ; Loss:  7.196263793610106 ; l2 norm of gradient:  1.5972358822255832 ; l2 norm of weights:  8.557340790682076\n",
            "Iteration#:  8032 ; Loss:  7.196176129612848 ; l2 norm of gradient:  1.5972272564757988 ; l2 norm of weights:  8.557290378107764\n",
            "Iteration#:  8036 ; Loss:  7.196088465967055 ; l2 norm of gradient:  1.597218630719597 ; l2 norm of weights:  8.557239965927593\n",
            "Iteration#:  8040 ; Loss:  7.196000807452341 ; l2 norm of gradient:  1.597210004956976 ; l2 norm of weights:  8.557189554141562\n",
            "Iteration#:  8044 ; Loss:  7.195913145426889 ; l2 norm of gradient:  1.5972013791879314 ; l2 norm of weights:  8.557139142749675\n",
            "Iteration#:  8048 ; Loss:  7.195825487668481 ; l2 norm of gradient:  1.5971927534124597 ; l2 norm of weights:  8.557088731751925\n",
            "Iteration#:  8052 ; Loss:  7.195737827675945 ; l2 norm of gradient:  1.5971841276305585 ; l2 norm of weights:  8.557038321148315\n",
            "Iteration#:  8056 ; Loss:  7.195650169669084 ; l2 norm of gradient:  1.5971755018422242 ; l2 norm of weights:  8.55698791093884\n",
            "Iteration#:  8060 ; Loss:  7.195562509179338 ; l2 norm of gradient:  1.5971668760474524 ; l2 norm of weights:  8.556937501123505\n",
            "Iteration#:  8064 ; Loss:  7.195474849898586 ; l2 norm of gradient:  1.597158250246242 ; l2 norm of weights:  8.556887091702304\n",
            "Iteration#:  8068 ; Loss:  7.195387194474073 ; l2 norm of gradient:  1.5971496244385883 ; l2 norm of weights:  8.556836682675238\n",
            "Iteration#:  8072 ; Loss:  7.195299535709304 ; l2 norm of gradient:  1.5971409986244858 ; l2 norm of weights:  8.556786274042308\n",
            "Iteration#:  8076 ; Loss:  7.195211879336036 ; l2 norm of gradient:  1.5971323728039348 ; l2 norm of weights:  8.55673586580351\n",
            "Iteration#:  8080 ; Loss:  7.195124224169866 ; l2 norm of gradient:  1.5971237469769297 ; l2 norm of weights:  8.556685457958846\n",
            "Iteration#:  8084 ; Loss:  7.1950365675414325 ; l2 norm of gradient:  1.597115121143468 ; l2 norm of weights:  8.556635050508314\n",
            "Iteration#:  8088 ; Loss:  7.194948913776108 ; l2 norm of gradient:  1.597106495303546 ; l2 norm of weights:  8.556584643451913\n",
            "Iteration#:  8092 ; Loss:  7.194861260304811 ; l2 norm of gradient:  1.5970978694571607 ; l2 norm of weights:  8.556534236789641\n",
            "Iteration#:  8096 ; Loss:  7.194773607438686 ; l2 norm of gradient:  1.5970892436043076 ; l2 norm of weights:  8.5564838305215\n",
            "Iteration#:  8100 ; Loss:  7.194685950830797 ; l2 norm of gradient:  1.5970806177449854 ; l2 norm of weights:  8.556433424647487\n",
            "Iteration#:  8104 ; Loss:  7.194598300100111 ; l2 norm of gradient:  1.5970719918791885 ; l2 norm of weights:  8.556383019167603\n",
            "Iteration#:  8108 ; Loss:  7.194510644487108 ; l2 norm of gradient:  1.5970633660069153 ; l2 norm of weights:  8.556332614081844\n",
            "Iteration#:  8112 ; Loss:  7.194422991714884 ; l2 norm of gradient:  1.597054740128162 ; l2 norm of weights:  8.556282209390213\n",
            "Iteration#:  8116 ; Loss:  7.194335341839709 ; l2 norm of gradient:  1.5970461142429244 ; l2 norm of weights:  8.556231805092708\n",
            "Iteration#:  8120 ; Loss:  7.194247690146877 ; l2 norm of gradient:  1.5970374883511993 ; l2 norm of weights:  8.556181401189326\n",
            "Iteration#:  8124 ; Loss:  7.194160038990127 ; l2 norm of gradient:  1.5970288624529838 ; l2 norm of weights:  8.556130997680068\n",
            "Iteration#:  8128 ; Loss:  7.194072391604403 ; l2 norm of gradient:  1.5970202365482746 ; l2 norm of weights:  8.556080594564934\n",
            "Iteration#:  8132 ; Loss:  7.193984740944817 ; l2 norm of gradient:  1.5970116106370684 ; l2 norm of weights:  8.55603019184392\n",
            "Iteration#:  8136 ; Loss:  7.193897092365942 ; l2 norm of gradient:  1.5970029847193614 ; l2 norm of weights:  8.55597978951703\n",
            "Iteration#:  8140 ; Loss:  7.193809446296941 ; l2 norm of gradient:  1.5969943587951505 ; l2 norm of weights:  8.55592938758426\n",
            "Iteration#:  8144 ; Loss:  7.193721795069452 ; l2 norm of gradient:  1.596985732864432 ; l2 norm of weights:  8.55587898604561\n",
            "Iteration#:  8148 ; Loss:  7.193634146680424 ; l2 norm of gradient:  1.5969771069272023 ; l2 norm of weights:  8.555828584901077\n",
            "Iteration#:  8152 ; Loss:  7.1935465006893295 ; l2 norm of gradient:  1.5969684809834586 ; l2 norm of weights:  8.555778184150663\n",
            "Iteration#:  8156 ; Loss:  7.193458854885052 ; l2 norm of gradient:  1.596959855033198 ; l2 norm of weights:  8.555727783794365\n",
            "Iteration#:  8160 ; Loss:  7.193371211859153 ; l2 norm of gradient:  1.596951229076416 ; l2 norm of weights:  8.555677383832187\n",
            "Iteration#:  8164 ; Loss:  7.193283566983585 ; l2 norm of gradient:  1.5969426031131102 ; l2 norm of weights:  8.55562698426412\n",
            "Iteration#:  8168 ; Loss:  7.193195922489692 ; l2 norm of gradient:  1.596933977143276 ; l2 norm of weights:  8.555576585090169\n",
            "Iteration#:  8172 ; Loss:  7.193108280520288 ; l2 norm of gradient:  1.5969253511669106 ; l2 norm of weights:  8.555526186310335\n",
            "Iteration#:  8176 ; Loss:  7.193020634551143 ; l2 norm of gradient:  1.5969167251840106 ; l2 norm of weights:  8.55547578792461\n",
            "Iteration#:  8180 ; Loss:  7.192932989958016 ; l2 norm of gradient:  1.5969080991945737 ; l2 norm of weights:  8.555425389933\n",
            "Iteration#:  8184 ; Loss:  7.192845348597571 ; l2 norm of gradient:  1.5968994731985955 ; l2 norm of weights:  8.5553749923355\n",
            "Iteration#:  8188 ; Loss:  7.192757703178588 ; l2 norm of gradient:  1.5968908471960719 ; l2 norm of weights:  8.55532459513211\n",
            "Iteration#:  8192 ; Loss:  7.1926700605099505 ; l2 norm of gradient:  1.5968822211870006 ; l2 norm of weights:  8.55527419832283\n",
            "Iteration#:  8196 ; Loss:  7.1925824180236475 ; l2 norm of gradient:  1.5968735951713777 ; l2 norm of weights:  8.55522380190766\n",
            "Iteration#:  8200 ; Loss:  7.192494781930412 ; l2 norm of gradient:  1.5968649691492007 ; l2 norm of weights:  8.555173405886599\n",
            "Iteration#:  8204 ; Loss:  7.192407140844889 ; l2 norm of gradient:  1.5968563431204648 ; l2 norm of weights:  8.555123010259644\n",
            "Iteration#:  8208 ; Loss:  7.192319501018655 ; l2 norm of gradient:  1.5968477170851672 ; l2 norm of weights:  8.555072615026795\n",
            "Iteration#:  8212 ; Loss:  7.192231860835623 ; l2 norm of gradient:  1.596839091043305 ; l2 norm of weights:  8.555022220188052\n",
            "Iteration#:  8216 ; Loss:  7.192144227419986 ; l2 norm of gradient:  1.5968304649948746 ; l2 norm of weights:  8.554971825743415\n",
            "Iteration#:  8220 ; Loss:  7.192056589313935 ; l2 norm of gradient:  1.596821838939872 ; l2 norm of weights:  8.554921431692883\n",
            "Iteration#:  8224 ; Loss:  7.1919689489697705 ; l2 norm of gradient:  1.5968132128782941 ; l2 norm of weights:  8.554871038036453\n",
            "Iteration#:  8228 ; Loss:  7.191881308566849 ; l2 norm of gradient:  1.5968045868101381 ; l2 norm of weights:  8.554820644774123\n",
            "Iteration#:  8232 ; Loss:  7.191793673654427 ; l2 norm of gradient:  1.5967959607353996 ; l2 norm of weights:  8.554770251905897\n",
            "Iteration#:  8236 ; Loss:  7.191706038676724 ; l2 norm of gradient:  1.596787334654076 ; l2 norm of weights:  8.55471985943177\n",
            "Iteration#:  8240 ; Loss:  7.191618400748695 ; l2 norm of gradient:  1.5967787085661636 ; l2 norm of weights:  8.554669467351745\n",
            "Iteration#:  8244 ; Loss:  7.1915307684086685 ; l2 norm of gradient:  1.5967700824716589 ; l2 norm of weights:  8.55461907566582\n",
            "Iteration#:  8248 ; Loss:  7.191443135760009 ; l2 norm of gradient:  1.596761456370559 ; l2 norm of weights:  8.554568684373992\n",
            "Iteration#:  8252 ; Loss:  7.1913555008516425 ; l2 norm of gradient:  1.5967528302628593 ; l2 norm of weights:  8.55451829347626\n",
            "Iteration#:  8256 ; Loss:  7.191267869763846 ; l2 norm of gradient:  1.5967442041485578 ; l2 norm of weights:  8.554467902972627\n",
            "Iteration#:  8260 ; Loss:  7.191180233790026 ; l2 norm of gradient:  1.5967355780276495 ; l2 norm of weights:  8.554417512863088\n",
            "Iteration#:  8264 ; Loss:  7.191092602636047 ; l2 norm of gradient:  1.5967269519001335 ; l2 norm of weights:  8.554367123147644\n",
            "Iteration#:  8268 ; Loss:  7.1910049722060325 ; l2 norm of gradient:  1.5967183257660034 ; l2 norm of weights:  8.554316733826294\n",
            "Iteration#:  8272 ; Loss:  7.190917340799482 ; l2 norm of gradient:  1.5967096996252588 ; l2 norm of weights:  8.554266344899037\n",
            "Iteration#:  8276 ; Loss:  7.1908297074767775 ; l2 norm of gradient:  1.596701073477893 ; l2 norm of weights:  8.554215956365875\n",
            "Iteration#:  8280 ; Loss:  7.1907420811765785 ; l2 norm of gradient:  1.5966924473239055 ; l2 norm of weights:  8.554165568226804\n",
            "Iteration#:  8284 ; Loss:  7.190654450452435 ; l2 norm of gradient:  1.5966838211632906 ; l2 norm of weights:  8.554115180481823\n",
            "Iteration#:  8288 ; Loss:  7.190566822185158 ; l2 norm of gradient:  1.5966751949960465 ; l2 norm of weights:  8.554064793130932\n",
            "Iteration#:  8292 ; Loss:  7.190479192055841 ; l2 norm of gradient:  1.5966665688221702 ; l2 norm of weights:  8.55401440617413\n",
            "Iteration#:  8296 ; Loss:  7.190391561323061 ; l2 norm of gradient:  1.5966579426416567 ; l2 norm of weights:  8.553964019611417\n",
            "Iteration#:  8300 ; Loss:  7.190303937767437 ; l2 norm of gradient:  1.5966493164545028 ; l2 norm of weights:  8.553913633442793\n",
            "Iteration#:  8304 ; Loss:  7.190216307381396 ; l2 norm of gradient:  1.596640690260706 ; l2 norm of weights:  8.553863247668254\n",
            "Iteration#:  8308 ; Loss:  7.190128683594057 ; l2 norm of gradient:  1.596632064060262 ; l2 norm of weights:  8.553812862287801\n",
            "Iteration#:  8312 ; Loss:  7.190041053417167 ; l2 norm of gradient:  1.5966234378531676 ; l2 norm of weights:  8.553762477301433\n",
            "Iteration#:  8316 ; Loss:  7.189953429519207 ; l2 norm of gradient:  1.5966148116394208 ; l2 norm of weights:  8.55371209270915\n",
            "Iteration#:  8320 ; Loss:  7.189865808870975 ; l2 norm of gradient:  1.5966061854190159 ; l2 norm of weights:  8.55366170851095\n",
            "Iteration#:  8324 ; Loss:  7.189778183897221 ; l2 norm of gradient:  1.596597559191951 ; l2 norm of weights:  8.553611324706834\n",
            "Iteration#:  8328 ; Loss:  7.189690562053288 ; l2 norm of gradient:  1.5965889329582215 ; l2 norm of weights:  8.553560941296798\n",
            "Iteration#:  8332 ; Loss:  7.189602934469882 ; l2 norm of gradient:  1.5965803067178252 ; l2 norm of weights:  8.553510558280845\n",
            "Iteration#:  8336 ; Loss:  7.1895153116692825 ; l2 norm of gradient:  1.5965716804707575 ; l2 norm of weights:  8.55346017565897\n",
            "Iteration#:  8340 ; Loss:  7.189427689899691 ; l2 norm of gradient:  1.5965630542170162 ; l2 norm of weights:  8.553409793431175\n",
            "Iteration#:  8344 ; Loss:  7.189340070458412 ; l2 norm of gradient:  1.5965544279565969 ; l2 norm of weights:  8.55335941159746\n",
            "Iteration#:  8348 ; Loss:  7.189252449168641 ; l2 norm of gradient:  1.596545801689497 ; l2 norm of weights:  8.55330903015782\n",
            "Iteration#:  8352 ; Loss:  7.189164825281802 ; l2 norm of gradient:  1.5965371754157125 ; l2 norm of weights:  8.553258649112259\n",
            "Iteration#:  8356 ; Loss:  7.189077209296654 ; l2 norm of gradient:  1.59652854913524 ; l2 norm of weights:  8.553208268460773\n",
            "Iteration#:  8360 ; Loss:  7.188989585614214 ; l2 norm of gradient:  1.596519922848076 ; l2 norm of weights:  8.553157888203364\n",
            "Iteration#:  8364 ; Loss:  7.188901968920028 ; l2 norm of gradient:  1.596511296554217 ; l2 norm of weights:  8.553107508340027\n",
            "Iteration#:  8368 ; Loss:  7.1888143481529365 ; l2 norm of gradient:  1.5965026702536596 ; l2 norm of weights:  8.553057128870767\n",
            "Iteration#:  8372 ; Loss:  7.188726729053501 ; l2 norm of gradient:  1.5964940439464002 ; l2 norm of weights:  8.553006749795577\n",
            "Iteration#:  8376 ; Loss:  7.188639109000363 ; l2 norm of gradient:  1.596485417632437 ; l2 norm of weights:  8.55295637111446\n",
            "Iteration#:  8380 ; Loss:  7.188551491354534 ; l2 norm of gradient:  1.5964767913117648 ; l2 norm of weights:  8.552905992827412\n",
            "Iteration#:  8384 ; Loss:  7.188463877555691 ; l2 norm of gradient:  1.5964681649843802 ; l2 norm of weights:  8.552855614934435\n",
            "Iteration#:  8388 ; Loss:  7.188376261163727 ; l2 norm of gradient:  1.59645953865028 ; l2 norm of weights:  8.552805237435532\n",
            "Iteration#:  8392 ; Loss:  7.188288646823556 ; l2 norm of gradient:  1.5964509123094612 ; l2 norm of weights:  8.552754860330692\n",
            "Iteration#:  8396 ; Loss:  7.188201027404022 ; l2 norm of gradient:  1.59644228596192 ; l2 norm of weights:  8.552704483619923\n",
            "Iteration#:  8400 ; Loss:  7.188113412667368 ; l2 norm of gradient:  1.5964336596076534 ; l2 norm of weights:  8.55265410730322\n",
            "Iteration#:  8404 ; Loss:  7.188025796777573 ; l2 norm of gradient:  1.5964250332466576 ; l2 norm of weights:  8.552603731380582\n",
            "Iteration#:  8408 ; Loss:  7.187938188979556 ; l2 norm of gradient:  1.5964164068789286 ; l2 norm of weights:  8.55255335585201\n",
            "Iteration#:  8412 ; Loss:  7.187850574670517 ; l2 norm of gradient:  1.596407780504464 ; l2 norm of weights:  8.552502980717504\n",
            "Iteration#:  8416 ; Loss:  7.187762960593785 ; l2 norm of gradient:  1.5963991541232587 ; l2 norm of weights:  8.55245260597706\n",
            "Iteration#:  8420 ; Loss:  7.1876753499373685 ; l2 norm of gradient:  1.5963905277353123 ; l2 norm of weights:  8.55240223163068\n",
            "Iteration#:  8424 ; Loss:  7.187587738548428 ; l2 norm of gradient:  1.5963819013406184 ; l2 norm of weights:  8.55235185767836\n",
            "Iteration#:  8428 ; Loss:  7.18750012891603 ; l2 norm of gradient:  1.5963732749391744 ; l2 norm of weights:  8.552301484120104\n",
            "Iteration#:  8432 ; Loss:  7.18741251867311 ; l2 norm of gradient:  1.5963646485309775 ; l2 norm of weights:  8.55225111095591\n",
            "Iteration#:  8436 ; Loss:  7.187324905746019 ; l2 norm of gradient:  1.5963560221160238 ; l2 norm of weights:  8.552200738185773\n",
            "Iteration#:  8440 ; Loss:  7.187237296066863 ; l2 norm of gradient:  1.596347395694309 ; l2 norm of weights:  8.552150365809695\n",
            "Iteration#:  8444 ; Loss:  7.187149690801041 ; l2 norm of gradient:  1.5963387692658308 ; l2 norm of weights:  8.552099993827674\n",
            "Iteration#:  8448 ; Loss:  7.187062078392656 ; l2 norm of gradient:  1.5963301428305858 ; l2 norm of weights:  8.552049622239712\n",
            "Iteration#:  8452 ; Loss:  7.186974472615516 ; l2 norm of gradient:  1.5963215163885702 ; l2 norm of weights:  8.551999251045805\n",
            "Iteration#:  8456 ; Loss:  7.186886865961821 ; l2 norm of gradient:  1.5963128899397805 ; l2 norm of weights:  8.551948880245954\n",
            "Iteration#:  8460 ; Loss:  7.186799259326946 ; l2 norm of gradient:  1.5963042634842126 ; l2 norm of weights:  8.551898509840157\n",
            "Iteration#:  8464 ; Loss:  7.186711651396267 ; l2 norm of gradient:  1.5962956370218646 ; l2 norm of weights:  8.551848139828415\n",
            "Iteration#:  8468 ; Loss:  7.186624045387852 ; l2 norm of gradient:  1.596287010552731 ; l2 norm of weights:  8.551797770210724\n",
            "Iteration#:  8472 ; Loss:  7.186536443664524 ; l2 norm of gradient:  1.59627838407681 ; l2 norm of weights:  8.551747400987088\n",
            "Iteration#:  8476 ; Loss:  7.186448834491639 ; l2 norm of gradient:  1.5962697575940985 ; l2 norm of weights:  8.551697032157502\n",
            "Iteration#:  8480 ; Loss:  7.186361228099887 ; l2 norm of gradient:  1.5962611311045913 ; l2 norm of weights:  8.551646663721966\n",
            "Iteration#:  8484 ; Loss:  7.186273624374048 ; l2 norm of gradient:  1.5962525046082858 ; l2 norm of weights:  8.551596295680481\n",
            "Iteration#:  8488 ; Loss:  7.186186022848913 ; l2 norm of gradient:  1.5962438781051784 ; l2 norm of weights:  8.551545928033045\n",
            "Iteration#:  8492 ; Loss:  7.186098420323379 ; l2 norm of gradient:  1.5962352515952662 ; l2 norm of weights:  8.551495560779657\n",
            "Iteration#:  8496 ; Loss:  7.18601081597407 ; l2 norm of gradient:  1.5962266250785446 ; l2 norm of weights:  8.551445193920314\n",
            "Iteration#:  8500 ; Loss:  7.185923217545053 ; l2 norm of gradient:  1.5962179985550107 ; l2 norm of weights:  8.55139482745502\n",
            "Iteration#:  8504 ; Loss:  7.185835611723731 ; l2 norm of gradient:  1.5962093720246613 ; l2 norm of weights:  8.551344461383769\n",
            "Iteration#:  8508 ; Loss:  7.185748014651086 ; l2 norm of gradient:  1.5962007454874934 ; l2 norm of weights:  8.551294095706565\n",
            "Iteration#:  8512 ; Loss:  7.185660412543767 ; l2 norm of gradient:  1.5961921189435029 ; l2 norm of weights:  8.551243730423405\n",
            "Iteration#:  8516 ; Loss:  7.1855728150107225 ; l2 norm of gradient:  1.5961834923926848 ; l2 norm of weights:  8.551193365534289\n",
            "Iteration#:  8520 ; Loss:  7.185485217920643 ; l2 norm of gradient:  1.5961748658350385 ; l2 norm of weights:  8.551143001039215\n",
            "Iteration#:  8524 ; Loss:  7.185397616993769 ; l2 norm of gradient:  1.5961662392705593 ; l2 norm of weights:  8.551092636938181\n",
            "Iteration#:  8528 ; Loss:  7.185310018995901 ; l2 norm of gradient:  1.5961576126992425 ; l2 norm of weights:  8.551042273231186\n",
            "Iteration#:  8532 ; Loss:  7.1852224242043 ; l2 norm of gradient:  1.5961489861210856 ; l2 norm of weights:  8.550991909918235\n",
            "Iteration#:  8536 ; Loss:  7.185134825884662 ; l2 norm of gradient:  1.5961403595360864 ; l2 norm of weights:  8.55094154699932\n",
            "Iteration#:  8540 ; Loss:  7.185047223706535 ; l2 norm of gradient:  1.5961317329442393 ; l2 norm of weights:  8.550891184474443\n",
            "Iteration#:  8544 ; Loss:  7.184959635205779 ; l2 norm of gradient:  1.5961231063455423 ; l2 norm of weights:  8.550840822343607\n",
            "Iteration#:  8548 ; Loss:  7.1848720374505195 ; l2 norm of gradient:  1.5961144797399909 ; l2 norm of weights:  8.550790460606803\n",
            "Iteration#:  8552 ; Loss:  7.184784442372097 ; l2 norm of gradient:  1.5961058531275822 ; l2 norm of weights:  8.550740099264038\n",
            "Iteration#:  8556 ; Loss:  7.184696845366286 ; l2 norm of gradient:  1.5960972265083129 ; l2 norm of weights:  8.550689738315308\n",
            "Iteration#:  8560 ; Loss:  7.184609254525162 ; l2 norm of gradient:  1.5960885998821792 ; l2 norm of weights:  8.550639377760609\n",
            "Iteration#:  8564 ; Loss:  7.18452166124483 ; l2 norm of gradient:  1.5960799732491773 ; l2 norm of weights:  8.550589017599947\n",
            "Iteration#:  8568 ; Loss:  7.184434065306944 ; l2 norm of gradient:  1.5960713466093042 ; l2 norm of weights:  8.550538657833314\n",
            "Iteration#:  8572 ; Loss:  7.1843464754964055 ; l2 norm of gradient:  1.5960627199625552 ; l2 norm of weights:  8.550488298460715\n",
            "Iteration#:  8576 ; Loss:  7.18425888369068 ; l2 norm of gradient:  1.5960540933089296 ; l2 norm of weights:  8.550437939482146\n",
            "Iteration#:  8580 ; Loss:  7.184171294172938 ; l2 norm of gradient:  1.5960454666484214 ; l2 norm of weights:  8.550387580897606\n",
            "Iteration#:  8584 ; Loss:  7.1840836988854635 ; l2 norm of gradient:  1.5960368399810274 ; l2 norm of weights:  8.550337222707096\n",
            "Iteration#:  8588 ; Loss:  7.183996110746378 ; l2 norm of gradient:  1.596028213306745 ; l2 norm of weights:  8.550286864910614\n",
            "Iteration#:  8592 ; Loss:  7.183908522483149 ; l2 norm of gradient:  1.5960195866255704 ; l2 norm of weights:  8.550236507508162\n",
            "Iteration#:  8596 ; Loss:  7.1838209317473805 ; l2 norm of gradient:  1.5960109599375 ; l2 norm of weights:  8.550186150499734\n",
            "Iteration#:  8600 ; Loss:  7.183733345064201 ; l2 norm of gradient:  1.5960023332425295 ; l2 norm of weights:  8.550135793885332\n",
            "Iteration#:  8604 ; Loss:  7.183645755126523 ; l2 norm of gradient:  1.5959937065406573 ; l2 norm of weights:  8.550085437664956\n",
            "Iteration#:  8608 ; Loss:  7.1835581677484655 ; l2 norm of gradient:  1.5959850798318778 ; l2 norm of weights:  8.550035081838603\n",
            "Iteration#:  8612 ; Loss:  7.183470578627427 ; l2 norm of gradient:  1.5959764531161889 ; l2 norm of weights:  8.549984726406274\n",
            "Iteration#:  8616 ; Loss:  7.183382991119374 ; l2 norm of gradient:  1.5959678263935873 ; l2 norm of weights:  8.549934371367968\n",
            "Iteration#:  8620 ; Loss:  7.183295403488136 ; l2 norm of gradient:  1.5959591996640685 ; l2 norm of weights:  8.549884016723682\n",
            "Iteration#:  8624 ; Loss:  7.183207819159822 ; l2 norm of gradient:  1.5959505729276293 ; l2 norm of weights:  8.549833662473418\n",
            "Iteration#:  8628 ; Loss:  7.183120235516622 ; l2 norm of gradient:  1.5959419461842663 ; l2 norm of weights:  8.549783308617178\n",
            "Iteration#:  8632 ; Loss:  7.183032654199314 ; l2 norm of gradient:  1.595933319433975 ; l2 norm of weights:  8.549732955154951\n",
            "Iteration#:  8636 ; Loss:  7.182945066047595 ; l2 norm of gradient:  1.5959246926767547 ; l2 norm of weights:  8.549682602086747\n",
            "Iteration#:  8640 ; Loss:  7.1828574833167345 ; l2 norm of gradient:  1.5959160659125984 ; l2 norm of weights:  8.549632249412557\n",
            "Iteration#:  8644 ; Loss:  7.182769902658357 ; l2 norm of gradient:  1.5959074391415051 ; l2 norm of weights:  8.549581897132386\n",
            "Iteration#:  8648 ; Loss:  7.182682321779952 ; l2 norm of gradient:  1.5958988123634705 ; l2 norm of weights:  8.54953154524623\n",
            "Iteration#:  8652 ; Loss:  7.182594738055102 ; l2 norm of gradient:  1.5958901855784906 ; l2 norm of weights:  8.549481193754088\n",
            "Iteration#:  8656 ; Loss:  7.182507158603584 ; l2 norm of gradient:  1.5958815587865618 ; l2 norm of weights:  8.549430842655964\n",
            "Iteration#:  8660 ; Loss:  7.182419577860212 ; l2 norm of gradient:  1.595872931987683 ; l2 norm of weights:  8.549380491951851\n",
            "Iteration#:  8664 ; Loss:  7.1823319922963105 ; l2 norm of gradient:  1.5958643051818475 ; l2 norm of weights:  8.549330141641748\n",
            "Iteration#:  8668 ; Loss:  7.182244417297444 ; l2 norm of gradient:  1.5958556783690534 ; l2 norm of weights:  8.54927979172566\n",
            "Iteration#:  8672 ; Loss:  7.182156835502251 ; l2 norm of gradient:  1.5958470515492966 ; l2 norm of weights:  8.549229442203584\n",
            "Iteration#:  8676 ; Loss:  7.182069256831654 ; l2 norm of gradient:  1.5958384247225736 ; l2 norm of weights:  8.549179093075518\n",
            "Iteration#:  8680 ; Loss:  7.181981677403368 ; l2 norm of gradient:  1.5958297978888816 ; l2 norm of weights:  8.54912874434146\n",
            "Iteration#:  8684 ; Loss:  7.181894100500916 ; l2 norm of gradient:  1.5958211710482162 ; l2 norm of weights:  8.54907839600141\n",
            "Iteration#:  8688 ; Loss:  7.181806520764013 ; l2 norm of gradient:  1.5958125442005753 ; l2 norm of weights:  8.54902804805537\n",
            "Iteration#:  8692 ; Loss:  7.181718945985564 ; l2 norm of gradient:  1.5958039173459535 ; l2 norm of weights:  8.548977700503334\n",
            "Iteration#:  8696 ; Loss:  7.18163137136554 ; l2 norm of gradient:  1.5957952904843489 ; l2 norm of weights:  8.548927353345304\n",
            "Iteration#:  8700 ; Loss:  7.181543795089146 ; l2 norm of gradient:  1.595786663615756 ; l2 norm of weights:  8.54887700658128\n",
            "Iteration#:  8704 ; Loss:  7.181456217644099 ; l2 norm of gradient:  1.5957780367401724 ; l2 norm of weights:  8.548826660211263\n",
            "Iteration#:  8708 ; Loss:  7.1813686432879535 ; l2 norm of gradient:  1.5957694098575967 ; l2 norm of weights:  8.548776314235248\n",
            "Iteration#:  8712 ; Loss:  7.181281071664968 ; l2 norm of gradient:  1.5957607829680225 ; l2 norm of weights:  8.548725968653235\n",
            "Iteration#:  8716 ; Loss:  7.181193494632012 ; l2 norm of gradient:  1.5957521560714456 ; l2 norm of weights:  8.548675623465224\n",
            "Iteration#:  8720 ; Loss:  7.181105920340038 ; l2 norm of gradient:  1.5957435291678654 ; l2 norm of weights:  8.548625278671214\n",
            "Iteration#:  8724 ; Loss:  7.181018346882459 ; l2 norm of gradient:  1.5957349022572762 ; l2 norm of weights:  8.548574934271205\n",
            "Iteration#:  8728 ; Loss:  7.180930772138913 ; l2 norm of gradient:  1.595726275339676 ; l2 norm of weights:  8.548524590265194\n",
            "Iteration#:  8732 ; Loss:  7.180843204465446 ; l2 norm of gradient:  1.5957176484150593 ; l2 norm of weights:  8.548474246653184\n",
            "Iteration#:  8736 ; Loss:  7.180755631329868 ; l2 norm of gradient:  1.5957090214834253 ; l2 norm of weights:  8.54842390343517\n",
            "Iteration#:  8740 ; Loss:  7.180668061647902 ; l2 norm of gradient:  1.5957003945447685 ; l2 norm of weights:  8.548373560611154\n",
            "Iteration#:  8744 ; Loss:  7.180580496216395 ; l2 norm of gradient:  1.5956917675990854 ; l2 norm of weights:  8.548323218181133\n",
            "Iteration#:  8748 ; Loss:  7.180492921163573 ; l2 norm of gradient:  1.5956831406463734 ; l2 norm of weights:  8.54827287614511\n",
            "Iteration#:  8752 ; Loss:  7.180405355148424 ; l2 norm of gradient:  1.5956745136866282 ; l2 norm of weights:  8.54822253450308\n",
            "Iteration#:  8756 ; Loss:  7.1803177890501 ; l2 norm of gradient:  1.5956658867198459 ; l2 norm of weights:  8.548172193255043\n",
            "Iteration#:  8760 ; Loss:  7.180230218001396 ; l2 norm of gradient:  1.5956572597460241 ; l2 norm of weights:  8.548121852401\n",
            "Iteration#:  8764 ; Loss:  7.180142654059589 ; l2 norm of gradient:  1.595648632765159 ; l2 norm of weights:  8.548071511940947\n",
            "Iteration#:  8768 ; Loss:  7.180055086801799 ; l2 norm of gradient:  1.5956400057772462 ; l2 norm of weights:  8.548021171874884\n",
            "Iteration#:  8772 ; Loss:  7.179967517469357 ; l2 norm of gradient:  1.5956313787822827 ; l2 norm of weights:  8.547970832202815\n",
            "Iteration#:  8776 ; Loss:  7.179879950003539 ; l2 norm of gradient:  1.5956227517802655 ; l2 norm of weights:  8.547920492924735\n",
            "Iteration#:  8780 ; Loss:  7.1797923843728135 ; l2 norm of gradient:  1.5956141247711908 ; l2 norm of weights:  8.547870154040643\n",
            "Iteration#:  8784 ; Loss:  7.179704820026789 ; l2 norm of gradient:  1.5956054977550544 ; l2 norm of weights:  8.54781981555054\n",
            "Iteration#:  8788 ; Loss:  7.179617254536199 ; l2 norm of gradient:  1.5955968707318533 ; l2 norm of weights:  8.547769477454423\n",
            "Iteration#:  8792 ; Loss:  7.179529692066824 ; l2 norm of gradient:  1.5955882437015823 ; l2 norm of weights:  8.547719139752292\n",
            "Iteration#:  8796 ; Loss:  7.179442128275333 ; l2 norm of gradient:  1.5955796166642413 ; l2 norm of weights:  8.547668802444147\n",
            "Iteration#:  8800 ; Loss:  7.179354568901771 ; l2 norm of gradient:  1.5955709896198238 ; l2 norm of weights:  8.547618465529988\n",
            "Iteration#:  8804 ; Loss:  7.17926700285733 ; l2 norm of gradient:  1.5955623625683273 ; l2 norm of weights:  8.54756812900981\n",
            "Iteration#:  8808 ; Loss:  7.179179440426653 ; l2 norm of gradient:  1.595553735509749 ; l2 norm of weights:  8.547517792883617\n",
            "Iteration#:  8812 ; Loss:  7.179091886166151 ; l2 norm of gradient:  1.5955451084440844 ; l2 norm of weights:  8.547467457151404\n",
            "Iteration#:  8816 ; Loss:  7.179004319625088 ; l2 norm of gradient:  1.5955364813713295 ; l2 norm of weights:  8.547417121813174\n",
            "Iteration#:  8820 ; Loss:  7.17891676037997 ; l2 norm of gradient:  1.5955278542914813 ; l2 norm of weights:  8.547366786868924\n",
            "Iteration#:  8824 ; Loss:  7.178829199868806 ; l2 norm of gradient:  1.5955192272045375 ; l2 norm of weights:  8.547316452318654\n",
            "Iteration#:  8828 ; Loss:  7.178741639446435 ; l2 norm of gradient:  1.5955106001104924 ; l2 norm of weights:  8.547266118162364\n",
            "Iteration#:  8832 ; Loss:  7.178654080893499 ; l2 norm of gradient:  1.595501973009343 ; l2 norm of weights:  8.54721578440005\n",
            "Iteration#:  8836 ; Loss:  7.178566522265547 ; l2 norm of gradient:  1.5954933459010867 ; l2 norm of weights:  8.547165451031715\n",
            "Iteration#:  8840 ; Loss:  7.17847896486638 ; l2 norm of gradient:  1.5954847187857197 ; l2 norm of weights:  8.547115118057354\n",
            "Iteration#:  8844 ; Loss:  7.178391406482914 ; l2 norm of gradient:  1.5954760916632367 ; l2 norm of weights:  8.54706478547697\n",
            "Iteration#:  8848 ; Loss:  7.178303853833687 ; l2 norm of gradient:  1.5954674645336375 ; l2 norm of weights:  8.54701445329056\n",
            "Iteration#:  8852 ; Loss:  7.178216295844238 ; l2 norm of gradient:  1.595458837396915 ; l2 norm of weights:  8.546964121498126\n",
            "Iteration#:  8856 ; Loss:  7.178128735488267 ; l2 norm of gradient:  1.5954502102530674 ; l2 norm of weights:  8.546913790099662\n",
            "Iteration#:  8860 ; Loss:  7.178041185522948 ; l2 norm of gradient:  1.595441583102091 ; l2 norm of weights:  8.54686345909517\n",
            "Iteration#:  8864 ; Loss:  7.177953626203661 ; l2 norm of gradient:  1.5954329559439824 ; l2 norm of weights:  8.546813128484652\n",
            "Iteration#:  8868 ; Loss:  7.177866074404101 ; l2 norm of gradient:  1.5954243287787377 ; l2 norm of weights:  8.546762798268105\n",
            "Iteration#:  8872 ; Loss:  7.177778526288044 ; l2 norm of gradient:  1.5954157016063537 ; l2 norm of weights:  8.546712468445525\n",
            "Iteration#:  8876 ; Loss:  7.177690969568927 ; l2 norm of gradient:  1.5954070744268258 ; l2 norm of weights:  8.546662139016918\n",
            "Iteration#:  8880 ; Loss:  7.177603415563632 ; l2 norm of gradient:  1.5953984472401521 ; l2 norm of weights:  8.546611809982275\n",
            "Iteration#:  8884 ; Loss:  7.177515865573396 ; l2 norm of gradient:  1.5953898200463275 ; l2 norm of weights:  8.5465614813416\n",
            "Iteration#:  8888 ; Loss:  7.177428316151035 ; l2 norm of gradient:  1.5953811928453496 ; l2 norm of weights:  8.546511153094894\n",
            "Iteration#:  8892 ; Loss:  7.1773407660467985 ; l2 norm of gradient:  1.5953725656372135 ; l2 norm of weights:  8.546460825242152\n",
            "Iteration#:  8896 ; Loss:  7.177253212545274 ; l2 norm of gradient:  1.5953639384219171 ; l2 norm of weights:  8.546410497783373\n",
            "Iteration#:  8900 ; Loss:  7.177165663075452 ; l2 norm of gradient:  1.5953553111994558 ; l2 norm of weights:  8.54636017071856\n",
            "Iteration#:  8904 ; Loss:  7.177078113681423 ; l2 norm of gradient:  1.595346683969827 ; l2 norm of weights:  8.54630984404771\n",
            "Iteration#:  8908 ; Loss:  7.176990566369805 ; l2 norm of gradient:  1.595338056733026 ; l2 norm of weights:  8.546259517770823\n",
            "Iteration#:  8912 ; Loss:  7.176903015928616 ; l2 norm of gradient:  1.59532942948905 ; l2 norm of weights:  8.546209191887895\n",
            "Iteration#:  8916 ; Loss:  7.176815469593841 ; l2 norm of gradient:  1.595320802237894 ; l2 norm of weights:  8.546158866398931\n",
            "Iteration#:  8920 ; Loss:  7.176727923808979 ; l2 norm of gradient:  1.5953121749795571 ; l2 norm of weights:  8.546108541303925\n",
            "Iteration#:  8924 ; Loss:  7.176640374153217 ; l2 norm of gradient:  1.595303547714033 ; l2 norm of weights:  8.546058216602878\n",
            "Iteration#:  8928 ; Loss:  7.176552833557624 ; l2 norm of gradient:  1.5952949204413196 ; l2 norm of weights:  8.54600789229579\n",
            "Iteration#:  8932 ; Loss:  7.1764652838831875 ; l2 norm of gradient:  1.5952862931614127 ; l2 norm of weights:  8.54595756838266\n",
            "Iteration#:  8936 ; Loss:  7.17637773836009 ; l2 norm of gradient:  1.5952776658743086 ; l2 norm of weights:  8.545907244863484\n",
            "Iteration#:  8940 ; Loss:  7.176290190435553 ; l2 norm of gradient:  1.5952690385800055 ; l2 norm of weights:  8.545856921738265\n",
            "Iteration#:  8944 ; Loss:  7.176202648568127 ; l2 norm of gradient:  1.5952604112784974 ; l2 norm of weights:  8.545806599007001\n",
            "Iteration#:  8948 ; Loss:  7.176115106204334 ; l2 norm of gradient:  1.5952517839697817 ; l2 norm of weights:  8.54575627666969\n",
            "Iteration#:  8952 ; Loss:  7.1760275606999055 ; l2 norm of gradient:  1.5952431566538559 ; l2 norm of weights:  8.545705954726335\n",
            "Iteration#:  8956 ; Loss:  7.175940017816906 ; l2 norm of gradient:  1.5952345293307142 ; l2 norm of weights:  8.54565563317693\n",
            "Iteration#:  8960 ; Loss:  7.175852478937047 ; l2 norm of gradient:  1.5952259020003547 ; l2 norm of weights:  8.545605312021475\n",
            "Iteration#:  8964 ; Loss:  7.175764933762331 ; l2 norm of gradient:  1.5952172746627726 ; l2 norm of weights:  8.545554991259975\n",
            "Iteration#:  8968 ; Loss:  7.175677391851082 ; l2 norm of gradient:  1.5952086473179659 ; l2 norm of weights:  8.545504670892424\n",
            "Iteration#:  8972 ; Loss:  7.175589849677408 ; l2 norm of gradient:  1.5952000199659289 ; l2 norm of weights:  8.545454350918819\n",
            "Iteration#:  8976 ; Loss:  7.1755023144801005 ; l2 norm of gradient:  1.5951913926066599 ; l2 norm of weights:  8.545404031339165\n",
            "Iteration#:  8980 ; Loss:  7.175414773983377 ; l2 norm of gradient:  1.595182765240154 ; l2 norm of weights:  8.545353712153458\n",
            "Iteration#:  8984 ; Loss:  7.17532723343569 ; l2 norm of gradient:  1.595174137866409 ; l2 norm of weights:  8.545303393361698\n",
            "Iteration#:  8988 ; Loss:  7.1752396955459385 ; l2 norm of gradient:  1.59516551048542 ; l2 norm of weights:  8.545253074963883\n",
            "Iteration#:  8992 ; Loss:  7.175152158556274 ; l2 norm of gradient:  1.5951568830971843 ; l2 norm of weights:  8.545202756960013\n",
            "Iteration#:  8996 ; Loss:  7.175064617580043 ; l2 norm of gradient:  1.5951482557016972 ; l2 norm of weights:  8.545152439350089\n",
            "Iteration#:  9000 ; Loss:  7.1749770810509395 ; l2 norm of gradient:  1.5951396282989563 ; l2 norm of weights:  8.545102122134105\n",
            "Iteration#:  9004 ; Loss:  7.174889543423012 ; l2 norm of gradient:  1.5951310008889574 ; l2 norm of weights:  8.545051805312065\n",
            "Iteration#:  9008 ; Loss:  7.174802011108498 ; l2 norm of gradient:  1.5951223734716973 ; l2 norm of weights:  8.545001488883967\n",
            "Iteration#:  9012 ; Loss:  7.174714474587793 ; l2 norm of gradient:  1.595113746047171 ; l2 norm of weights:  8.54495117284981\n",
            "Iteration#:  9016 ; Loss:  7.174626940952628 ; l2 norm of gradient:  1.5951051186153764 ; l2 norm of weights:  8.544900857209594\n",
            "Iteration#:  9020 ; Loss:  7.17453940770265 ; l2 norm of gradient:  1.5950964911763101 ; l2 norm of weights:  8.544850541963315\n",
            "Iteration#:  9024 ; Loss:  7.174451869884269 ; l2 norm of gradient:  1.5950878637299675 ; l2 norm of weights:  8.544800227110976\n",
            "Iteration#:  9028 ; Loss:  7.174364338145152 ; l2 norm of gradient:  1.595079236276345 ; l2 norm of weights:  8.544749912652572\n",
            "Iteration#:  9032 ; Loss:  7.174276807295559 ; l2 norm of gradient:  1.5950706088154394 ; l2 norm of weights:  8.544699598588108\n",
            "Iteration#:  9036 ; Loss:  7.174189275544968 ; l2 norm of gradient:  1.5950619813472466 ; l2 norm of weights:  8.544649284917579\n",
            "Iteration#:  9040 ; Loss:  7.174101745403148 ; l2 norm of gradient:  1.5950533538717644 ; l2 norm of weights:  8.544598971640983\n",
            "Iteration#:  9044 ; Loss:  7.174014209430119 ; l2 norm of gradient:  1.5950447263889875 ; l2 norm of weights:  8.544548658758325\n",
            "Iteration#:  9048 ; Loss:  7.173926679850582 ; l2 norm of gradient:  1.595036098898913 ; l2 norm of weights:  8.544498346269599\n",
            "Iteration#:  9052 ; Loss:  7.173839144333612 ; l2 norm of gradient:  1.5950274714015364 ; l2 norm of weights:  8.544448034174804\n",
            "Iteration#:  9056 ; Loss:  7.17375161898879 ; l2 norm of gradient:  1.595018843896856 ; l2 norm of weights:  8.544397722473942\n",
            "Iteration#:  9060 ; Loss:  7.173664092409699 ; l2 norm of gradient:  1.595010216384867 ; l2 norm of weights:  8.54434741116701\n",
            "Iteration#:  9064 ; Loss:  7.173576558396659 ; l2 norm of gradient:  1.5950015888655658 ; l2 norm of weights:  8.54429710025401\n",
            "Iteration#:  9068 ; Loss:  7.173489030589932 ; l2 norm of gradient:  1.5949929613389486 ; l2 norm of weights:  8.544246789734938\n",
            "Iteration#:  9072 ; Loss:  7.173401503279413 ; l2 norm of gradient:  1.5949843338050118 ; l2 norm of weights:  8.544196479609795\n",
            "Iteration#:  9076 ; Loss:  7.1733139782639626 ; l2 norm of gradient:  1.594975706263753 ; l2 norm of weights:  8.54414616987858\n",
            "Iteration#:  9080 ; Loss:  7.173226451052051 ; l2 norm of gradient:  1.5949670787151666 ; l2 norm of weights:  8.544095860541288\n",
            "Iteration#:  9084 ; Loss:  7.173138927378985 ; l2 norm of gradient:  1.59495845115925 ; l2 norm of weights:  8.544045551597925\n",
            "Iteration#:  9088 ; Loss:  7.173051400703128 ; l2 norm of gradient:  1.5949498235960005 ; l2 norm of weights:  8.543995243048489\n",
            "Iteration#:  9092 ; Loss:  7.172963874307784 ; l2 norm of gradient:  1.5949411960254118 ; l2 norm of weights:  8.543944934892975\n",
            "Iteration#:  9096 ; Loss:  7.172876348434615 ; l2 norm of gradient:  1.5949325684474838 ; l2 norm of weights:  8.543894627131385\n",
            "Iteration#:  9100 ; Loss:  7.172788826915674 ; l2 norm of gradient:  1.5949239408622093 ; l2 norm of weights:  8.543844319763718\n",
            "Iteration#:  9104 ; Loss:  7.17270130255674 ; l2 norm of gradient:  1.5949153132695881 ; l2 norm of weights:  8.543794012789972\n",
            "Iteration#:  9108 ; Loss:  7.172613777871724 ; l2 norm of gradient:  1.5949066856696137 ; l2 norm of weights:  8.543743706210147\n",
            "Iteration#:  9112 ; Loss:  7.172526258836978 ; l2 norm of gradient:  1.5948980580622836 ; l2 norm of weights:  8.543693400024242\n",
            "Iteration#:  9116 ; Loss:  7.17243873415102 ; l2 norm of gradient:  1.594889430447595 ; l2 norm of weights:  8.543643094232259\n",
            "Iteration#:  9120 ; Loss:  7.172351211404695 ; l2 norm of gradient:  1.5948808028255435 ; l2 norm of weights:  8.543592788834191\n",
            "Iteration#:  9124 ; Loss:  7.172263689932128 ; l2 norm of gradient:  1.5948721751961241 ; l2 norm of weights:  8.543542483830043\n",
            "Iteration#:  9128 ; Loss:  7.172176170610426 ; l2 norm of gradient:  1.5948635475593353 ; l2 norm of weights:  8.543492179219811\n",
            "Iteration#:  9132 ; Loss:  7.172088650420777 ; l2 norm of gradient:  1.594854919915173 ; l2 norm of weights:  8.543441875003495\n",
            "Iteration#:  9136 ; Loss:  7.17200113001679 ; l2 norm of gradient:  1.5948462922636324 ; l2 norm of weights:  8.543391571181093\n",
            "Iteration#:  9140 ; Loss:  7.171913611611307 ; l2 norm of gradient:  1.5948376646047113 ; l2 norm of weights:  8.54334126775261\n",
            "Iteration#:  9144 ; Loss:  7.171826093554562 ; l2 norm of gradient:  1.5948290369384062 ; l2 norm of weights:  8.543290964718036\n",
            "Iteration#:  9148 ; Loss:  7.171738580436453 ; l2 norm of gradient:  1.5948204092647111 ; l2 norm of weights:  8.543240662077377\n",
            "Iteration#:  9152 ; Loss:  7.171651057455816 ; l2 norm of gradient:  1.5948117815836251 ; l2 norm of weights:  8.543190359830627\n",
            "Iteration#:  9156 ; Loss:  7.171563540054396 ; l2 norm of gradient:  1.5948031538951426 ; l2 norm of weights:  8.54314005797779\n",
            "Iteration#:  9160 ; Loss:  7.17147602583478 ; l2 norm of gradient:  1.5947945261992607 ; l2 norm of weights:  8.543089756518867\n",
            "Iteration#:  9164 ; Loss:  7.171388509243599 ; l2 norm of gradient:  1.5947858984959769 ; l2 norm of weights:  8.54303945545385\n",
            "Iteration#:  9168 ; Loss:  7.171300994219963 ; l2 norm of gradient:  1.5947772707852856 ; l2 norm of weights:  8.54298915478274\n",
            "Iteration#:  9172 ; Loss:  7.1712134794735185 ; l2 norm of gradient:  1.5947686430671837 ; l2 norm of weights:  8.542938854505541\n",
            "Iteration#:  9176 ; Loss:  7.171125965749956 ; l2 norm of gradient:  1.5947600153416681 ; l2 norm of weights:  8.542888554622245\n",
            "Iteration#:  9180 ; Loss:  7.171038451060649 ; l2 norm of gradient:  1.594751387608735 ; l2 norm of weights:  8.542838255132859\n",
            "Iteration#:  9184 ; Loss:  7.1709509378003204 ; l2 norm of gradient:  1.594742759868381 ; l2 norm of weights:  8.542787956037378\n",
            "Iteration#:  9188 ; Loss:  7.1708634245872 ; l2 norm of gradient:  1.594734132120602 ; l2 norm of weights:  8.5427376573358\n",
            "Iteration#:  9192 ; Loss:  7.17077591541943 ; l2 norm of gradient:  1.5947255043653943 ; l2 norm of weights:  8.542687359028124\n",
            "Iteration#:  9196 ; Loss:  7.170688401459996 ; l2 norm of gradient:  1.5947168766027533 ; l2 norm of weights:  8.542637061114354\n",
            "Iteration#:  9200 ; Loss:  7.170600889379761 ; l2 norm of gradient:  1.5947082488326774 ; l2 norm of weights:  8.542586763594484\n",
            "Iteration#:  9204 ; Loss:  7.170513385274051 ; l2 norm of gradient:  1.5946996210551623 ; l2 norm of weights:  8.542536466468517\n",
            "Iteration#:  9208 ; Loss:  7.170425870353146 ; l2 norm of gradient:  1.5946909932702034 ; l2 norm of weights:  8.542486169736447\n",
            "Iteration#:  9212 ; Loss:  7.170338359207026 ; l2 norm of gradient:  1.594682365477798 ; l2 norm of weights:  8.542435873398281\n",
            "Iteration#:  9216 ; Loss:  7.170250850971124 ; l2 norm of gradient:  1.594673737677942 ; l2 norm of weights:  8.54238557745401\n",
            "Iteration#:  9220 ; Loss:  7.170163342696482 ; l2 norm of gradient:  1.5946651098706317 ; l2 norm of weights:  8.54233528190364\n",
            "Iteration#:  9224 ; Loss:  7.170075835928008 ; l2 norm of gradient:  1.5946564820558637 ; l2 norm of weights:  8.542284986747164\n",
            "Iteration#:  9228 ; Loss:  7.1699883291295485 ; l2 norm of gradient:  1.594647854233634 ; l2 norm of weights:  8.542234691984588\n",
            "Iteration#:  9232 ; Loss:  7.169900822133224 ; l2 norm of gradient:  1.5946392264039395 ; l2 norm of weights:  8.542184397615904\n",
            "Iteration#:  9236 ; Loss:  7.169813309504402 ; l2 norm of gradient:  1.5946305985667757 ; l2 norm of weights:  8.542134103641116\n",
            "Iteration#:  9240 ; Loss:  7.169725807166486 ; l2 norm of gradient:  1.594621970722139 ; l2 norm of weights:  8.542083810060221\n",
            "Iteration#:  9244 ; Loss:  7.169638302021814 ; l2 norm of gradient:  1.5946133428700273 ; l2 norm of weights:  8.54203351687322\n",
            "Iteration#:  9248 ; Loss:  7.169550798081133 ; l2 norm of gradient:  1.5946047150104354 ; l2 norm of weights:  8.54198322408011\n",
            "Iteration#:  9252 ; Loss:  7.1694632911238 ; l2 norm of gradient:  1.5945960871433595 ; l2 norm of weights:  8.541932931680892\n",
            "Iteration#:  9256 ; Loss:  7.1693757888526175 ; l2 norm of gradient:  1.5945874592687963 ; l2 norm of weights:  8.541882639675563\n",
            "Iteration#:  9260 ; Loss:  7.1692882824976465 ; l2 norm of gradient:  1.5945788313867424 ; l2 norm of weights:  8.541832348064126\n",
            "Iteration#:  9264 ; Loss:  7.169200780247708 ; l2 norm of gradient:  1.5945702034971938 ; l2 norm of weights:  8.541782056846575\n",
            "Iteration#:  9268 ; Loss:  7.1691132775309665 ; l2 norm of gradient:  1.5945615756001479 ; l2 norm of weights:  8.541731766022913\n",
            "Iteration#:  9272 ; Loss:  7.169025777179437 ; l2 norm of gradient:  1.5945529476955989 ; l2 norm of weights:  8.541681475593139\n",
            "Iteration#:  9276 ; Loss:  7.168938277616208 ; l2 norm of gradient:  1.5945443197835447 ; l2 norm of weights:  8.54163118555725\n",
            "Iteration#:  9280 ; Loss:  7.168850776191257 ; l2 norm of gradient:  1.5945356918639813 ; l2 norm of weights:  8.54158089591525\n",
            "Iteration#:  9284 ; Loss:  7.168763274443435 ; l2 norm of gradient:  1.5945270639369051 ; l2 norm of weights:  8.541530606667129\n",
            "Iteration#:  9288 ; Loss:  7.168675776321041 ; l2 norm of gradient:  1.5945184360023124 ; l2 norm of weights:  8.541480317812894\n",
            "Iteration#:  9292 ; Loss:  7.16858827551283 ; l2 norm of gradient:  1.5945098080601992 ; l2 norm of weights:  8.541430029352545\n",
            "Iteration#:  9296 ; Loss:  7.168500775879192 ; l2 norm of gradient:  1.594501180110562 ; l2 norm of weights:  8.541379741286075\n",
            "Iteration#:  9300 ; Loss:  7.168413281114558 ; l2 norm of gradient:  1.5944925521533975 ; l2 norm of weights:  8.54132945361349\n",
            "Iteration#:  9304 ; Loss:  7.168325781955242 ; l2 norm of gradient:  1.5944839241886997 ; l2 norm of weights:  8.54127916633478\n",
            "Iteration#:  9308 ; Loss:  7.16823828184539 ; l2 norm of gradient:  1.5944752962164694 ; l2 norm of weights:  8.541228879449951\n",
            "Iteration#:  9312 ; Loss:  7.1681507897661065 ; l2 norm of gradient:  1.594466668236699 ; l2 norm of weights:  8.541178592959001\n",
            "Iteration#:  9316 ; Loss:  7.168063289467867 ; l2 norm of gradient:  1.5944580402493866 ; l2 norm of weights:  8.541128306861932\n",
            "Iteration#:  9320 ; Loss:  7.167975793724011 ; l2 norm of gradient:  1.594449412254528 ; l2 norm of weights:  8.54107802115874\n",
            "Iteration#:  9324 ; Loss:  7.1678882982308085 ; l2 norm of gradient:  1.5944407842521195 ; l2 norm of weights:  8.541027735849422\n",
            "Iteration#:  9328 ; Loss:  7.167800804283863 ; l2 norm of gradient:  1.5944321562421575 ; l2 norm of weights:  8.54097745093398\n",
            "Iteration#:  9332 ; Loss:  7.167713312395909 ; l2 norm of gradient:  1.5944235282246382 ; l2 norm of weights:  8.540927166412413\n",
            "Iteration#:  9336 ; Loss:  7.167625819145564 ; l2 norm of gradient:  1.5944149001995578 ; l2 norm of weights:  8.540876882284717\n",
            "Iteration#:  9340 ; Loss:  7.16753832581592 ; l2 norm of gradient:  1.5944062721669132 ; l2 norm of weights:  8.5408265985509\n",
            "Iteration#:  9344 ; Loss:  7.167450833974862 ; l2 norm of gradient:  1.5943976441267007 ; l2 norm of weights:  8.540776315210952\n",
            "Iteration#:  9348 ; Loss:  7.167363340196086 ; l2 norm of gradient:  1.5943890160789151 ; l2 norm of weights:  8.540726032264875\n",
            "Iteration#:  9352 ; Loss:  7.167275849439324 ; l2 norm of gradient:  1.5943803880235543 ; l2 norm of weights:  8.540675749712669\n",
            "Iteration#:  9356 ; Loss:  7.16718835931653 ; l2 norm of gradient:  1.5943717599606146 ; l2 norm of weights:  8.540625467554333\n",
            "Iteration#:  9360 ; Loss:  7.167100869033147 ; l2 norm of gradient:  1.5943631318900917 ; l2 norm of weights:  8.540575185789866\n",
            "Iteration#:  9364 ; Loss:  7.167013379869145 ; l2 norm of gradient:  1.5943545038119815 ; l2 norm of weights:  8.540524904419268\n",
            "Iteration#:  9368 ; Loss:  7.1669258862602625 ; l2 norm of gradient:  1.594345875726281 ; l2 norm of weights:  8.540474623442535\n",
            "Iteration#:  9372 ; Loss:  7.166838398150524 ; l2 norm of gradient:  1.5943372476329867 ; l2 norm of weights:  8.54042434285967\n",
            "Iteration#:  9376 ; Loss:  7.16675090993397 ; l2 norm of gradient:  1.5943286195320936 ; l2 norm of weights:  8.540374062670669\n",
            "Iteration#:  9380 ; Loss:  7.166663423739933 ; l2 norm of gradient:  1.5943199914235997 ; l2 norm of weights:  8.540323782875534\n",
            "Iteration#:  9384 ; Loss:  7.166575931750826 ; l2 norm of gradient:  1.5943113633075003 ; l2 norm of weights:  8.540273503474262\n",
            "Iteration#:  9388 ; Loss:  7.166488452005932 ; l2 norm of gradient:  1.5943027351837917 ; l2 norm of weights:  8.540223224466857\n",
            "Iteration#:  9392 ; Loss:  7.16640096042681 ; l2 norm of gradient:  1.5942941070524697 ; l2 norm of weights:  8.540172945853309\n",
            "Iteration#:  9396 ; Loss:  7.166313476763876 ; l2 norm of gradient:  1.5942854789135317 ; l2 norm of weights:  8.540122667633625\n",
            "Iteration#:  9400 ; Loss:  7.166225988683362 ; l2 norm of gradient:  1.5942768507669745 ; l2 norm of weights:  8.5400723898078\n",
            "Iteration#:  9404 ; Loss:  7.166138506731533 ; l2 norm of gradient:  1.5942682226127927 ; l2 norm of weights:  8.540022112375837\n",
            "Iteration#:  9408 ; Loss:  7.16605102306945 ; l2 norm of gradient:  1.594259594450983 ; l2 norm of weights:  8.539971835337731\n",
            "Iteration#:  9412 ; Loss:  7.165963537265739 ; l2 norm of gradient:  1.5942509662815425 ; l2 norm of weights:  8.539921558693484\n",
            "Iteration#:  9416 ; Loss:  7.165876056082828 ; l2 norm of gradient:  1.594242338104467 ; l2 norm of weights:  8.539871282443094\n",
            "Iteration#:  9420 ; Loss:  7.165788574012501 ; l2 norm of gradient:  1.594233709919752 ; l2 norm of weights:  8.539821006586562\n",
            "Iteration#:  9424 ; Loss:  7.165701091024031 ; l2 norm of gradient:  1.5942250817273949 ; l2 norm of weights:  8.539770731123884\n",
            "Iteration#:  9428 ; Loss:  7.165613606637558 ; l2 norm of gradient:  1.5942164535273906 ; l2 norm of weights:  8.539720456055063\n",
            "Iteration#:  9432 ; Loss:  7.165526130707812 ; l2 norm of gradient:  1.5942078253197383 ; l2 norm of weights:  8.539670181380092\n",
            "Iteration#:  9436 ; Loss:  7.165438646468743 ; l2 norm of gradient:  1.5941991971044318 ; l2 norm of weights:  8.539619907098977\n",
            "Iteration#:  9440 ; Loss:  7.165351165431649 ; l2 norm of gradient:  1.5941905688814673 ; l2 norm of weights:  8.539569633211714\n",
            "Iteration#:  9444 ; Loss:  7.165263687712068 ; l2 norm of gradient:  1.5941819406508422 ; l2 norm of weights:  8.539519359718303\n",
            "Iteration#:  9448 ; Loss:  7.165176206207329 ; l2 norm of gradient:  1.5941733124125521 ; l2 norm of weights:  8.539469086618743\n",
            "Iteration#:  9452 ; Loss:  7.1650887271741075 ; l2 norm of gradient:  1.5941646841665935 ; l2 norm of weights:  8.53941881391303\n",
            "Iteration#:  9456 ; Loss:  7.165001251935465 ; l2 norm of gradient:  1.5941560559129626 ; l2 norm of weights:  8.539368541601169\n",
            "Iteration#:  9460 ; Loss:  7.164913776016791 ; l2 norm of gradient:  1.5941474276516558 ; l2 norm of weights:  8.539318269683156\n",
            "Iteration#:  9464 ; Loss:  7.16482629809789 ; l2 norm of gradient:  1.5941387993826692 ; l2 norm of weights:  8.539267998158987\n",
            "Iteration#:  9468 ; Loss:  7.164738820264682 ; l2 norm of gradient:  1.5941301711059992 ; l2 norm of weights:  8.539217727028667\n",
            "Iteration#:  9472 ; Loss:  7.164651345589727 ; l2 norm of gradient:  1.5941215428216415 ; l2 norm of weights:  8.539167456292196\n",
            "Iteration#:  9476 ; Loss:  7.164563868933506 ; l2 norm of gradient:  1.5941129145295931 ; l2 norm of weights:  8.539117185949566\n",
            "Iteration#:  9480 ; Loss:  7.16447639351358 ; l2 norm of gradient:  1.5941042862298507 ; l2 norm of weights:  8.53906691600078\n",
            "Iteration#:  9484 ; Loss:  7.164388922070698 ; l2 norm of gradient:  1.594095657922409 ; l2 norm of weights:  8.539016646445837\n",
            "Iteration#:  9488 ; Loss:  7.164301444619201 ; l2 norm of gradient:  1.5940870296072658 ; l2 norm of weights:  8.538966377284737\n",
            "Iteration#:  9492 ; Loss:  7.164213973476343 ; l2 norm of gradient:  1.5940784012844165 ; l2 norm of weights:  8.538916108517478\n",
            "Iteration#:  9496 ; Loss:  7.164126498575966 ; l2 norm of gradient:  1.5940697729538564 ; l2 norm of weights:  8.538865840144062\n",
            "Iteration#:  9500 ; Loss:  7.164039029822227 ; l2 norm of gradient:  1.5940611446155852 ; l2 norm of weights:  8.538815572164484\n",
            "Iteration#:  9504 ; Loss:  7.163951556326428 ; l2 norm of gradient:  1.5940525162695949 ; l2 norm of weights:  8.538765304578748\n",
            "Iteration#:  9508 ; Loss:  7.163864083686036 ; l2 norm of gradient:  1.5940438879158858 ; l2 norm of weights:  8.538715037386845\n",
            "Iteration#:  9512 ; Loss:  7.163776613779662 ; l2 norm of gradient:  1.5940352595544507 ; l2 norm of weights:  8.538664770588783\n",
            "Iteration#:  9516 ; Loss:  7.163689141954832 ; l2 norm of gradient:  1.5940266311852875 ; l2 norm of weights:  8.53861450418456\n",
            "Iteration#:  9520 ; Loss:  7.163601671124285 ; l2 norm of gradient:  1.5940180028083921 ; l2 norm of weights:  8.538564238174166\n",
            "Iteration#:  9524 ; Loss:  7.163514204069422 ; l2 norm of gradient:  1.5940093744237611 ; l2 norm of weights:  8.538513972557611\n",
            "Iteration#:  9528 ; Loss:  7.163426733569231 ; l2 norm of gradient:  1.59400074603139 ; l2 norm of weights:  8.538463707334891\n",
            "Iteration#:  9532 ; Loss:  7.163339266920657 ; l2 norm of gradient:  1.5939921176312761 ; l2 norm of weights:  8.538413442506002\n",
            "Iteration#:  9536 ; Loss:  7.163251799506913 ; l2 norm of gradient:  1.5939834892234153 ; l2 norm of weights:  8.538363178070949\n",
            "Iteration#:  9540 ; Loss:  7.163164331890749 ; l2 norm of gradient:  1.5939748608078033 ; l2 norm of weights:  8.538312914029724\n",
            "Iteration#:  9544 ; Loss:  7.16307685896116 ; l2 norm of gradient:  1.5939662323844368 ; l2 norm of weights:  8.538262650382332\n",
            "Iteration#:  9548 ; Loss:  7.162989402076059 ; l2 norm of gradient:  1.5939576039533119 ; l2 norm of weights:  8.53821238712877\n",
            "Iteration#:  9552 ; Loss:  7.162901934138086 ; l2 norm of gradient:  1.593948975514425 ; l2 norm of weights:  8.538162124269036\n",
            "Iteration#:  9556 ; Loss:  7.162814469185205 ; l2 norm of gradient:  1.5939403470677724 ; l2 norm of weights:  8.538111861803129\n",
            "Iteration#:  9560 ; Loss:  7.162727002078782 ; l2 norm of gradient:  1.5939317186133493 ; l2 norm of weights:  8.538061599731051\n",
            "Iteration#:  9564 ; Loss:  7.162639538036679 ; l2 norm of gradient:  1.5939230901511539 ; l2 norm of weights:  8.538011338052799\n",
            "Iteration#:  9568 ; Loss:  7.162552073468059 ; l2 norm of gradient:  1.5939144616811813 ; l2 norm of weights:  8.537961076768372\n",
            "Iteration#:  9572 ; Loss:  7.162464612560559 ; l2 norm of gradient:  1.5939058332034273 ; l2 norm of weights:  8.537910815877774\n",
            "Iteration#:  9576 ; Loss:  7.162377147701351 ; l2 norm of gradient:  1.5938972047178888 ; l2 norm of weights:  8.537860555380998\n",
            "Iteration#:  9580 ; Loss:  7.162289686591867 ; l2 norm of gradient:  1.5938885762245618 ; l2 norm of weights:  8.537810295278046\n",
            "Iteration#:  9584 ; Loss:  7.162202225563552 ; l2 norm of gradient:  1.593879947723443 ; l2 norm of weights:  8.537760035568914\n",
            "Iteration#:  9588 ; Loss:  7.1621147630234745 ; l2 norm of gradient:  1.5938713192145284 ; l2 norm of weights:  8.537709776253605\n",
            "Iteration#:  9592 ; Loss:  7.1620273043028035 ; l2 norm of gradient:  1.593862690697813 ; l2 norm of weights:  8.537659517332116\n",
            "Iteration#:  9596 ; Loss:  7.161939847423449 ; l2 norm of gradient:  1.5938540621732946 ; l2 norm of weights:  8.53760925880445\n",
            "Iteration#:  9600 ; Loss:  7.161852384350079 ; l2 norm of gradient:  1.5938454336409695 ; l2 norm of weights:  8.5375590006706\n",
            "Iteration#:  9604 ; Loss:  7.161764925702707 ; l2 norm of gradient:  1.5938368051008331 ; l2 norm of weights:  8.53750874293057\n",
            "Iteration#:  9608 ; Loss:  7.161677464981464 ; l2 norm of gradient:  1.5938281765528823 ; l2 norm of weights:  8.537458485584358\n",
            "Iteration#:  9612 ; Loss:  7.161590007219381 ; l2 norm of gradient:  1.5938195479971118 ; l2 norm of weights:  8.53740822863196\n",
            "Iteration#:  9616 ; Loss:  7.161502555109145 ; l2 norm of gradient:  1.59381091943352 ; l2 norm of weights:  8.53735797207338\n",
            "Iteration#:  9620 ; Loss:  7.161415095460653 ; l2 norm of gradient:  1.5938022908621021 ; l2 norm of weights:  8.537307715908614\n",
            "Iteration#:  9624 ; Loss:  7.161327643883127 ; l2 norm of gradient:  1.5937936622828535 ; l2 norm of weights:  8.537257460137663\n",
            "Iteration#:  9628 ; Loss:  7.1612401831321755 ; l2 norm of gradient:  1.5937850336957717 ; l2 norm of weights:  8.537207204760525\n",
            "Iteration#:  9632 ; Loss:  7.16115273090862 ; l2 norm of gradient:  1.593776405100853 ; l2 norm of weights:  8.5371569497772\n",
            "Iteration#:  9636 ; Loss:  7.161065275570399 ; l2 norm of gradient:  1.5937677764980918 ; l2 norm of weights:  8.537106695187688\n",
            "Iteration#:  9640 ; Loss:  7.160977817561664 ; l2 norm of gradient:  1.5937591478874866 ; l2 norm of weights:  8.537056440991984\n",
            "Iteration#:  9644 ; Loss:  7.160890361452087 ; l2 norm of gradient:  1.5937505192690316 ; l2 norm of weights:  8.53700618719009\n",
            "Iteration#:  9648 ; Loss:  7.16080290935213 ; l2 norm of gradient:  1.5937418906427256 ; l2 norm of weights:  8.536955933782007\n",
            "Iteration#:  9652 ; Loss:  7.160715459685395 ; l2 norm of gradient:  1.593733262008562 ; l2 norm of weights:  8.536905680767733\n",
            "Iteration#:  9656 ; Loss:  7.160628007638184 ; l2 norm of gradient:  1.593724633366539 ; l2 norm of weights:  8.536855428147266\n",
            "Iteration#:  9660 ; Loss:  7.160540553197437 ; l2 norm of gradient:  1.5937160047166512 ; l2 norm of weights:  8.536805175920605\n",
            "Iteration#:  9664 ; Loss:  7.160453103734231 ; l2 norm of gradient:  1.593707376058896 ; l2 norm of weights:  8.53675492408775\n",
            "Iteration#:  9668 ; Loss:  7.160365655043211 ; l2 norm of gradient:  1.5936987473932698 ; l2 norm of weights:  8.536704672648701\n",
            "Iteration#:  9672 ; Loss:  7.160278198822575 ; l2 norm of gradient:  1.5936901187197685 ; l2 norm of weights:  8.536654421603457\n",
            "Iteration#:  9676 ; Loss:  7.160190755996709 ; l2 norm of gradient:  1.5936814900383882 ; l2 norm of weights:  8.536604170952016\n",
            "Iteration#:  9680 ; Loss:  7.160103300324302 ; l2 norm of gradient:  1.5936728613491238 ; l2 norm of weights:  8.536553920694375\n",
            "Iteration#:  9684 ; Loss:  7.160015852455091 ; l2 norm of gradient:  1.5936642326519732 ; l2 norm of weights:  8.536503670830538\n",
            "Iteration#:  9688 ; Loss:  7.159928407086033 ; l2 norm of gradient:  1.5936556039469327 ; l2 norm of weights:  8.536453421360502\n",
            "Iteration#:  9692 ; Loss:  7.159840957497963 ; l2 norm of gradient:  1.593646975233998 ; l2 norm of weights:  8.536403172284267\n",
            "Iteration#:  9696 ; Loss:  7.159753512614515 ; l2 norm of gradient:  1.5936383465131643 ; l2 norm of weights:  8.536352923601829\n",
            "Iteration#:  9700 ; Loss:  7.159666068747977 ; l2 norm of gradient:  1.5936297177844296 ; l2 norm of weights:  8.536302675313191\n",
            "Iteration#:  9704 ; Loss:  7.159578619382289 ; l2 norm of gradient:  1.5936210890477895 ; l2 norm of weights:  8.53625242741835\n",
            "Iteration#:  9708 ; Loss:  7.15949117095149 ; l2 norm of gradient:  1.5936124603032387 ; l2 norm of weights:  8.536202179917305\n",
            "Iteration#:  9712 ; Loss:  7.159403727963837 ; l2 norm of gradient:  1.5936038315507763 ; l2 norm of weights:  8.536151932810059\n",
            "Iteration#:  9716 ; Loss:  7.159316287776548 ; l2 norm of gradient:  1.5935952027903957 ; l2 norm of weights:  8.536101686096606\n",
            "Iteration#:  9720 ; Loss:  7.159228837880818 ; l2 norm of gradient:  1.593586574022094 ; l2 norm of weights:  8.536051439776944\n",
            "Iteration#:  9724 ; Loss:  7.159141395532689 ; l2 norm of gradient:  1.593577945245868 ; l2 norm of weights:  8.53600119385108\n",
            "Iteration#:  9728 ; Loss:  7.159053950712719 ; l2 norm of gradient:  1.5935693164617146 ; l2 norm of weights:  8.535950948319007\n",
            "Iteration#:  9732 ; Loss:  7.158966512598855 ; l2 norm of gradient:  1.5935606876696273 ; l2 norm of weights:  8.53590070318073\n",
            "Iteration#:  9736 ; Loss:  7.158879064140459 ; l2 norm of gradient:  1.5935520588696055 ; l2 norm of weights:  8.535850458436238\n",
            "Iteration#:  9740 ; Loss:  7.158791627822213 ; l2 norm of gradient:  1.593543430061643 ; l2 norm of weights:  8.535800214085539\n",
            "Iteration#:  9744 ; Loss:  7.158704185113822 ; l2 norm of gradient:  1.5935348012457362 ; l2 norm of weights:  8.535749970128629\n",
            "Iteration#:  9748 ; Loss:  7.158616745098589 ; l2 norm of gradient:  1.5935261724218828 ; l2 norm of weights:  8.535699726565506\n",
            "Iteration#:  9752 ; Loss:  7.158529305404709 ; l2 norm of gradient:  1.5935175435900784 ; l2 norm of weights:  8.535649483396172\n",
            "Iteration#:  9756 ; Loss:  7.158441865871159 ; l2 norm of gradient:  1.593508914750318 ; l2 norm of weights:  8.535599240620625\n",
            "Iteration#:  9760 ; Loss:  7.158354427959303 ; l2 norm of gradient:  1.593500285902599 ; l2 norm of weights:  8.535548998238864\n",
            "Iteration#:  9764 ; Loss:  7.158266992024728 ; l2 norm of gradient:  1.5934916570469173 ; l2 norm of weights:  8.535498756250888\n",
            "Iteration#:  9768 ; Loss:  7.158179555053006 ; l2 norm of gradient:  1.5934830281832684 ; l2 norm of weights:  8.535448514656697\n",
            "Iteration#:  9772 ; Loss:  7.158092112624992 ; l2 norm of gradient:  1.5934743993116502 ; l2 norm of weights:  8.535398273456288\n",
            "Iteration#:  9776 ; Loss:  7.158004676455038 ; l2 norm of gradient:  1.5934657704320576 ; l2 norm of weights:  8.535348032649663\n",
            "Iteration#:  9780 ; Loss:  7.1579172366708255 ; l2 norm of gradient:  1.593457141544486 ; l2 norm of weights:  8.53529779223682\n",
            "Iteration#:  9784 ; Loss:  7.157829802258789 ; l2 norm of gradient:  1.5934485126489326 ; l2 norm of weights:  8.535247552217758\n",
            "Iteration#:  9788 ; Loss:  7.157742367547945 ; l2 norm of gradient:  1.5934398837453942 ; l2 norm of weights:  8.535197312592475\n",
            "Iteration#:  9792 ; Loss:  7.157654936623402 ; l2 norm of gradient:  1.5934312548338663 ; l2 norm of weights:  8.535147073360973\n",
            "Iteration#:  9796 ; Loss:  7.157567501084129 ; l2 norm of gradient:  1.5934226259143456 ; l2 norm of weights:  8.535096834523248\n",
            "Iteration#:  9800 ; Loss:  7.157480069556811 ; l2 norm of gradient:  1.5934139969868264 ; l2 norm of weights:  8.535046596079301\n",
            "Iteration#:  9804 ; Loss:  7.1573926326711685 ; l2 norm of gradient:  1.593405368051307 ; l2 norm of weights:  8.534996358029131\n",
            "Iteration#:  9808 ; Loss:  7.15730520130852 ; l2 norm of gradient:  1.593396739107783 ; l2 norm of weights:  8.534946120372737\n",
            "Iteration#:  9812 ; Loss:  7.157217770902232 ; l2 norm of gradient:  1.5933881101562497 ; l2 norm of weights:  8.534895883110119\n",
            "Iteration#:  9816 ; Loss:  7.157130341226154 ; l2 norm of gradient:  1.5933794811967048 ; l2 norm of weights:  8.534845646241275\n",
            "Iteration#:  9820 ; Loss:  7.157042905596006 ; l2 norm of gradient:  1.5933708522291432 ; l2 norm of weights:  8.534795409766204\n",
            "Iteration#:  9824 ; Loss:  7.156955473993985 ; l2 norm of gradient:  1.5933622232535616 ; l2 norm of weights:  8.534745173684907\n",
            "Iteration#:  9828 ; Loss:  7.156868045493511 ; l2 norm of gradient:  1.5933535942699553 ; l2 norm of weights:  8.534694937997381\n",
            "Iteration#:  9832 ; Loss:  7.156780618168203 ; l2 norm of gradient:  1.5933449652783216 ; l2 norm of weights:  8.534644702703625\n",
            "Iteration#:  9836 ; Loss:  7.156693183430656 ; l2 norm of gradient:  1.5933363362786563 ; l2 norm of weights:  8.534594467803641\n",
            "Iteration#:  9840 ; Loss:  7.156605759988219 ; l2 norm of gradient:  1.5933277072709562 ; l2 norm of weights:  8.534544233297426\n",
            "Iteration#:  9844 ; Loss:  7.1565183303099404 ; l2 norm of gradient:  1.5933190782552153 ; l2 norm of weights:  8.534493999184981\n",
            "Iteration#:  9848 ; Loss:  7.1564309060357765 ; l2 norm of gradient:  1.593310449231433 ; l2 norm of weights:  8.534443765466301\n",
            "Iteration#:  9852 ; Loss:  7.156343475830104 ; l2 norm of gradient:  1.5933018201996028 ; l2 norm of weights:  8.53439353214139\n",
            "Iteration#:  9856 ; Loss:  7.156256050408441 ; l2 norm of gradient:  1.593293191159722 ; l2 norm of weights:  8.534343299210244\n",
            "Iteration#:  9860 ; Loss:  7.156168622988215 ; l2 norm of gradient:  1.5932845621117862 ; l2 norm of weights:  8.534293066672863\n",
            "Iteration#:  9864 ; Loss:  7.1560811992203135 ; l2 norm of gradient:  1.593275933055792 ; l2 norm of weights:  8.534242834529248\n",
            "Iteration#:  9868 ; Loss:  7.155993776569386 ; l2 norm of gradient:  1.593267303991735 ; l2 norm of weights:  8.534192602779395\n",
            "Iteration#:  9872 ; Loss:  7.155906349466003 ; l2 norm of gradient:  1.5932586749196131 ; l2 norm of weights:  8.534142371423304\n",
            "Iteration#:  9876 ; Loss:  7.1558189267539 ; l2 norm of gradient:  1.5932500458394199 ; l2 norm of weights:  8.534092140460979\n",
            "Iteration#:  9880 ; Loss:  7.155731501975268 ; l2 norm of gradient:  1.5932414167511535 ; l2 norm of weights:  8.53404190989241\n",
            "Iteration#:  9884 ; Loss:  7.155644080587322 ; l2 norm of gradient:  1.5932327876548085 ; l2 norm of weights:  8.533991679717603\n",
            "Iteration#:  9888 ; Loss:  7.155556657175397 ; l2 norm of gradient:  1.5932241585503826 ; l2 norm of weights:  8.533941449936556\n",
            "Iteration#:  9892 ; Loss:  7.155469237831436 ; l2 norm of gradient:  1.5932155294378705 ; l2 norm of weights:  8.533891220549268\n",
            "Iteration#:  9896 ; Loss:  7.155381814407026 ; l2 norm of gradient:  1.59320690031727 ; l2 norm of weights:  8.533840991555737\n",
            "Iteration#:  9900 ; Loss:  7.155294394489228 ; l2 norm of gradient:  1.5931982711885755 ; l2 norm of weights:  8.533790762955965\n",
            "Iteration#:  9904 ; Loss:  7.155206973577313 ; l2 norm of gradient:  1.593189642051785 ; l2 norm of weights:  8.533740534749946\n",
            "Iteration#:  9908 ; Loss:  7.155119552597688 ; l2 norm of gradient:  1.5931810129068937 ; l2 norm of weights:  8.533690306937684\n",
            "Iteration#:  9912 ; Loss:  7.1550321321030355 ; l2 norm of gradient:  1.5931723837538965 ; l2 norm of weights:  8.533640079519177\n",
            "Iteration#:  9916 ; Loss:  7.154944712673288 ; l2 norm of gradient:  1.5931637545927917 ; l2 norm of weights:  8.533589852494421\n",
            "Iteration#:  9920 ; Loss:  7.154857295900678 ; l2 norm of gradient:  1.5931551254235734 ; l2 norm of weights:  8.53353962586342\n",
            "Iteration#:  9924 ; Loss:  7.15476988029409 ; l2 norm of gradient:  1.5931464962462394 ; l2 norm of weights:  8.533489399626172\n",
            "Iteration#:  9928 ; Loss:  7.154682466913502 ; l2 norm of gradient:  1.5931378670607854 ; l2 norm of weights:  8.533439173782673\n",
            "Iteration#:  9932 ; Loss:  7.154595040277439 ; l2 norm of gradient:  1.5931292378672064 ; l2 norm of weights:  8.533388948332924\n",
            "Iteration#:  9936 ; Loss:  7.154507630346493 ; l2 norm of gradient:  1.5931206086655014 ; l2 norm of weights:  8.533338723276927\n",
            "Iteration#:  9940 ; Loss:  7.154420216339153 ; l2 norm of gradient:  1.5931119794556623 ; l2 norm of weights:  8.533288498614677\n",
            "Iteration#:  9944 ; Loss:  7.154332799903035 ; l2 norm of gradient:  1.5931033502376892 ; l2 norm of weights:  8.533238274346175\n",
            "Iteration#:  9948 ; Loss:  7.154245386867398 ; l2 norm of gradient:  1.5930947210115758 ; l2 norm of weights:  8.53318805047142\n",
            "Iteration#:  9952 ; Loss:  7.154157969538057 ; l2 norm of gradient:  1.5930860917773209 ; l2 norm of weights:  8.533137826990412\n",
            "Iteration#:  9956 ; Loss:  7.1540705555794695 ; l2 norm of gradient:  1.5930774625349158 ; l2 norm of weights:  8.533087603903148\n",
            "Iteration#:  9960 ; Loss:  7.1539831491039125 ; l2 norm of gradient:  1.5930688332843612 ; l2 norm of weights:  8.53303738120963\n",
            "Iteration#:  9964 ; Loss:  7.1538957297516035 ; l2 norm of gradient:  1.5930602040256514 ; l2 norm of weights:  8.532987158909854\n",
            "Iteration#:  9968 ; Loss:  7.153808318537192 ; l2 norm of gradient:  1.5930515747587826 ; l2 norm of weights:  8.53293693700382\n",
            "Iteration#:  9972 ; Loss:  7.153720907474575 ; l2 norm of gradient:  1.5930429454837503 ; l2 norm of weights:  8.532886715491529\n",
            "Iteration#:  9976 ; Loss:  7.153633497912567 ; l2 norm of gradient:  1.593034316200553 ; l2 norm of weights:  8.53283649437298\n",
            "Iteration#:  9980 ; Loss:  7.153546085323956 ; l2 norm of gradient:  1.5930256869091848 ; l2 norm of weights:  8.532786273648172\n",
            "Iteration#:  9984 ; Loss:  7.153458678497715 ; l2 norm of gradient:  1.5930170576096412 ; l2 norm of weights:  8.532736053317102\n",
            "Iteration#:  9988 ; Loss:  7.153371269189758 ; l2 norm of gradient:  1.5930084283019201 ; l2 norm of weights:  8.532685833379773\n",
            "Iteration#:  9992 ; Loss:  7.153283862161647 ; l2 norm of gradient:  1.5929997989860163 ; l2 norm of weights:  8.53263561383618\n",
            "Iteration#:  9996 ; Loss:  7.153196447873083 ; l2 norm of gradient:  1.592991169661927 ; l2 norm of weights:  8.532585394686322\n",
            "1e-05 0.1 0.5924075924075924\n",
            "Iteration#:  0 ; Loss:  3.1141392143671647 ; l2 norm of gradient:  1.1428812455768378 ; l2 norm of weights:  7.2177915821273695\n",
            "Iteration#:  4 ; Loss:  3.1140867060360313 ; l2 norm of gradient:  1.1428637178430716 ; l2 norm of weights:  7.217773640375809\n",
            "Iteration#:  8 ; Loss:  3.1140341994861593 ; l2 norm of gradient:  1.1428461902681943 ; l2 norm of weights:  7.217755698888635\n",
            "Iteration#:  12 ; Loss:  3.1139816942539533 ; l2 norm of gradient:  1.1428286628521298 ; l2 norm of weights:  7.217737757665843\n",
            "Iteration#:  16 ; Loss:  3.1139291908892943 ; l2 norm of gradient:  1.1428111355947967 ; l2 norm of weights:  7.217719816707422\n",
            "Iteration#:  20 ; Loss:  3.1138766891864877 ; l2 norm of gradient:  1.14279360849612 ; l2 norm of weights:  7.217701876013371\n",
            "Iteration#:  24 ; Loss:  3.1138241887275293 ; l2 norm of gradient:  1.142776081556019 ; l2 norm of weights:  7.217683935583676\n",
            "Iteration#:  28 ; Loss:  3.113771690281844 ; l2 norm of gradient:  1.1427585547744168 ; l2 norm of weights:  7.217665995418335\n",
            "Iteration#:  32 ; Loss:  3.1137191930358616 ; l2 norm of gradient:  1.142741028151234 ; l2 norm of weights:  7.217648055517339\n",
            "Iteration#:  36 ; Loss:  3.113666697254694 ; l2 norm of gradient:  1.1427235016863937 ; l2 norm of weights:  7.217630115880679\n",
            "Iteration#:  40 ; Loss:  3.1136142037312515 ; l2 norm of gradient:  1.1427059753798157 ; l2 norm of weights:  7.217612176508351\n",
            "Iteration#:  44 ; Loss:  3.1135617113815015 ; l2 norm of gradient:  1.1426884492314235 ; l2 norm of weights:  7.217594237400347\n",
            "Iteration#:  48 ; Loss:  3.1135092208622925 ; l2 norm of gradient:  1.1426709232411376 ; l2 norm of weights:  7.217576298556658\n",
            "Iteration#:  52 ; Loss:  3.113456731564308 ; l2 norm of gradient:  1.1426533974088795 ; l2 norm of weights:  7.217558359977278\n",
            "Iteration#:  56 ; Loss:  3.1134042437064204 ; l2 norm of gradient:  1.142635871734572 ; l2 norm of weights:  7.217540421662202\n",
            "Iteration#:  60 ; Loss:  3.113351758709094 ; l2 norm of gradient:  1.1426183462181356 ; l2 norm of weights:  7.217522483611419\n",
            "Iteration#:  64 ; Loss:  3.1132992738282774 ; l2 norm of gradient:  1.142600820859493 ; l2 norm of weights:  7.217504545824925\n",
            "Iteration#:  68 ; Loss:  3.113246792088524 ; l2 norm of gradient:  1.1425832956585653 ; l2 norm of weights:  7.217486608302711\n",
            "Iteration#:  72 ; Loss:  3.113194310350866 ; l2 norm of gradient:  1.1425657706152734 ; l2 norm of weights:  7.217468671044771\n",
            "Iteration#:  76 ; Loss:  3.113141831096096 ; l2 norm of gradient:  1.1425482457295402 ; l2 norm of weights:  7.217450734051098\n",
            "Iteration#:  80 ; Loss:  3.113089353553578 ; l2 norm of gradient:  1.142530721001287 ; l2 norm of weights:  7.217432797321682\n",
            "Iteration#:  84 ; Loss:  3.1130368770900407 ; l2 norm of gradient:  1.1425131964304347 ; l2 norm of weights:  7.217414860856522\n",
            "Iteration#:  88 ; Loss:  3.1129844027380953 ; l2 norm of gradient:  1.1424956720169066 ; l2 norm of weights:  7.217396924655604\n",
            "Iteration#:  92 ; Loss:  3.1129319295413396 ; l2 norm of gradient:  1.142478147760623 ; l2 norm of weights:  7.2173789887189255\n",
            "Iteration#:  96 ; Loss:  3.112879458532383 ; l2 norm of gradient:  1.142460623661505 ; l2 norm of weights:  7.217361053046478\n",
            "Iteration#:  100 ; Loss:  3.112826988108592 ; l2 norm of gradient:  1.1424430997194752 ; l2 norm of weights:  7.2173431176382525\n",
            "Iteration#:  104 ; Loss:  3.112774520004642 ; l2 norm of gradient:  1.1424255759344566 ; l2 norm of weights:  7.217325182494244\n",
            "Iteration#:  108 ; Loss:  3.1127220536398768 ; l2 norm of gradient:  1.1424080523063673 ; l2 norm of weights:  7.217307247614445\n",
            "Iteration#:  112 ; Loss:  3.1126695887261264 ; l2 norm of gradient:  1.1423905288351326 ; l2 norm of weights:  7.217289312998849\n",
            "Iteration#:  116 ; Loss:  3.112617125704231 ; l2 norm of gradient:  1.1423730055206711 ; l2 norm of weights:  7.217271378647448\n",
            "Iteration#:  120 ; Loss:  3.1125646635291115 ; l2 norm of gradient:  1.1423554823629067 ; l2 norm of weights:  7.217253444560234\n",
            "Iteration#:  124 ; Loss:  3.1125122029222485 ; l2 norm of gradient:  1.1423379593617606 ; l2 norm of weights:  7.217235510737203\n",
            "Iteration#:  128 ; Loss:  3.11245974491292 ; l2 norm of gradient:  1.142320436517153 ; l2 norm of weights:  7.217217577178343\n",
            "Iteration#:  132 ; Loss:  3.1124072876972857 ; l2 norm of gradient:  1.1423029138290073 ; l2 norm of weights:  7.217199643883651\n",
            "Iteration#:  136 ; Loss:  3.1123548327700195 ; l2 norm of gradient:  1.1422853912972437 ; l2 norm of weights:  7.217181710853118\n",
            "Iteration#:  140 ; Loss:  3.112302378827342 ; l2 norm of gradient:  1.142267868921784 ; l2 norm of weights:  7.217163778086738\n",
            "Iteration#:  144 ; Loss:  3.1122499260289618 ; l2 norm of gradient:  1.1422503467025507 ; l2 norm of weights:  7.217145845584503\n",
            "Iteration#:  148 ; Loss:  3.112197475716995 ; l2 norm of gradient:  1.142232824639465 ; l2 norm of weights:  7.217127913346406\n",
            "Iteration#:  152 ; Loss:  3.112145026969822 ; l2 norm of gradient:  1.1422153027324489 ; l2 norm of weights:  7.217109981372439\n",
            "Iteration#:  156 ; Loss:  3.1120925798733206 ; l2 norm of gradient:  1.1421977809814226 ; l2 norm of weights:  7.217092049662598\n",
            "Iteration#:  160 ; Loss:  3.1120401340259893 ; l2 norm of gradient:  1.1421802593863093 ; l2 norm of weights:  7.217074118216871\n",
            "Iteration#:  164 ; Loss:  3.111987689448359 ; l2 norm of gradient:  1.142162737947029 ; l2 norm of weights:  7.217056187035256\n",
            "Iteration#:  168 ; Loss:  3.1119352472579287 ; l2 norm of gradient:  1.142145216663506 ; l2 norm of weights:  7.217038256117741\n",
            "Iteration#:  172 ; Loss:  3.111882806315405 ; l2 norm of gradient:  1.142127695535659 ; l2 norm of weights:  7.217020325464323\n",
            "Iteration#:  176 ; Loss:  3.1118303672533867 ; l2 norm of gradient:  1.1421101745634108 ; l2 norm of weights:  7.217002395074992\n",
            "Iteration#:  180 ; Loss:  3.111777928811329 ; l2 norm of gradient:  1.1420926537466831 ; l2 norm of weights:  7.216984464949744\n",
            "Iteration#:  184 ; Loss:  3.111725492836622 ; l2 norm of gradient:  1.1420751330853973 ; l2 norm of weights:  7.216966535088568\n",
            "Iteration#:  188 ; Loss:  3.1116730585380967 ; l2 norm of gradient:  1.142057612579476 ; l2 norm of weights:  7.216948605491459\n",
            "Iteration#:  192 ; Loss:  3.1116206250630585 ; l2 norm of gradient:  1.142040092228838 ; l2 norm of weights:  7.216930676158411\n",
            "Iteration#:  196 ; Loss:  3.1115681947212552 ; l2 norm of gradient:  1.1420225720334076 ; l2 norm of weights:  7.216912747089414\n",
            "Iteration#:  200 ; Loss:  3.111515764312264 ; l2 norm of gradient:  1.1420050519931064 ; l2 norm of weights:  7.216894818284463\n",
            "Iteration#:  204 ; Loss:  3.1114633359436956 ; l2 norm of gradient:  1.1419875321078534 ; l2 norm of weights:  7.216876889743548\n",
            "Iteration#:  208 ; Loss:  3.1114109099645684 ; l2 norm of gradient:  1.1419700123775727 ; l2 norm of weights:  7.216858961466668\n",
            "Iteration#:  212 ; Loss:  3.1113584846361686 ; l2 norm of gradient:  1.1419524928021854 ; l2 norm of weights:  7.216841033453809\n",
            "Iteration#:  216 ; Loss:  3.111306061924724 ; l2 norm of gradient:  1.1419349733816124 ; l2 norm of weights:  7.216823105704969\n",
            "Iteration#:  220 ; Loss:  3.1112536396849304 ; l2 norm of gradient:  1.1419174541157757 ; l2 norm of weights:  7.216805178220137\n",
            "Iteration#:  224 ; Loss:  3.111201219498419 ; l2 norm of gradient:  1.1418999350045973 ; l2 norm of weights:  7.216787250999309\n",
            "Iteration#:  228 ; Loss:  3.11114880125238 ; l2 norm of gradient:  1.1418824160479972 ; l2 norm of weights:  7.216769324042476\n",
            "Iteration#:  232 ; Loss:  3.1110963840521073 ; l2 norm of gradient:  1.141864897245899 ; l2 norm of weights:  7.216751397349631\n",
            "Iteration#:  236 ; Loss:  3.1110439683919484 ; l2 norm of gradient:  1.1418473785982226 ; l2 norm of weights:  7.216733470920768\n",
            "Iteration#:  240 ; Loss:  3.110991555045077 ; l2 norm of gradient:  1.1418298601048904 ; l2 norm of weights:  7.2167155447558775\n",
            "Iteration#:  244 ; Loss:  3.110939142972407 ; l2 norm of gradient:  1.141812341765823 ; l2 norm of weights:  7.216697618854956\n",
            "Iteration#:  248 ; Loss:  3.1108867328880785 ; l2 norm of gradient:  1.1417948235809459 ; l2 norm of weights:  7.216679693217992\n",
            "Iteration#:  252 ; Loss:  3.1108343233667957 ; l2 norm of gradient:  1.1417773055501748 ; l2 norm of weights:  7.2166617678449825\n",
            "Iteration#:  256 ; Loss:  3.110781916118638 ; l2 norm of gradient:  1.1417597876734353 ; l2 norm of weights:  7.21664384273592\n",
            "Iteration#:  260 ; Loss:  3.1107295105764274 ; l2 norm of gradient:  1.1417422699506476 ; l2 norm of weights:  7.216625917890793\n",
            "Iteration#:  264 ; Loss:  3.1106771062424796 ; l2 norm of gradient:  1.1417247523817333 ; l2 norm of weights:  7.216607993309599\n",
            "Iteration#:  268 ; Loss:  3.1106247043990236 ; l2 norm of gradient:  1.1417072349666149 ; l2 norm of weights:  7.216590068992328\n",
            "Iteration#:  272 ; Loss:  3.1105723032460353 ; l2 norm of gradient:  1.141689717705211 ; l2 norm of weights:  7.216572144938975\n",
            "Iteration#:  276 ; Loss:  3.1105199035348963 ; l2 norm of gradient:  1.1416722005974471 ; l2 norm of weights:  7.216554221149531\n",
            "Iteration#:  280 ; Loss:  3.110467506213398 ; l2 norm of gradient:  1.1416546836432424 ; l2 norm of weights:  7.216536297623991\n",
            "Iteration#:  284 ; Loss:  3.110415110105338 ; l2 norm of gradient:  1.1416371668425191 ; l2 norm of weights:  7.216518374362345\n",
            "Iteration#:  288 ; Loss:  3.110362715818565 ; l2 norm of gradient:  1.1416196501951978 ; l2 norm of weights:  7.21650045136459\n",
            "Iteration#:  292 ; Loss:  3.110310322556831 ; l2 norm of gradient:  1.1416021337012026 ; l2 norm of weights:  7.216482528630714\n",
            "Iteration#:  296 ; Loss:  3.110257930856576 ; l2 norm of gradient:  1.1415846173604525 ; l2 norm of weights:  7.216464606160713\n",
            "Iteration#:  300 ; Loss:  3.1102055418903634 ; l2 norm of gradient:  1.141567101172871 ; l2 norm of weights:  7.216446683954581\n",
            "Iteration#:  304 ; Loss:  3.1101531534517766 ; l2 norm of gradient:  1.1415495851383772 ; l2 norm of weights:  7.216428762012307\n",
            "Iteration#:  308 ; Loss:  3.1101007667099325 ; l2 norm of gradient:  1.1415320692568944 ; l2 norm of weights:  7.216410840333885\n",
            "Iteration#:  312 ; Loss:  3.1100483822198965 ; l2 norm of gradient:  1.1415145535283437 ; l2 norm of weights:  7.21639291891931\n",
            "Iteration#:  316 ; Loss:  3.1099959988166903 ; l2 norm of gradient:  1.1414970379526472 ; l2 norm of weights:  7.216374997768575\n",
            "Iteration#:  320 ; Loss:  3.1099436175808792 ; l2 norm of gradient:  1.1414795225297254 ; l2 norm of weights:  7.216357076881671\n",
            "Iteration#:  324 ; Loss:  3.109891236913783 ; l2 norm of gradient:  1.1414620072595014 ; l2 norm of weights:  7.216339156258591\n",
            "Iteration#:  328 ; Loss:  3.109838858206027 ; l2 norm of gradient:  1.141444492141895 ; l2 norm of weights:  7.216321235899327\n",
            "Iteration#:  332 ; Loss:  3.109786482009745 ; l2 norm of gradient:  1.1414269771768293 ; l2 norm of weights:  7.2163033158038745\n",
            "Iteration#:  336 ; Loss:  3.1097341066264113 ; l2 norm of gradient:  1.1414094623642241 ; l2 norm of weights:  7.216285395972226\n",
            "Iteration#:  340 ; Loss:  3.109681733193951 ; l2 norm of gradient:  1.1413919477040024 ; l2 norm of weights:  7.216267476404371\n",
            "Iteration#:  344 ; Loss:  3.1096293606253407 ; l2 norm of gradient:  1.1413744331960856 ; l2 norm of weights:  7.216249557100307\n",
            "Iteration#:  348 ; Loss:  3.1095769900902304 ; l2 norm of gradient:  1.1413569188403947 ; l2 norm of weights:  7.216231638060024\n",
            "Iteration#:  352 ; Loss:  3.1095246215631795 ; l2 norm of gradient:  1.1413394046368512 ; l2 norm of weights:  7.216213719283513\n",
            "Iteration#:  356 ; Loss:  3.1094722539954867 ; l2 norm of gradient:  1.1413218905853775 ; l2 norm of weights:  7.216195800770772\n",
            "Iteration#:  360 ; Loss:  3.109419887931744 ; l2 norm of gradient:  1.1413043766858941 ; l2 norm of weights:  7.216177882521791\n",
            "Iteration#:  364 ; Loss:  3.109367524301522 ; l2 norm of gradient:  1.141286862938323 ; l2 norm of weights:  7.216159964536564\n",
            "Iteration#:  368 ; Loss:  3.109315161860569 ; l2 norm of gradient:  1.141269349342586 ; l2 norm of weights:  7.216142046815082\n",
            "Iteration#:  372 ; Loss:  3.109262801496194 ; l2 norm of gradient:  1.1412518358986037 ; l2 norm of weights:  7.216124129357338\n",
            "Iteration#:  376 ; Loss:  3.1092104419019644 ; l2 norm of gradient:  1.1412343226062984 ; l2 norm of weights:  7.216106212163326\n",
            "Iteration#:  380 ; Loss:  3.1091580837235693 ; l2 norm of gradient:  1.1412168094655928 ; l2 norm of weights:  7.216088295233041\n",
            "Iteration#:  384 ; Loss:  3.1091057279980845 ; l2 norm of gradient:  1.1411992964764064 ; l2 norm of weights:  7.216070378566472\n",
            "Iteration#:  388 ; Loss:  3.109053373002352 ; l2 norm of gradient:  1.141181783638661 ; l2 norm of weights:  7.216052462163613\n",
            "Iteration#:  392 ; Loss:  3.1090010211515597 ; l2 norm of gradient:  1.1411642709522791 ; l2 norm of weights:  7.2160345460244555\n",
            "Iteration#:  396 ; Loss:  3.108948669504455 ; l2 norm of gradient:  1.1411467584171817 ; l2 norm of weights:  7.216016630148997\n",
            "Iteration#:  400 ; Loss:  3.108896319620414 ; l2 norm of gradient:  1.1411292460332902 ; l2 norm of weights:  7.215998714537227\n",
            "Iteration#:  404 ; Loss:  3.1088439718410155 ; l2 norm of gradient:  1.141111733800528 ; l2 norm of weights:  7.215980799189138\n",
            "Iteration#:  408 ; Loss:  3.108791624935409 ; l2 norm of gradient:  1.141094221718813 ; l2 norm of weights:  7.215962884104724\n",
            "Iteration#:  412 ; Loss:  3.108739279683621 ; l2 norm of gradient:  1.1410767097880699 ; l2 norm of weights:  7.215944969283977\n",
            "Iteration#:  416 ; Loss:  3.1086869372150563 ; l2 norm of gradient:  1.1410591980082188 ; l2 norm of weights:  7.215927054726893\n",
            "Iteration#:  420 ; Loss:  3.108634595206325 ; l2 norm of gradient:  1.1410416863791806 ; l2 norm of weights:  7.215909140433461\n",
            "Iteration#:  424 ; Loss:  3.1085822554354805 ; l2 norm of gradient:  1.141024174900879 ; l2 norm of weights:  7.2158912264036745\n",
            "Iteration#:  428 ; Loss:  3.108529916760498 ; l2 norm of gradient:  1.1410066635732337 ; l2 norm of weights:  7.215873312637528\n",
            "Iteration#:  432 ; Loss:  3.108477579484288 ; l2 norm of gradient:  1.1409891523961666 ; l2 norm of weights:  7.215855399135013\n",
            "Iteration#:  436 ; Loss:  3.1084252448610075 ; l2 norm of gradient:  1.1409716413695998 ; l2 norm of weights:  7.215837485896123\n",
            "Iteration#:  440 ; Loss:  3.108372910841589 ; l2 norm of gradient:  1.1409541304934547 ; l2 norm of weights:  7.215819572920851\n",
            "Iteration#:  444 ; Loss:  3.1083205785696535 ; l2 norm of gradient:  1.1409366197676523 ; l2 norm of weights:  7.215801660209189\n",
            "Iteration#:  448 ; Loss:  3.1082682487749125 ; l2 norm of gradient:  1.1409191091921136 ; l2 norm of weights:  7.215783747761131\n",
            "Iteration#:  452 ; Loss:  3.108215919363442 ; l2 norm of gradient:  1.1409015987667623 ; l2 norm of weights:  7.215765835576669\n",
            "Iteration#:  456 ; Loss:  3.108163592646557 ; l2 norm of gradient:  1.140884088491518 ; l2 norm of weights:  7.2157479236557975\n",
            "Iteration#:  460 ; Loss:  3.1081112664893435 ; l2 norm of gradient:  1.140866578366303 ; l2 norm of weights:  7.215730011998508\n",
            "Iteration#:  464 ; Loss:  3.1080589421789186 ; l2 norm of gradient:  1.140849068391038 ; l2 norm of weights:  7.215712100604792\n",
            "Iteration#:  468 ; Loss:  3.1080066200664844 ; l2 norm of gradient:  1.140831558565646 ; l2 norm of weights:  7.215694189474645\n",
            "Iteration#:  472 ; Loss:  3.1079542988073126 ; l2 norm of gradient:  1.1408140488900476 ; l2 norm of weights:  7.2156762786080595\n",
            "Iteration#:  476 ; Loss:  3.1079019797135974 ; l2 norm of gradient:  1.1407965393641641 ; l2 norm of weights:  7.215658368005027\n",
            "Iteration#:  480 ; Loss:  3.107849662495074 ; l2 norm of gradient:  1.1407790299879172 ; l2 norm of weights:  7.2156404576655415\n",
            "Iteration#:  484 ; Loss:  3.1077973458696158 ; l2 norm of gradient:  1.1407615207612294 ; l2 norm of weights:  7.215622547589593\n",
            "Iteration#:  488 ; Loss:  3.1077450318374753 ; l2 norm of gradient:  1.1407440116840202 ; l2 norm of weights:  7.215604637777178\n",
            "Iteration#:  492 ; Loss:  3.1076927187325416 ; l2 norm of gradient:  1.1407265027562123 ; l2 norm of weights:  7.21558672822829\n",
            "Iteration#:  496 ; Loss:  3.1076404071880543 ; l2 norm of gradient:  1.140708993977729 ; l2 norm of weights:  7.215568818942918\n",
            "Iteration#:  500 ; Loss:  3.107588098048783 ; l2 norm of gradient:  1.1406914853484893 ; l2 norm of weights:  7.215550909921058\n",
            "Iteration#:  504 ; Loss:  3.1075357897631757 ; l2 norm of gradient:  1.140673976868415 ; l2 norm of weights:  7.215533001162703\n",
            "Iteration#:  508 ; Loss:  3.10748348271006 ; l2 norm of gradient:  1.1406564685374294 ; l2 norm of weights:  7.215515092667842\n",
            "Iteration#:  512 ; Loss:  3.1074311784752986 ; l2 norm of gradient:  1.1406389603554514 ; l2 norm of weights:  7.2154971844364715\n",
            "Iteration#:  516 ; Loss:  3.107378874727059 ; l2 norm of gradient:  1.140621452322405 ; l2 norm of weights:  7.215479276468584\n",
            "Iteration#:  520 ; Loss:  3.107326572774575 ; l2 norm of gradient:  1.1406039444382097 ; l2 norm of weights:  7.2154613687641715\n",
            "Iteration#:  524 ; Loss:  3.1072742730738105 ; l2 norm of gradient:  1.1405864367027885 ; l2 norm of weights:  7.215443461323228\n",
            "Iteration#:  528 ; Loss:  3.107221974989841 ; l2 norm of gradient:  1.1405689291160623 ; l2 norm of weights:  7.215425554145744\n",
            "Iteration#:  532 ; Loss:  3.1071696784428755 ; l2 norm of gradient:  1.1405514216779542 ; l2 norm of weights:  7.215407647231715\n",
            "Iteration#:  536 ; Loss:  3.10711738284747 ; l2 norm of gradient:  1.1405339143883821 ; l2 norm of weights:  7.215389740581133\n",
            "Iteration#:  540 ; Loss:  3.1070650886929805 ; l2 norm of gradient:  1.140516407247271 ; l2 norm of weights:  7.2153718341939905\n",
            "Iteration#:  544 ; Loss:  3.1070127970340127 ; l2 norm of gradient:  1.1404989002545414 ; l2 norm of weights:  7.2153539280702805\n",
            "Iteration#:  548 ; Loss:  3.1069605063346164 ; l2 norm of gradient:  1.1404813934101135 ; l2 norm of weights:  7.215336022209996\n",
            "Iteration#:  552 ; Loss:  3.106908217070619 ; l2 norm of gradient:  1.1404638867139105 ; l2 norm of weights:  7.21531811661313\n",
            "Iteration#:  556 ; Loss:  3.10685593030663 ; l2 norm of gradient:  1.1404463801658533 ; l2 norm of weights:  7.215300211279675\n",
            "Iteration#:  560 ; Loss:  3.106803644414895 ; l2 norm of gradient:  1.1404288737658643 ; l2 norm of weights:  7.215282306209624\n",
            "Iteration#:  564 ; Loss:  3.1067513600435084 ; l2 norm of gradient:  1.140411367513863 ; l2 norm of weights:  7.21526440140297\n",
            "Iteration#:  568 ; Loss:  3.106699078103101 ; l2 norm of gradient:  1.1403938614097728 ; l2 norm of weights:  7.215246496859706\n",
            "Iteration#:  572 ; Loss:  3.1066467967071616 ; l2 norm of gradient:  1.1403763554535142 ; l2 norm of weights:  7.215228592579825\n",
            "Iteration#:  576 ; Loss:  3.106594517946667 ; l2 norm of gradient:  1.1403588496450099 ; l2 norm of weights:  7.21521068856332\n",
            "Iteration#:  580 ; Loss:  3.1065422398102327 ; l2 norm of gradient:  1.1403413439841792 ; l2 norm of weights:  7.215192784810182\n",
            "Iteration#:  584 ; Loss:  3.1064899635281065 ; l2 norm of gradient:  1.1403238384709462 ; l2 norm of weights:  7.215174881320406\n",
            "Iteration#:  588 ; Loss:  3.10643768944328 ; l2 norm of gradient:  1.1403063331052317 ; l2 norm of weights:  7.215156978093985\n",
            "Iteration#:  592 ; Loss:  3.1063854164160682 ; l2 norm of gradient:  1.1402888278869576 ; l2 norm of weights:  7.21513907513091\n",
            "Iteration#:  596 ; Loss:  3.1063331447998705 ; l2 norm of gradient:  1.1402713228160426 ; l2 norm of weights:  7.215121172431176\n",
            "Iteration#:  600 ; Loss:  3.1062808755391127 ; l2 norm of gradient:  1.1402538178924118 ; l2 norm of weights:  7.215103269994775\n",
            "Iteration#:  604 ; Loss:  3.1062286071244336 ; l2 norm of gradient:  1.1402363131159847 ; l2 norm of weights:  7.2150853678216995\n",
            "Iteration#:  608 ; Loss:  3.106176340394051 ; l2 norm of gradient:  1.1402188084866844 ; l2 norm of weights:  7.215067465911942\n",
            "Iteration#:  612 ; Loss:  3.1061240759527187 ; l2 norm of gradient:  1.1402013040044299 ; l2 norm of weights:  7.215049564265496\n",
            "Iteration#:  616 ; Loss:  3.1060718122396382 ; l2 norm of gradient:  1.1401837996691455 ; l2 norm of weights:  7.215031662882356\n",
            "Iteration#:  620 ; Loss:  3.1060195511878903 ; l2 norm of gradient:  1.140166295480751 ; l2 norm of weights:  7.215013761762512\n",
            "Iteration#:  624 ; Loss:  3.1059672908480067 ; l2 norm of gradient:  1.1401487914391693 ; l2 norm of weights:  7.214995860905957\n",
            "Iteration#:  628 ; Loss:  3.1059150320231943 ; l2 norm of gradient:  1.1401312875443206 ; l2 norm of weights:  7.214977960312688\n",
            "Iteration#:  632 ; Loss:  3.1058627757297446 ; l2 norm of gradient:  1.1401137837961268 ; l2 norm of weights:  7.214960059982693\n",
            "Iteration#:  636 ; Loss:  3.1058105201259205 ; l2 norm of gradient:  1.1400962801945098 ; l2 norm of weights:  7.214942159915968\n",
            "Iteration#:  640 ; Loss:  3.1057582661167626 ; l2 norm of gradient:  1.1400787767393907 ; l2 norm of weights:  7.214924260112503\n",
            "Iteration#:  644 ; Loss:  3.1057060144423936 ; l2 norm of gradient:  1.1400612734306919 ; l2 norm of weights:  7.214906360572294\n",
            "Iteration#:  648 ; Loss:  3.105653763676939 ; l2 norm of gradient:  1.1400437702683346 ; l2 norm of weights:  7.214888461295331\n",
            "Iteration#:  652 ; Loss:  3.105601514352501 ; l2 norm of gradient:  1.1400262672522405 ; l2 norm of weights:  7.214870562281611\n",
            "Iteration#:  656 ; Loss:  3.105549267407217 ; l2 norm of gradient:  1.1400087643823296 ; l2 norm of weights:  7.214852663531122\n",
            "Iteration#:  660 ; Loss:  3.1054970212634085 ; l2 norm of gradient:  1.1399912616585257 ; l2 norm of weights:  7.21483476504386\n",
            "Iteration#:  664 ; Loss:  3.1054447767612454 ; l2 norm of gradient:  1.1399737590807488 ; l2 norm of weights:  7.214816866819817\n",
            "Iteration#:  668 ; Loss:  3.1053925348372715 ; l2 norm of gradient:  1.1399562566489205 ; l2 norm of weights:  7.214798968858985\n",
            "Iteration#:  672 ; Loss:  3.105340293583337 ; l2 norm of gradient:  1.1399387543629635 ; l2 norm of weights:  7.21478107116136\n",
            "Iteration#:  676 ; Loss:  3.105288053960727 ; l2 norm of gradient:  1.1399212522227988 ; l2 norm of weights:  7.214763173726929\n",
            "Iteration#:  680 ; Loss:  3.105235816676834 ; l2 norm of gradient:  1.1399037502283473 ; l2 norm of weights:  7.214745276555692\n",
            "Iteration#:  684 ; Loss:  3.1051835803014565 ; l2 norm of gradient:  1.1398862483795322 ; l2 norm of weights:  7.214727379647636\n",
            "Iteration#:  688 ; Loss:  3.105131346361682 ; l2 norm of gradient:  1.139868746676272 ; l2 norm of weights:  7.214709483002757\n",
            "Iteration#:  692 ; Loss:  3.1050791133528155 ; l2 norm of gradient:  1.1398512451184915 ; l2 norm of weights:  7.214691586621048\n",
            "Iteration#:  696 ; Loss:  3.1050268818765856 ; l2 norm of gradient:  1.1398337437061108 ; l2 norm of weights:  7.214673690502501\n",
            "Iteration#:  700 ; Loss:  3.1049746523379045 ; l2 norm of gradient:  1.1398162424390517 ; l2 norm of weights:  7.214655794647109\n",
            "Iteration#:  704 ; Loss:  3.104922423623406 ; l2 norm of gradient:  1.139798741317236 ; l2 norm of weights:  7.214637899054864\n",
            "Iteration#:  708 ; Loss:  3.1048701967462087 ; l2 norm of gradient:  1.1397812403405843 ; l2 norm of weights:  7.214620003725759\n",
            "Iteration#:  712 ; Loss:  3.1048179724341924 ; l2 norm of gradient:  1.1397637395090197 ; l2 norm of weights:  7.21460210865979\n",
            "Iteration#:  716 ; Loss:  3.104765748744496 ; l2 norm of gradient:  1.1397462388224624 ; l2 norm of weights:  7.214584213856945\n",
            "Iteration#:  720 ; Loss:  3.104713526966306 ; l2 norm of gradient:  1.1397287382808348 ; l2 norm of weights:  7.21456631931722\n",
            "Iteration#:  724 ; Loss:  3.1046613073987346 ; l2 norm of gradient:  1.1397112378840566 ; l2 norm of weights:  7.214548425040607\n",
            "Iteration#:  728 ; Loss:  3.1046090883365505 ; l2 norm of gradient:  1.1396937376320526 ; l2 norm of weights:  7.2145305310270995\n",
            "Iteration#:  732 ; Loss:  3.104556871159548 ; l2 norm of gradient:  1.1396762375247418 ; l2 norm of weights:  7.21451263727669\n",
            "Iteration#:  736 ; Loss:  3.1045046564058776 ; l2 norm of gradient:  1.139658737562047 ; l2 norm of weights:  7.2144947437893725\n",
            "Iteration#:  740 ; Loss:  3.104452441879494 ; l2 norm of gradient:  1.1396412377438896 ; l2 norm of weights:  7.214476850565138\n",
            "Iteration#:  744 ; Loss:  3.104400229523053 ; l2 norm of gradient:  1.1396237380701917 ; l2 norm of weights:  7.21445895760398\n",
            "Iteration#:  748 ; Loss:  3.104348019459012 ; l2 norm of gradient:  1.139606238540873 ; l2 norm of weights:  7.21444106490589\n",
            "Iteration#:  752 ; Loss:  3.1042958103647145 ; l2 norm of gradient:  1.1395887391558563 ; l2 norm of weights:  7.214423172470863\n",
            "Iteration#:  756 ; Loss:  3.1042436027090505 ; l2 norm of gradient:  1.139571239915064 ; l2 norm of weights:  7.214405280298892\n",
            "Iteration#:  760 ; Loss:  3.1041913974400877 ; l2 norm of gradient:  1.139553740818416 ; l2 norm of weights:  7.21438738838997\n",
            "Iteration#:  764 ; Loss:  3.104139193041824 ; l2 norm of gradient:  1.1395362418658348 ; l2 norm of weights:  7.214369496744088\n",
            "Iteration#:  768 ; Loss:  3.1040869906279243 ; l2 norm of gradient:  1.139518743057243 ; l2 norm of weights:  7.214351605361241\n",
            "Iteration#:  772 ; Loss:  3.1040347893948423 ; l2 norm of gradient:  1.1395012443925607 ; l2 norm of weights:  7.21433371424142\n",
            "Iteration#:  776 ; Loss:  3.103982589946281 ; l2 norm of gradient:  1.1394837458717089 ; l2 norm of weights:  7.214315823384618\n",
            "Iteration#:  780 ; Loss:  3.1039303924943127 ; l2 norm of gradient:  1.1394662474946113 ; l2 norm of weights:  7.21429793279083\n",
            "Iteration#:  784 ; Loss:  3.103878195893909 ; l2 norm of gradient:  1.1394487492611882 ; l2 norm of weights:  7.214280042460048\n",
            "Iteration#:  788 ; Loss:  3.1038260013311136 ; l2 norm of gradient:  1.1394312511713613 ; l2 norm of weights:  7.214262152392263\n",
            "Iteration#:  792 ; Loss:  3.103773808346185 ; l2 norm of gradient:  1.139413753225052 ; l2 norm of weights:  7.214244262587469\n",
            "Iteration#:  796 ; Loss:  3.1037216168664155 ; l2 norm of gradient:  1.1393962554221824 ; l2 norm of weights:  7.214226373045659\n",
            "Iteration#:  800 ; Loss:  3.1036694267712015 ; l2 norm of gradient:  1.139378757762674 ; l2 norm of weights:  7.214208483766827\n",
            "Iteration#:  804 ; Loss:  3.103617239161469 ; l2 norm of gradient:  1.1393612602464478 ; l2 norm of weights:  7.2141905947509635\n",
            "Iteration#:  808 ; Loss:  3.1035650523835745 ; l2 norm of gradient:  1.1393437628734266 ; l2 norm of weights:  7.214172705998065\n",
            "Iteration#:  812 ; Loss:  3.1035128665711382 ; l2 norm of gradient:  1.1393262656435308 ; l2 norm of weights:  7.214154817508121\n",
            "Iteration#:  816 ; Loss:  3.1034606837785486 ; l2 norm of gradient:  1.139308768556683 ; l2 norm of weights:  7.214136929281125\n",
            "Iteration#:  820 ; Loss:  3.103408501598987 ; l2 norm of gradient:  1.1392912716128036 ; l2 norm of weights:  7.21411904131707\n",
            "Iteration#:  824 ; Loss:  3.10335632127632 ; l2 norm of gradient:  1.139273774811815 ; l2 norm of weights:  7.214101153615951\n",
            "Iteration#:  828 ; Loss:  3.103304143288698 ; l2 norm of gradient:  1.1392562781536393 ; l2 norm of weights:  7.2140832661777585\n",
            "Iteration#:  832 ; Loss:  3.1032519655092057 ; l2 norm of gradient:  1.139238781638198 ; l2 norm of weights:  7.214065379002485\n",
            "Iteration#:  836 ; Loss:  3.1031997898793113 ; l2 norm of gradient:  1.1392212852654113 ; l2 norm of weights:  7.214047492090126\n",
            "Iteration#:  840 ; Loss:  3.1031476164974463 ; l2 norm of gradient:  1.1392037890352016 ; l2 norm of weights:  7.214029605440673\n",
            "Iteration#:  844 ; Loss:  3.103095443913963 ; l2 norm of gradient:  1.1391862929474914 ; l2 norm of weights:  7.214011719054117\n",
            "Iteration#:  848 ; Loss:  3.1030432729297464 ; l2 norm of gradient:  1.1391687970022015 ; l2 norm of weights:  7.213993832930455\n",
            "Iteration#:  852 ; Loss:  3.1029911044325824 ; l2 norm of gradient:  1.1391513011992527 ; l2 norm of weights:  7.213975947069676\n",
            "Iteration#:  856 ; Loss:  3.102938936662382 ; l2 norm of gradient:  1.1391338055385687 ; l2 norm of weights:  7.213958061471774\n",
            "Iteration#:  860 ; Loss:  3.102886770618781 ; l2 norm of gradient:  1.1391163100200699 ; l2 norm of weights:  7.213940176136743\n",
            "Iteration#:  864 ; Loss:  3.1028346063962595 ; l2 norm of gradient:  1.1390988146436776 ; l2 norm of weights:  7.213922291064576\n",
            "Iteration#:  868 ; Loss:  3.10278244351728 ; l2 norm of gradient:  1.139081319409315 ; l2 norm of weights:  7.213904406255265\n",
            "Iteration#:  872 ; Loss:  3.1027302823638863 ; l2 norm of gradient:  1.1390638243169018 ; l2 norm of weights:  7.213886521708801\n",
            "Iteration#:  876 ; Loss:  3.102678123442717 ; l2 norm of gradient:  1.1390463293663606 ; l2 norm of weights:  7.213868637425181\n",
            "Iteration#:  880 ; Loss:  3.102625964943989 ; l2 norm of gradient:  1.1390288345576125 ; l2 norm of weights:  7.213850753404394\n",
            "Iteration#:  884 ; Loss:  3.10257380867471 ; l2 norm of gradient:  1.13901133989058 ; l2 norm of weights:  7.213832869646435\n",
            "Iteration#:  888 ; Loss:  3.1025216543679934 ; l2 norm of gradient:  1.138993845365185 ; l2 norm of weights:  7.213814986151296\n",
            "Iteration#:  892 ; Loss:  3.102469501169336 ; l2 norm of gradient:  1.138976350981347 ; l2 norm of weights:  7.213797102918971\n",
            "Iteration#:  896 ; Loss:  3.1024173491066063 ; l2 norm of gradient:  1.1389588567389892 ; l2 norm of weights:  7.213779219949452\n",
            "Iteration#:  900 ; Loss:  3.102365199679407 ; l2 norm of gradient:  1.1389413626380342 ; l2 norm of weights:  7.213761337242733\n",
            "Iteration#:  904 ; Loss:  3.1023130510810293 ; l2 norm of gradient:  1.138923868678402 ; l2 norm of weights:  7.213743454798806\n",
            "Iteration#:  908 ; Loss:  3.1022609043208575 ; l2 norm of gradient:  1.1389063748600157 ; l2 norm of weights:  7.213725572617662\n",
            "Iteration#:  912 ; Loss:  3.1022087593420546 ; l2 norm of gradient:  1.1388888811827957 ; l2 norm of weights:  7.213707690699298\n",
            "Iteration#:  916 ; Loss:  3.1021566155639313 ; l2 norm of gradient:  1.1388713876466638 ; l2 norm of weights:  7.213689809043703\n",
            "Iteration#:  920 ; Loss:  3.102104473688474 ; l2 norm of gradient:  1.1388538942515423 ; l2 norm of weights:  7.213671927650872\n",
            "Iteration#:  924 ; Loss:  3.1020523335922174 ; l2 norm of gradient:  1.1388364009973515 ; l2 norm of weights:  7.213654046520799\n",
            "Iteration#:  928 ; Loss:  3.1020001946837725 ; l2 norm of gradient:  1.138818907884016 ; l2 norm of weights:  7.213636165653474\n",
            "Iteration#:  932 ; Loss:  3.101948057422034 ; l2 norm of gradient:  1.1388014149114538 ; l2 norm of weights:  7.213618285048891\n",
            "Iteration#:  936 ; Loss:  3.1018959221245153 ; l2 norm of gradient:  1.1387839220795892 ; l2 norm of weights:  7.2136004047070434\n",
            "Iteration#:  940 ; Loss:  3.1018437876324017 ; l2 norm of gradient:  1.1387664293883426 ; l2 norm of weights:  7.213582524627924\n",
            "Iteration#:  944 ; Loss:  3.101791655175327 ; l2 norm of gradient:  1.1387489368376367 ; l2 norm of weights:  7.213564644811526\n",
            "Iteration#:  948 ; Loss:  3.101739524848628 ; l2 norm of gradient:  1.1387314444273924 ; l2 norm of weights:  7.213546765257841\n",
            "Iteration#:  952 ; Loss:  3.1016873954608664 ; l2 norm of gradient:  1.1387139521575316 ; l2 norm of weights:  7.213528885966863\n",
            "Iteration#:  956 ; Loss:  3.1016352676549364 ; l2 norm of gradient:  1.1386964600279759 ; l2 norm of weights:  7.213511006938585\n",
            "Iteration#:  960 ; Loss:  3.1015831418888733 ; l2 norm of gradient:  1.1386789680386458 ; l2 norm of weights:  7.213493128172999\n",
            "Iteration#:  964 ; Loss:  3.1015310173278774 ; l2 norm of gradient:  1.1386614761894664 ; l2 norm of weights:  7.213475249670099\n",
            "Iteration#:  968 ; Loss:  3.1014788944585754 ; l2 norm of gradient:  1.1386439844803558 ; l2 norm of weights:  7.213457371429876\n",
            "Iteration#:  972 ; Loss:  3.101426773529557 ; l2 norm of gradient:  1.138626492911237 ; l2 norm of weights:  7.213439493452325\n",
            "Iteration#:  976 ; Loss:  3.1013746536108546 ; l2 norm of gradient:  1.1386090014820327 ; l2 norm of weights:  7.213421615737438\n",
            "Iteration#:  980 ; Loss:  3.101322535500879 ; l2 norm of gradient:  1.1385915101926638 ; l2 norm of weights:  7.213403738285206\n",
            "Iteration#:  984 ; Loss:  3.1012704194969523 ; l2 norm of gradient:  1.1385740190430522 ; l2 norm of weights:  7.2133858610956265\n",
            "Iteration#:  988 ; Loss:  3.101218304521964 ; l2 norm of gradient:  1.1385565280331182 ; l2 norm of weights:  7.213367984168689\n",
            "Iteration#:  992 ; Loss:  3.10116619104345 ; l2 norm of gradient:  1.1385390371627855 ; l2 norm of weights:  7.213350107504386\n",
            "Iteration#:  996 ; Loss:  3.1011140798088555 ; l2 norm of gradient:  1.1385215464319747 ; l2 norm of weights:  7.2133322311027115\n",
            "Iteration#:  1000 ; Loss:  3.101061969643463 ; l2 norm of gradient:  1.1385040558406083 ; l2 norm of weights:  7.213314354963659\n",
            "Iteration#:  1004 ; Loss:  3.101009860838341 ; l2 norm of gradient:  1.1384865653886065 ; l2 norm of weights:  7.213296479087221\n",
            "Iteration#:  1008 ; Loss:  3.1009577542333004 ; l2 norm of gradient:  1.138469075075893 ; l2 norm of weights:  7.21327860347339\n",
            "Iteration#:  1012 ; Loss:  3.10090564894913 ; l2 norm of gradient:  1.1384515849023877 ; l2 norm of weights:  7.213260728122158\n",
            "Iteration#:  1016 ; Loss:  3.1008535446018697 ; l2 norm of gradient:  1.1384340948680145 ; l2 norm of weights:  7.21324285303352\n",
            "Iteration#:  1020 ; Loss:  3.1008014425048915 ; l2 norm of gradient:  1.1384166049726931 ; l2 norm of weights:  7.213224978207467\n",
            "Iteration#:  1024 ; Loss:  3.100749342822639 ; l2 norm of gradient:  1.138399115216346 ; l2 norm of weights:  7.2132071036439935\n",
            "Iteration#:  1028 ; Loss:  3.100697243274213 ; l2 norm of gradient:  1.1383816255988954 ; l2 norm of weights:  7.213189229343091\n",
            "Iteration#:  1032 ; Loss:  3.10064514583991 ; l2 norm of gradient:  1.1383641361202612 ; l2 norm of weights:  7.213171355304752\n",
            "Iteration#:  1036 ; Loss:  3.1005930510375834 ; l2 norm of gradient:  1.1383466467803682 ; l2 norm of weights:  7.213153481528973\n",
            "Iteration#:  1040 ; Loss:  3.1005409561971717 ; l2 norm of gradient:  1.138329157579136 ; l2 norm of weights:  7.213135608015742\n",
            "Iteration#:  1044 ; Loss:  3.10048886357093 ; l2 norm of gradient:  1.1383116685164871 ; l2 norm of weights:  7.213117734765055\n",
            "Iteration#:  1048 ; Loss:  3.100436772948034 ; l2 norm of gradient:  1.1382941795923418 ; l2 norm of weights:  7.213099861776902\n",
            "Iteration#:  1052 ; Loss:  3.100384683582323 ; l2 norm of gradient:  1.138276690806624 ; l2 norm of weights:  7.213081989051281\n",
            "Iteration#:  1056 ; Loss:  3.1003325952422998 ; l2 norm of gradient:  1.138259202159254 ; l2 norm of weights:  7.213064116588179\n",
            "Iteration#:  1060 ; Loss:  3.1002805098374164 ; l2 norm of gradient:  1.138241713650155 ; l2 norm of weights:  7.213046244387593\n",
            "Iteration#:  1064 ; Loss:  3.100228425161212 ; l2 norm of gradient:  1.138224225279247 ; l2 norm of weights:  7.213028372449514\n",
            "Iteration#:  1068 ; Loss:  3.100176341806472 ; l2 norm of gradient:  1.1382067370464526 ; l2 norm of weights:  7.213010500773937\n",
            "Iteration#:  1072 ; Loss:  3.100124261166492 ; l2 norm of gradient:  1.1381892489516952 ; l2 norm of weights:  7.212992629360852\n",
            "Iteration#:  1076 ; Loss:  3.1000721807838714 ; l2 norm of gradient:  1.138171760994893 ; l2 norm of weights:  7.212974758210253\n",
            "Iteration#:  1080 ; Loss:  3.1000201026140135 ; l2 norm of gradient:  1.1381542731759697 ; l2 norm of weights:  7.212956887322133\n",
            "Iteration#:  1084 ; Loss:  3.0999680265367897 ; l2 norm of gradient:  1.1381367854948483 ; l2 norm of weights:  7.212939016696486\n",
            "Iteration#:  1088 ; Loss:  3.0999159512743226 ; l2 norm of gradient:  1.1381192979514483 ; l2 norm of weights:  7.212921146333304\n",
            "Iteration#:  1092 ; Loss:  3.0998638779834353 ; l2 norm of gradient:  1.1381018105456944 ; l2 norm of weights:  7.212903276232579\n",
            "Iteration#:  1096 ; Loss:  3.0998118064549693 ; l2 norm of gradient:  1.138084323277505 ; l2 norm of weights:  7.212885406394305\n",
            "Iteration#:  1100 ; Loss:  3.0997597360521674 ; l2 norm of gradient:  1.1380668361468043 ; l2 norm of weights:  7.212867536818474\n",
            "Iteration#:  1104 ; Loss:  3.0997076668684045 ; l2 norm of gradient:  1.1380493491535129 ; l2 norm of weights:  7.21284966750508\n",
            "Iteration#:  1108 ; Loss:  3.0996556000015922 ; l2 norm of gradient:  1.138031862297554 ; l2 norm of weights:  7.212831798454116\n",
            "Iteration#:  1112 ; Loss:  3.0996035348408735 ; l2 norm of gradient:  1.1380143755788468 ; l2 norm of weights:  7.212813929665573\n",
            "Iteration#:  1116 ; Loss:  3.099551470893038 ; l2 norm of gradient:  1.1379968889973155 ; l2 norm of weights:  7.2127960611394455\n",
            "Iteration#:  1120 ; Loss:  3.0994994082059533 ; l2 norm of gradient:  1.1379794025528818 ; l2 norm of weights:  7.212778192875726\n",
            "Iteration#:  1124 ; Loss:  3.0994473485264304 ; l2 norm of gradient:  1.137961916245466 ; l2 norm of weights:  7.2127603248744085\n",
            "Iteration#:  1128 ; Loss:  3.0993952885721563 ; l2 norm of gradient:  1.1379444300749906 ; l2 norm of weights:  7.212742457135485\n",
            "Iteration#:  1132 ; Loss:  3.099343231101433 ; l2 norm of gradient:  1.1379269440413793 ; l2 norm of weights:  7.212724589658945\n",
            "Iteration#:  1136 ; Loss:  3.099291175699988 ; l2 norm of gradient:  1.1379094581445512 ; l2 norm of weights:  7.212706722444787\n",
            "Iteration#:  1140 ; Loss:  3.0992391214221655 ; l2 norm of gradient:  1.1378919723844292 ; l2 norm of weights:  7.212688855493002\n",
            "Iteration#:  1144 ; Loss:  3.099187068208747 ; l2 norm of gradient:  1.137874486760935 ; l2 norm of weights:  7.2126709888035805\n",
            "Iteration#:  1148 ; Loss:  3.099135018033728 ; l2 norm of gradient:  1.1378570012739908 ; l2 norm of weights:  7.212653122376519\n",
            "Iteration#:  1152 ; Loss:  3.099082967820473 ; l2 norm of gradient:  1.1378395159235186 ; l2 norm of weights:  7.212635256211807\n",
            "Iteration#:  1156 ; Loss:  3.0990309200374635 ; l2 norm of gradient:  1.1378220307094395 ; l2 norm of weights:  7.212617390309441\n",
            "Iteration#:  1160 ; Loss:  3.0989788733210406 ; l2 norm of gradient:  1.1378045456316757 ; l2 norm of weights:  7.212599524669411\n",
            "Iteration#:  1164 ; Loss:  3.098926829122258 ; l2 norm of gradient:  1.1377870606901488 ; l2 norm of weights:  7.212581659291709\n",
            "Iteration#:  1168 ; Loss:  3.098874785578368 ; l2 norm of gradient:  1.137769575884782 ; l2 norm of weights:  7.212563794176332\n",
            "Iteration#:  1172 ; Loss:  3.098822744294904 ; l2 norm of gradient:  1.137752091215496 ; l2 norm of weights:  7.21254592932327\n",
            "Iteration#:  1176 ; Loss:  3.0987707046193242 ; l2 norm of gradient:  1.1377346066822114 ; l2 norm of weights:  7.212528064732517\n",
            "Iteration#:  1180 ; Loss:  3.0987186663053046 ; l2 norm of gradient:  1.1377171222848528 ; l2 norm of weights:  7.212510200404064\n",
            "Iteration#:  1184 ; Loss:  3.0986666290887315 ; l2 norm of gradient:  1.137699638023341 ; l2 norm of weights:  7.212492336337906\n",
            "Iteration#:  1188 ; Loss:  3.0986145946306447 ; l2 norm of gradient:  1.1376821538975967 ; l2 norm of weights:  7.212474472534036\n",
            "Iteration#:  1192 ; Loss:  3.0985625604938463 ; l2 norm of gradient:  1.137664669907543 ; l2 norm of weights:  7.212456608992445\n",
            "Iteration#:  1196 ; Loss:  3.0985105285775885 ; l2 norm of gradient:  1.1376471860531023 ; l2 norm of weights:  7.212438745713127\n",
            "Iteration#:  1200 ; Loss:  3.0984584985726666 ; l2 norm of gradient:  1.1376297023341946 ; l2 norm of weights:  7.212420882696074\n",
            "Iteration#:  1204 ; Loss:  3.098406469311175 ; l2 norm of gradient:  1.1376122187507431 ; l2 norm of weights:  7.212403019941282\n",
            "Iteration#:  1208 ; Loss:  3.0983544421860083 ; l2 norm of gradient:  1.1375947353026703 ; l2 norm of weights:  7.21238515744874\n",
            "Iteration#:  1212 ; Loss:  3.098302416269206 ; l2 norm of gradient:  1.1375772519898963 ; l2 norm of weights:  7.212367295218443\n",
            "Iteration#:  1216 ; Loss:  3.098250392828702 ; l2 norm of gradient:  1.1375597688123458 ; l2 norm of weights:  7.212349433250384\n",
            "Iteration#:  1220 ; Loss:  3.098198369889764 ; l2 norm of gradient:  1.1375422857699375 ; l2 norm of weights:  7.212331571544555\n",
            "Iteration#:  1224 ; Loss:  3.098146349195577 ; l2 norm of gradient:  1.1375248028625946 ; l2 norm of weights:  7.212313710100949\n",
            "Iteration#:  1228 ; Loss:  3.0980943303599537 ; l2 norm of gradient:  1.1375073200902404 ; l2 norm of weights:  7.212295848919558\n",
            "Iteration#:  1232 ; Loss:  3.098042312213294 ; l2 norm of gradient:  1.1374898374527944 ; l2 norm of weights:  7.212277988000377\n",
            "Iteration#:  1236 ; Loss:  3.0979902961908015 ; l2 norm of gradient:  1.1374723549501802 ; l2 norm of weights:  7.212260127343399\n",
            "Iteration#:  1240 ; Loss:  3.097938282079875 ; l2 norm of gradient:  1.137454872582319 ; l2 norm of weights:  7.212242266948613\n",
            "Iteration#:  1244 ; Loss:  3.0978862692246523 ; l2 norm of gradient:  1.1374373903491337 ; l2 norm of weights:  7.212224406816017\n",
            "Iteration#:  1248 ; Loss:  3.0978342575788673 ; l2 norm of gradient:  1.1374199082505456 ; l2 norm of weights:  7.2122065469456\n",
            "Iteration#:  1252 ; Loss:  3.0977822479766863 ; l2 norm of gradient:  1.1374024262864764 ; l2 norm of weights:  7.212188687337357\n",
            "Iteration#:  1256 ; Loss:  3.0977302403583367 ; l2 norm of gradient:  1.1373849444568476 ; l2 norm of weights:  7.212170827991281\n",
            "Iteration#:  1260 ; Loss:  3.0976782334111967 ; l2 norm of gradient:  1.1373674627615828 ; l2 norm of weights:  7.212152968907364\n",
            "Iteration#:  1264 ; Loss:  3.097626228397483 ; l2 norm of gradient:  1.1373499812006023 ; l2 norm of weights:  7.212135110085599\n",
            "Iteration#:  1268 ; Loss:  3.097574225655355 ; l2 norm of gradient:  1.1373324997738288 ; l2 norm of weights:  7.212117251525979\n",
            "Iteration#:  1272 ; Loss:  3.0975222237255844 ; l2 norm of gradient:  1.137315018481185 ; l2 norm of weights:  7.212099393228497\n",
            "Iteration#:  1276 ; Loss:  3.097470223364646 ; l2 norm of gradient:  1.1372975373225913 ; l2 norm of weights:  7.212081535193145\n",
            "Iteration#:  1280 ; Loss:  3.0974182250418787 ; l2 norm of gradient:  1.137280056297971 ; l2 norm of weights:  7.212063677419916\n",
            "Iteration#:  1284 ; Loss:  3.0973662284318264 ; l2 norm of gradient:  1.1372625754072456 ; l2 norm of weights:  7.2120458199088056\n",
            "Iteration#:  1288 ; Loss:  3.0973142324907887 ; l2 norm of gradient:  1.1372450946503367 ; l2 norm of weights:  7.212027962659804\n",
            "Iteration#:  1292 ; Loss:  3.097262238370245 ; l2 norm of gradient:  1.1372276140271669 ; l2 norm of weights:  7.2120101056729045\n",
            "Iteration#:  1296 ; Loss:  3.0972102468815956 ; l2 norm of gradient:  1.1372101335376585 ; l2 norm of weights:  7.211992248948101\n",
            "Iteration#:  1300 ; Loss:  3.09715825588708 ; l2 norm of gradient:  1.137192653181732 ; l2 norm of weights:  7.211974392485385\n",
            "Iteration#:  1304 ; Loss:  3.0971062666167737 ; l2 norm of gradient:  1.1371751729593107 ; l2 norm of weights:  7.211956536284751\n",
            "Iteration#:  1308 ; Loss:  3.0970542800139684 ; l2 norm of gradient:  1.1371576928703169 ; l2 norm of weights:  7.211938680346188\n",
            "Iteration#:  1312 ; Loss:  3.0970022938180124 ; l2 norm of gradient:  1.137140212914672 ; l2 norm of weights:  7.211920824669694\n",
            "Iteration#:  1316 ; Loss:  3.0969503096720383 ; l2 norm of gradient:  1.1371227330922975 ; l2 norm of weights:  7.21190296925526\n",
            "Iteration#:  1320 ; Loss:  3.096898327681438 ; l2 norm of gradient:  1.137105253403116 ; l2 norm of weights:  7.211885114102879\n",
            "Iteration#:  1324 ; Loss:  3.096846346287455 ; l2 norm of gradient:  1.1370877738470486 ; l2 norm of weights:  7.211867259212543\n",
            "Iteration#:  1328 ; Loss:  3.0967943667385955 ; l2 norm of gradient:  1.1370702944240199 ; l2 norm of weights:  7.211849404584245\n",
            "Iteration#:  1332 ; Loss:  3.096742388653095 ; l2 norm of gradient:  1.1370528151339494 ; l2 norm of weights:  7.211831550217979\n",
            "Iteration#:  1336 ; Loss:  3.0966904126438903 ; l2 norm of gradient:  1.1370353359767595 ; l2 norm of weights:  7.211813696113737\n",
            "Iteration#:  1340 ; Loss:  3.09663843822052 ; l2 norm of gradient:  1.137017856952374 ; l2 norm of weights:  7.211795842271512\n",
            "Iteration#:  1344 ; Loss:  3.096586464679712 ; l2 norm of gradient:  1.1370003780607125 ; l2 norm of weights:  7.211777988691296\n",
            "Iteration#:  1348 ; Loss:  3.096534493604199 ; l2 norm of gradient:  1.1369828993016997 ; l2 norm of weights:  7.211760135373084\n",
            "Iteration#:  1352 ; Loss:  3.096482523499872 ; l2 norm of gradient:  1.136965420675255 ; l2 norm of weights:  7.211742282316868\n",
            "Iteration#:  1356 ; Loss:  3.0964305551306435 ; l2 norm of gradient:  1.136947942181302 ; l2 norm of weights:  7.21172442952264\n",
            "Iteration#:  1360 ; Loss:  3.096378588060693 ; l2 norm of gradient:  1.1369304638197626 ; l2 norm of weights:  7.211706576990394\n",
            "Iteration#:  1364 ; Loss:  3.0963266234108726 ; l2 norm of gradient:  1.1369129855905593 ; l2 norm of weights:  7.211688724720122\n",
            "Iteration#:  1368 ; Loss:  3.0962746600139046 ; l2 norm of gradient:  1.1368955074936133 ; l2 norm of weights:  7.211670872711818\n",
            "Iteration#:  1372 ; Loss:  3.0962226976330363 ; l2 norm of gradient:  1.1368780295288468 ; l2 norm of weights:  7.211653020965474\n",
            "Iteration#:  1376 ; Loss:  3.0961707377907306 ; l2 norm of gradient:  1.1368605516961812 ; l2 norm of weights:  7.211635169481083\n",
            "Iteration#:  1380 ; Loss:  3.0961187791484264 ; l2 norm of gradient:  1.1368430739955415 ; l2 norm of weights:  7.211617318258639\n",
            "Iteration#:  1384 ; Loss:  3.0960668214780136 ; l2 norm of gradient:  1.1368255964268472 ; l2 norm of weights:  7.211599467298132\n",
            "Iteration#:  1388 ; Loss:  3.0960148655217656 ; l2 norm of gradient:  1.1368081189900214 ; l2 norm of weights:  7.211581616599559\n",
            "Iteration#:  1392 ; Loss:  3.095962912225369 ; l2 norm of gradient:  1.1367906416849844 ; l2 norm of weights:  7.211563766162909\n",
            "Iteration#:  1396 ; Loss:  3.0959109597410923 ; l2 norm of gradient:  1.1367731645116603 ; l2 norm of weights:  7.211545915988178\n",
            "Iteration#:  1400 ; Loss:  3.0958590088425018 ; l2 norm of gradient:  1.1367556874699722 ; l2 norm of weights:  7.211528066075356\n",
            "Iteration#:  1404 ; Loss:  3.0958070599522864 ; l2 norm of gradient:  1.1367382105598394 ; l2 norm of weights:  7.211510216424439\n",
            "Iteration#:  1408 ; Loss:  3.095755111837815 ; l2 norm of gradient:  1.1367207337811858 ; l2 norm of weights:  7.211492367035416\n",
            "Iteration#:  1412 ; Loss:  3.095703166147839 ; l2 norm of gradient:  1.1367032571339333 ; l2 norm of weights:  7.211474517908285\n",
            "Iteration#:  1416 ; Loss:  3.0956512212852014 ; l2 norm of gradient:  1.1366857806180035 ; l2 norm of weights:  7.211456669043035\n",
            "Iteration#:  1420 ; Loss:  3.095599278826761 ; l2 norm of gradient:  1.1366683042333185 ; l2 norm of weights:  7.21143882043966\n",
            "Iteration#:  1424 ; Loss:  3.0955473372717184 ; l2 norm of gradient:  1.1366508279798022 ; l2 norm of weights:  7.211420972098153\n",
            "Iteration#:  1428 ; Loss:  3.0954953977960935 ; l2 norm of gradient:  1.1366333518573752 ; l2 norm of weights:  7.211403124018507\n",
            "Iteration#:  1432 ; Loss:  3.0954434602039878 ; l2 norm of gradient:  1.1366158758659588 ; l2 norm of weights:  7.211385276200714\n",
            "Iteration#:  1436 ; Loss:  3.0953915234479203 ; l2 norm of gradient:  1.1365984000054767 ; l2 norm of weights:  7.211367428644768\n",
            "Iteration#:  1440 ; Loss:  3.0953395882412016 ; l2 norm of gradient:  1.1365809242758511 ; l2 norm of weights:  7.211349581350661\n",
            "Iteration#:  1444 ; Loss:  3.095287655153735 ; l2 norm of gradient:  1.136563448677004 ; l2 norm of weights:  7.211331734318387\n",
            "Iteration#:  1448 ; Loss:  3.0952357237082575 ; l2 norm of gradient:  1.1365459732088563 ; l2 norm of weights:  7.211313887547938\n",
            "Iteration#:  1452 ; Loss:  3.0951837933586654 ; l2 norm of gradient:  1.1365284978713324 ; l2 norm of weights:  7.211296041039307\n",
            "Iteration#:  1456 ; Loss:  3.0951318645339843 ; l2 norm of gradient:  1.1365110226643524 ; l2 norm of weights:  7.211278194792487\n",
            "Iteration#:  1460 ; Loss:  3.0950799383770287 ; l2 norm of gradient:  1.1364935475878402 ; l2 norm of weights:  7.211260348807472\n",
            "Iteration#:  1464 ; Loss:  3.095028012623543 ; l2 norm of gradient:  1.136476072641716 ; l2 norm of weights:  7.211242503084253\n",
            "Iteration#:  1468 ; Loss:  3.0949760887816433 ; l2 norm of gradient:  1.1364585978259045 ; l2 norm of weights:  7.211224657622823\n",
            "Iteration#:  1472 ; Loss:  3.094924166092854 ; l2 norm of gradient:  1.1364411231403269 ; l2 norm of weights:  7.211206812423177\n",
            "Iteration#:  1476 ; Loss:  3.094872246001605 ; l2 norm of gradient:  1.1364236485849046 ; l2 norm of weights:  7.211188967485307\n",
            "Iteration#:  1480 ; Loss:  3.094820327436893 ; l2 norm of gradient:  1.1364061741595604 ; l2 norm of weights:  7.211171122809204\n",
            "Iteration#:  1484 ; Loss:  3.094768409619283 ; l2 norm of gradient:  1.1363886998642174 ; l2 norm of weights:  7.211153278394863\n",
            "Iteration#:  1488 ; Loss:  3.0947164943218373 ; l2 norm of gradient:  1.136371225698797 ; l2 norm of weights:  7.211135434242277\n",
            "Iteration#:  1492 ; Loss:  3.0946645797548165 ; l2 norm of gradient:  1.1363537516632207 ; l2 norm of weights:  7.211117590351438\n",
            "Iteration#:  1496 ; Loss:  3.094612666911421 ; l2 norm of gradient:  1.1363362777574113 ; l2 norm of weights:  7.211099746722338\n",
            "Iteration#:  1500 ; Loss:  3.0945607559975663 ; l2 norm of gradient:  1.1363188039812926 ; l2 norm of weights:  7.211081903354971\n",
            "Iteration#:  1504 ; Loss:  3.094508846978618 ; l2 norm of gradient:  1.136301330334785 ; l2 norm of weights:  7.211064060249329\n",
            "Iteration#:  1508 ; Loss:  3.0944569387626166 ; l2 norm of gradient:  1.1362838568178113 ; l2 norm of weights:  7.211046217405408\n",
            "Iteration#:  1512 ; Loss:  3.094405032464973 ; l2 norm of gradient:  1.136266383430294 ; l2 norm of weights:  7.211028374823196\n",
            "Iteration#:  1516 ; Loss:  3.094353127285758 ; l2 norm of gradient:  1.1362489101721551 ; l2 norm of weights:  7.211010532502692\n",
            "Iteration#:  1520 ; Loss:  3.094301224778786 ; l2 norm of gradient:  1.1362314370433166 ; l2 norm of weights:  7.210992690443882\n",
            "Iteration#:  1524 ; Loss:  3.0942493232532877 ; l2 norm of gradient:  1.136213964043702 ; l2 norm of weights:  7.210974848646765\n",
            "Iteration#:  1528 ; Loss:  3.0941974232035037 ; l2 norm of gradient:  1.1361964911732327 ; l2 norm of weights:  7.21095700711133\n",
            "Iteration#:  1532 ; Loss:  3.094145525479207 ; l2 norm of gradient:  1.1361790184318314 ; l2 norm of weights:  7.21093916583757\n",
            "Iteration#:  1536 ; Loss:  3.0940936283914167 ; l2 norm of gradient:  1.1361615458194199 ; l2 norm of weights:  7.210921324825479\n",
            "Iteration#:  1540 ; Loss:  3.0940417331482157 ; l2 norm of gradient:  1.1361440733359198 ; l2 norm of weights:  7.210903484075051\n",
            "Iteration#:  1544 ; Loss:  3.0939898393025627 ; l2 norm of gradient:  1.1361266009812556 ; l2 norm of weights:  7.210885643586279\n",
            "Iteration#:  1548 ; Loss:  3.0939379478710722 ; l2 norm of gradient:  1.136109128755348 ; l2 norm of weights:  7.210867803359153\n",
            "Iteration#:  1552 ; Loss:  3.093886057630886 ; l2 norm of gradient:  1.1360916566581203 ; l2 norm of weights:  7.2108499633936685\n",
            "Iteration#:  1556 ; Loss:  3.0938341686605715 ; l2 norm of gradient:  1.1360741846894937 ; l2 norm of weights:  7.210832123689817\n",
            "Iteration#:  1560 ; Loss:  3.0937822812447138 ; l2 norm of gradient:  1.1360567128493912 ; l2 norm of weights:  7.210814284247592\n",
            "Iteration#:  1564 ; Loss:  3.0937303957810545 ; l2 norm of gradient:  1.1360392411377358 ; l2 norm of weights:  7.210796445066985\n",
            "Iteration#:  1568 ; Loss:  3.0936785115959085 ; l2 norm of gradient:  1.1360217695544492 ; l2 norm of weights:  7.210778606147992\n",
            "Iteration#:  1572 ; Loss:  3.0936266290404575 ; l2 norm of gradient:  1.1360042980994527 ; l2 norm of weights:  7.210760767490603\n",
            "Iteration#:  1576 ; Loss:  3.0935747486866223 ; l2 norm of gradient:  1.135986826772671 ; l2 norm of weights:  7.210742929094812\n",
            "Iteration#:  1580 ; Loss:  3.0935228691138934 ; l2 norm of gradient:  1.1359693555740245 ; l2 norm of weights:  7.210725090960612\n",
            "Iteration#:  1584 ; Loss:  3.093470991429935 ; l2 norm of gradient:  1.1359518845034366 ; l2 norm of weights:  7.210707253087998\n",
            "Iteration#:  1588 ; Loss:  3.0934191154873547 ; l2 norm of gradient:  1.1359344135608296 ; l2 norm of weights:  7.210689415476958\n",
            "Iteration#:  1592 ; Loss:  3.093367241378705 ; l2 norm of gradient:  1.1359169427461255 ; l2 norm of weights:  7.210671578127489\n",
            "Iteration#:  1596 ; Loss:  3.0933153684777746 ; l2 norm of gradient:  1.1358994720592461 ; l2 norm of weights:  7.2106537410395815\n",
            "Iteration#:  1600 ; Loss:  3.093263496886396 ; l2 norm of gradient:  1.1358820015001159 ; l2 norm of weights:  7.210635904213231\n",
            "Iteration#:  1604 ; Loss:  3.0932116269498486 ; l2 norm of gradient:  1.1358645310686557 ; l2 norm of weights:  7.2106180676484275\n",
            "Iteration#:  1608 ; Loss:  3.0931597592417237 ; l2 norm of gradient:  1.1358470607647875 ; l2 norm of weights:  7.210600231345165\n",
            "Iteration#:  1612 ; Loss:  3.093107892378743 ; l2 norm of gradient:  1.135829590588436 ; l2 norm of weights:  7.210582395303437\n",
            "Iteration#:  1616 ; Loss:  3.093056027366089 ; l2 norm of gradient:  1.1358121205395215 ; l2 norm of weights:  7.210564559523236\n",
            "Iteration#:  1620 ; Loss:  3.0930041637345385 ; l2 norm of gradient:  1.1357946506179672 ; l2 norm of weights:  7.210546724004554\n",
            "Iteration#:  1624 ; Loss:  3.092952302520108 ; l2 norm of gradient:  1.1357771808236952 ; l2 norm of weights:  7.210528888747386\n",
            "Iteration#:  1628 ; Loss:  3.092900442307813 ; l2 norm of gradient:  1.1357597111566282 ; l2 norm of weights:  7.2105110537517225\n",
            "Iteration#:  1632 ; Loss:  3.092848583266943 ; l2 norm of gradient:  1.135742241616689 ; l2 norm of weights:  7.210493219017559\n",
            "Iteration#:  1636 ; Loss:  3.0927967269118493 ; l2 norm of gradient:  1.1357247722037986 ; l2 norm of weights:  7.210475384544886\n",
            "Iteration#:  1640 ; Loss:  3.0927448713879673 ; l2 norm of gradient:  1.1357073029178821 ; l2 norm of weights:  7.210457550333698\n",
            "Iteration#:  1644 ; Loss:  3.092693017560151 ; l2 norm of gradient:  1.1356898337588597 ; l2 norm of weights:  7.210439716383986\n",
            "Iteration#:  1648 ; Loss:  3.0926411650004826 ; l2 norm of gradient:  1.135672364726656 ; l2 norm of weights:  7.210421882695745\n",
            "Iteration#:  1652 ; Loss:  3.0925893151240356 ; l2 norm of gradient:  1.1356548958211912 ; l2 norm of weights:  7.210404049268967\n",
            "Iteration#:  1656 ; Loss:  3.0925374657302864 ; l2 norm of gradient:  1.1356374270423892 ; l2 norm of weights:  7.210386216103644\n",
            "Iteration#:  1660 ; Loss:  3.0924856180453126 ; l2 norm of gradient:  1.1356199583901714 ; l2 norm of weights:  7.210368383199772\n",
            "Iteration#:  1664 ; Loss:  3.092433772161696 ; l2 norm of gradient:  1.1356024898644619 ; l2 norm of weights:  7.210350550557341\n",
            "Iteration#:  1668 ; Loss:  3.092381928418333 ; l2 norm of gradient:  1.1355850214651815 ; l2 norm of weights:  7.210332718176343\n",
            "Iteration#:  1672 ; Loss:  3.0923300855898326 ; l2 norm of gradient:  1.1355675531922536 ; l2 norm of weights:  7.210314886056774\n",
            "Iteration#:  1676 ; Loss:  3.0922782441225465 ; l2 norm of gradient:  1.1355500850456017 ; l2 norm of weights:  7.210297054198625\n",
            "Iteration#:  1680 ; Loss:  3.0922264044446663 ; l2 norm of gradient:  1.1355326170251463 ; l2 norm of weights:  7.210279222601891\n",
            "Iteration#:  1684 ; Loss:  3.0921745671919534 ; l2 norm of gradient:  1.1355151491308124 ; l2 norm of weights:  7.210261391266561\n",
            "Iteration#:  1688 ; Loss:  3.092122730323807 ; l2 norm of gradient:  1.135497681362521 ; l2 norm of weights:  7.210243560192631\n",
            "Iteration#:  1692 ; Loss:  3.0920708955155365 ; l2 norm of gradient:  1.1354802137201938 ; l2 norm of weights:  7.2102257293800935\n",
            "Iteration#:  1696 ; Loss:  3.092019062326881 ; l2 norm of gradient:  1.1354627462037548 ; l2 norm of weights:  7.21020789882894\n",
            "Iteration#:  1700 ; Loss:  3.091967231097927 ; l2 norm of gradient:  1.1354452788131257 ; l2 norm of weights:  7.210190068539165\n",
            "Iteration#:  1704 ; Loss:  3.09191540107346 ; l2 norm of gradient:  1.1354278115482317 ; l2 norm of weights:  7.2101722385107605\n",
            "Iteration#:  1708 ; Loss:  3.091863572239895 ; l2 norm of gradient:  1.1354103444089922 ; l2 norm of weights:  7.210154408743721\n",
            "Iteration#:  1712 ; Loss:  3.091811746139317 ; l2 norm of gradient:  1.1353928773953303 ; l2 norm of weights:  7.2101365792380365\n",
            "Iteration#:  1716 ; Loss:  3.0917599207719713 ; l2 norm of gradient:  1.1353754105071692 ; l2 norm of weights:  7.210118749993702\n",
            "Iteration#:  1720 ; Loss:  3.091708096722189 ; l2 norm of gradient:  1.1353579437444323 ; l2 norm of weights:  7.210100921010709\n",
            "Iteration#:  1724 ; Loss:  3.0916562746992793 ; l2 norm of gradient:  1.1353404771070412 ; l2 norm of weights:  7.210083092289053\n",
            "Iteration#:  1728 ; Loss:  3.0916044549804043 ; l2 norm of gradient:  1.1353230105949181 ; l2 norm of weights:  7.2100652638287235\n",
            "Iteration#:  1732 ; Loss:  3.091552635794902 ; l2 norm of gradient:  1.135305544207987 ; l2 norm of weights:  7.210047435629716\n",
            "Iteration#:  1736 ; Loss:  3.0915008183463986 ; l2 norm of gradient:  1.13528807794617 ; l2 norm of weights:  7.210029607692022\n",
            "Iteration#:  1740 ; Loss:  3.0914490023955628 ; l2 norm of gradient:  1.1352706118093898 ; l2 norm of weights:  7.210011780015637\n",
            "Iteration#:  1744 ; Loss:  3.0913971889093963 ; l2 norm of gradient:  1.1352531457975672 ; l2 norm of weights:  7.209993952600548\n",
            "Iteration#:  1748 ; Loss:  3.091345375585286 ; l2 norm of gradient:  1.1352356799106287 ; l2 norm of weights:  7.209976125446754\n",
            "Iteration#:  1752 ; Loss:  3.091293564717337 ; l2 norm of gradient:  1.1352182141484932 ; l2 norm of weights:  7.209958298554245\n",
            "Iteration#:  1756 ; Loss:  3.0912417552473794 ; l2 norm of gradient:  1.1352007485110858 ; l2 norm of weights:  7.209940471923014\n",
            "Iteration#:  1760 ; Loss:  3.091189947782633 ; l2 norm of gradient:  1.1351832829983277 ; l2 norm of weights:  7.209922645553055\n",
            "Iteration#:  1764 ; Loss:  3.091138141558527 ; l2 norm of gradient:  1.1351658176101438 ; l2 norm of weights:  7.20990481944436\n",
            "Iteration#:  1768 ; Loss:  3.0910863369926824 ; l2 norm of gradient:  1.135148352346453 ; l2 norm of weights:  7.209886993596922\n",
            "Iteration#:  1772 ; Loss:  3.09103453372256 ; l2 norm of gradient:  1.1351308872071806 ; l2 norm of weights:  7.209869168010735\n",
            "Iteration#:  1776 ; Loss:  3.090982732854619 ; l2 norm of gradient:  1.135113422192251 ; l2 norm of weights:  7.2098513426857895\n",
            "Iteration#:  1780 ; Loss:  3.0909309331544526 ; l2 norm of gradient:  1.135095957301583 ; l2 norm of weights:  7.209833517622081\n",
            "Iteration#:  1784 ; Loss:  3.0908791345690125 ; l2 norm of gradient:  1.1350784925351012 ; l2 norm of weights:  7.209815692819601\n",
            "Iteration#:  1788 ; Loss:  3.0908273377036255 ; l2 norm of gradient:  1.1350610278927298 ; l2 norm of weights:  7.209797868278343\n",
            "Iteration#:  1792 ; Loss:  3.0907755433785242 ; l2 norm of gradient:  1.1350435633743894 ; l2 norm of weights:  7.209780043998299\n",
            "Iteration#:  1796 ; Loss:  3.0907237496448476 ; l2 norm of gradient:  1.135026098980002 ; l2 norm of weights:  7.209762219979463\n",
            "Iteration#:  1800 ; Loss:  3.0906719576294526 ; l2 norm of gradient:  1.1350086347094928 ; l2 norm of weights:  7.209744396221826\n",
            "Iteration#:  1804 ; Loss:  3.0906201667895203 ; l2 norm of gradient:  1.1349911705627849 ; l2 norm of weights:  7.2097265727253825\n",
            "Iteration#:  1808 ; Loss:  3.0905683787995164 ; l2 norm of gradient:  1.134973706539799 ; l2 norm of weights:  7.209708749490128\n",
            "Iteration#:  1812 ; Loss:  3.0905165914663164 ; l2 norm of gradient:  1.134956242640458 ; l2 norm of weights:  7.20969092651605\n",
            "Iteration#:  1816 ; Loss:  3.0904648057396096 ; l2 norm of gradient:  1.1349387788646854 ; l2 norm of weights:  7.209673103803143\n",
            "Iteration#:  1820 ; Loss:  3.09041302164517 ; l2 norm of gradient:  1.1349213152124042 ; l2 norm of weights:  7.209655281351403\n",
            "Iteration#:  1824 ; Loss:  3.090361239866674 ; l2 norm of gradient:  1.1349038516835366 ; l2 norm of weights:  7.20963745916082\n",
            "Iteration#:  1828 ; Loss:  3.090309459081405 ; l2 norm of gradient:  1.1348863882780063 ; l2 norm of weights:  7.2096196372313885\n",
            "Iteration#:  1832 ; Loss:  3.0902576796374785 ; l2 norm of gradient:  1.1348689249957356 ; l2 norm of weights:  7.2096018155631\n",
            "Iteration#:  1836 ; Loss:  3.090205901608388 ; l2 norm of gradient:  1.1348514618366465 ; l2 norm of weights:  7.209583994155948\n",
            "Iteration#:  1840 ; Loss:  3.0901541263060484 ; l2 norm of gradient:  1.134833998800663 ; l2 norm of weights:  7.209566173009926\n",
            "Iteration#:  1844 ; Loss:  3.090102351649697 ; l2 norm of gradient:  1.1348165358877071 ; l2 norm of weights:  7.209548352125026\n",
            "Iteration#:  1848 ; Loss:  3.0900505786251657 ; l2 norm of gradient:  1.134799073097703 ; l2 norm of weights:  7.20953053150124\n",
            "Iteration#:  1852 ; Loss:  3.0899988074060314 ; l2 norm of gradient:  1.134781610430571 ; l2 norm of weights:  7.209512711138562\n",
            "Iteration#:  1856 ; Loss:  3.089947038406578 ; l2 norm of gradient:  1.1347641478862371 ; l2 norm of weights:  7.209494891036988\n",
            "Iteration#:  1860 ; Loss:  3.0898952702243543 ; l2 norm of gradient:  1.1347466854646224 ; l2 norm of weights:  7.209477071196505\n",
            "Iteration#:  1864 ; Loss:  3.0898435029863642 ; l2 norm of gradient:  1.13472922316565 ; l2 norm of weights:  7.20945925161711\n",
            "Iteration#:  1868 ; Loss:  3.089791737967853 ; l2 norm of gradient:  1.134711760989243 ; l2 norm of weights:  7.209441432298795\n",
            "Iteration#:  1872 ; Loss:  3.0897399745904517 ; l2 norm of gradient:  1.1346942989353244 ; l2 norm of weights:  7.209423613241552\n",
            "Iteration#:  1876 ; Loss:  3.0896882136094104 ; l2 norm of gradient:  1.1346768370038165 ; l2 norm of weights:  7.209405794445375\n",
            "Iteration#:  1880 ; Loss:  3.0896364530768254 ; l2 norm of gradient:  1.134659375194642 ; l2 norm of weights:  7.209387975910256\n",
            "Iteration#:  1884 ; Loss:  3.0895846945399272 ; l2 norm of gradient:  1.134641913507726 ; l2 norm of weights:  7.209370157636187\n",
            "Iteration#:  1888 ; Loss:  3.0895329371963403 ; l2 norm of gradient:  1.1346244519429876 ; l2 norm of weights:  7.209352339623163\n",
            "Iteration#:  1892 ; Loss:  3.089481182489224 ; l2 norm of gradient:  1.1346069905003542 ; l2 norm of weights:  7.209334521871178\n",
            "Iteration#:  1896 ; Loss:  3.0894294282998342 ; l2 norm of gradient:  1.1345895291797443 ; l2 norm of weights:  7.209316704380221\n",
            "Iteration#:  1900 ; Loss:  3.089377676235821 ; l2 norm of gradient:  1.134572067981084 ; l2 norm of weights:  7.209298887150287\n",
            "Iteration#:  1904 ; Loss:  3.089325925333868 ; l2 norm of gradient:  1.1345546069042958 ; l2 norm of weights:  7.20928107018137\n",
            "Iteration#:  1908 ; Loss:  3.089274177024714 ; l2 norm of gradient:  1.1345371459493017 ; l2 norm of weights:  7.20926325347346\n",
            "Iteration#:  1912 ; Loss:  3.089222428889326 ; l2 norm of gradient:  1.1345196851160253 ; l2 norm of weights:  7.209245437026554\n",
            "Iteration#:  1916 ; Loss:  3.0891706830343924 ; l2 norm of gradient:  1.1345022244043894 ; l2 norm of weights:  7.2092276208406405\n",
            "Iteration#:  1920 ; Loss:  3.08911893861117 ; l2 norm of gradient:  1.1344847638143176 ; l2 norm of weights:  7.209209804915716\n",
            "Iteration#:  1924 ; Loss:  3.0890671967038563 ; l2 norm of gradient:  1.1344673033457309 ; l2 norm of weights:  7.209191989251771\n",
            "Iteration#:  1928 ; Loss:  3.089015455523541 ; l2 norm of gradient:  1.134449842998555 ; l2 norm of weights:  7.209174173848799\n",
            "Iteration#:  1932 ; Loss:  3.0889637154734926 ; l2 norm of gradient:  1.134432382772711 ; l2 norm of weights:  7.209156358706792\n",
            "Iteration#:  1936 ; Loss:  3.088911977624208 ; l2 norm of gradient:  1.134414922668123 ; l2 norm of weights:  7.209138543825747\n",
            "Iteration#:  1940 ; Loss:  3.0888602418153317 ; l2 norm of gradient:  1.1343974626847129 ; l2 norm of weights:  7.209120729205653\n",
            "Iteration#:  1944 ; Loss:  3.0888085069729625 ; l2 norm of gradient:  1.1343800028224054 ; l2 norm of weights:  7.209102914846502\n",
            "Iteration#:  1948 ; Loss:  3.0887567731892553 ; l2 norm of gradient:  1.1343625430811215 ; l2 norm of weights:  7.2090851007482915\n",
            "Iteration#:  1952 ; Loss:  3.0887050415731987 ; l2 norm of gradient:  1.1343450834607867 ; l2 norm of weights:  7.209067286911009\n",
            "Iteration#:  1956 ; Loss:  3.0886533114428083 ; l2 norm of gradient:  1.1343276239613214 ; l2 norm of weights:  7.209049473334652\n",
            "Iteration#:  1960 ; Loss:  3.088601583856784 ; l2 norm of gradient:  1.1343101645826505 ; l2 norm of weights:  7.209031660019211\n",
            "Iteration#:  1964 ; Loss:  3.0885498564093017 ; l2 norm of gradient:  1.134292705324697 ; l2 norm of weights:  7.209013846964679\n",
            "Iteration#:  1968 ; Loss:  3.0884981311255197 ; l2 norm of gradient:  1.1342752461873826 ; l2 norm of weights:  7.208996034171049\n",
            "Iteration#:  1972 ; Loss:  3.088446407341965 ; l2 norm of gradient:  1.1342577871706325 ; l2 norm of weights:  7.208978221638315\n",
            "Iteration#:  1976 ; Loss:  3.0883946860151754 ; l2 norm of gradient:  1.1342403282743685 ; l2 norm of weights:  7.208960409366469\n",
            "Iteration#:  1980 ; Loss:  3.088342965024428 ; l2 norm of gradient:  1.1342228694985126 ; l2 norm of weights:  7.208942597355504\n",
            "Iteration#:  1984 ; Loss:  3.0882912460856784 ; l2 norm of gradient:  1.1342054108429902 ; l2 norm of weights:  7.2089247856054115\n",
            "Iteration#:  1988 ; Loss:  3.0882395286525353 ; l2 norm of gradient:  1.134187952307723 ; l2 norm of weights:  7.2089069741161875\n",
            "Iteration#:  1992 ; Loss:  3.088187813732949 ; l2 norm of gradient:  1.134170493892635 ; l2 norm of weights:  7.208889162887823\n",
            "Iteration#:  1996 ; Loss:  3.088136098954726 ; l2 norm of gradient:  1.1341530355976488 ; l2 norm of weights:  7.20887135192031\n",
            "Iteration#:  2000 ; Loss:  3.0880843863155567 ; l2 norm of gradient:  1.1341355774226873 ; l2 norm of weights:  7.208853541213643\n",
            "Iteration#:  2004 ; Loss:  3.0880326754452287 ; l2 norm of gradient:  1.1341181193676746 ; l2 norm of weights:  7.208835730767814\n",
            "Iteration#:  2008 ; Loss:  3.08798096551823 ; l2 norm of gradient:  1.134100661432533 ; l2 norm of weights:  7.208817920582818\n",
            "Iteration#:  2012 ; Loss:  3.0879292585152216 ; l2 norm of gradient:  1.1340832036171862 ; l2 norm of weights:  7.208800110658644\n",
            "Iteration#:  2016 ; Loss:  3.087877552164067 ; l2 norm of gradient:  1.1340657459215564 ; l2 norm of weights:  7.208782300995289\n",
            "Iteration#:  2020 ; Loss:  3.0878258473392117 ; l2 norm of gradient:  1.134048288345569 ; l2 norm of weights:  7.208764491592743\n",
            "Iteration#:  2024 ; Loss:  3.087774143962817 ; l2 norm of gradient:  1.1340308308891445 ; l2 norm of weights:  7.208746682450999\n",
            "Iteration#:  2028 ; Loss:  3.087722443260358 ; l2 norm of gradient:  1.1340133735522084 ; l2 norm of weights:  7.208728873570054\n",
            "Iteration#:  2032 ; Loss:  3.0876707432742756 ; l2 norm of gradient:  1.1339959163346827 ; l2 norm of weights:  7.208711064949895\n",
            "Iteration#:  2036 ; Loss:  3.0876190444242857 ; l2 norm of gradient:  1.133978459236492 ; l2 norm of weights:  7.208693256590518\n",
            "Iteration#:  2040 ; Loss:  3.087567347785819 ; l2 norm of gradient:  1.1339610022575561 ; l2 norm of weights:  7.208675448491916\n",
            "Iteration#:  2044 ; Loss:  3.0875156534415704 ; l2 norm of gradient:  1.1339435453978017 ; l2 norm of weights:  7.2086576406540805\n",
            "Iteration#:  2048 ; Loss:  3.087463959438033 ; l2 norm of gradient:  1.1339260886571514 ; l2 norm of weights:  7.208639833077006\n",
            "Iteration#:  2052 ; Loss:  3.0874122674662168 ; l2 norm of gradient:  1.1339086320355278 ; l2 norm of weights:  7.208622025760685\n",
            "Iteration#:  2056 ; Loss:  3.0873605771227663 ; l2 norm of gradient:  1.133891175532854 ; l2 norm of weights:  7.20860421870511\n",
            "Iteration#:  2060 ; Loss:  3.087308887699209 ; l2 norm of gradient:  1.1338737191490538 ; l2 norm of weights:  7.208586411910274\n",
            "Iteration#:  2064 ; Loss:  3.0872572013794377 ; l2 norm of gradient:  1.133856262884051 ; l2 norm of weights:  7.208568605376171\n",
            "Iteration#:  2068 ; Loss:  3.087205515526461 ; l2 norm of gradient:  1.1338388067377674 ; l2 norm of weights:  7.208550799102792\n",
            "Iteration#:  2072 ; Loss:  3.0871538312289792 ; l2 norm of gradient:  1.1338213507101276 ; l2 norm of weights:  7.20853299309013\n",
            "Iteration#:  2076 ; Loss:  3.0871021486247274 ; l2 norm of gradient:  1.1338038948010545 ; l2 norm of weights:  7.20851518733818\n",
            "Iteration#:  2080 ; Loss:  3.0870504687602147 ; l2 norm of gradient:  1.1337864390104724 ; l2 norm of weights:  7.208497381846933\n",
            "Iteration#:  2084 ; Loss:  3.0869987889744674 ; l2 norm of gradient:  1.1337689833383018 ; l2 norm of weights:  7.208479576616382\n",
            "Iteration#:  2088 ; Loss:  3.0869471112910074 ; l2 norm of gradient:  1.1337515277844696 ; l2 norm of weights:  7.208461771646521\n",
            "Iteration#:  2092 ; Loss:  3.086895434975755 ; l2 norm of gradient:  1.1337340723488962 ; l2 norm of weights:  7.208443966937343\n",
            "Iteration#:  2096 ; Loss:  3.0868437600345517 ; l2 norm of gradient:  1.1337166170315072 ; l2 norm of weights:  7.208426162488839\n",
            "Iteration#:  2100 ; Loss:  3.0867920879159794 ; l2 norm of gradient:  1.1336991618322252 ; l2 norm of weights:  7.208408358301003\n",
            "Iteration#:  2104 ; Loss:  3.086740416286118 ; l2 norm of gradient:  1.133681706750973 ; l2 norm of weights:  7.208390554373828\n",
            "Iteration#:  2108 ; Loss:  3.0866887464294903 ; l2 norm of gradient:  1.133664251787675 ; l2 norm of weights:  7.2083727507073085\n",
            "Iteration#:  2112 ; Loss:  3.0866370783070813 ; l2 norm of gradient:  1.1336467969422532 ; l2 norm of weights:  7.208354947301435\n",
            "Iteration#:  2116 ; Loss:  3.0865854120665923 ; l2 norm of gradient:  1.1336293422146329 ; l2 norm of weights:  7.2083371441562\n",
            "Iteration#:  2120 ; Loss:  3.0865337469721696 ; l2 norm of gradient:  1.1336118876047356 ; l2 norm of weights:  7.208319341271599\n",
            "Iteration#:  2124 ; Loss:  3.0864820835220854 ; l2 norm of gradient:  1.1335944331124859 ; l2 norm of weights:  7.208301538647624\n",
            "Iteration#:  2128 ; Loss:  3.0864304210703457 ; l2 norm of gradient:  1.1335769787378078 ; l2 norm of weights:  7.208283736284266\n",
            "Iteration#:  2132 ; Loss:  3.0863787610296978 ; l2 norm of gradient:  1.133559524480623 ; l2 norm of weights:  7.20826593418152\n",
            "Iteration#:  2136 ; Loss:  3.0863271025893493 ; l2 norm of gradient:  1.1335420703408559 ; l2 norm of weights:  7.208248132339379\n",
            "Iteration#:  2140 ; Loss:  3.0862754456691817 ; l2 norm of gradient:  1.1335246163184305 ; l2 norm of weights:  7.208230330757833\n",
            "Iteration#:  2144 ; Loss:  3.086223790212454 ; l2 norm of gradient:  1.1335071624132695 ; l2 norm of weights:  7.208212529436879\n",
            "Iteration#:  2148 ; Loss:  3.086172135826548 ; l2 norm of gradient:  1.1334897086252966 ; l2 norm of weights:  7.208194728376507\n",
            "Iteration#:  2152 ; Loss:  3.0861204841517376 ; l2 norm of gradient:  1.1334722549544354 ; l2 norm of weights:  7.208176927576711\n",
            "Iteration#:  2156 ; Loss:  3.0860688330092683 ; l2 norm of gradient:  1.1334548014006092 ; l2 norm of weights:  7.208159127037485\n",
            "Iteration#:  2160 ; Loss:  3.0860171838292882 ; l2 norm of gradient:  1.133437347963742 ; l2 norm of weights:  7.2081413267588195\n",
            "Iteration#:  2164 ; Loss:  3.085965535792236 ; l2 norm of gradient:  1.133419894643756 ; l2 norm of weights:  7.208123526740708\n",
            "Iteration#:  2168 ; Loss:  3.085913889834212 ; l2 norm of gradient:  1.1334024414405763 ; l2 norm of weights:  7.2081057269831454\n",
            "Iteration#:  2172 ; Loss:  3.0858622463024608 ; l2 norm of gradient:  1.1333849883541258 ; l2 norm of weights:  7.2080879274861225\n",
            "Iteration#:  2176 ; Loss:  3.0858106031658656 ; l2 norm of gradient:  1.1333675353843287 ; l2 norm of weights:  7.2080701282496324\n",
            "Iteration#:  2180 ; Loss:  3.0857589617323615 ; l2 norm of gradient:  1.1333500825311078 ; l2 norm of weights:  7.208052329273669\n",
            "Iteration#:  2184 ; Loss:  3.0857073218452467 ; l2 norm of gradient:  1.1333326297943864 ; l2 norm of weights:  7.208034530558227\n",
            "Iteration#:  2188 ; Loss:  3.0856556839780502 ; l2 norm of gradient:  1.1333151771740888 ; l2 norm of weights:  7.208016732103293\n",
            "Iteration#:  2192 ; Loss:  3.085604047755144 ; l2 norm of gradient:  1.1332977246701381 ; l2 norm of weights:  7.207998933908867\n",
            "Iteration#:  2196 ; Loss:  3.08555241288335 ; l2 norm of gradient:  1.1332802722824589 ; l2 norm of weights:  7.207981135974936\n",
            "Iteration#:  2200 ; Loss:  3.085500779111446 ; l2 norm of gradient:  1.1332628200109744 ; l2 norm of weights:  7.207963338301498\n",
            "Iteration#:  2204 ; Loss:  3.085449147438412 ; l2 norm of gradient:  1.133245367855607 ; l2 norm of weights:  7.207945540888543\n",
            "Iteration#:  2208 ; Loss:  3.08539751672273 ; l2 norm of gradient:  1.1332279158162812 ; l2 norm of weights:  7.207927743736065\n",
            "Iteration#:  2212 ; Loss:  3.085345889268111 ; l2 norm of gradient:  1.1332104638929201 ; l2 norm of weights:  7.207909946844056\n",
            "Iteration#:  2216 ; Loss:  3.0852942617834787 ; l2 norm of gradient:  1.1331930120854488 ; l2 norm of weights:  7.207892150212509\n",
            "Iteration#:  2220 ; Loss:  3.0852426364349204 ; l2 norm of gradient:  1.1331755603937907 ; l2 norm of weights:  7.207874353841419\n",
            "Iteration#:  2224 ; Loss:  3.0851910121911104 ; l2 norm of gradient:  1.133158108817868 ; l2 norm of weights:  7.207856557730776\n",
            "Iteration#:  2228 ; Loss:  3.085139390661575 ; l2 norm of gradient:  1.1331406573576044 ; l2 norm of weights:  7.207838761880574\n",
            "Iteration#:  2232 ; Loss:  3.085087770047249 ; l2 norm of gradient:  1.1331232060129255 ; l2 norm of weights:  7.2078209662908055\n",
            "Iteration#:  2236 ; Loss:  3.0850361508132544 ; l2 norm of gradient:  1.1331057547837535 ; l2 norm of weights:  7.207803170961465\n",
            "Iteration#:  2240 ; Loss:  3.0849845333707693 ; l2 norm of gradient:  1.1330883036700128 ; l2 norm of weights:  7.207785375892544\n",
            "Iteration#:  2244 ; Loss:  3.084932916857551 ; l2 norm of gradient:  1.1330708526716269 ; l2 norm of weights:  7.207767581084036\n",
            "Iteration#:  2248 ; Loss:  3.0848813030855458 ; l2 norm of gradient:  1.1330534017885192 ; l2 norm of weights:  7.207749786535933\n",
            "Iteration#:  2252 ; Loss:  3.084829690383638 ; l2 norm of gradient:  1.133035951020614 ; l2 norm of weights:  7.207731992248228\n",
            "Iteration#:  2256 ; Loss:  3.084778078983453 ; l2 norm of gradient:  1.133018500367835 ; l2 norm of weights:  7.207714198220916\n",
            "Iteration#:  2260 ; Loss:  3.0847264693437797 ; l2 norm of gradient:  1.133001049830105 ; l2 norm of weights:  7.207696404453987\n",
            "Iteration#:  2264 ; Loss:  3.084674860922839 ; l2 norm of gradient:  1.1329835994073487 ; l2 norm of weights:  7.207678610947435\n",
            "Iteration#:  2268 ; Loss:  3.0846232555647295 ; l2 norm of gradient:  1.1329661490994896 ; l2 norm of weights:  7.207660817701255\n",
            "Iteration#:  2272 ; Loss:  3.084571650350087 ; l2 norm of gradient:  1.132948698906452 ; l2 norm of weights:  7.207643024715436\n",
            "Iteration#:  2276 ; Loss:  3.084520047177612 ; l2 norm of gradient:  1.1329312488281595 ; l2 norm of weights:  7.207625231989974\n",
            "Iteration#:  2280 ; Loss:  3.0844684451192865 ; l2 norm of gradient:  1.1329137988645346 ; l2 norm of weights:  7.2076074395248595\n",
            "Iteration#:  2284 ; Loss:  3.08441684517862 ; l2 norm of gradient:  1.1328963490155037 ; l2 norm of weights:  7.207589647320089\n",
            "Iteration#:  2288 ; Loss:  3.084365247041671 ; l2 norm of gradient:  1.1328788992809884 ; l2 norm of weights:  7.207571855375653\n",
            "Iteration#:  2292 ; Loss:  3.0843136499852104 ; l2 norm of gradient:  1.1328614496609128 ; l2 norm of weights:  7.207554063691543\n",
            "Iteration#:  2296 ; Loss:  3.084262054395119 ; l2 norm of gradient:  1.1328440001552011 ; l2 norm of weights:  7.207536272267754\n",
            "Iteration#:  2300 ; Loss:  3.0842104609197074 ; l2 norm of gradient:  1.1328265507637783 ; l2 norm of weights:  7.2075184811042785\n",
            "Iteration#:  2304 ; Loss:  3.0841588681818255 ; l2 norm of gradient:  1.1328091014865667 ; l2 norm of weights:  7.207500690201109\n",
            "Iteration#:  2308 ; Loss:  3.0841072781602623 ; l2 norm of gradient:  1.1327916523234904 ; l2 norm of weights:  7.20748289955824\n",
            "Iteration#:  2312 ; Loss:  3.0840556892438657 ; l2 norm of gradient:  1.132774203274474 ; l2 norm of weights:  7.207465109175661\n",
            "Iteration#:  2316 ; Loss:  3.0840041015585955 ; l2 norm of gradient:  1.1327567543394421 ; l2 norm of weights:  7.207447319053369\n",
            "Iteration#:  2320 ; Loss:  3.0839525157395626 ; l2 norm of gradient:  1.132739305518316 ; l2 norm of weights:  7.207429529191354\n",
            "Iteration#:  2324 ; Loss:  3.0839009313457706 ; l2 norm of gradient:  1.1327218568110216 ; l2 norm of weights:  7.2074117395896105\n",
            "Iteration#:  2328 ; Loss:  3.083849349615841 ; l2 norm of gradient:  1.1327044082174822 ; l2 norm of weights:  7.20739395024813\n",
            "Iteration#:  2332 ; Loss:  3.0837977680536572 ; l2 norm of gradient:  1.1326869597376223 ; l2 norm of weights:  7.207376161166906\n",
            "Iteration#:  2336 ; Loss:  3.083746188165812 ; l2 norm of gradient:  1.1326695113713663 ; l2 norm of weights:  7.207358372345932\n",
            "Iteration#:  2340 ; Loss:  3.0836946104201077 ; l2 norm of gradient:  1.1326520631186365 ; l2 norm of weights:  7.207340583785201\n",
            "Iteration#:  2344 ; Loss:  3.083643033770961 ; l2 norm of gradient:  1.1326346149793578 ; l2 norm of weights:  7.207322795484705\n",
            "Iteration#:  2348 ; Loss:  3.083591459856162 ; l2 norm of gradient:  1.1326171669534542 ; l2 norm of weights:  7.207305007444436\n",
            "Iteration#:  2352 ; Loss:  3.0835398864322117 ; l2 norm of gradient:  1.1325997190408492 ; l2 norm of weights:  7.207287219664391\n",
            "Iteration#:  2356 ; Loss:  3.0834883144655256 ; l2 norm of gradient:  1.1325822712414677 ; l2 norm of weights:  7.207269432144558\n",
            "Iteration#:  2360 ; Loss:  3.083436744606834 ; l2 norm of gradient:  1.1325648235552328 ; l2 norm of weights:  7.2072516448849315\n",
            "Iteration#:  2364 ; Loss:  3.083385175961604 ; l2 norm of gradient:  1.1325473759820692 ; l2 norm of weights:  7.207233857885506\n",
            "Iteration#:  2368 ; Loss:  3.083333609858047 ; l2 norm of gradient:  1.1325299285219017 ; l2 norm of weights:  7.207216071146274\n",
            "Iteration#:  2372 ; Loss:  3.08328204443445 ; l2 norm of gradient:  1.1325124811746516 ; l2 norm of weights:  7.207198284667227\n",
            "Iteration#:  2376 ; Loss:  3.083230480365982 ; l2 norm of gradient:  1.1324950339402455 ; l2 norm of weights:  7.207180498448358\n",
            "Iteration#:  2380 ; Loss:  3.083178918311469 ; l2 norm of gradient:  1.1324775868186074 ; l2 norm of weights:  7.207162712489662\n",
            "Iteration#:  2384 ; Loss:  3.0831273577344005 ; l2 norm of gradient:  1.1324601398096603 ; l2 norm of weights:  7.207144926791129\n",
            "Iteration#:  2388 ; Loss:  3.0830757992159454 ; l2 norm of gradient:  1.1324426929133278 ; l2 norm of weights:  7.207127141352754\n",
            "Iteration#:  2392 ; Loss:  3.083024241842722 ; l2 norm of gradient:  1.1324252461295354 ; l2 norm of weights:  7.207109356174529\n",
            "Iteration#:  2396 ; Loss:  3.0829726858310114 ; l2 norm of gradient:  1.1324077994582071 ; l2 norm of weights:  7.207091571256448\n",
            "Iteration#:  2400 ; Loss:  3.082921131370207 ; l2 norm of gradient:  1.1323903528992658 ; l2 norm of weights:  7.207073786598502\n",
            "Iteration#:  2404 ; Loss:  3.0828695789336504 ; l2 norm of gradient:  1.1323729064526373 ; l2 norm of weights:  7.2070560022006855\n",
            "Iteration#:  2408 ; Loss:  3.0828180283179547 ; l2 norm of gradient:  1.1323554601182437 ; l2 norm of weights:  7.207038218062991\n",
            "Iteration#:  2412 ; Loss:  3.082766478767339 ; l2 norm of gradient:  1.1323380138960117 ; l2 norm of weights:  7.207020434185411\n",
            "Iteration#:  2416 ; Loss:  3.082714930913804 ; l2 norm of gradient:  1.1323205677858634 ; l2 norm of weights:  7.20700265056794\n",
            "Iteration#:  2420 ; Loss:  3.082663384351432 ; l2 norm of gradient:  1.1323031217877235 ; l2 norm of weights:  7.206984867210568\n",
            "Iteration#:  2424 ; Loss:  3.0826118395727446 ; l2 norm of gradient:  1.1322856759015172 ; l2 norm of weights:  7.20696708411329\n",
            "Iteration#:  2428 ; Loss:  3.082560296384466 ; l2 norm of gradient:  1.1322682301271667 ; l2 norm of weights:  7.206949301276099\n",
            "Iteration#:  2432 ; Loss:  3.0825087554523325 ; l2 norm of gradient:  1.1322507844645977 ; l2 norm of weights:  7.206931518698987\n",
            "Iteration#:  2436 ; Loss:  3.0824572151730587 ; l2 norm of gradient:  1.1322333389137345 ; l2 norm of weights:  7.206913736381946\n",
            "Iteration#:  2440 ; Loss:  3.0824056768241928 ; l2 norm of gradient:  1.1322158934745012 ; l2 norm of weights:  7.206895954324971\n",
            "Iteration#:  2444 ; Loss:  3.0823541399453767 ; l2 norm of gradient:  1.132198448146821 ; l2 norm of weights:  7.206878172528055\n",
            "Iteration#:  2448 ; Loss:  3.0823026045176967 ; l2 norm of gradient:  1.1321810029306194 ; l2 norm of weights:  7.2068603909911895\n",
            "Iteration#:  2452 ; Loss:  3.0822510717922 ; l2 norm of gradient:  1.13216355782582 ; l2 norm of weights:  7.206842609714368\n",
            "Iteration#:  2456 ; Loss:  3.0821995394231156 ; l2 norm of gradient:  1.1321461128323482 ; l2 norm of weights:  7.206824828697583\n",
            "Iteration#:  2460 ; Loss:  3.082148008653511 ; l2 norm of gradient:  1.1321286679501257 ; l2 norm of weights:  7.206807047940829\n",
            "Iteration#:  2464 ; Loss:  3.082096479569455 ; l2 norm of gradient:  1.1321112231790793 ; l2 norm of weights:  7.206789267444095\n",
            "Iteration#:  2468 ; Loss:  3.0820449524447864 ; l2 norm of gradient:  1.132093778519132 ; l2 norm of weights:  7.206771487207378\n",
            "Iteration#:  2472 ; Loss:  3.0819934273049663 ; l2 norm of gradient:  1.1320763339702087 ; l2 norm of weights:  7.2067537072306695\n",
            "Iteration#:  2476 ; Loss:  3.0819419027291164 ; l2 norm of gradient:  1.132058889532234 ; l2 norm of weights:  7.206735927513963\n",
            "Iteration#:  2480 ; Loss:  3.081890380449864 ; l2 norm of gradient:  1.1320414452051322 ; l2 norm of weights:  7.20671814805725\n",
            "Iteration#:  2484 ; Loss:  3.0818388592977684 ; l2 norm of gradient:  1.132024000988826 ; l2 norm of weights:  7.206700368860524\n",
            "Iteration#:  2488 ; Loss:  3.0817873395580695 ; l2 norm of gradient:  1.1320065568832427 ; l2 norm of weights:  7.206682589923779\n",
            "Iteration#:  2492 ; Loss:  3.081735821703851 ; l2 norm of gradient:  1.1319891128883037 ; l2 norm of weights:  7.2066648112470055\n",
            "Iteration#:  2496 ; Loss:  3.0816843063888206 ; l2 norm of gradient:  1.1319716690039356 ; l2 norm of weights:  7.206647032830198\n",
            "Iteration#:  2500 ; Loss:  3.0816327914965864 ; l2 norm of gradient:  1.1319542252300616 ; l2 norm of weights:  7.20662925467335\n",
            "Iteration#:  2504 ; Loss:  3.0815812779713543 ; l2 norm of gradient:  1.131936781566606 ; l2 norm of weights:  7.206611476776454\n",
            "Iteration#:  2508 ; Loss:  3.0815297664911645 ; l2 norm of gradient:  1.131919338013494 ; l2 norm of weights:  7.2065936991395025\n",
            "Iteration#:  2512 ; Loss:  3.0814782568475447 ; l2 norm of gradient:  1.1319018945706507 ; l2 norm of weights:  7.206575921762488\n",
            "Iteration#:  2516 ; Loss:  3.081426748947679 ; l2 norm of gradient:  1.1318844512379984 ; l2 norm of weights:  7.2065581446454035\n",
            "Iteration#:  2520 ; Loss:  3.0813752420476375 ; l2 norm of gradient:  1.1318670080154627 ; l2 norm of weights:  7.206540367788244\n",
            "Iteration#:  2524 ; Loss:  3.0813237365848947 ; l2 norm of gradient:  1.1318495649029683 ; l2 norm of weights:  7.206522591191\n",
            "Iteration#:  2528 ; Loss:  3.0812722333259384 ; l2 norm of gradient:  1.13183212190044 ; l2 norm of weights:  7.206504814853664\n",
            "Iteration#:  2532 ; Loss:  3.0812207308179316 ; l2 norm of gradient:  1.1318146790078003 ; l2 norm of weights:  7.20648703877623\n",
            "Iteration#:  2536 ; Loss:  3.0811692303698663 ; l2 norm of gradient:  1.1317972362249766 ; l2 norm of weights:  7.206469262958693\n",
            "Iteration#:  2540 ; Loss:  3.0811177321504886 ; l2 norm of gradient:  1.1317797935518918 ; l2 norm of weights:  7.206451487401043\n",
            "Iteration#:  2544 ; Loss:  3.0810662348357543 ; l2 norm of gradient:  1.1317623509884693 ; l2 norm of weights:  7.206433712103274\n",
            "Iteration#:  2548 ; Loss:  3.0810147392241465 ; l2 norm of gradient:  1.1317449085346363 ; l2 norm of weights:  7.206415937065377\n",
            "Iteration#:  2552 ; Loss:  3.0809632447123665 ; l2 norm of gradient:  1.1317274661903152 ; l2 norm of weights:  7.206398162287348\n",
            "Iteration#:  2556 ; Loss:  3.080911752153386 ; l2 norm of gradient:  1.1317100239554316 ; l2 norm of weights:  7.206380387769178\n",
            "Iteration#:  2560 ; Loss:  3.0808602616585743 ; l2 norm of gradient:  1.1316925818299093 ; l2 norm of weights:  7.206362613510861\n",
            "Iteration#:  2564 ; Loss:  3.0808087724686293 ; l2 norm of gradient:  1.1316751398136733 ; l2 norm of weights:  7.20634483951239\n",
            "Iteration#:  2568 ; Loss:  3.080757284422336 ; l2 norm of gradient:  1.13165769790665 ; l2 norm of weights:  7.206327065773756\n",
            "Iteration#:  2572 ; Loss:  3.0807057980410546 ; l2 norm of gradient:  1.1316402561087604 ; l2 norm of weights:  7.206309292294953\n",
            "Iteration#:  2576 ; Loss:  3.0806543134043616 ; l2 norm of gradient:  1.1316228144199314 ; l2 norm of weights:  7.206291519075975\n",
            "Iteration#:  2580 ; Loss:  3.080602829926898 ; l2 norm of gradient:  1.1316053728400874 ; l2 norm of weights:  7.206273746116814\n",
            "Iteration#:  2584 ; Loss:  3.080551349170605 ; l2 norm of gradient:  1.1315879313691524 ; l2 norm of weights:  7.206255973417462\n",
            "Iteration#:  2588 ; Loss:  3.080499869526861 ; l2 norm of gradient:  1.1315704900070525 ; l2 norm of weights:  7.206238200977913\n",
            "Iteration#:  2592 ; Loss:  3.0804483911443272 ; l2 norm of gradient:  1.1315530487537102 ; l2 norm of weights:  7.206220428798161\n",
            "Iteration#:  2596 ; Loss:  3.0803969142986425 ; l2 norm of gradient:  1.131535607609052 ; l2 norm of weights:  7.206202656878196\n",
            "Iteration#:  2600 ; Loss:  3.080345438951682 ; l2 norm of gradient:  1.1315181665730023 ; l2 norm of weights:  7.206184885218012\n",
            "Iteration#:  2604 ; Loss:  3.080293965165148 ; l2 norm of gradient:  1.1315007256454848 ; l2 norm of weights:  7.206167113817604\n",
            "Iteration#:  2608 ; Loss:  3.080242493690117 ; l2 norm of gradient:  1.1314832848264254 ; l2 norm of weights:  7.206149342676963\n",
            "Iteration#:  2612 ; Loss:  3.080191023716998 ; l2 norm of gradient:  1.1314658441157477 ; l2 norm of weights:  7.206131571796082\n",
            "Iteration#:  2616 ; Loss:  3.08013955457114 ; l2 norm of gradient:  1.1314484035133783 ; l2 norm of weights:  7.206113801174954\n",
            "Iteration#:  2620 ; Loss:  3.080088087209486 ; l2 norm of gradient:  1.1314309630192378 ; l2 norm of weights:  7.206096030813572\n",
            "Iteration#:  2624 ; Loss:  3.08003662147797 ; l2 norm of gradient:  1.1314135226332562 ; l2 norm of weights:  7.206078260711929\n",
            "Iteration#:  2628 ; Loss:  3.0799851572752983 ; l2 norm of gradient:  1.1313960823553555 ; l2 norm of weights:  7.206060490870018\n",
            "Iteration#:  2632 ; Loss:  3.079933695291797 ; l2 norm of gradient:  1.1313786421854597 ; l2 norm of weights:  7.206042721287831\n",
            "Iteration#:  2636 ; Loss:  3.079882234317486 ; l2 norm of gradient:  1.1313612021234958 ; l2 norm of weights:  7.206024951965363\n",
            "Iteration#:  2640 ; Loss:  3.0798307751163407 ; l2 norm of gradient:  1.1313437621693883 ; l2 norm of weights:  7.206007182902605\n",
            "Iteration#:  2644 ; Loss:  3.079779317176208 ; l2 norm of gradient:  1.131326322323061 ; l2 norm of weights:  7.20598941409955\n",
            "Iteration#:  2648 ; Loss:  3.079727861033115 ; l2 norm of gradient:  1.1313088825844375 ; l2 norm of weights:  7.205971645556193\n",
            "Iteration#:  2652 ; Loss:  3.079676406001398 ; l2 norm of gradient:  1.1312914429534449 ; l2 norm of weights:  7.205953877272525\n",
            "Iteration#:  2656 ; Loss:  3.0796249536874476 ; l2 norm of gradient:  1.1312740034300075 ; l2 norm of weights:  7.205936109248539\n",
            "Iteration#:  2660 ; Loss:  3.0795735022041724 ; l2 norm of gradient:  1.13125656401405 ; l2 norm of weights:  7.205918341484227\n",
            "Iteration#:  2664 ; Loss:  3.0795220520004225 ; l2 norm of gradient:  1.1312391247054974 ; l2 norm of weights:  7.205900573979584\n",
            "Iteration#:  2668 ; Loss:  3.0794706039122812 ; l2 norm of gradient:  1.1312216855042743 ; l2 norm of weights:  7.205882806734601\n",
            "Iteration#:  2672 ; Loss:  3.0794191569318365 ; l2 norm of gradient:  1.1312042464103051 ; l2 norm of weights:  7.205865039749273\n",
            "Iteration#:  2676 ; Loss:  3.0793677122392658 ; l2 norm of gradient:  1.131186807423516 ; l2 norm of weights:  7.205847273023592\n",
            "Iteration#:  2680 ; Loss:  3.079316269275437 ; l2 norm of gradient:  1.1311693685438315 ; l2 norm of weights:  7.20582950655755\n",
            "Iteration#:  2684 ; Loss:  3.0792648271254137 ; l2 norm of gradient:  1.1311519297711763 ; l2 norm of weights:  7.205811740351141\n",
            "Iteration#:  2688 ; Loss:  3.079213386409722 ; l2 norm of gradient:  1.131134491105475 ; l2 norm of weights:  7.205793974404357\n",
            "Iteration#:  2692 ; Loss:  3.0791619474842813 ; l2 norm of gradient:  1.1311170525466532 ; l2 norm of weights:  7.205776208717192\n",
            "Iteration#:  2696 ; Loss:  3.07911051018058 ; l2 norm of gradient:  1.1310996140946359 ; l2 norm of weights:  7.205758443289637\n",
            "Iteration#:  2700 ; Loss:  3.0790590744043795 ; l2 norm of gradient:  1.1310821757493472 ; l2 norm of weights:  7.205740678121689\n",
            "Iteration#:  2704 ; Loss:  3.079007641109572 ; l2 norm of gradient:  1.1310647375107135 ; l2 norm of weights:  7.205722913213336\n",
            "Iteration#:  2708 ; Loss:  3.0789562084553053 ; l2 norm of gradient:  1.1310472993786589 ; l2 norm of weights:  7.205705148564573\n",
            "Iteration#:  2712 ; Loss:  3.078904777525287 ; l2 norm of gradient:  1.131029861353108 ; l2 norm of weights:  7.205687384175394\n",
            "Iteration#:  2716 ; Loss:  3.078853347929712 ; l2 norm of gradient:  1.1310124234339873 ; l2 norm of weights:  7.205669620045791\n",
            "Iteration#:  2720 ; Loss:  3.078801920112522 ; l2 norm of gradient:  1.13099498562122 ; l2 norm of weights:  7.205651856175756\n",
            "Iteration#:  2724 ; Loss:  3.078750493748972 ; l2 norm of gradient:  1.130977547914733 ; l2 norm of weights:  7.205634092565283\n",
            "Iteration#:  2728 ; Loss:  3.078699069876817 ; l2 norm of gradient:  1.130960110314449 ; l2 norm of weights:  7.205616329214364\n",
            "Iteration#:  2732 ; Loss:  3.0786476467979482 ; l2 norm of gradient:  1.1309426728202963 ; l2 norm of weights:  7.205598566122993\n",
            "Iteration#:  2736 ; Loss:  3.0785962252897026 ; l2 norm of gradient:  1.1309252354321977 ; l2 norm of weights:  7.205580803291163\n",
            "Iteration#:  2740 ; Loss:  3.0785448056768367 ; l2 norm of gradient:  1.1309077981500784 ; l2 norm of weights:  7.205563040718866\n",
            "Iteration#:  2744 ; Loss:  3.078493387298069 ; l2 norm of gradient:  1.1308903609738645 ; l2 norm of weights:  7.205545278406095\n",
            "Iteration#:  2748 ; Loss:  3.07844197042874 ; l2 norm of gradient:  1.1308729239034814 ; l2 norm of weights:  7.205527516352843\n",
            "Iteration#:  2752 ; Loss:  3.0783905559621085 ; l2 norm of gradient:  1.1308554869388525 ; l2 norm of weights:  7.205509754559104\n",
            "Iteration#:  2756 ; Loss:  3.0783391424834368 ; l2 norm of gradient:  1.1308380500799047 ; l2 norm of weights:  7.205491993024871\n",
            "Iteration#:  2760 ; Loss:  3.078287730379119 ; l2 norm of gradient:  1.1308206133265621 ; l2 norm of weights:  7.2054742317501335\n",
            "Iteration#:  2764 ; Loss:  3.0782363199436684 ; l2 norm of gradient:  1.130803176678751 ; l2 norm of weights:  7.205456470734888\n",
            "Iteration#:  2768 ; Loss:  3.078184911060996 ; l2 norm of gradient:  1.1307857401363945 ; l2 norm of weights:  7.205438709979126\n",
            "Iteration#:  2772 ; Loss:  3.078133503520468 ; l2 norm of gradient:  1.13076830369942 ; l2 norm of weights:  7.20542094948284\n",
            "Iteration#:  2776 ; Loss:  3.078082098715062 ; l2 norm of gradient:  1.1307508673677518 ; l2 norm of weights:  7.2054031892460255\n",
            "Iteration#:  2780 ; Loss:  3.078030694667079 ; l2 norm of gradient:  1.130733431141315 ; l2 norm of weights:  7.205385429268671\n",
            "Iteration#:  2784 ; Loss:  3.0779792918962583 ; l2 norm of gradient:  1.1307159950200354 ; l2 norm of weights:  7.205367669550774\n",
            "Iteration#:  2788 ; Loss:  3.077927891119968 ; l2 norm of gradient:  1.130698559003839 ; l2 norm of weights:  7.205349910092324\n",
            "Iteration#:  2792 ; Loss:  3.077876491637792 ; l2 norm of gradient:  1.1306811230926483 ; l2 norm of weights:  7.205332150893315\n",
            "Iteration#:  2796 ; Loss:  3.0778250937088094 ; l2 norm of gradient:  1.1306636872863924 ; l2 norm of weights:  7.205314391953741\n",
            "Iteration#:  2800 ; Loss:  3.077773697594184 ; l2 norm of gradient:  1.1306462515849933 ; l2 norm of weights:  7.205296633273594\n",
            "Iteration#:  2804 ; Loss:  3.0777223036701797 ; l2 norm of gradient:  1.1306288159883764 ; l2 norm of weights:  7.205278874852866\n",
            "Iteration#:  2808 ; Loss:  3.0776709102123325 ; l2 norm of gradient:  1.1306113804964706 ; l2 norm of weights:  7.205261116691551\n",
            "Iteration#:  2812 ; Loss:  3.077619518536482 ; l2 norm of gradient:  1.1305939451091969 ; l2 norm of weights:  7.205243358789643\n",
            "Iteration#:  2816 ; Loss:  3.077568128715981 ; l2 norm of gradient:  1.1305765098264844 ; l2 norm of weights:  7.205225601147132\n",
            "Iteration#:  2820 ; Loss:  3.0775167405010357 ; l2 norm of gradient:  1.1305590746482552 ; l2 norm of weights:  7.205207843764014\n",
            "Iteration#:  2824 ; Loss:  3.0774653536628023 ; l2 norm of gradient:  1.130541639574437 ; l2 norm of weights:  7.205190086640278\n",
            "Iteration#:  2828 ; Loss:  3.077413969138986 ; l2 norm of gradient:  1.1305242046049535 ; l2 norm of weights:  7.205172329775922\n",
            "Iteration#:  2832 ; Loss:  3.077362585516135 ; l2 norm of gradient:  1.1305067697397315 ; l2 norm of weights:  7.205154573170935\n",
            "Iteration#:  2836 ; Loss:  3.0773112036801646 ; l2 norm of gradient:  1.1304893349786966 ; l2 norm of weights:  7.205136816825311\n",
            "Iteration#:  2840 ; Loss:  3.077259823014926 ; l2 norm of gradient:  1.1304719003217731 ; l2 norm of weights:  7.205119060739045\n",
            "Iteration#:  2844 ; Loss:  3.0772084442267427 ; l2 norm of gradient:  1.130454465768886 ; l2 norm of weights:  7.205101304912126\n",
            "Iteration#:  2848 ; Loss:  3.0771570669652246 ; l2 norm of gradient:  1.1304370313199634 ; l2 norm of weights:  7.2050835493445495\n",
            "Iteration#:  2852 ; Loss:  3.0771056911357406 ; l2 norm of gradient:  1.1304195969749276 ; l2 norm of weights:  7.205065794036308\n",
            "Iteration#:  2856 ; Loss:  3.077054317959992 ; l2 norm of gradient:  1.1304021627337055 ; l2 norm of weights:  7.205048038987394\n",
            "Iteration#:  2860 ; Loss:  3.0770029453510657 ; l2 norm of gradient:  1.130384728596223 ; l2 norm of weights:  7.205030284197803\n",
            "Iteration#:  2864 ; Loss:  3.0769515742767246 ; l2 norm of gradient:  1.130367294562406 ; l2 norm of weights:  7.205012529667523\n",
            "Iteration#:  2868 ; Loss:  3.0769002048989877 ; l2 norm of gradient:  1.1303498606321778 ; l2 norm of weights:  7.20499477539655\n",
            "Iteration#:  2872 ; Loss:  3.076848836800807 ; l2 norm of gradient:  1.1303324268054669 ; l2 norm of weights:  7.2049770213848765\n",
            "Iteration#:  2876 ; Loss:  3.076797470628843 ; l2 norm of gradient:  1.1303149930821972 ; l2 norm of weights:  7.2049592676324945\n",
            "Iteration#:  2880 ; Loss:  3.076746105938931 ; l2 norm of gradient:  1.130297559462294 ; l2 norm of weights:  7.204941514139399\n",
            "Iteration#:  2884 ; Loss:  3.0766947435997913 ; l2 norm of gradient:  1.130280125945683 ; l2 norm of weights:  7.20492376090558\n",
            "Iteration#:  2888 ; Loss:  3.0766433821804493 ; l2 norm of gradient:  1.1302626925322903 ; l2 norm of weights:  7.204906007931033\n",
            "Iteration#:  2892 ; Loss:  3.0765920221171577 ; l2 norm of gradient:  1.1302452592220413 ; l2 norm of weights:  7.20488825521575\n",
            "Iteration#:  2896 ; Loss:  3.07654066393784 ; l2 norm of gradient:  1.1302278260148622 ; l2 norm of weights:  7.204870502759724\n",
            "Iteration#:  2900 ; Loss:  3.076489307096638 ; l2 norm of gradient:  1.1302103929106777 ; l2 norm of weights:  7.2048527505629485\n",
            "Iteration#:  2904 ; Loss:  3.07643795212743 ; l2 norm of gradient:  1.1301929599094134 ; l2 norm of weights:  7.204834998625415\n",
            "Iteration#:  2908 ; Loss:  3.076386597916967 ; l2 norm of gradient:  1.1301755270109963 ; l2 norm of weights:  7.204817246947117\n",
            "Iteration#:  2912 ; Loss:  3.076335246955552 ; l2 norm of gradient:  1.1301580942153506 ; l2 norm of weights:  7.204799495528048\n",
            "Iteration#:  2916 ; Loss:  3.0762838964763097 ; l2 norm of gradient:  1.1301406615224028 ; l2 norm of weights:  7.2047817443682\n",
            "Iteration#:  2920 ; Loss:  3.0762325474433685 ; l2 norm of gradient:  1.1301232289320784 ; l2 norm of weights:  7.204763993467566\n",
            "Iteration#:  2924 ; Loss:  3.0761812002598674 ; l2 norm of gradient:  1.1301057964443026 ; l2 norm of weights:  7.20474624282614\n",
            "Iteration#:  2928 ; Loss:  3.076129854451913 ; l2 norm of gradient:  1.1300883640590018 ; l2 norm of weights:  7.2047284924439134\n",
            "Iteration#:  2932 ; Loss:  3.0760785104854618 ; l2 norm of gradient:  1.1300709317761015 ; l2 norm of weights:  7.204710742320881\n",
            "Iteration#:  2936 ; Loss:  3.0760271674113864 ; l2 norm of gradient:  1.1300534995955283 ; l2 norm of weights:  7.204692992457033\n",
            "Iteration#:  2940 ; Loss:  3.0759758272780364 ; l2 norm of gradient:  1.130036067517206 ; l2 norm of weights:  7.204675242852365\n",
            "Iteration#:  2944 ; Loss:  3.075924488005856 ; l2 norm of gradient:  1.130018635541062 ; l2 norm of weights:  7.2046574935068675\n",
            "Iteration#:  2948 ; Loss:  3.075873150037694 ; l2 norm of gradient:  1.1300012036670222 ; l2 norm of weights:  7.204639744420536\n",
            "Iteration#:  2952 ; Loss:  3.0758218138145392 ; l2 norm of gradient:  1.1299837718950119 ; l2 norm of weights:  7.204621995593362\n",
            "Iteration#:  2956 ; Loss:  3.0757704795085976 ; l2 norm of gradient:  1.129966340224956 ; l2 norm of weights:  7.204604247025338\n",
            "Iteration#:  2960 ; Loss:  3.0757191462367905 ; l2 norm of gradient:  1.1299489086567818 ; l2 norm of weights:  7.204586498716457\n",
            "Iteration#:  2964 ; Loss:  3.0756678144909917 ; l2 norm of gradient:  1.129931477190415 ; l2 norm of weights:  7.204568750666715\n",
            "Iteration#:  2968 ; Loss:  3.075616485229952 ; l2 norm of gradient:  1.1299140458257804 ; l2 norm of weights:  7.204551002876099\n",
            "Iteration#:  2972 ; Loss:  3.075565156858838 ; l2 norm of gradient:  1.1298966145628049 ; l2 norm of weights:  7.204533255344607\n",
            "Iteration#:  2976 ; Loss:  3.07551383002442 ; l2 norm of gradient:  1.1298791834014137 ; l2 norm of weights:  7.20451550807223\n",
            "Iteration#:  2980 ; Loss:  3.075462505067859 ; l2 norm of gradient:  1.1298617523415333 ; l2 norm of weights:  7.204497761058961\n",
            "Iteration#:  2984 ; Loss:  3.07541118110249 ; l2 norm of gradient:  1.12984432138309 ; l2 norm of weights:  7.204480014304792\n",
            "Iteration#:  2988 ; Loss:  3.075359859096107 ; l2 norm of gradient:  1.1298268905260078 ; l2 norm of weights:  7.204462267809718\n",
            "Iteration#:  2992 ; Loss:  3.0753085387015022 ; l2 norm of gradient:  1.129809459770215 ; l2 norm of weights:  7.2044445215737305\n",
            "Iteration#:  2996 ; Loss:  3.0752572207735893 ; l2 norm of gradient:  1.1297920291156363 ; l2 norm of weights:  7.204426775596822\n",
            "Iteration#:  3000 ; Loss:  3.0752059031581562 ; l2 norm of gradient:  1.129774598562199 ; l2 norm of weights:  7.204409029878986\n",
            "Iteration#:  3004 ; Loss:  3.0751545871064963 ; l2 norm of gradient:  1.1297571681098264 ; l2 norm of weights:  7.2043912844202165\n",
            "Iteration#:  3008 ; Loss:  3.0751032731557806 ; l2 norm of gradient:  1.1297397377584462 ; l2 norm of weights:  7.204373539220503\n",
            "Iteration#:  3012 ; Loss:  3.0750519603690436 ; l2 norm of gradient:  1.1297223075079839 ; l2 norm of weights:  7.204355794279844\n",
            "Iteration#:  3016 ; Loss:  3.075000649620205 ; l2 norm of gradient:  1.1297048773583669 ; l2 norm of weights:  7.204338049598228\n",
            "Iteration#:  3020 ; Loss:  3.074949339750278 ; l2 norm of gradient:  1.1296874473095202 ; l2 norm of weights:  7.204320305175648\n",
            "Iteration#:  3024 ; Loss:  3.074898032056034 ; l2 norm of gradient:  1.1296700173613707 ; l2 norm of weights:  7.204302561012099\n",
            "Iteration#:  3028 ; Loss:  3.0748467265943527 ; l2 norm of gradient:  1.1296525875138421 ; l2 norm of weights:  7.204284817107573\n",
            "Iteration#:  3032 ; Loss:  3.0747954217869418 ; l2 norm of gradient:  1.129635157766863 ; l2 norm of weights:  7.204267073462063\n",
            "Iteration#:  3036 ; Loss:  3.0747441186048707 ; l2 norm of gradient:  1.1296177281203592 ; l2 norm of weights:  7.204249330075561\n",
            "Iteration#:  3040 ; Loss:  3.0746928170859182 ; l2 norm of gradient:  1.1296002985742555 ; l2 norm of weights:  7.204231586948061\n",
            "Iteration#:  3044 ; Loss:  3.0746415170999217 ; l2 norm of gradient:  1.1295828691284795 ; l2 norm of weights:  7.204213844079557\n",
            "Iteration#:  3048 ; Loss:  3.0745902189618652 ; l2 norm of gradient:  1.1295654397829553 ; l2 norm of weights:  7.204196101470038\n",
            "Iteration#:  3052 ; Loss:  3.074538921781069 ; l2 norm of gradient:  1.1295480105376121 ; l2 norm of weights:  7.2041783591195\n",
            "Iteration#:  3056 ; Loss:  3.074487626561614 ; l2 norm of gradient:  1.1295305813923733 ; l2 norm of weights:  7.204160617027937\n",
            "Iteration#:  3060 ; Loss:  3.074436333475121 ; l2 norm of gradient:  1.1295131523471673 ; l2 norm of weights:  7.204142875195339\n",
            "Iteration#:  3064 ; Loss:  3.074385041526828 ; l2 norm of gradient:  1.1294957234019178 ; l2 norm of weights:  7.204125133621701\n",
            "Iteration#:  3068 ; Loss:  3.0743337507764474 ; l2 norm of gradient:  1.1294782945565534 ; l2 norm of weights:  7.204107392307015\n",
            "Iteration#:  3072 ; Loss:  3.074282461969061 ; l2 norm of gradient:  1.1294608658109986 ; l2 norm of weights:  7.204089651251272\n",
            "Iteration#:  3076 ; Loss:  3.0742311747948934 ; l2 norm of gradient:  1.1294434371651807 ; l2 norm of weights:  7.204071910454469\n",
            "Iteration#:  3080 ; Loss:  3.0741798890244953 ; l2 norm of gradient:  1.129426008619026 ; l2 norm of weights:  7.2040541699165965\n",
            "Iteration#:  3084 ; Loss:  3.0741286044683935 ; l2 norm of gradient:  1.1294085801724603 ; l2 norm of weights:  7.204036429637648\n",
            "Iteration#:  3088 ; Loss:  3.0740773227122196 ; l2 norm of gradient:  1.1293911518254096 ; l2 norm of weights:  7.204018689617615\n",
            "Iteration#:  3092 ; Loss:  3.0740260416234473 ; l2 norm of gradient:  1.1293737235778005 ; l2 norm of weights:  7.204000949856492\n",
            "Iteration#:  3096 ; Loss:  3.0739747618788797 ; l2 norm of gradient:  1.1293562954295593 ; l2 norm of weights:  7.203983210354271\n",
            "Iteration#:  3100 ; Loss:  3.0739234843461642 ; l2 norm of gradient:  1.1293388673806142 ; l2 norm of weights:  7.203965471110944\n",
            "Iteration#:  3104 ; Loss:  3.0738722081575647 ; l2 norm of gradient:  1.1293214394308875 ; l2 norm of weights:  7.203947732126508\n",
            "Iteration#:  3108 ; Loss:  3.073820932928478 ; l2 norm of gradient:  1.1293040115803101 ; l2 norm of weights:  7.203929993400953\n",
            "Iteration#:  3112 ; Loss:  3.0737696597004716 ; l2 norm of gradient:  1.1292865838288046 ; l2 norm of weights:  7.203912254934271\n",
            "Iteration#:  3116 ; Loss:  3.0737183881637398 ; l2 norm of gradient:  1.1292691561762997 ; l2 norm of weights:  7.203894516726455\n",
            "Iteration#:  3120 ; Loss:  3.0736671187803095 ; l2 norm of gradient:  1.129251728622721 ; l2 norm of weights:  7.2038767787775\n",
            "Iteration#:  3124 ; Loss:  3.073615850573966 ; l2 norm of gradient:  1.1292343011679948 ; l2 norm of weights:  7.203859041087399\n",
            "Iteration#:  3128 ; Loss:  3.0735645836028467 ; l2 norm of gradient:  1.129216873812047 ; l2 norm of weights:  7.203841303656142\n",
            "Iteration#:  3132 ; Loss:  3.0735133181951286 ; l2 norm of gradient:  1.1291994465548065 ; l2 norm of weights:  7.203823566483725\n",
            "Iteration#:  3136 ; Loss:  3.073462054441695 ; l2 norm of gradient:  1.1291820193961966 ; l2 norm of weights:  7.203805829570139\n",
            "Iteration#:  3140 ; Loss:  3.073410792557979 ; l2 norm of gradient:  1.1291645923361455 ; l2 norm of weights:  7.203788092915377\n",
            "Iteration#:  3144 ; Loss:  3.0733595316430673 ; l2 norm of gradient:  1.1291471653745802 ; l2 norm of weights:  7.203770356519431\n",
            "Iteration#:  3148 ; Loss:  3.0733082727072776 ; l2 norm of gradient:  1.129129738511426 ; l2 norm of weights:  7.203752620382298\n",
            "Iteration#:  3152 ; Loss:  3.0732570155121155 ; l2 norm of gradient:  1.129112311746609 ; l2 norm of weights:  7.203734884503968\n",
            "Iteration#:  3156 ; Loss:  3.0732057601665312 ; l2 norm of gradient:  1.129094885080057 ; l2 norm of weights:  7.203717148884433\n",
            "Iteration#:  3160 ; Loss:  3.0731545060933523 ; l2 norm of gradient:  1.1290774585116967 ; l2 norm of weights:  7.203699413523688\n",
            "Iteration#:  3164 ; Loss:  3.073103253589765 ; l2 norm of gradient:  1.1290600320414537 ; l2 norm of weights:  7.203681678421723\n",
            "Iteration#:  3168 ; Loss:  3.073052002150596 ; l2 norm of gradient:  1.1290426056692553 ; l2 norm of weights:  7.203663943578536\n",
            "Iteration#:  3172 ; Loss:  3.0730007527361543 ; l2 norm of gradient:  1.1290251793950281 ; l2 norm of weights:  7.203646208994114\n",
            "Iteration#:  3176 ; Loss:  3.0729495043913846 ; l2 norm of gradient:  1.1290077532186977 ; l2 norm of weights:  7.203628474668453\n",
            "Iteration#:  3180 ; Loss:  3.072898258189184 ; l2 norm of gradient:  1.1289903271401918 ; l2 norm of weights:  7.203610740601547\n",
            "Iteration#:  3184 ; Loss:  3.0728470136472414 ; l2 norm of gradient:  1.1289729011594367 ; l2 norm of weights:  7.203593006793387\n",
            "Iteration#:  3188 ; Loss:  3.072795770778656 ; l2 norm of gradient:  1.1289554752763589 ; l2 norm of weights:  7.203575273243965\n",
            "Iteration#:  3192 ; Loss:  3.0727445292169633 ; l2 norm of gradient:  1.1289380494908845 ; l2 norm of weights:  7.203557539953277\n",
            "Iteration#:  3196 ; Loss:  3.0726932892433765 ; l2 norm of gradient:  1.1289206238029426 ; l2 norm of weights:  7.203539806921314\n",
            "Iteration#:  3200 ; Loss:  3.072642050603827 ; l2 norm of gradient:  1.1289031982124564 ; l2 norm of weights:  7.203522074148068\n",
            "Iteration#:  3204 ; Loss:  3.0725908137799385 ; l2 norm of gradient:  1.1288857727193553 ; l2 norm of weights:  7.203504341633535\n",
            "Iteration#:  3208 ; Loss:  3.0725395779878726 ; l2 norm of gradient:  1.1288683473235657 ; l2 norm of weights:  7.203486609377704\n",
            "Iteration#:  3212 ; Loss:  3.0724883443867603 ; l2 norm of gradient:  1.1288509220250134 ; l2 norm of weights:  7.203468877380571\n",
            "Iteration#:  3216 ; Loss:  3.072437111972323 ; l2 norm of gradient:  1.1288334968236253 ; l2 norm of weights:  7.203451145642127\n",
            "Iteration#:  3220 ; Loss:  3.0723858814097142 ; l2 norm of gradient:  1.1288160717193287 ; l2 norm of weights:  7.203433414162365\n",
            "Iteration#:  3224 ; Loss:  3.072334653342795 ; l2 norm of gradient:  1.1287986467120503 ; l2 norm of weights:  7.203415682941282\n",
            "Iteration#:  3228 ; Loss:  3.072283425579497 ; l2 norm of gradient:  1.1287812218017166 ; l2 norm of weights:  7.203397951978865\n",
            "Iteration#:  3232 ; Loss:  3.072232199938924 ; l2 norm of gradient:  1.1287637969882542 ; l2 norm of weights:  7.2033802212751095\n",
            "Iteration#:  3236 ; Loss:  3.0721809753999643 ; l2 norm of gradient:  1.1287463722715914 ; l2 norm of weights:  7.203362490830008\n",
            "Iteration#:  3240 ; Loss:  3.0721297529675438 ; l2 norm of gradient:  1.1287289476516533 ; l2 norm of weights:  7.203344760643554\n",
            "Iteration#:  3244 ; Loss:  3.0720785313134127 ; l2 norm of gradient:  1.1287115231283675 ; l2 norm of weights:  7.203327030715742\n",
            "Iteration#:  3248 ; Loss:  3.0720273119967723 ; l2 norm of gradient:  1.128694098701661 ; l2 norm of weights:  7.203309301046561\n",
            "Iteration#:  3252 ; Loss:  3.0719760939616707 ; l2 norm of gradient:  1.1286766743714605 ; l2 norm of weights:  7.203291571636007\n",
            "Iteration#:  3256 ; Loss:  3.0719248775427483 ; l2 norm of gradient:  1.1286592501376924 ; l2 norm of weights:  7.203273842484071\n",
            "Iteration#:  3260 ; Loss:  3.0718736635164996 ; l2 norm of gradient:  1.1286418260002842 ; l2 norm of weights:  7.203256113590748\n",
            "Iteration#:  3264 ; Loss:  3.0718224498261284 ; l2 norm of gradient:  1.1286244019591636 ; l2 norm of weights:  7.2032383849560295\n",
            "Iteration#:  3268 ; Loss:  3.071771238227644 ; l2 norm of gradient:  1.1286069780142567 ; l2 norm of weights:  7.203220656579908\n",
            "Iteration#:  3272 ; Loss:  3.071720028067467 ; l2 norm of gradient:  1.12858955416549 ; l2 norm of weights:  7.203202928462377\n",
            "Iteration#:  3276 ; Loss:  3.0716688196784054 ; l2 norm of gradient:  1.1285721304127914 ; l2 norm of weights:  7.20318520060343\n",
            "Iteration#:  3280 ; Loss:  3.071617612473759 ; l2 norm of gradient:  1.1285547067560877 ; l2 norm of weights:  7.2031674730030595\n",
            "Iteration#:  3284 ; Loss:  3.071566407228312 ; l2 norm of gradient:  1.128537283195305 ; l2 norm of weights:  7.203149745661258\n",
            "Iteration#:  3288 ; Loss:  3.071515202990282 ; l2 norm of gradient:  1.1285198597303725 ; l2 norm of weights:  7.203132018578018\n",
            "Iteration#:  3292 ; Loss:  3.071464000948441 ; l2 norm of gradient:  1.1285024363612148 ; l2 norm of weights:  7.203114291753335\n",
            "Iteration#:  3296 ; Loss:  3.0714128007227073 ; l2 norm of gradient:  1.128485013087761 ; l2 norm of weights:  7.203096565187199\n",
            "Iteration#:  3300 ; Loss:  3.071361601657331 ; l2 norm of gradient:  1.128467589909936 ; l2 norm of weights:  7.203078838879604\n",
            "Iteration#:  3304 ; Loss:  3.0713104040906525 ; l2 norm of gradient:  1.1284501668276699 ; l2 norm of weights:  7.203061112830543\n",
            "Iteration#:  3308 ; Loss:  3.0712592083053254 ; l2 norm of gradient:  1.128432743840886 ; l2 norm of weights:  7.203043387040008\n",
            "Iteration#:  3312 ; Loss:  3.071208013570446 ; l2 norm of gradient:  1.1284153209495151 ; l2 norm of weights:  7.2030256615079935\n",
            "Iteration#:  3316 ; Loss:  3.0711568209211264 ; l2 norm of gradient:  1.1283978981534828 ; l2 norm of weights:  7.203007936234491\n",
            "Iteration#:  3320 ; Loss:  3.071105629542756 ; l2 norm of gradient:  1.1283804754527151 ; l2 norm of weights:  7.202990211219495\n",
            "Iteration#:  3324 ; Loss:  3.071054440165741 ; l2 norm of gradient:  1.128363052847141 ; l2 norm of weights:  7.202972486462998\n",
            "Iteration#:  3328 ; Loss:  3.071003251687996 ; l2 norm of gradient:  1.1283456303366877 ; l2 norm of weights:  7.20295476196499\n",
            "Iteration#:  3332 ; Loss:  3.070952065462708 ; l2 norm of gradient:  1.1283282079212802 ; l2 norm of weights:  7.202937037725468\n",
            "Iteration#:  3336 ; Loss:  3.0709008809095537 ; l2 norm of gradient:  1.128310785600848 ; l2 norm of weights:  7.202919313744423\n",
            "Iteration#:  3340 ; Loss:  3.0708496977213344 ; l2 norm of gradient:  1.1282933633753174 ; l2 norm of weights:  7.202901590021846\n",
            "Iteration#:  3344 ; Loss:  3.070798515891423 ; l2 norm of gradient:  1.1282759412446162 ; l2 norm of weights:  7.202883866557734\n",
            "Iteration#:  3348 ; Loss:  3.070747335848121 ; l2 norm of gradient:  1.1282585192086712 ; l2 norm of weights:  7.202866143352079\n",
            "Iteration#:  3352 ; Loss:  3.070696157008294 ; l2 norm of gradient:  1.128241097267409 ; l2 norm of weights:  7.202848420404871\n",
            "Iteration#:  3356 ; Loss:  3.070644979916688 ; l2 norm of gradient:  1.1282236754207589 ; l2 norm of weights:  7.202830697716105\n",
            "Iteration#:  3360 ; Loss:  3.0705938045268777 ; l2 norm of gradient:  1.1282062536686464 ; l2 norm of weights:  7.202812975285774\n",
            "Iteration#:  3364 ; Loss:  3.0705426301699483 ; l2 norm of gradient:  1.1281888320109992 ; l2 norm of weights:  7.20279525311387\n",
            "Iteration#:  3368 ; Loss:  3.0704914582203844 ; l2 norm of gradient:  1.1281714104477452 ; l2 norm of weights:  7.202777531200387\n",
            "Iteration#:  3372 ; Loss:  3.070440287075507 ; l2 norm of gradient:  1.1281539889788115 ; l2 norm of weights:  7.202759809545318\n",
            "Iteration#:  3376 ; Loss:  3.070389118741488 ; l2 norm of gradient:  1.1281365676041253 ; l2 norm of weights:  7.2027420881486535\n",
            "Iteration#:  3380 ; Loss:  3.0703379511983293 ; l2 norm of gradient:  1.1281191463236149 ; l2 norm of weights:  7.20272436701039\n",
            "Iteration#:  3384 ; Loss:  3.0702867852978106 ; l2 norm of gradient:  1.128101725137207 ; l2 norm of weights:  7.202706646130518\n",
            "Iteration#:  3388 ; Loss:  3.0702356203959797 ; l2 norm of gradient:  1.1280843040448278 ; l2 norm of weights:  7.2026889255090305\n",
            "Iteration#:  3392 ; Loss:  3.070184457351836 ; l2 norm of gradient:  1.1280668830464067 ; l2 norm of weights:  7.202671205145921\n",
            "Iteration#:  3396 ; Loss:  3.070133296254712 ; l2 norm of gradient:  1.1280494621418706 ; l2 norm of weights:  7.202653485041184\n",
            "Iteration#:  3400 ; Loss:  3.070082136347547 ; l2 norm of gradient:  1.1280320413311469 ; l2 norm of weights:  7.202635765194809\n",
            "Iteration#:  3404 ; Loss:  3.0700309783638806 ; l2 norm of gradient:  1.1280146206141628 ; l2 norm of weights:  7.20261804560679\n",
            "Iteration#:  3408 ; Loss:  3.0699798216776926 ; l2 norm of gradient:  1.1279971999908456 ; l2 norm of weights:  7.202600326277122\n",
            "Iteration#:  3412 ; Loss:  3.069928666335758 ; l2 norm of gradient:  1.1279797794611242 ; l2 norm of weights:  7.202582607205796\n",
            "Iteration#:  3416 ; Loss:  3.0698775138153644 ; l2 norm of gradient:  1.1279623590249246 ; l2 norm of weights:  7.202564888392805\n",
            "Iteration#:  3420 ; Loss:  3.069826361749925 ; l2 norm of gradient:  1.127944938682175 ; l2 norm of weights:  7.202547169838144\n",
            "Iteration#:  3424 ; Loss:  3.069775211716528 ; l2 norm of gradient:  1.1279275184328033 ; l2 norm of weights:  7.202529451541803\n",
            "Iteration#:  3428 ; Loss:  3.0697240627271922 ; l2 norm of gradient:  1.1279100982767358 ; l2 norm of weights:  7.202511733503777\n",
            "Iteration#:  3432 ; Loss:  3.069672915505272 ; l2 norm of gradient:  1.1278926782139023 ; l2 norm of weights:  7.202494015724057\n",
            "Iteration#:  3436 ; Loss:  3.0696217701276622 ; l2 norm of gradient:  1.127875258244228 ; l2 norm of weights:  7.202476298202638\n",
            "Iteration#:  3440 ; Loss:  3.0695706258592486 ; l2 norm of gradient:  1.1278578383676419 ; l2 norm of weights:  7.202458580939512\n",
            "Iteration#:  3444 ; Loss:  3.0695194831319634 ; l2 norm of gradient:  1.1278404185840722 ; l2 norm of weights:  7.202440863934671\n",
            "Iteration#:  3448 ; Loss:  3.0694683423995586 ; l2 norm of gradient:  1.127822998893446 ; l2 norm of weights:  7.2024231471881075\n",
            "Iteration#:  3452 ; Loss:  3.069417203123076 ; l2 norm of gradient:  1.1278055792956896 ; l2 norm of weights:  7.202405430699818\n",
            "Iteration#:  3456 ; Loss:  3.0693660656069706 ; l2 norm of gradient:  1.1277881597907338 ; l2 norm of weights:  7.202387714469791\n",
            "Iteration#:  3460 ; Loss:  3.069314930079893 ; l2 norm of gradient:  1.1277707403785031 ; l2 norm of weights:  7.202369998498022\n",
            "Iteration#:  3464 ; Loss:  3.0692637953677826 ; l2 norm of gradient:  1.1277533210589268 ; l2 norm of weights:  7.202352282784505\n",
            "Iteration#:  3468 ; Loss:  3.069212662675929 ; l2 norm of gradient:  1.1277359018319317 ; l2 norm of weights:  7.202334567329229\n",
            "Iteration#:  3472 ; Loss:  3.069161530916839 ; l2 norm of gradient:  1.127718482697448 ; l2 norm of weights:  7.20231685213219\n",
            "Iteration#:  3476 ; Loss:  3.069110401021466 ; l2 norm of gradient:  1.1277010636554015 ; l2 norm of weights:  7.202299137193381\n",
            "Iteration#:  3480 ; Loss:  3.069059272883281 ; l2 norm of gradient:  1.1276836447057201 ; l2 norm of weights:  7.202281422512794\n",
            "Iteration#:  3484 ; Loss:  3.0690081461247773 ; l2 norm of gradient:  1.1276662258483319 ; l2 norm of weights:  7.20226370809042\n",
            "Iteration#:  3488 ; Loss:  3.0689570207949823 ; l2 norm of gradient:  1.1276488070831647 ; l2 norm of weights:  7.202245993926256\n",
            "Iteration#:  3492 ; Loss:  3.0689058974757977 ; l2 norm of gradient:  1.1276313884101465 ; l2 norm of weights:  7.202228280020291\n",
            "Iteration#:  3496 ; Loss:  3.0688547752978175 ; l2 norm of gradient:  1.1276139698292045 ; l2 norm of weights:  7.20221056637252\n",
            "Iteration#:  3500 ; Loss:  3.0688036547819944 ; l2 norm of gradient:  1.127596551340268 ; l2 norm of weights:  7.202192852982937\n",
            "Iteration#:  3504 ; Loss:  3.0687525369852047 ; l2 norm of gradient:  1.1275791329432636 ; l2 norm of weights:  7.202175139851533\n",
            "Iteration#:  3508 ; Loss:  3.0687014195063935 ; l2 norm of gradient:  1.1275617146381196 ; l2 norm of weights:  7.2021574269783\n",
            "Iteration#:  3512 ; Loss:  3.068650303725568 ; l2 norm of gradient:  1.1275442964247642 ; l2 norm of weights:  7.202139714363234\n",
            "Iteration#:  3516 ; Loss:  3.0685991893258495 ; l2 norm of gradient:  1.127526878303125 ; l2 norm of weights:  7.202122002006326\n",
            "Iteration#:  3520 ; Loss:  3.0685480772337925 ; l2 norm of gradient:  1.1275094602731295 ; l2 norm of weights:  7.202104289907569\n",
            "Iteration#:  3524 ; Loss:  3.068496965853073 ; l2 norm of gradient:  1.127492042334707 ; l2 norm of weights:  7.202086578066956\n",
            "Iteration#:  3528 ; Loss:  3.068445856434711 ; l2 norm of gradient:  1.1274746244877853 ; l2 norm of weights:  7.2020688664844785\n",
            "Iteration#:  3532 ; Loss:  3.0683947489245886 ; l2 norm of gradient:  1.1274572067322917 ; l2 norm of weights:  7.202051155160133\n",
            "Iteration#:  3536 ; Loss:  3.068343642520157 ; l2 norm of gradient:  1.127439789068155 ; l2 norm of weights:  7.202033444093908\n",
            "Iteration#:  3540 ; Loss:  3.0682925377337864 ; l2 norm of gradient:  1.127422371495302 ; l2 norm of weights:  7.202015733285801\n",
            "Iteration#:  3544 ; Loss:  3.06824143437908 ; l2 norm of gradient:  1.127404954013661 ; l2 norm of weights:  7.2019980227358005\n",
            "Iteration#:  3548 ; Loss:  3.0681903329610822 ; l2 norm of gradient:  1.1273875366231618 ; l2 norm of weights:  7.2019803124439035\n",
            "Iteration#:  3552 ; Loss:  3.0681392340127718 ; l2 norm of gradient:  1.1273701193237302 ; l2 norm of weights:  7.2019626024101\n",
            "Iteration#:  3556 ; Loss:  3.068088135331744 ; l2 norm of gradient:  1.1273527021152958 ; l2 norm of weights:  7.201944892634384\n",
            "Iteration#:  3560 ; Loss:  3.068037038387239 ; l2 norm of gradient:  1.1273352849977873 ; l2 norm of weights:  7.201927183116747\n",
            "Iteration#:  3564 ; Loss:  3.067985943393695 ; l2 norm of gradient:  1.1273178679711318 ; l2 norm of weights:  7.201909473857186\n",
            "Iteration#:  3568 ; Loss:  3.0679348495851357 ; l2 norm of gradient:  1.1273004510352564 ; l2 norm of weights:  7.201891764855689\n",
            "Iteration#:  3572 ; Loss:  3.0678837573893882 ; l2 norm of gradient:  1.1272830341900908 ; l2 norm of weights:  7.201874056112252\n",
            "Iteration#:  3576 ; Loss:  3.067832666784247 ; l2 norm of gradient:  1.1272656174355642 ; l2 norm of weights:  7.201856347626866\n",
            "Iteration#:  3580 ; Loss:  3.067781577892521 ; l2 norm of gradient:  1.1272482007716023 ; l2 norm of weights:  7.201838639399525\n",
            "Iteration#:  3584 ; Loss:  3.0677304905628247 ; l2 norm of gradient:  1.127230784198135 ; l2 norm of weights:  7.2018209314302215\n",
            "Iteration#:  3588 ; Loss:  3.067679404718188 ; l2 norm of gradient:  1.1272133677150897 ; l2 norm of weights:  7.201803223718949\n",
            "Iteration#:  3592 ; Loss:  3.0676283200507846 ; l2 norm of gradient:  1.1271959513223955 ; l2 norm of weights:  7.2017855162657005\n",
            "Iteration#:  3596 ; Loss:  3.06757723787995 ; l2 norm of gradient:  1.1271785350199806 ; l2 norm of weights:  7.201767809070468\n",
            "Iteration#:  3600 ; Loss:  3.0675261565854295 ; l2 norm of gradient:  1.127161118807773 ; l2 norm of weights:  7.2017501021332455\n",
            "Iteration#:  3604 ; Loss:  3.06747507751618 ; l2 norm of gradient:  1.1271437026857014 ; l2 norm of weights:  7.201732395454025\n",
            "Iteration#:  3608 ; Loss:  3.0674239996775947 ; l2 norm of gradient:  1.1271262866536926 ; l2 norm of weights:  7.2017146890328005\n",
            "Iteration#:  3612 ; Loss:  3.067372923266276 ; l2 norm of gradient:  1.127108870711677 ; l2 norm of weights:  7.201696982869563\n",
            "Iteration#:  3616 ; Loss:  3.0673218486337697 ; l2 norm of gradient:  1.1270914548595823 ; l2 norm of weights:  7.201679276964305\n",
            "Iteration#:  3620 ; Loss:  3.067270775359117 ; l2 norm of gradient:  1.1270740390973366 ; l2 norm of weights:  7.2016615713170244\n",
            "Iteration#:  3624 ; Loss:  3.0672197035211366 ; l2 norm of gradient:  1.1270566234248678 ; l2 norm of weights:  7.201643865927709\n",
            "Iteration#:  3628 ; Loss:  3.0671686332149797 ; l2 norm of gradient:  1.1270392078421057 ; l2 norm of weights:  7.201626160796353\n",
            "Iteration#:  3632 ; Loss:  3.0671175648215145 ; l2 norm of gradient:  1.1270217923489783 ; l2 norm of weights:  7.20160845592295\n",
            "Iteration#:  3636 ; Loss:  3.0670664982100355 ; l2 norm of gradient:  1.1270043769454134 ; l2 norm of weights:  7.201590751307493\n",
            "Iteration#:  3640 ; Loss:  3.067015432458273 ; l2 norm of gradient:  1.1269869616313397 ; l2 norm of weights:  7.2015730469499735\n",
            "Iteration#:  3644 ; Loss:  3.0669643688157837 ; l2 norm of gradient:  1.1269695464066858 ; l2 norm of weights:  7.201555342850386\n",
            "Iteration#:  3648 ; Loss:  3.0669133062696115 ; l2 norm of gradient:  1.1269521312713813 ; l2 norm of weights:  7.201537639008723\n",
            "Iteration#:  3652 ; Loss:  3.0668622455970187 ; l2 norm of gradient:  1.1269347162253525 ; l2 norm of weights:  7.201519935424978\n",
            "Iteration#:  3656 ; Loss:  3.0668111863385428 ; l2 norm of gradient:  1.12691730126853 ; l2 norm of weights:  7.201502232099142\n",
            "Iteration#:  3660 ; Loss:  3.0667601300584106 ; l2 norm of gradient:  1.1268998864008417 ; l2 norm of weights:  7.201484529031209\n",
            "Iteration#:  3664 ; Loss:  3.066709073935689 ; l2 norm of gradient:  1.1268824716222157 ; l2 norm of weights:  7.201466826221173\n",
            "Iteration#:  3668 ; Loss:  3.06665801941656 ; l2 norm of gradient:  1.1268650569325813 ; l2 norm of weights:  7.201449123669024\n",
            "Iteration#:  3672 ; Loss:  3.0666069664931426 ; l2 norm of gradient:  1.126847642331866 ; l2 norm of weights:  7.2014314213747594\n",
            "Iteration#:  3676 ; Loss:  3.0665559151806647 ; l2 norm of gradient:  1.1268302278199995 ; l2 norm of weights:  7.201413719338368\n",
            "Iteration#:  3680 ; Loss:  3.0665048654622424 ; l2 norm of gradient:  1.1268128133969106 ; l2 norm of weights:  7.201396017559843\n",
            "Iteration#:  3684 ; Loss:  3.066453817773837 ; l2 norm of gradient:  1.1267953990625277 ; l2 norm of weights:  7.2013783160391815\n",
            "Iteration#:  3688 ; Loss:  3.0664027712651087 ; l2 norm of gradient:  1.1267779848167785 ; l2 norm of weights:  7.201360614776372\n",
            "Iteration#:  3692 ; Loss:  3.0663517261286595 ; l2 norm of gradient:  1.1267605706595927 ; l2 norm of weights:  7.201342913771408\n",
            "Iteration#:  3696 ; Loss:  3.0663006827706076 ; l2 norm of gradient:  1.1267431565908994 ; l2 norm of weights:  7.201325213024285\n",
            "Iteration#:  3700 ; Loss:  3.0662496408238993 ; l2 norm of gradient:  1.1267257426106272 ; l2 norm of weights:  7.201307512534993\n",
            "Iteration#:  3704 ; Loss:  3.0661986005027386 ; l2 norm of gradient:  1.1267083287187036 ; l2 norm of weights:  7.201289812303527\n",
            "Iteration#:  3708 ; Loss:  3.066147562006952 ; l2 norm of gradient:  1.1266909149150581 ; l2 norm of weights:  7.201272112329878\n",
            "Iteration#:  3712 ; Loss:  3.0660965248026515 ; l2 norm of gradient:  1.1266735011996207 ; l2 norm of weights:  7.201254412614039\n",
            "Iteration#:  3716 ; Loss:  3.066045489032583 ; l2 norm of gradient:  1.1266560875723188 ; l2 norm of weights:  7.201236713156007\n",
            "Iteration#:  3720 ; Loss:  3.065994456483084 ; l2 norm of gradient:  1.1266386740330812 ; l2 norm of weights:  7.201219013955769\n",
            "Iteration#:  3724 ; Loss:  3.065943423918119 ; l2 norm of gradient:  1.1266212605818373 ; l2 norm of weights:  7.201201315013322\n",
            "Iteration#:  3728 ; Loss:  3.065892392961483 ; l2 norm of gradient:  1.1266038472185167 ; l2 norm of weights:  7.201183616328657\n",
            "Iteration#:  3732 ; Loss:  3.065841363770199 ; l2 norm of gradient:  1.126586433943047 ; l2 norm of weights:  7.2011659179017675\n",
            "Iteration#:  3736 ; Loss:  3.065790336012169 ; l2 norm of gradient:  1.1265690207553571 ; l2 norm of weights:  7.201148219732646\n",
            "Iteration#:  3740 ; Loss:  3.065739309794983 ; l2 norm of gradient:  1.1265516076553765 ; l2 norm of weights:  7.2011305218212875\n",
            "Iteration#:  3744 ; Loss:  3.0656882854106033 ; l2 norm of gradient:  1.1265341946430338 ; l2 norm of weights:  7.201112824167683\n",
            "Iteration#:  3748 ; Loss:  3.0656372624642034 ; l2 norm of gradient:  1.1265167817182586 ; l2 norm of weights:  7.201095126771825\n",
            "Iteration#:  3752 ; Loss:  3.065586240799628 ; l2 norm of gradient:  1.1264993688809795 ; l2 norm of weights:  7.201077429633706\n",
            "Iteration#:  3756 ; Loss:  3.0655352210627216 ; l2 norm of gradient:  1.1264819561311248 ; l2 norm of weights:  7.201059732753322\n",
            "Iteration#:  3760 ; Loss:  3.0654842027881726 ; l2 norm of gradient:  1.1264645434686245 ; l2 norm of weights:  7.201042036130662\n",
            "Iteration#:  3764 ; Loss:  3.065433185995797 ; l2 norm of gradient:  1.1264471308934072 ; l2 norm of weights:  7.201024339765722\n",
            "Iteration#:  3768 ; Loss:  3.065382171413436 ; l2 norm of gradient:  1.1264297184054017 ; l2 norm of weights:  7.201006643658493\n",
            "Iteration#:  3772 ; Loss:  3.065331157755881 ; l2 norm of gradient:  1.1264123060045383 ; l2 norm of weights:  7.200988947808969\n",
            "Iteration#:  3776 ; Loss:  3.06528014570771 ; l2 norm of gradient:  1.1263948936907444 ; l2 norm of weights:  7.200971252217142\n",
            "Iteration#:  3780 ; Loss:  3.065229135513819 ; l2 norm of gradient:  1.1263774814639498 ; l2 norm of weights:  7.2009535568830065\n",
            "Iteration#:  3784 ; Loss:  3.0651781272845886 ; l2 norm of gradient:  1.1263600693240836 ; l2 norm of weights:  7.200935861806553\n",
            "Iteration#:  3788 ; Loss:  3.0651271199826553 ; l2 norm of gradient:  1.1263426572710764 ; l2 norm of weights:  7.200918166987777\n",
            "Iteration#:  3792 ; Loss:  3.0650761143418843 ; l2 norm of gradient:  1.126325245304855 ; l2 norm of weights:  7.200900472426669\n",
            "Iteration#:  3796 ; Loss:  3.0650251103050126 ; l2 norm of gradient:  1.1263078334253498 ; l2 norm of weights:  7.200882778123224\n",
            "Iteration#:  3800 ; Loss:  3.064974107641003 ; l2 norm of gradient:  1.126290421632489 ; l2 norm of weights:  7.200865084077433\n",
            "Iteration#:  3804 ; Loss:  3.064923106804639 ; l2 norm of gradient:  1.1262730099262024 ; l2 norm of weights:  7.200847390289292\n",
            "Iteration#:  3808 ; Loss:  3.0648721071001637 ; l2 norm of gradient:  1.126255598306421 ; l2 norm of weights:  7.20082969675879\n",
            "Iteration#:  3812 ; Loss:  3.0648211094040727 ; l2 norm of gradient:  1.126238186773071 ; l2 norm of weights:  7.200812003485922\n",
            "Iteration#:  3816 ; Loss:  3.06477011321021 ; l2 norm of gradient:  1.126220775326084 ; l2 norm of weights:  7.200794310470681\n",
            "Iteration#:  3820 ; Loss:  3.0647191184716163 ; l2 norm of gradient:  1.1262033639653872 ; l2 norm of weights:  7.2007766177130605\n",
            "Iteration#:  3824 ; Loss:  3.0646681254187618 ; l2 norm of gradient:  1.1261859526909126 ; l2 norm of weights:  7.200758925213051\n",
            "Iteration#:  3828 ; Loss:  3.0646171338736035 ; l2 norm of gradient:  1.1261685415025866 ; l2 norm of weights:  7.200741232970649\n",
            "Iteration#:  3832 ; Loss:  3.064566143705046 ; l2 norm of gradient:  1.1261511304003409 ; l2 norm of weights:  7.200723540985842\n",
            "Iteration#:  3836 ; Loss:  3.0645151555225274 ; l2 norm of gradient:  1.1261337193841028 ; l2 norm of weights:  7.200705849258627\n",
            "Iteration#:  3840 ; Loss:  3.064464168558291 ; l2 norm of gradient:  1.126116308453804 ; l2 norm of weights:  7.200688157788998\n",
            "Iteration#:  3844 ; Loss:  3.0644131834291337 ; l2 norm of gradient:  1.1260988976093727 ; l2 norm of weights:  7.200670466576946\n",
            "Iteration#:  3848 ; Loss:  3.0643621997400814 ; l2 norm of gradient:  1.1260814868507372 ; l2 norm of weights:  7.200652775622462\n",
            "Iteration#:  3852 ; Loss:  3.064311217581178 ; l2 norm of gradient:  1.1260640761778287 ; l2 norm of weights:  7.200635084925543\n",
            "Iteration#:  3856 ; Loss:  3.0642602378023134 ; l2 norm of gradient:  1.1260466655905752 ; l2 norm of weights:  7.200617394486178\n",
            "Iteration#:  3860 ; Loss:  3.0642092589218697 ; l2 norm of gradient:  1.1260292550889075 ; l2 norm of weights:  7.200599704304363\n",
            "Iteration#:  3864 ; Loss:  3.0641582813158665 ; l2 norm of gradient:  1.1260118446727538 ; l2 norm of weights:  7.200582014380088\n",
            "Iteration#:  3868 ; Loss:  3.064107305562437 ; l2 norm of gradient:  1.1259944343420456 ; l2 norm of weights:  7.2005643247133495\n",
            "Iteration#:  3872 ; Loss:  3.0640563314104106 ; l2 norm of gradient:  1.1259770240967097 ; l2 norm of weights:  7.200546635304137\n",
            "Iteration#:  3876 ; Loss:  3.064005358611005 ; l2 norm of gradient:  1.1259596139366785 ; l2 norm of weights:  7.2005289461524455\n",
            "Iteration#:  3880 ; Loss:  3.0639543874925375 ; l2 norm of gradient:  1.1259422038618794 ; l2 norm of weights:  7.200511257258267\n",
            "Iteration#:  3884 ; Loss:  3.0639034179016287 ; l2 norm of gradient:  1.1259247938722428 ; l2 norm of weights:  7.200493568621596\n",
            "Iteration#:  3888 ; Loss:  3.063852449494717 ; l2 norm of gradient:  1.1259073839676972 ; l2 norm of weights:  7.200475880242422\n",
            "Iteration#:  3892 ; Loss:  3.063801483354516 ; l2 norm of gradient:  1.1258899741481747 ; l2 norm of weights:  7.20045819212074\n",
            "Iteration#:  3896 ; Loss:  3.063750518403459 ; l2 norm of gradient:  1.1258725644136027 ; l2 norm of weights:  7.200440504256544\n",
            "Iteration#:  3900 ; Loss:  3.0636995550752455 ; l2 norm of gradient:  1.125855154763912 ; l2 norm of weights:  7.200422816649826\n",
            "Iteration#:  3904 ; Loss:  3.0636485933194653 ; l2 norm of gradient:  1.1258377451990316 ; l2 norm of weights:  7.200405129300578\n",
            "Iteration#:  3908 ; Loss:  3.063597633337218 ; l2 norm of gradient:  1.1258203357188918 ; l2 norm of weights:  7.200387442208794\n",
            "Iteration#:  3912 ; Loss:  3.0635466748712403 ; l2 norm of gradient:  1.1258029263234217 ; l2 norm of weights:  7.200369755374465\n",
            "Iteration#:  3916 ; Loss:  3.063495717926025 ; l2 norm of gradient:  1.1257855170125506 ; l2 norm of weights:  7.200352068797588\n",
            "Iteration#:  3920 ; Loss:  3.0634447622013825 ; l2 norm of gradient:  1.1257681077862094 ; l2 norm of weights:  7.200334382478152\n",
            "Iteration#:  3924 ; Loss:  3.0633938086165244 ; l2 norm of gradient:  1.1257506986443275 ; l2 norm of weights:  7.200316696416151\n",
            "Iteration#:  3928 ; Loss:  3.063342856399722 ; l2 norm of gradient:  1.1257332895868344 ; l2 norm of weights:  7.20029901061158\n",
            "Iteration#:  3932 ; Loss:  3.0632919054765044 ; l2 norm of gradient:  1.1257158806136602 ; l2 norm of weights:  7.200281325064427\n",
            "Iteration#:  3936 ; Loss:  3.0632409562144556 ; l2 norm of gradient:  1.125698471724734 ; l2 norm of weights:  7.200263639774691\n",
            "Iteration#:  3940 ; Loss:  3.0631900086249098 ; l2 norm of gradient:  1.125681062919988 ; l2 norm of weights:  7.2002459547423605\n",
            "Iteration#:  3944 ; Loss:  3.0631390632791247 ; l2 norm of gradient:  1.1256636541993492 ; l2 norm of weights:  7.20022826996743\n",
            "Iteration#:  3948 ; Loss:  3.0630881190745165 ; l2 norm of gradient:  1.125646245562748 ; l2 norm of weights:  7.2002105854498915\n",
            "Iteration#:  3952 ; Loss:  3.0630371762114987 ; l2 norm of gradient:  1.125628837010115 ; l2 norm of weights:  7.20019290118974\n",
            "Iteration#:  3956 ; Loss:  3.062986234823777 ; l2 norm of gradient:  1.125611428541381 ; l2 norm of weights:  7.200175217186967\n",
            "Iteration#:  3960 ; Loss:  3.062935295165683 ; l2 norm of gradient:  1.1255940201564734 ; l2 norm of weights:  7.2001575334415655\n",
            "Iteration#:  3964 ; Loss:  3.0628843570519066 ; l2 norm of gradient:  1.1255766118553256 ; l2 norm of weights:  7.200139849953528\n",
            "Iteration#:  3968 ; Loss:  3.0628334206022307 ; l2 norm of gradient:  1.1255592036378643 ; l2 norm of weights:  7.200122166722848\n",
            "Iteration#:  3972 ; Loss:  3.0627824852743646 ; l2 norm of gradient:  1.12554179550402 ; l2 norm of weights:  7.200104483749517\n",
            "Iteration#:  3976 ; Loss:  3.06273155170011 ; l2 norm of gradient:  1.1255243874537255 ; l2 norm of weights:  7.20008680103353\n",
            "Iteration#:  3980 ; Loss:  3.0626806198152923 ; l2 norm of gradient:  1.1255069794869075 ; l2 norm of weights:  7.20006911857488\n",
            "Iteration#:  3984 ; Loss:  3.062629689702856 ; l2 norm of gradient:  1.1254895716034978 ; l2 norm of weights:  7.200051436373558\n",
            "Iteration#:  3988 ; Loss:  3.0625787609383295 ; l2 norm of gradient:  1.1254721638034266 ; l2 norm of weights:  7.200033754429558\n",
            "Iteration#:  3992 ; Loss:  3.0625278339544444 ; l2 norm of gradient:  1.1254547560866224 ; l2 norm of weights:  7.200016072742873\n",
            "Iteration#:  3996 ; Loss:  3.0624769078121075 ; l2 norm of gradient:  1.1254373484530167 ; l2 norm of weights:  7.199998391313495\n",
            "Iteration#:  4000 ; Loss:  3.0624259841009813 ; l2 norm of gradient:  1.1254199409025396 ; l2 norm of weights:  7.199980710141419\n",
            "Iteration#:  4004 ; Loss:  3.0623750615711494 ; l2 norm of gradient:  1.1254025334351212 ; l2 norm of weights:  7.199963029226635\n",
            "Iteration#:  4008 ; Loss:  3.062324140745911 ; l2 norm of gradient:  1.125385126050691 ; l2 norm of weights:  7.1999453485691385\n",
            "Iteration#:  4012 ; Loss:  3.062273221346465 ; l2 norm of gradient:  1.1253677187491797 ; l2 norm of weights:  7.199927668168921\n",
            "Iteration#:  4016 ; Loss:  3.062222303718172 ; l2 norm of gradient:  1.1253503115305172 ; l2 norm of weights:  7.1999099880259765\n",
            "Iteration#:  4020 ; Loss:  3.062171387183497 ; l2 norm of gradient:  1.1253329043946334 ; l2 norm of weights:  7.199892308140297\n",
            "Iteration#:  4024 ; Loss:  3.062120472590013 ; l2 norm of gradient:  1.1253154973414603 ; l2 norm of weights:  7.199874628511874\n",
            "Iteration#:  4028 ; Loss:  3.0620695597656944 ; l2 norm of gradient:  1.1252980903709253 ; l2 norm of weights:  7.1998569491407025\n",
            "Iteration#:  4032 ; Loss:  3.0620186483757776 ; l2 norm of gradient:  1.1252806834829607 ; l2 norm of weights:  7.199839270026776\n",
            "Iteration#:  4036 ; Loss:  3.061967738335665 ; l2 norm of gradient:  1.125263276677497 ; l2 norm of weights:  7.199821591170085\n",
            "Iteration#:  4040 ; Loss:  3.0619168297378065 ; l2 norm of gradient:  1.1252458699544645 ; l2 norm of weights:  7.199803912570625\n",
            "Iteration#:  4044 ; Loss:  3.0618659231455325 ; l2 norm of gradient:  1.1252284633137912 ; l2 norm of weights:  7.199786234228387\n",
            "Iteration#:  4048 ; Loss:  3.0618150178482266 ; l2 norm of gradient:  1.12521105675541 ; l2 norm of weights:  7.199768556143364\n",
            "Iteration#:  4052 ; Loss:  3.0617641149805648 ; l2 norm of gradient:  1.12519365027925 ; l2 norm of weights:  7.19975087831555\n",
            "Iteration#:  4056 ; Loss:  3.061713213134095 ; l2 norm of gradient:  1.1251762438852422 ; l2 norm of weights:  7.199733200744937\n",
            "Iteration#:  4060 ; Loss:  3.0616623122154936 ; l2 norm of gradient:  1.1251588375733172 ; l2 norm of weights:  7.199715523431519\n",
            "Iteration#:  4064 ; Loss:  3.061611413380226 ; l2 norm of gradient:  1.125141431343405 ; l2 norm of weights:  7.1996978463752885\n",
            "Iteration#:  4068 ; Loss:  3.061560516081329 ; l2 norm of gradient:  1.1251240251954366 ; l2 norm of weights:  7.199680169576237\n",
            "Iteration#:  4072 ; Loss:  3.0615096203709498 ; l2 norm of gradient:  1.1251066191293413 ; l2 norm of weights:  7.199662493034359\n",
            "Iteration#:  4076 ; Loss:  3.061458725853431 ; l2 norm of gradient:  1.12508921314505 ; l2 norm of weights:  7.199644816749648\n",
            "Iteration#:  4080 ; Loss:  3.061407833254723 ; l2 norm of gradient:  1.1250718072424943 ; l2 norm of weights:  7.199627140722094\n",
            "Iteration#:  4084 ; Loss:  3.061356942352743 ; l2 norm of gradient:  1.125054401421603 ; l2 norm of weights:  7.199609464951693\n",
            "Iteration#:  4088 ; Loss:  3.061306053039753 ; l2 norm of gradient:  1.125036995682309 ; l2 norm of weights:  7.199591789438435\n",
            "Iteration#:  4092 ; Loss:  3.061255164704587 ; l2 norm of gradient:  1.1250195900245403 ; l2 norm of weights:  7.199574114182315\n",
            "Iteration#:  4096 ; Loss:  3.0612042783451145 ; l2 norm of gradient:  1.1250021844482292 ; l2 norm of weights:  7.199556439183327\n",
            "Iteration#:  4100 ; Loss:  3.0611533938378637 ; l2 norm of gradient:  1.1249847789533052 ; l2 norm of weights:  7.199538764441462\n",
            "Iteration#:  4104 ; Loss:  3.0611025105178076 ; l2 norm of gradient:  1.1249673735396992 ; l2 norm of weights:  7.199521089956713\n",
            "Iteration#:  4108 ; Loss:  3.0610516285696496 ; l2 norm of gradient:  1.124949968207343 ; l2 norm of weights:  7.199503415729073\n",
            "Iteration#:  4112 ; Loss:  3.0610007485563284 ; l2 norm of gradient:  1.1249325629561653 ; l2 norm of weights:  7.199485741758535\n",
            "Iteration#:  4116 ; Loss:  3.060949870040515 ; l2 norm of gradient:  1.124915157786099 ; l2 norm of weights:  7.199468068045093\n",
            "Iteration#:  4120 ; Loss:  3.0608989932139377 ; l2 norm of gradient:  1.1248977526970734 ; l2 norm of weights:  7.199450394588738\n",
            "Iteration#:  4124 ; Loss:  3.0608481177395244 ; l2 norm of gradient:  1.1248803476890188 ; l2 norm of weights:  7.199432721389464\n",
            "Iteration#:  4128 ; Loss:  3.0607972438849558 ; l2 norm of gradient:  1.1248629427618677 ; l2 norm of weights:  7.199415048447264\n",
            "Iteration#:  4132 ; Loss:  3.060746371688578 ; l2 norm of gradient:  1.1248455379155489 ; l2 norm of weights:  7.199397375762131\n",
            "Iteration#:  4136 ; Loss:  3.060695500766805 ; l2 norm of gradient:  1.1248281331499947 ; l2 norm of weights:  7.199379703334058\n",
            "Iteration#:  4140 ; Loss:  3.0606446316874933 ; l2 norm of gradient:  1.1248107284651345 ; l2 norm of weights:  7.1993620311630355\n",
            "Iteration#:  4144 ; Loss:  3.0605937642428795 ; l2 norm of gradient:  1.1247933238609005 ; l2 norm of weights:  7.199344359249061\n",
            "Iteration#:  4148 ; Loss:  3.060542898041563 ; l2 norm of gradient:  1.1247759193372227 ; l2 norm of weights:  7.199326687592123\n",
            "Iteration#:  4152 ; Loss:  3.060492033539142 ; l2 norm of gradient:  1.124758514894032 ; l2 norm of weights:  7.199309016192217\n",
            "Iteration#:  4156 ; Loss:  3.0604411709532635 ; l2 norm of gradient:  1.1247411105312604 ; l2 norm of weights:  7.1992913450493345\n",
            "Iteration#:  4160 ; Loss:  3.0603903092983344 ; l2 norm of gradient:  1.1247237062488373 ; l2 norm of weights:  7.199273674163469\n",
            "Iteration#:  4164 ; Loss:  3.0603394496735996 ; l2 norm of gradient:  1.1247063020466948 ; l2 norm of weights:  7.199256003534615\n",
            "Iteration#:  4168 ; Loss:  3.060288591632645 ; l2 norm of gradient:  1.1246888979247627 ; l2 norm of weights:  7.199238333162763\n",
            "Iteration#:  4172 ; Loss:  3.0602377350558165 ; l2 norm of gradient:  1.124671493882973 ; l2 norm of weights:  7.199220663047906\n",
            "Iteration#:  4176 ; Loss:  3.060186880060885 ; l2 norm of gradient:  1.124654089921256 ; l2 norm of weights:  7.199202993190038\n",
            "Iteration#:  4180 ; Loss:  3.060136026592526 ; l2 norm of gradient:  1.1246366860395427 ; l2 norm of weights:  7.199185323589153\n",
            "Iteration#:  4184 ; Loss:  3.060085174466133 ; l2 norm of gradient:  1.124619282237765 ; l2 norm of weights:  7.199167654245241\n",
            "Iteration#:  4188 ; Loss:  3.0600343243581207 ; l2 norm of gradient:  1.124601878515852 ; l2 norm of weights:  7.199149985158297\n",
            "Iteration#:  4192 ; Loss:  3.059983475683045 ; l2 norm of gradient:  1.1245844748737377 ; l2 norm of weights:  7.1991323163283125\n",
            "Iteration#:  4196 ; Loss:  3.0599326283659107 ; l2 norm of gradient:  1.1245670713113503 ; l2 norm of weights:  7.199114647755283\n",
            "Iteration#:  4200 ; Loss:  3.059881782569494 ; l2 norm of gradient:  1.124549667828623 ; l2 norm of weights:  7.199096979439199\n",
            "Iteration#:  4204 ; Loss:  3.0598309388545513 ; l2 norm of gradient:  1.124532264425486 ; l2 norm of weights:  7.199079311380053\n",
            "Iteration#:  4208 ; Loss:  3.059780096097871 ; l2 norm of gradient:  1.1245148611018705 ; l2 norm of weights:  7.199061643577841\n",
            "Iteration#:  4212 ; Loss:  3.0597292552698474 ; l2 norm of gradient:  1.1244974578577076 ; l2 norm of weights:  7.199043976032552\n",
            "Iteration#:  4216 ; Loss:  3.0596784162595734 ; l2 norm of gradient:  1.1244800546929283 ; l2 norm of weights:  7.199026308744182\n",
            "Iteration#:  4220 ; Loss:  3.059627578051254 ; l2 norm of gradient:  1.124462651607464 ; l2 norm of weights:  7.199008641712721\n",
            "Iteration#:  4224 ; Loss:  3.0595767426437237 ; l2 norm of gradient:  1.1244452486012466 ; l2 norm of weights:  7.198990974938164\n",
            "Iteration#:  4228 ; Loss:  3.0595259080400847 ; l2 norm of gradient:  1.1244278456742067 ; l2 norm of weights:  7.198973308420505\n",
            "Iteration#:  4232 ; Loss:  3.0594750750469526 ; l2 norm of gradient:  1.1244104428262753 ; l2 norm of weights:  7.198955642159736\n",
            "Iteration#:  4236 ; Loss:  3.0594242436433716 ; l2 norm of gradient:  1.1243930400573845 ; l2 norm of weights:  7.198937976155848\n",
            "Iteration#:  4240 ; Loss:  3.059373413434163 ; l2 norm of gradient:  1.124375637367465 ; l2 norm of weights:  7.198920310408836\n",
            "Iteration#:  4244 ; Loss:  3.059322585233234 ; l2 norm of gradient:  1.1243582347564474 ; l2 norm of weights:  7.198902644918692\n",
            "Iteration#:  4248 ; Loss:  3.0592717582113136 ; l2 norm of gradient:  1.124340832224265 ; l2 norm of weights:  7.198884979685408\n",
            "Iteration#:  4252 ; Loss:  3.059220933210144 ; l2 norm of gradient:  1.1243234297708469 ; l2 norm of weights:  7.19886731470898\n",
            "Iteration#:  4256 ; Loss:  3.059170109490267 ; l2 norm of gradient:  1.1243060273961265 ; l2 norm of weights:  7.198849649989397\n",
            "Iteration#:  4260 ; Loss:  3.0591192872704642 ; l2 norm of gradient:  1.1242886251000337 ; l2 norm of weights:  7.198831985526655\n",
            "Iteration#:  4264 ; Loss:  3.0590684668323855 ; l2 norm of gradient:  1.1242712228825018 ; l2 norm of weights:  7.198814321320746\n",
            "Iteration#:  4268 ; Loss:  3.0590176474979947 ; l2 norm of gradient:  1.12425382074346 ; l2 norm of weights:  7.198796657371662\n",
            "Iteration#:  4272 ; Loss:  3.0589668304902835 ; l2 norm of gradient:  1.1242364186828417 ; l2 norm of weights:  7.198778993679396\n",
            "Iteration#:  4276 ; Loss:  3.0589160147571834 ; l2 norm of gradient:  1.124219016700576 ; l2 norm of weights:  7.198761330243943\n",
            "Iteration#:  4280 ; Loss:  3.058865200058418 ; l2 norm of gradient:  1.124201614796597 ; l2 norm of weights:  7.198743667065293\n",
            "Iteration#:  4284 ; Loss:  3.058814387772674 ; l2 norm of gradient:  1.124184212970835 ; l2 norm of weights:  7.198726004143441\n",
            "Iteration#:  4288 ; Loss:  3.058763576266257 ; l2 norm of gradient:  1.124166811223222 ; l2 norm of weights:  7.198708341478378\n",
            "Iteration#:  4292 ; Loss:  3.0587127669395873 ; l2 norm of gradient:  1.1241494095536886 ; l2 norm of weights:  7.1986906790701\n",
            "Iteration#:  4296 ; Loss:  3.05866195861974 ; l2 norm of gradient:  1.1241320079621673 ; l2 norm of weights:  7.1986730169185975\n",
            "Iteration#:  4300 ; Loss:  3.0586111524916317 ; l2 norm of gradient:  1.1241146064485896 ; l2 norm of weights:  7.198655355023864\n",
            "Iteration#:  4304 ; Loss:  3.058560347371102 ; l2 norm of gradient:  1.124097205012887 ; l2 norm of weights:  7.198637693385892\n",
            "Iteration#:  4308 ; Loss:  3.058509544363333 ; l2 norm of gradient:  1.1240798036549913 ; l2 norm of weights:  7.1986200320046745\n",
            "Iteration#:  4312 ; Loss:  3.058458742143789 ; l2 norm of gradient:  1.1240624023748336 ; l2 norm of weights:  7.198602370880207\n",
            "Iteration#:  4316 ; Loss:  3.0584079422366903 ; l2 norm of gradient:  1.1240450011723462 ; l2 norm of weights:  7.198584710012477\n",
            "Iteration#:  4320 ; Loss:  3.0583571438597037 ; l2 norm of gradient:  1.1240276000474605 ; l2 norm of weights:  7.198567049401482\n",
            "Iteration#:  4324 ; Loss:  3.058306346650619 ; l2 norm of gradient:  1.124010199000109 ; l2 norm of weights:  7.198549389047213\n",
            "Iteration#:  4328 ; Loss:  3.0582555514680547 ; l2 norm of gradient:  1.123992798030223 ; l2 norm of weights:  7.198531728949663\n",
            "Iteration#:  4332 ; Loss:  3.058204757171597 ; l2 norm of gradient:  1.1239753971377333 ; l2 norm of weights:  7.1985140691088265\n",
            "Iteration#:  4336 ; Loss:  3.05815396499783 ; l2 norm of gradient:  1.123957996322573 ; l2 norm of weights:  7.198496409524694\n",
            "Iteration#:  4340 ; Loss:  3.0581031742079645 ; l2 norm of gradient:  1.1239405955846724 ; l2 norm of weights:  7.19847875019726\n",
            "Iteration#:  4344 ; Loss:  3.058052385172795 ; l2 norm of gradient:  1.1239231949239663 ; l2 norm of weights:  7.198461091126516\n",
            "Iteration#:  4348 ; Loss:  3.0580015972697194 ; l2 norm of gradient:  1.1239057943403838 ; l2 norm of weights:  7.198443432312456\n",
            "Iteration#:  4352 ; Loss:  3.057950811422415 ; l2 norm of gradient:  1.1238883938338573 ; l2 norm of weights:  7.198425773755075\n",
            "Iteration#:  4356 ; Loss:  3.0579000267053513 ; l2 norm of gradient:  1.1238709934043183 ; l2 norm of weights:  7.198408115454361\n",
            "Iteration#:  4360 ; Loss:  3.0578492440595895 ; l2 norm of gradient:  1.1238535930517006 ; l2 norm of weights:  7.19839045741031\n",
            "Iteration#:  4364 ; Loss:  3.0577984624471757 ; l2 norm of gradient:  1.1238361927759353 ; l2 norm of weights:  7.198372799622915\n",
            "Iteration#:  4368 ; Loss:  3.0577476826813133 ; l2 norm of gradient:  1.1238187925769536 ; l2 norm of weights:  7.198355142092169\n",
            "Iteration#:  4372 ; Loss:  3.0576969044248274 ; l2 norm of gradient:  1.1238013924546872 ; l2 norm of weights:  7.198337484818064\n",
            "Iteration#:  4376 ; Loss:  3.057646127542998 ; l2 norm of gradient:  1.1237839924090698 ; l2 norm of weights:  7.198319827800595\n",
            "Iteration#:  4380 ; Loss:  3.057595352574196 ; l2 norm of gradient:  1.1237665924400329 ; l2 norm of weights:  7.19830217103975\n",
            "Iteration#:  4384 ; Loss:  3.0575445788783977 ; l2 norm of gradient:  1.1237491925475074 ; l2 norm of weights:  7.198284514535526\n",
            "Iteration#:  4388 ; Loss:  3.0574938071712894 ; l2 norm of gradient:  1.1237317927314265 ; l2 norm of weights:  7.198266858287916\n",
            "Iteration#:  4392 ; Loss:  3.0574430363589467 ; l2 norm of gradient:  1.1237143929917208 ; l2 norm of weights:  7.198249202296911\n",
            "Iteration#:  4396 ; Loss:  3.0573922677757737 ; l2 norm of gradient:  1.1236969933283252 ; l2 norm of weights:  7.198231546562506\n",
            "Iteration#:  4400 ; Loss:  3.0573415004817237 ; l2 norm of gradient:  1.1236795937411692 ; l2 norm of weights:  7.198213891084691\n",
            "Iteration#:  4404 ; Loss:  3.0572907350943566 ; l2 norm of gradient:  1.1236621942301865 ; l2 norm of weights:  7.198196235863462\n",
            "Iteration#:  4408 ; Loss:  3.057239970679196 ; l2 norm of gradient:  1.123644794795308 ; l2 norm of weights:  7.19817858089881\n",
            "Iteration#:  4412 ; Loss:  3.057189208326416 ; l2 norm of gradient:  1.1236273954364668 ; l2 norm of weights:  7.198160926190728\n",
            "Iteration#:  4416 ; Loss:  3.057138447165511 ; l2 norm of gradient:  1.1236099961535957 ; l2 norm of weights:  7.198143271739209\n",
            "Iteration#:  4420 ; Loss:  3.0570876874517334 ; l2 norm of gradient:  1.1235925969466258 ; l2 norm of weights:  7.198125617544248\n",
            "Iteration#:  4424 ; Loss:  3.057036929830511 ; l2 norm of gradient:  1.123575197815489 ; l2 norm of weights:  7.1981079636058345\n",
            "Iteration#:  4428 ; Loss:  3.0569861734586143 ; l2 norm of gradient:  1.123557798760119 ; l2 norm of weights:  7.1980903099239635\n",
            "Iteration#:  4432 ; Loss:  3.0569354187833366 ; l2 norm of gradient:  1.1235403997804474 ; l2 norm of weights:  7.198072656498628\n",
            "Iteration#:  4436 ; Loss:  3.056884665476404 ; l2 norm of gradient:  1.1235230008764057 ; l2 norm of weights:  7.198055003329818\n",
            "Iteration#:  4440 ; Loss:  3.056833913591052 ; l2 norm of gradient:  1.1235056020479277 ; l2 norm of weights:  7.198037350417532\n",
            "Iteration#:  4444 ; Loss:  3.056783164020029 ; l2 norm of gradient:  1.1234882032949447 ; l2 norm of weights:  7.198019697761759\n",
            "Iteration#:  4448 ; Loss:  3.0567324153225384 ; l2 norm of gradient:  1.1234708046173905 ; l2 norm of weights:  7.19800204536249\n",
            "Iteration#:  4452 ; Loss:  3.0566816685757283 ; l2 norm of gradient:  1.1234534060151962 ; l2 norm of weights:  7.197984393219724\n",
            "Iteration#:  4456 ; Loss:  3.0566309229122948 ; l2 norm of gradient:  1.123436007488294 ; l2 norm of weights:  7.197966741333449\n",
            "Iteration#:  4460 ; Loss:  3.056580178942495 ; l2 norm of gradient:  1.1234186090366185 ; l2 norm of weights:  7.197949089703659\n",
            "Iteration#:  4464 ; Loss:  3.056529437041219 ; l2 norm of gradient:  1.1234012106600992 ; l2 norm of weights:  7.197931438330348\n",
            "Iteration#:  4468 ; Loss:  3.056478696269304 ; l2 norm of gradient:  1.1233838123586704 ; l2 norm of weights:  7.1979137872135075\n",
            "Iteration#:  4472 ; Loss:  3.0564279573342255 ; l2 norm of gradient:  1.1233664141322637 ; l2 norm of weights:  7.197896136353132\n",
            "Iteration#:  4476 ; Loss:  3.056377219889043 ; l2 norm of gradient:  1.1233490159808128 ; l2 norm of weights:  7.197878485749212\n",
            "Iteration#:  4480 ; Loss:  3.056326483742274 ; l2 norm of gradient:  1.1233316179042498 ; l2 norm of weights:  7.197860835401745\n",
            "Iteration#:  4484 ; Loss:  3.0562757495217134 ; l2 norm of gradient:  1.1233142199025066 ; l2 norm of weights:  7.197843185310718\n",
            "Iteration#:  4488 ; Loss:  3.056225016568431 ; l2 norm of gradient:  1.1232968219755166 ; l2 norm of weights:  7.1978255354761265\n",
            "Iteration#:  4492 ; Loss:  3.056174285126855 ; l2 norm of gradient:  1.1232794241232114 ; l2 norm of weights:  7.197807885897965\n",
            "Iteration#:  4496 ; Loss:  3.056123555776322 ; l2 norm of gradient:  1.1232620263455257 ; l2 norm of weights:  7.197790236576225\n",
            "Iteration#:  4500 ; Loss:  3.056072827311883 ; l2 norm of gradient:  1.1232446286423898 ; l2 norm of weights:  7.197772587510898\n",
            "Iteration#:  4504 ; Loss:  3.0560221006561052 ; l2 norm of gradient:  1.1232272310137381 ; l2 norm of weights:  7.19775493870198\n",
            "Iteration#:  4508 ; Loss:  3.0559713760255494 ; l2 norm of gradient:  1.1232098334595018 ; l2 norm of weights:  7.197737290149461\n",
            "Iteration#:  4512 ; Loss:  3.055920652411783 ; l2 norm of gradient:  1.1231924359796153 ; l2 norm of weights:  7.197719641853335\n",
            "Iteration#:  4516 ; Loss:  3.0558699296053877 ; l2 norm of gradient:  1.1231750385740105 ; l2 norm of weights:  7.197701993813596\n",
            "Iteration#:  4520 ; Loss:  3.055819209517935 ; l2 norm of gradient:  1.1231576412426203 ; l2 norm of weights:  7.197684346030236\n",
            "Iteration#:  4524 ; Loss:  3.055768490863329 ; l2 norm of gradient:  1.1231402439853773 ; l2 norm of weights:  7.197666698503247\n",
            "Iteration#:  4528 ; Loss:  3.0557177734989116 ; l2 norm of gradient:  1.1231228468022139 ; l2 norm of weights:  7.1976490512326246\n",
            "Iteration#:  4532 ; Loss:  3.0556670580271037 ; l2 norm of gradient:  1.123105449693064 ; l2 norm of weights:  7.197631404218358\n",
            "Iteration#:  4536 ; Loss:  3.0556163439278836 ; l2 norm of gradient:  1.12308805265786 ; l2 norm of weights:  7.197613757460442\n",
            "Iteration#:  4540 ; Loss:  3.0555656312513224 ; l2 norm of gradient:  1.1230706556965342 ; l2 norm of weights:  7.197596110958871\n",
            "Iteration#:  4544 ; Loss:  3.0555149205657943 ; l2 norm of gradient:  1.123053258809021 ; l2 norm of weights:  7.197578464713635\n",
            "Iteration#:  4548 ; Loss:  3.0554642111906456 ; l2 norm of gradient:  1.1230358619952516 ; l2 norm of weights:  7.197560818724729\n",
            "Iteration#:  4552 ; Loss:  3.0554135032282494 ; l2 norm of gradient:  1.1230184652551605 ; l2 norm of weights:  7.1975431729921455\n",
            "Iteration#:  4556 ; Loss:  3.0553627972651842 ; l2 norm of gradient:  1.1230010685886793 ; l2 norm of weights:  7.197525527515877\n",
            "Iteration#:  4560 ; Loss:  3.055312092519966 ; l2 norm of gradient:  1.1229836719957407 ; l2 norm of weights:  7.197507882295917\n",
            "Iteration#:  4564 ; Loss:  3.055261389341104 ; l2 norm of gradient:  1.12296627547628 ; l2 norm of weights:  7.197490237332257\n",
            "Iteration#:  4568 ; Loss:  3.0552106877022465 ; l2 norm of gradient:  1.1229488790302284 ; l2 norm of weights:  7.197472592624892\n",
            "Iteration#:  4572 ; Loss:  3.0551599879610425 ; l2 norm of gradient:  1.1229314826575194 ; l2 norm of weights:  7.197454948173813\n",
            "Iteration#:  4576 ; Loss:  3.055109289509422 ; l2 norm of gradient:  1.1229140863580866 ; l2 norm of weights:  7.197437303979014\n",
            "Iteration#:  4580 ; Loss:  3.055058592492136 ; l2 norm of gradient:  1.1228966901318616 ; l2 norm of weights:  7.197419660040488\n",
            "Iteration#:  4584 ; Loss:  3.0550078977093476 ; l2 norm of gradient:  1.1228792939787795 ; l2 norm of weights:  7.197402016358226\n",
            "Iteration#:  4588 ; Loss:  3.0549572035640176 ; l2 norm of gradient:  1.1228618978987708 ; l2 norm of weights:  7.197384372932225\n",
            "Iteration#:  4592 ; Loss:  3.054906511408143 ; l2 norm of gradient:  1.122844501891772 ; l2 norm of weights:  7.197366729762473\n",
            "Iteration#:  4596 ; Loss:  3.0548558208704337 ; l2 norm of gradient:  1.122827105957714 ; l2 norm of weights:  7.197349086848966\n",
            "Iteration#:  4600 ; Loss:  3.054805132065383 ; l2 norm of gradient:  1.1228097100965313 ; l2 norm of weights:  7.197331444191696\n",
            "Iteration#:  4604 ; Loss:  3.0547544445600536 ; l2 norm of gradient:  1.1227923143081553 ; l2 norm of weights:  7.197313801790656\n",
            "Iteration#:  4608 ; Loss:  3.05470375863254 ; l2 norm of gradient:  1.1227749185925215 ; l2 norm of weights:  7.197296159645839\n",
            "Iteration#:  4612 ; Loss:  3.054653074135241 ; l2 norm of gradient:  1.1227575229495617 ; l2 norm of weights:  7.197278517757239\n",
            "Iteration#:  4616 ; Loss:  3.054602391828074 ; l2 norm of gradient:  1.1227401273792095 ; l2 norm of weights:  7.197260876124847\n",
            "Iteration#:  4620 ; Loss:  3.054551710518611 ; l2 norm of gradient:  1.1227227318813982 ; l2 norm of weights:  7.197243234748657\n",
            "Iteration#:  4624 ; Loss:  3.054501030749564 ; l2 norm of gradient:  1.1227053364560617 ; l2 norm of weights:  7.197225593628661\n",
            "Iteration#:  4628 ; Loss:  3.0544503525803064 ; l2 norm of gradient:  1.122687941103132 ; l2 norm of weights:  7.1972079527648525\n",
            "Iteration#:  4632 ; Loss:  3.054399676066915 ; l2 norm of gradient:  1.1226705458225448 ; l2 norm of weights:  7.197190312157225\n",
            "Iteration#:  4636 ; Loss:  3.054349001486588 ; l2 norm of gradient:  1.1226531506142317 ; l2 norm of weights:  7.197172671805771\n",
            "Iteration#:  4640 ; Loss:  3.0542983280319813 ; l2 norm of gradient:  1.122635755478127 ; l2 norm of weights:  7.197155031710482\n",
            "Iteration#:  4644 ; Loss:  3.0542476558291547 ; l2 norm of gradient:  1.1226183604141626 ; l2 norm of weights:  7.197137391871354\n",
            "Iteration#:  4648 ; Loss:  3.0541969857025584 ; l2 norm of gradient:  1.1226009654222742 ; l2 norm of weights:  7.197119752288377\n",
            "Iteration#:  4652 ; Loss:  3.054146316858251 ; l2 norm of gradient:  1.1225835705023937 ; l2 norm of weights:  7.197102112961545\n",
            "Iteration#:  4656 ; Loss:  3.0540956501964893 ; l2 norm of gradient:  1.1225661756544558 ; l2 norm of weights:  7.19708447389085\n",
            "Iteration#:  4660 ; Loss:  3.0540449843728013 ; l2 norm of gradient:  1.1225487808783918 ; l2 norm of weights:  7.197066835076288\n",
            "Iteration#:  4664 ; Loss:  3.0539943204966096 ; l2 norm of gradient:  1.1225313861741377 ; l2 norm of weights:  7.197049196517848\n",
            "Iteration#:  4668 ; Loss:  3.0539436578616166 ; l2 norm of gradient:  1.1225139915416267 ; l2 norm of weights:  7.197031558215525\n",
            "Iteration#:  4672 ; Loss:  3.0538929970039383 ; l2 norm of gradient:  1.122496596980792 ; l2 norm of weights:  7.197013920169311\n",
            "Iteration#:  4676 ; Loss:  3.053842337909898 ; l2 norm of gradient:  1.1224792024915675 ; l2 norm of weights:  7.196996282379201\n",
            "Iteration#:  4680 ; Loss:  3.053791680409499 ; l2 norm of gradient:  1.1224618080738855 ; l2 norm of weights:  7.196978644845185\n",
            "Iteration#:  4684 ; Loss:  3.0537410241040144 ; l2 norm of gradient:  1.1224444137276812 ; l2 norm of weights:  7.196961007567257\n",
            "Iteration#:  4688 ; Loss:  3.053690368432251 ; l2 norm of gradient:  1.1224270194528878 ; l2 norm of weights:  7.196943370545411\n",
            "Iteration#:  4692 ; Loss:  3.0536397154702755 ; l2 norm of gradient:  1.1224096252494409 ; l2 norm of weights:  7.196925733779638\n",
            "Iteration#:  4696 ; Loss:  3.053589063948649 ; l2 norm of gradient:  1.1223922311172705 ; l2 norm of weights:  7.196908097269933\n",
            "Iteration#:  4700 ; Loss:  3.0535384143661792 ; l2 norm of gradient:  1.1223748370563118 ; l2 norm of weights:  7.196890461016287\n",
            "Iteration#:  4704 ; Loss:  3.053487765788271 ; l2 norm of gradient:  1.1223574430665006 ; l2 norm of weights:  7.196872825018695\n",
            "Iteration#:  4708 ; Loss:  3.053437119211249 ; l2 norm of gradient:  1.1223400491477684 ; l2 norm of weights:  7.196855189277148\n",
            "Iteration#:  4712 ; Loss:  3.0533864738295904 ; l2 norm of gradient:  1.12232265530005 ; l2 norm of weights:  7.19683755379164\n",
            "Iteration#:  4716 ; Loss:  3.053335830215005 ; l2 norm of gradient:  1.122305261523279 ; l2 norm of weights:  7.196819918562163\n",
            "Iteration#:  4720 ; Loss:  3.0532851879523193 ; l2 norm of gradient:  1.1222878678173898 ; l2 norm of weights:  7.196802283588709\n",
            "Iteration#:  4724 ; Loss:  3.0532345476000633 ; l2 norm of gradient:  1.1222704741823166 ; l2 norm of weights:  7.196784648871274\n",
            "Iteration#:  4728 ; Loss:  3.0531839089240718 ; l2 norm of gradient:  1.1222530806179916 ; l2 norm of weights:  7.196767014409849\n",
            "Iteration#:  4732 ; Loss:  3.0531332713636328 ; l2 norm of gradient:  1.1222356871243493 ; l2 norm of weights:  7.196749380204427\n",
            "Iteration#:  4736 ; Loss:  3.0530826354816116 ; l2 norm of gradient:  1.1222182937013259 ; l2 norm of weights:  7.196731746255001\n",
            "Iteration#:  4740 ; Loss:  3.0530320011909677 ; l2 norm of gradient:  1.1222009003488522 ; l2 norm of weights:  7.196714112561565\n",
            "Iteration#:  4744 ; Loss:  3.0529813685000313 ; l2 norm of gradient:  1.122183507066864 ; l2 norm of weights:  7.196696479124109\n",
            "Iteration#:  4748 ; Loss:  3.052930737405137 ; l2 norm of gradient:  1.122166113855295 ; l2 norm of weights:  7.196678845942629\n",
            "Iteration#:  4752 ; Loss:  3.052880107500543 ; l2 norm of gradient:  1.1221487207140801 ; l2 norm of weights:  7.196661213017117\n",
            "Iteration#:  4756 ; Loss:  3.052829479597492 ; l2 norm of gradient:  1.1221313276431517 ; l2 norm of weights:  7.196643580347564\n",
            "Iteration#:  4760 ; Loss:  3.0527788530633164 ; l2 norm of gradient:  1.1221139346424447 ; l2 norm of weights:  7.196625947933965\n",
            "Iteration#:  4764 ; Loss:  3.052728228502815 ; l2 norm of gradient:  1.122096541711894 ; l2 norm of weights:  7.196608315776314\n",
            "Iteration#:  4768 ; Loss:  3.0526776052151243 ; l2 norm of gradient:  1.1220791488514337 ; l2 norm of weights:  7.1965906838746\n",
            "Iteration#:  4772 ; Loss:  3.052626983375629 ; l2 norm of gradient:  1.1220617560609958 ; l2 norm of weights:  7.196573052228819\n",
            "Iteration#:  4776 ; Loss:  3.0525763629842095 ; l2 norm of gradient:  1.1220443633405168 ; l2 norm of weights:  7.196555420838964\n",
            "Iteration#:  4780 ; Loss:  3.0525257445807377 ; l2 norm of gradient:  1.1220269706899302 ; l2 norm of weights:  7.196537789705027\n",
            "Iteration#:  4784 ; Loss:  3.0524751274258923 ; l2 norm of gradient:  1.1220095781091706 ; l2 norm of weights:  7.196520158827\n",
            "Iteration#:  4788 ; Loss:  3.0524245117425104 ; l2 norm of gradient:  1.121992185598171 ; l2 norm of weights:  7.196502528204876\n",
            "Iteration#:  4792 ; Loss:  3.0523738979428137 ; l2 norm of gradient:  1.1219747931568669 ; l2 norm of weights:  7.19648489783865\n",
            "Iteration#:  4796 ; Loss:  3.052323284731152 ; l2 norm of gradient:  1.1219574007851933 ; l2 norm of weights:  7.1964672677283135\n",
            "Iteration#:  4800 ; Loss:  3.052272673896695 ; l2 norm of gradient:  1.1219400084830822 ; l2 norm of weights:  7.19644963787386\n",
            "Iteration#:  4804 ; Loss:  3.0522220644147198 ; l2 norm of gradient:  1.1219226162504707 ; l2 norm of weights:  7.196432008275282\n",
            "Iteration#:  4808 ; Loss:  3.0521714567042606 ; l2 norm of gradient:  1.1219052240872913 ; l2 norm of weights:  7.196414378932571\n",
            "Iteration#:  4812 ; Loss:  3.0521208506567046 ; l2 norm of gradient:  1.1218878319934784 ; l2 norm of weights:  7.196396749845723\n",
            "Iteration#:  4816 ; Loss:  3.052070245958147 ; l2 norm of gradient:  1.1218704399689674 ; l2 norm of weights:  7.196379121014729\n",
            "Iteration#:  4820 ; Loss:  3.0520196430259836 ; l2 norm of gradient:  1.1218530480136921 ; l2 norm of weights:  7.19636149243958\n",
            "Iteration#:  4824 ; Loss:  3.0519690415142575 ; l2 norm of gradient:  1.1218356561275873 ; l2 norm of weights:  7.196343864120273\n",
            "Iteration#:  4828 ; Loss:  3.051918441772405 ; l2 norm of gradient:  1.121818264310588 ; l2 norm of weights:  7.196326236056798\n",
            "Iteration#:  4832 ; Loss:  3.051867843385266 ; l2 norm of gradient:  1.1218008725626274 ; l2 norm of weights:  7.19630860824915\n",
            "Iteration#:  4836 ; Loss:  3.051817246495277 ; l2 norm of gradient:  1.121783480883641 ; l2 norm of weights:  7.196290980697318\n",
            "Iteration#:  4840 ; Loss:  3.0517666517039723 ; l2 norm of gradient:  1.121766089273564 ; l2 norm of weights:  7.196273353401301\n",
            "Iteration#:  4844 ; Loss:  3.0517160580244425 ; l2 norm of gradient:  1.1217486977323292 ; l2 norm of weights:  7.196255726361087\n",
            "Iteration#:  4848 ; Loss:  3.0516654659216105 ; l2 norm of gradient:  1.121731306259871 ; l2 norm of weights:  7.19623809957667\n",
            "Iteration#:  4852 ; Loss:  3.051614875507999 ; l2 norm of gradient:  1.1217139148561268 ; l2 norm of weights:  7.196220473048043\n",
            "Iteration#:  4856 ; Loss:  3.0515642864536385 ; l2 norm of gradient:  1.1216965235210297 ; l2 norm of weights:  7.196202846775201\n",
            "Iteration#:  4860 ; Loss:  3.051513699218943 ; l2 norm of gradient:  1.121679132254514 ; l2 norm of weights:  7.196185220758133\n",
            "Iteration#:  4864 ; Loss:  3.05146311336619 ; l2 norm of gradient:  1.1216617410565144 ; l2 norm of weights:  7.196167594996837\n",
            "Iteration#:  4868 ; Loss:  3.051412529142465 ; l2 norm of gradient:  1.1216443499269666 ; l2 norm of weights:  7.1961499694913\n",
            "Iteration#:  4872 ; Loss:  3.0513619464761845 ; l2 norm of gradient:  1.121626958865805 ; l2 norm of weights:  7.196132344241519\n",
            "Iteration#:  4876 ; Loss:  3.051311364903355 ; l2 norm of gradient:  1.1216095678729632 ; l2 norm of weights:  7.196114719247486\n",
            "Iteration#:  4880 ; Loss:  3.051260784708339 ; l2 norm of gradient:  1.1215921769483777 ; l2 norm of weights:  7.196097094509194\n",
            "Iteration#:  4884 ; Loss:  3.0512102065713287 ; l2 norm of gradient:  1.121574786091983 ; l2 norm of weights:  7.196079470026635\n",
            "Iteration#:  4888 ; Loss:  3.0511596300325112 ; l2 norm of gradient:  1.1215573953037123 ; l2 norm of weights:  7.196061845799802\n",
            "Iteration#:  4892 ; Loss:  3.05110905548888 ; l2 norm of gradient:  1.1215400045835016 ; l2 norm of weights:  7.196044221828689\n",
            "Iteration#:  4896 ; Loss:  3.0510584821428455 ; l2 norm of gradient:  1.1215226139312873 ; l2 norm of weights:  7.1960265981132885\n",
            "Iteration#:  4900 ; Loss:  3.051007910380326 ; l2 norm of gradient:  1.121505223347002 ; l2 norm of weights:  7.196008974653592\n",
            "Iteration#:  4904 ; Loss:  3.0509573402147097 ; l2 norm of gradient:  1.1214878328305813 ; l2 norm of weights:  7.195991351449595\n",
            "Iteration#:  4908 ; Loss:  3.0509067715776395 ; l2 norm of gradient:  1.1214704423819613 ; l2 norm of weights:  7.19597372850129\n",
            "Iteration#:  4912 ; Loss:  3.0508562043548686 ; l2 norm of gradient:  1.1214530520010757 ; l2 norm of weights:  7.1959561058086665\n",
            "Iteration#:  4916 ; Loss:  3.050805638966373 ; l2 norm of gradient:  1.1214356616878596 ; l2 norm of weights:  7.195938483371721\n",
            "Iteration#:  4920 ; Loss:  3.0507550751146084 ; l2 norm of gradient:  1.1214182714422491 ; l2 norm of weights:  7.195920861190444\n",
            "Iteration#:  4924 ; Loss:  3.0507045125114485 ; l2 norm of gradient:  1.121400881264178 ; l2 norm of weights:  7.195903239264832\n",
            "Iteration#:  4928 ; Loss:  3.0506539519122096 ; l2 norm of gradient:  1.1213834911535818 ; l2 norm of weights:  7.195885617594874\n",
            "Iteration#:  4932 ; Loss:  3.0506033925976856 ; l2 norm of gradient:  1.1213661011103961 ; l2 norm of weights:  7.1958679961805645\n",
            "Iteration#:  4936 ; Loss:  3.0505528351157993 ; l2 norm of gradient:  1.1213487111345561 ; l2 norm of weights:  7.195850375021897\n",
            "Iteration#:  4940 ; Loss:  3.0505022788254106 ; l2 norm of gradient:  1.1213313212259959 ; l2 norm of weights:  7.195832754118864\n",
            "Iteration#:  4944 ; Loss:  3.0504517242884606 ; l2 norm of gradient:  1.1213139313846514 ; l2 norm of weights:  7.1958151334714575\n",
            "Iteration#:  4948 ; Loss:  3.0504011707780503 ; l2 norm of gradient:  1.1212965416104583 ; l2 norm of weights:  7.195797513079671\n",
            "Iteration#:  4952 ; Loss:  3.0503506188854956 ; l2 norm of gradient:  1.1212791519033503 ; l2 norm of weights:  7.195779892943499\n",
            "Iteration#:  4956 ; Loss:  3.0503000689027897 ; l2 norm of gradient:  1.1212617622632641 ; l2 norm of weights:  7.195762273062932\n",
            "Iteration#:  4960 ; Loss:  3.0502495205034768 ; l2 norm of gradient:  1.1212443726901338 ; l2 norm of weights:  7.195744653437962\n",
            "Iteration#:  4964 ; Loss:  3.050198973767122 ; l2 norm of gradient:  1.1212269831838966 ; l2 norm of weights:  7.195727034068584\n",
            "Iteration#:  4968 ; Loss:  3.0501484287945777 ; l2 norm of gradient:  1.1212095937444866 ; l2 norm of weights:  7.195709414954793\n",
            "Iteration#:  4972 ; Loss:  3.0500978851824967 ; l2 norm of gradient:  1.1211922043718376 ; l2 norm of weights:  7.195691796096578\n",
            "Iteration#:  4976 ; Loss:  3.0500473429883552 ; l2 norm of gradient:  1.1211748150658873 ; l2 norm of weights:  7.195674177493934\n",
            "Iteration#:  4980 ; Loss:  3.049996802635278 ; l2 norm of gradient:  1.1211574258265706 ; l2 norm of weights:  7.195656559146853\n",
            "Iteration#:  4984 ; Loss:  3.049946263254884 ; l2 norm of gradient:  1.121140036653823 ; l2 norm of weights:  7.195638941055328\n",
            "Iteration#:  4988 ; Loss:  3.0498957260808752 ; l2 norm of gradient:  1.1211226475475786 ; l2 norm of weights:  7.195621323219353\n",
            "Iteration#:  4992 ; Loss:  3.0498451901391253 ; l2 norm of gradient:  1.1211052585077748 ; l2 norm of weights:  7.195603705638919\n",
            "Iteration#:  4996 ; Loss:  3.0497946561434697 ; l2 norm of gradient:  1.1210878695343454 ; l2 norm of weights:  7.19558608831402\n",
            "Iteration#:  5000 ; Loss:  3.049744123379855 ; l2 norm of gradient:  1.121070480627227 ; l2 norm of weights:  7.195568471244649\n",
            "Iteration#:  5004 ; Loss:  3.04969359234255 ; l2 norm of gradient:  1.1210530917863548 ; l2 norm of weights:  7.195550854430799\n",
            "Iteration#:  5008 ; Loss:  3.049643062769195 ; l2 norm of gradient:  1.1210357030116644 ; l2 norm of weights:  7.1955332378724615\n",
            "Iteration#:  5012 ; Loss:  3.049592534845906 ; l2 norm of gradient:  1.1210183143030914 ; l2 norm of weights:  7.195515621569632\n",
            "Iteration#:  5016 ; Loss:  3.049542007266638 ; l2 norm of gradient:  1.1210009256605704 ; l2 norm of weights:  7.1954980055223015\n",
            "Iteration#:  5020 ; Loss:  3.0494914823731474 ; l2 norm of gradient:  1.1209835370840395 ; l2 norm of weights:  7.195480389730465\n",
            "Iteration#:  5024 ; Loss:  3.0494409589222045 ; l2 norm of gradient:  1.1209661485734321 ; l2 norm of weights:  7.195462774194111\n",
            "Iteration#:  5028 ; Loss:  3.0493904373787215 ; l2 norm of gradient:  1.1209487601286836 ; l2 norm of weights:  7.195445158913236\n",
            "Iteration#:  5032 ; Loss:  3.0493399171997373 ; l2 norm of gradient:  1.120931371749732 ; l2 norm of weights:  7.195427543887833\n",
            "Iteration#:  5036 ; Loss:  3.0492893985253 ; l2 norm of gradient:  1.1209139834365116 ; l2 norm of weights:  7.1954099291178935\n",
            "Iteration#:  5040 ; Loss:  3.0492388813525593 ; l2 norm of gradient:  1.1208965951889582 ; l2 norm of weights:  7.195392314603411\n",
            "Iteration#:  5044 ; Loss:  3.0491883657064527 ; l2 norm of gradient:  1.1208792070070073 ; l2 norm of weights:  7.195374700344378\n",
            "Iteration#:  5048 ; Loss:  3.049137851815128 ; l2 norm of gradient:  1.1208618188905952 ; l2 norm of weights:  7.195357086340789\n",
            "Iteration#:  5052 ; Loss:  3.0490873395973233 ; l2 norm of gradient:  1.1208444308396575 ; l2 norm of weights:  7.195339472592634\n",
            "Iteration#:  5056 ; Loss:  3.0490368288975334 ; l2 norm of gradient:  1.1208270428541307 ; l2 norm of weights:  7.1953218590999075\n",
            "Iteration#:  5060 ; Loss:  3.048986319151669 ; l2 norm of gradient:  1.1208096549339504 ; l2 norm of weights:  7.195304245862603\n",
            "Iteration#:  5064 ; Loss:  3.0489358114698124 ; l2 norm of gradient:  1.1207922670790513 ; l2 norm of weights:  7.1952866328807135\n",
            "Iteration#:  5068 ; Loss:  3.0488853053795175 ; l2 norm of gradient:  1.1207748792893706 ; l2 norm of weights:  7.19526902015423\n",
            "Iteration#:  5072 ; Loss:  3.048834800975787 ; l2 norm of gradient:  1.120757491564844 ; l2 norm of weights:  7.1952514076831475\n",
            "Iteration#:  5076 ; Loss:  3.048784297053827 ; l2 norm of gradient:  1.1207401039054072 ; l2 norm of weights:  7.195233795467458\n",
            "Iteration#:  5080 ; Loss:  3.0487337952817226 ; l2 norm of gradient:  1.1207227163109967 ; l2 norm of weights:  7.195216183507154\n",
            "Iteration#:  5084 ; Loss:  3.048683295485744 ; l2 norm of gradient:  1.1207053287815476 ; l2 norm of weights:  7.195198571802228\n",
            "Iteration#:  5088 ; Loss:  3.0486327970534393 ; l2 norm of gradient:  1.1206879413169968 ; l2 norm of weights:  7.195180960352676\n",
            "Iteration#:  5092 ; Loss:  3.0485823005267085 ; l2 norm of gradient:  1.1206705539172814 ; l2 norm of weights:  7.195163349158487\n",
            "Iteration#:  5096 ; Loss:  3.0485318048755703 ; l2 norm of gradient:  1.1206531665823354 ; l2 norm of weights:  7.195145738219657\n",
            "Iteration#:  5100 ; Loss:  3.0484813114575036 ; l2 norm of gradient:  1.120635779312094 ; l2 norm of weights:  7.195128127536177\n",
            "Iteration#:  5104 ; Loss:  3.0484308193980896 ; l2 norm of gradient:  1.1206183921064976 ; l2 norm of weights:  7.195110517108039\n",
            "Iteration#:  5108 ; Loss:  3.0483803289279043 ; l2 norm of gradient:  1.1206010049654789 ; l2 norm of weights:  7.195092906935238\n",
            "Iteration#:  5112 ; Loss:  3.0483298395766596 ; l2 norm of gradient:  1.1205836178889743 ; l2 norm of weights:  7.195075297017766\n",
            "Iteration#:  5116 ; Loss:  3.0482793521318228 ; l2 norm of gradient:  1.120566230876921 ; l2 norm of weights:  7.195057687355616\n",
            "Iteration#:  5120 ; Loss:  3.04822886644309 ; l2 norm of gradient:  1.1205488439292564 ; l2 norm of weights:  7.195040077948782\n",
            "Iteration#:  5124 ; Loss:  3.048178382267193 ; l2 norm of gradient:  1.1205314570459142 ; l2 norm of weights:  7.195022468797254\n",
            "Iteration#:  5128 ; Loss:  3.048127899356168 ; l2 norm of gradient:  1.120514070226832 ; l2 norm of weights:  7.195004859901029\n",
            "Iteration#:  5132 ; Loss:  3.0480774174360885 ; l2 norm of gradient:  1.1204966834719463 ; l2 norm of weights:  7.1949872512600965\n",
            "Iteration#:  5136 ; Loss:  3.0480269378007816 ; l2 norm of gradient:  1.1204792967811916 ; l2 norm of weights:  7.19496964287445\n",
            "Iteration#:  5140 ; Loss:  3.0479764598433525 ; l2 norm of gradient:  1.1204619101545075 ; l2 norm of weights:  7.194952034744085\n",
            "Iteration#:  5144 ; Loss:  3.047925982993405 ; l2 norm of gradient:  1.1204445235918274 ; l2 norm of weights:  7.194934426868991\n",
            "Iteration#:  5148 ; Loss:  3.04787550821771 ; l2 norm of gradient:  1.1204271370930898 ; l2 norm of weights:  7.194916819249162\n",
            "Iteration#:  5152 ; Loss:  3.047825034796052 ; l2 norm of gradient:  1.12040975065823 ; l2 norm of weights:  7.194899211884591\n",
            "Iteration#:  5156 ; Loss:  3.0477745629524673 ; l2 norm of gradient:  1.120392364287185 ; l2 norm of weights:  7.194881604775273\n",
            "Iteration#:  5160 ; Loss:  3.047724092652858 ; l2 norm of gradient:  1.1203749779798904 ; l2 norm of weights:  7.194863997921197\n",
            "Iteration#:  5164 ; Loss:  3.0476736240038296 ; l2 norm of gradient:  1.1203575917362851 ; l2 norm of weights:  7.194846391322359\n",
            "Iteration#:  5168 ; Loss:  3.047623156546461 ; l2 norm of gradient:  1.1203402055563012 ; l2 norm of weights:  7.194828784978751\n",
            "Iteration#:  5172 ; Loss:  3.0475726912418297 ; l2 norm of gradient:  1.1203228194398798 ; l2 norm of weights:  7.194811178890365\n",
            "Iteration#:  5176 ; Loss:  3.047522227306101 ; l2 norm of gradient:  1.1203054333869558 ; l2 norm of weights:  7.194793573057194\n",
            "Iteration#:  5180 ; Loss:  3.047471764244734 ; l2 norm of gradient:  1.1202880473974648 ; l2 norm of weights:  7.194775967479233\n",
            "Iteration#:  5184 ; Loss:  3.0474213030884427 ; l2 norm of gradient:  1.1202706614713445 ; l2 norm of weights:  7.194758362156473\n",
            "Iteration#:  5188 ; Loss:  3.0473708436853713 ; l2 norm of gradient:  1.1202532756085317 ; l2 norm of weights:  7.194740757088908\n",
            "Iteration#:  5192 ; Loss:  3.0473203861112435 ; l2 norm of gradient:  1.1202358898089617 ; l2 norm of weights:  7.19472315227653\n",
            "Iteration#:  5196 ; Loss:  3.047269929489424 ; l2 norm of gradient:  1.1202185040725723 ; l2 norm of weights:  7.194705547719331\n",
            "Iteration#:  5200 ; Loss:  3.047219475104647 ; l2 norm of gradient:  1.1202011183993015 ; l2 norm of weights:  7.194687943417306\n",
            "Iteration#:  5204 ; Loss:  3.047169021973389 ; l2 norm of gradient:  1.1201837327890833 ; l2 norm of weights:  7.194670339370447\n",
            "Iteration#:  5208 ; Loss:  3.0471185702983323 ; l2 norm of gradient:  1.1201663472418575 ; l2 norm of weights:  7.194652735578746\n",
            "Iteration#:  5212 ; Loss:  3.047068120455374 ; l2 norm of gradient:  1.120148961757558 ; l2 norm of weights:  7.1946351320421975\n",
            "Iteration#:  5216 ; Loss:  3.047017671554042 ; l2 norm of gradient:  1.1201315763361233 ; l2 norm of weights:  7.194617528760793\n",
            "Iteration#:  5220 ; Loss:  3.046967224896683 ; l2 norm of gradient:  1.12011419097749 ; l2 norm of weights:  7.194599925734527\n",
            "Iteration#:  5224 ; Loss:  3.0469167797512973 ; l2 norm of gradient:  1.1200968056815948 ; l2 norm of weights:  7.1945823229633925\n",
            "Iteration#:  5228 ; Loss:  3.0468663349153946 ; l2 norm of gradient:  1.1200794204483737 ; l2 norm of weights:  7.19456472044738\n",
            "Iteration#:  5232 ; Loss:  3.0468158929524076 ; l2 norm of gradient:  1.120062035277766 ; l2 norm of weights:  7.194547118186482\n",
            "Iteration#:  5236 ; Loss:  3.0467654524297294 ; l2 norm of gradient:  1.1200446501697074 ; l2 norm of weights:  7.194529516180695\n",
            "Iteration#:  5240 ; Loss:  3.0467150133073644 ; l2 norm of gradient:  1.120027265124134 ; l2 norm of weights:  7.19451191443001\n",
            "Iteration#:  5244 ; Loss:  3.0466645757282746 ; l2 norm of gradient:  1.1200098801409841 ; l2 norm of weights:  7.194494312934419\n",
            "Iteration#:  5248 ; Loss:  3.046614139963247 ; l2 norm of gradient:  1.1199924952201936 ; l2 norm of weights:  7.194476711693917\n",
            "Iteration#:  5252 ; Loss:  3.0465637053190506 ; l2 norm of gradient:  1.1199751103617015 ; l2 norm of weights:  7.194459110708496\n",
            "Iteration#:  5256 ; Loss:  3.0465132726836113 ; l2 norm of gradient:  1.119957725565441 ; l2 norm of weights:  7.194441509978148\n",
            "Iteration#:  5260 ; Loss:  3.046462841120403 ; l2 norm of gradient:  1.119940340831354 ; l2 norm of weights:  7.194423909502867\n",
            "Iteration#:  5264 ; Loss:  3.0464124116590066 ; l2 norm of gradient:  1.1199229561593746 ; l2 norm of weights:  7.194406309282645\n",
            "Iteration#:  5268 ; Loss:  3.0463619836299864 ; l2 norm of gradient:  1.119905571549442 ; l2 norm of weights:  7.194388709317476\n",
            "Iteration#:  5272 ; Loss:  3.0463115559337557 ; l2 norm of gradient:  1.1198881870014903 ; l2 norm of weights:  7.194371109607352\n",
            "Iteration#:  5276 ; Loss:  3.046261131232075 ; l2 norm of gradient:  1.1198708025154596 ; l2 norm of weights:  7.194353510152266\n",
            "Iteration#:  5280 ; Loss:  3.0462107073380906 ; l2 norm of gradient:  1.1198534180912854 ; l2 norm of weights:  7.194335910952212\n",
            "Iteration#:  5284 ; Loss:  3.0461602856611045 ; l2 norm of gradient:  1.1198360337289057 ; l2 norm of weights:  7.194318312007181\n",
            "Iteration#:  5288 ; Loss:  3.046109865109208 ; l2 norm of gradient:  1.1198186494282591 ; l2 norm of weights:  7.194300713317167\n",
            "Iteration#:  5292 ; Loss:  3.0460594465526087 ; l2 norm of gradient:  1.1198012651892797 ; l2 norm of weights:  7.194283114882164\n",
            "Iteration#:  5296 ; Loss:  3.0460090293502553 ; l2 norm of gradient:  1.1197838810119072 ; l2 norm of weights:  7.194265516702162\n",
            "Iteration#:  5300 ; Loss:  3.045958613569371 ; l2 norm of gradient:  1.1197664968960792 ; l2 norm of weights:  7.194247918777156\n",
            "Iteration#:  5304 ; Loss:  3.0459081995415302 ; l2 norm of gradient:  1.1197491128417318 ; l2 norm of weights:  7.194230321107139\n",
            "Iteration#:  5308 ; Loss:  3.0458577867876375 ; l2 norm of gradient:  1.1197317288488033 ; l2 norm of weights:  7.194212723692105\n",
            "Iteration#:  5312 ; Loss:  3.0458073759414535 ; l2 norm of gradient:  1.11971434491723 ; l2 norm of weights:  7.194195126532043\n",
            "Iteration#:  5316 ; Loss:  3.045756965519981 ; l2 norm of gradient:  1.1196969610469512 ; l2 norm of weights:  7.1941775296269475\n",
            "Iteration#:  5320 ; Loss:  3.0457065578577303 ; l2 norm of gradient:  1.1196795772379022 ; l2 norm of weights:  7.194159932976815\n",
            "Iteration#:  5324 ; Loss:  3.04565615123137 ; l2 norm of gradient:  1.1196621934900228 ; l2 norm of weights:  7.194142336581633\n",
            "Iteration#:  5328 ; Loss:  3.0456057466761823 ; l2 norm of gradient:  1.1196448098032477 ; l2 norm of weights:  7.194124740441398\n",
            "Iteration#:  5332 ; Loss:  3.0455553431659865 ; l2 norm of gradient:  1.119627426177517 ; l2 norm of weights:  7.194107144556102\n",
            "Iteration#:  5336 ; Loss:  3.0455049418607323 ; l2 norm of gradient:  1.119610042612768 ; l2 norm of weights:  7.194089548925736\n",
            "Iteration#:  5340 ; Loss:  3.045454541926539 ; l2 norm of gradient:  1.119592659108937 ; l2 norm of weights:  7.1940719535502975\n",
            "Iteration#:  5344 ; Loss:  3.045404143115204 ; l2 norm of gradient:  1.1195752756659636 ; l2 norm of weights:  7.194054358429774\n",
            "Iteration#:  5348 ; Loss:  3.045353746289586 ; l2 norm of gradient:  1.119557892283783 ; l2 norm of weights:  7.194036763564163\n",
            "Iteration#:  5352 ; Loss:  3.0453033507253586 ; l2 norm of gradient:  1.1195405089623345 ; l2 norm of weights:  7.194019168953454\n",
            "Iteration#:  5356 ; Loss:  3.0452529564374315 ; l2 norm of gradient:  1.119523125701555 ; l2 norm of weights:  7.194001574597642\n",
            "Iteration#:  5360 ; Loss:  3.0452025637527704 ; l2 norm of gradient:  1.1195057425013835 ; l2 norm of weights:  7.193983980496719\n",
            "Iteration#:  5364 ; Loss:  3.045152173358689 ; l2 norm of gradient:  1.119488359361756 ; l2 norm of weights:  7.1939663866506764\n",
            "Iteration#:  5368 ; Loss:  3.0451017838532493 ; l2 norm of gradient:  1.1194709762826116 ; l2 norm of weights:  7.19394879305951\n",
            "Iteration#:  5372 ; Loss:  3.045051396409149 ; l2 norm of gradient:  1.1194535932638885 ; l2 norm of weights:  7.193931199723212\n",
            "Iteration#:  5376 ; Loss:  3.0450010101424367 ; l2 norm of gradient:  1.1194362103055226 ; l2 norm of weights:  7.1939136066417735\n",
            "Iteration#:  5380 ; Loss:  3.0449506260560955 ; l2 norm of gradient:  1.1194188274074537 ; l2 norm of weights:  7.19389601381519\n",
            "Iteration#:  5384 ; Loss:  3.04490024291394 ; l2 norm of gradient:  1.1194014445696188 ; l2 norm of weights:  7.193878421243452\n",
            "Iteration#:  5388 ; Loss:  3.044849861417308 ; l2 norm of gradient:  1.1193840617919562 ; l2 norm of weights:  7.193860828926554\n",
            "Iteration#:  5392 ; Loss:  3.0447994816143042 ; l2 norm of gradient:  1.1193666790744032 ; l2 norm of weights:  7.193843236864489\n",
            "Iteration#:  5396 ; Loss:  3.0447491023017346 ; l2 norm of gradient:  1.1193492964168985 ; l2 norm of weights:  7.193825645057248\n",
            "Iteration#:  5400 ; Loss:  3.044698725926784 ; l2 norm of gradient:  1.11933191381938 ; l2 norm of weights:  7.193808053504825\n",
            "Iteration#:  5404 ; Loss:  3.044648350724055 ; l2 norm of gradient:  1.119314531281785 ; l2 norm of weights:  7.193790462207214\n",
            "Iteration#:  5408 ; Loss:  3.044597977063743 ; l2 norm of gradient:  1.119297148804053 ; l2 norm of weights:  7.1937728711644064\n",
            "Iteration#:  5412 ; Loss:  3.044547605119574 ; l2 norm of gradient:  1.1192797663861207 ; l2 norm of weights:  7.193755280376396\n",
            "Iteration#:  5416 ; Loss:  3.044497234629041 ; l2 norm of gradient:  1.1192623840279263 ; l2 norm of weights:  7.193737689843176\n",
            "Iteration#:  5420 ; Loss:  3.0444468657945385 ; l2 norm of gradient:  1.1192450017294087 ; l2 norm of weights:  7.193720099564737\n",
            "Iteration#:  5424 ; Loss:  3.0443964984993457 ; l2 norm of gradient:  1.119227619490506 ; l2 norm of weights:  7.193702509541074\n",
            "Iteration#:  5428 ; Loss:  3.044346132448127 ; l2 norm of gradient:  1.1192102373111552 ; l2 norm of weights:  7.19368491977218\n",
            "Iteration#:  5432 ; Loss:  3.0442957673011515 ; l2 norm of gradient:  1.119192855191296 ; l2 norm of weights:  7.193667330258047\n",
            "Iteration#:  5436 ; Loss:  3.044245404761904 ; l2 norm of gradient:  1.1191754731308656 ; l2 norm of weights:  7.193649740998668\n",
            "Iteration#:  5440 ; Loss:  3.0441950434157388 ; l2 norm of gradient:  1.1191580911298022 ; l2 norm of weights:  7.193632151994038\n",
            "Iteration#:  5444 ; Loss:  3.044144684281058 ; l2 norm of gradient:  1.1191407091880452 ; l2 norm of weights:  7.193614563244146\n",
            "Iteration#:  5448 ; Loss:  3.0440943261208777 ; l2 norm of gradient:  1.1191233273055319 ; l2 norm of weights:  7.193596974748988\n",
            "Iteration#:  5452 ; Loss:  3.0440439699491906 ; l2 norm of gradient:  1.1191059454822014 ; l2 norm of weights:  7.193579386508555\n",
            "Iteration#:  5456 ; Loss:  3.043993614957053 ; l2 norm of gradient:  1.1190885637179917 ; l2 norm of weights:  7.193561798522841\n",
            "Iteration#:  5460 ; Loss:  3.0439432612631467 ; l2 norm of gradient:  1.1190711820128407 ; l2 norm of weights:  7.193544210791838\n",
            "Iteration#:  5464 ; Loss:  3.0438929098587524 ; l2 norm of gradient:  1.1190538003666877 ; l2 norm of weights:  7.19352662331554\n",
            "Iteration#:  5468 ; Loss:  3.043842559396222 ; l2 norm of gradient:  1.119036418779469 ; l2 norm of weights:  7.193509036093941\n",
            "Iteration#:  5472 ; Loss:  3.0437922103244874 ; l2 norm of gradient:  1.1190190372511257 ; l2 norm of weights:  7.1934914491270305\n",
            "Iteration#:  5476 ; Loss:  3.04374186298363 ; l2 norm of gradient:  1.1190016557815958 ; l2 norm of weights:  7.193473862414804\n",
            "Iteration#:  5480 ; Loss:  3.043691517145406 ; l2 norm of gradient:  1.118984274370817 ; l2 norm of weights:  7.193456275957253\n",
            "Iteration#:  5484 ; Loss:  3.043641173442957 ; l2 norm of gradient:  1.1189668930187278 ; l2 norm of weights:  7.193438689754372\n",
            "Iteration#:  5488 ; Loss:  3.043590830869182 ; l2 norm of gradient:  1.1189495117252675 ; l2 norm of weights:  7.193421103806152\n",
            "Iteration#:  5492 ; Loss:  3.043540489891515 ; l2 norm of gradient:  1.118932130490374 ; l2 norm of weights:  7.1934035181125875\n",
            "Iteration#:  5496 ; Loss:  3.0434901507411736 ; l2 norm of gradient:  1.118914749313987 ; l2 norm of weights:  7.193385932673671\n",
            "Iteration#:  5500 ; Loss:  3.043439812935948 ; l2 norm of gradient:  1.118897368196044 ; l2 norm of weights:  7.193368347489395\n",
            "Iteration#:  5504 ; Loss:  3.0433894761836204 ; l2 norm of gradient:  1.118879987136484 ; l2 norm of weights:  7.193350762559751\n",
            "Iteration#:  5508 ; Loss:  3.0433391415580666 ; l2 norm of gradient:  1.1188626061352462 ; l2 norm of weights:  7.1933331778847345\n",
            "Iteration#:  5512 ; Loss:  3.0432888080624068 ; l2 norm of gradient:  1.1188452251922683 ; l2 norm of weights:  7.193315593464337\n",
            "Iteration#:  5516 ; Loss:  3.043238477019702 ; l2 norm of gradient:  1.1188278443074897 ; l2 norm of weights:  7.193298009298553\n",
            "Iteration#:  5520 ; Loss:  3.043188147000764 ; l2 norm of gradient:  1.1188104634808491 ; l2 norm of weights:  7.193280425387374\n",
            "Iteration#:  5524 ; Loss:  3.043137818273541 ; l2 norm of gradient:  1.1187930827122865 ; l2 norm of weights:  7.193262841730792\n",
            "Iteration#:  5528 ; Loss:  3.0430874916901036 ; l2 norm of gradient:  1.118775702001738 ; l2 norm of weights:  7.1932452583288\n",
            "Iteration#:  5532 ; Loss:  3.04303716644708 ; l2 norm of gradient:  1.118758321349145 ; l2 norm of weights:  7.193227675181395\n",
            "Iteration#:  5536 ; Loss:  3.042986842478548 ; l2 norm of gradient:  1.1187409407544444 ; l2 norm of weights:  7.193210092288564\n",
            "Iteration#:  5540 ; Loss:  3.0429365197297247 ; l2 norm of gradient:  1.1187235602175762 ; l2 norm of weights:  7.193192509650303\n",
            "Iteration#:  5544 ; Loss:  3.042886199008093 ; l2 norm of gradient:  1.11870617973848 ; l2 norm of weights:  7.193174927266605\n",
            "Iteration#:  5548 ; Loss:  3.0428358799789614 ; l2 norm of gradient:  1.1186887993170946 ; l2 norm of weights:  7.193157345137463\n",
            "Iteration#:  5552 ; Loss:  3.0427855627696028 ; l2 norm of gradient:  1.1186714189533564 ; l2 norm of weights:  7.1931397632628675\n",
            "Iteration#:  5556 ; Loss:  3.042735246756912 ; l2 norm of gradient:  1.118654038647208 ; l2 norm of weights:  7.193122181642815\n",
            "Iteration#:  5560 ; Loss:  3.042684931930391 ; l2 norm of gradient:  1.1186366583985867 ; l2 norm of weights:  7.193104600277296\n",
            "Iteration#:  5564 ; Loss:  3.042634619029608 ; l2 norm of gradient:  1.1186192782074313 ; l2 norm of weights:  7.193087019166305\n",
            "Iteration#:  5568 ; Loss:  3.0425843081158304 ; l2 norm of gradient:  1.1186018980736812 ; l2 norm of weights:  7.193069438309832\n",
            "Iteration#:  5572 ; Loss:  3.0425339974404526 ; l2 norm of gradient:  1.118584517997276 ; l2 norm of weights:  7.193051857707874\n",
            "Iteration#:  5576 ; Loss:  3.04248368913228 ; l2 norm of gradient:  1.1185671379781539 ; l2 norm of weights:  7.193034277360421\n",
            "Iteration#:  5580 ; Loss:  3.0424333828235968 ; l2 norm of gradient:  1.1185497580162567 ; l2 norm of weights:  7.193016697267466\n",
            "Iteration#:  5584 ; Loss:  3.042383077697633 ; l2 norm of gradient:  1.11853237811152 ; l2 norm of weights:  7.192999117429003\n",
            "Iteration#:  5588 ; Loss:  3.0423327740238033 ; l2 norm of gradient:  1.1185149982638851 ; l2 norm of weights:  7.192981537845023\n",
            "Iteration#:  5592 ; Loss:  3.0422824719109 ; l2 norm of gradient:  1.11849761847329 ; l2 norm of weights:  7.192963958515523\n",
            "Iteration#:  5596 ; Loss:  3.0422321721177625 ; l2 norm of gradient:  1.1184802387396744 ; l2 norm of weights:  7.1929463794404915\n",
            "Iteration#:  5600 ; Loss:  3.0421818731234316 ; l2 norm of gradient:  1.118462859062979 ; l2 norm of weights:  7.192928800619922\n",
            "Iteration#:  5604 ; Loss:  3.0421315748584514 ; l2 norm of gradient:  1.118445479443141 ; l2 norm of weights:  7.19291122205381\n",
            "Iteration#:  5608 ; Loss:  3.042081279037092 ; l2 norm of gradient:  1.1184280998801024 ; l2 norm of weights:  7.192893643742147\n",
            "Iteration#:  5612 ; Loss:  3.042030985301701 ; l2 norm of gradient:  1.1184107203737996 ; l2 norm of weights:  7.192876065684925\n",
            "Iteration#:  5616 ; Loss:  3.0419806926486688 ; l2 norm of gradient:  1.1183933409241742 ; l2 norm of weights:  7.192858487882139\n",
            "Iteration#:  5620 ; Loss:  3.0419304013539135 ; l2 norm of gradient:  1.118375961531165 ; l2 norm of weights:  7.192840910333779\n",
            "Iteration#:  5624 ; Loss:  3.041880111831978 ; l2 norm of gradient:  1.118358582194711 ; l2 norm of weights:  7.192823333039841\n",
            "Iteration#:  5628 ; Loss:  3.0418298242725132 ; l2 norm of gradient:  1.1183412029147517 ; l2 norm of weights:  7.192805756000316\n",
            "Iteration#:  5632 ; Loss:  3.041779537918206 ; l2 norm of gradient:  1.1183238236912274 ; l2 norm of weights:  7.192788179215197\n",
            "Iteration#:  5636 ; Loss:  3.0417292521994397 ; l2 norm of gradient:  1.1183064445240773 ; l2 norm of weights:  7.192770602684476\n",
            "Iteration#:  5640 ; Loss:  3.0416789687154395 ; l2 norm of gradient:  1.1182890654132407 ; l2 norm of weights:  7.192753026408148\n",
            "Iteration#:  5644 ; Loss:  3.0416286873603493 ; l2 norm of gradient:  1.1182716863586575 ; l2 norm of weights:  7.1927354503862055\n",
            "Iteration#:  5648 ; Loss:  3.041578407121103 ; l2 norm of gradient:  1.1182543073602673 ; l2 norm of weights:  7.192717874618641\n",
            "Iteration#:  5652 ; Loss:  3.0415281284727373 ; l2 norm of gradient:  1.1182369284180094 ; l2 norm of weights:  7.1927002991054465\n",
            "Iteration#:  5656 ; Loss:  3.041477851414081 ; l2 norm of gradient:  1.118219549531824 ; l2 norm of weights:  7.192682723846616\n",
            "Iteration#:  5660 ; Loss:  3.0414275758741955 ; l2 norm of gradient:  1.1182021707016505 ; l2 norm of weights:  7.192665148842142\n",
            "Iteration#:  5664 ; Loss:  3.041377302226535 ; l2 norm of gradient:  1.1181847919274286 ; l2 norm of weights:  7.192647574092017\n",
            "Iteration#:  5668 ; Loss:  3.0413270291560126 ; l2 norm of gradient:  1.118167413209098 ; l2 norm of weights:  7.192629999596235\n",
            "Iteration#:  5672 ; Loss:  3.0412767582115325 ; l2 norm of gradient:  1.118150034546599 ; l2 norm of weights:  7.192612425354788\n",
            "Iteration#:  5676 ; Loss:  3.041226489026589 ; l2 norm of gradient:  1.1181326559398703 ; l2 norm of weights:  7.19259485136767\n",
            "Iteration#:  5680 ; Loss:  3.041176221031837 ; l2 norm of gradient:  1.118115277388853 ; l2 norm of weights:  7.192577277634872\n",
            "Iteration#:  5684 ; Loss:  3.0411259554337935 ; l2 norm of gradient:  1.1180978988934869 ; l2 norm of weights:  7.1925597041563885\n",
            "Iteration#:  5688 ; Loss:  3.041075690683618 ; l2 norm of gradient:  1.118080520453711 ; l2 norm of weights:  7.19254213093221\n",
            "Iteration#:  5692 ; Loss:  3.0410254276129445 ; l2 norm of gradient:  1.118063142069467 ; l2 norm of weights:  7.192524557962334\n",
            "Iteration#:  5696 ; Loss:  3.04097516519824 ; l2 norm of gradient:  1.1180457637406918 ; l2 norm of weights:  7.19250698524675\n",
            "Iteration#:  5700 ; Loss:  3.0409249055366683 ; l2 norm of gradient:  1.1180283854673279 ; l2 norm of weights:  7.19248941278545\n",
            "Iteration#:  5704 ; Loss:  3.0408746470787937 ; l2 norm of gradient:  1.1180110072493143 ; l2 norm of weights:  7.192471840578429\n",
            "Iteration#:  5708 ; Loss:  3.040824390361535 ; l2 norm of gradient:  1.1179936290865917 ; l2 norm of weights:  7.19245426862568\n",
            "Iteration#:  5712 ; Loss:  3.0407741350852224 ; l2 norm of gradient:  1.1179762509790994 ; l2 norm of weights:  7.192436696927196\n",
            "Iteration#:  5716 ; Loss:  3.0407238814762367 ; l2 norm of gradient:  1.117958872926777 ; l2 norm of weights:  7.192419125482968\n",
            "Iteration#:  5720 ; Loss:  3.0406736293708585 ; l2 norm of gradient:  1.1179414949295674 ; l2 norm of weights:  7.1924015542929896\n",
            "Iteration#:  5724 ; Loss:  3.0406233785428602 ; l2 norm of gradient:  1.1179241169874066 ; l2 norm of weights:  7.192383983357255\n",
            "Iteration#:  5728 ; Loss:  3.0405731287623428 ; l2 norm of gradient:  1.117906739100239 ; l2 norm of weights:  7.1923664126757565\n",
            "Iteration#:  5732 ; Loss:  3.04052288141144 ; l2 norm of gradient:  1.1178893612680016 ; l2 norm of weights:  7.192348842248484\n",
            "Iteration#:  5736 ; Loss:  3.0404726358173604 ; l2 norm of gradient:  1.1178719834906359 ; l2 norm of weights:  7.192331272075436\n",
            "Iteration#:  5740 ; Loss:  3.040422391352471 ; l2 norm of gradient:  1.1178546057680823 ; l2 norm of weights:  7.192313702156603\n",
            "Iteration#:  5744 ; Loss:  3.040372148690336 ; l2 norm of gradient:  1.1178372281002804 ; l2 norm of weights:  7.192296132491975\n",
            "Iteration#:  5748 ; Loss:  3.040321907403536 ; l2 norm of gradient:  1.1178198504871708 ; l2 norm of weights:  7.1922785630815484\n",
            "Iteration#:  5752 ; Loss:  3.040271667624787 ; l2 norm of gradient:  1.1178024729286933 ; l2 norm of weights:  7.192260993925316\n",
            "Iteration#:  5756 ; Loss:  3.0402214287245135 ; l2 norm of gradient:  1.1177850954247905 ; l2 norm of weights:  7.1922434250232685\n",
            "Iteration#:  5760 ; Loss:  3.0401711920317025 ; l2 norm of gradient:  1.1177677179753998 ; l2 norm of weights:  7.1922258563754\n",
            "Iteration#:  5764 ; Loss:  3.040120957087554 ; l2 norm of gradient:  1.1177503405804636 ; l2 norm of weights:  7.192208287981702\n",
            "Iteration#:  5768 ; Loss:  3.040070723495981 ; l2 norm of gradient:  1.117732963239922 ; l2 norm of weights:  7.192190719842171\n",
            "Iteration#:  5772 ; Loss:  3.0400204918988276 ; l2 norm of gradient:  1.1177155859537156 ; l2 norm of weights:  7.192173151956797\n",
            "Iteration#:  5776 ; Loss:  3.0399702617161477 ; l2 norm of gradient:  1.1176982087217835 ; l2 norm of weights:  7.192155584325573\n",
            "Iteration#:  5780 ; Loss:  3.039920032663076 ; l2 norm of gradient:  1.1176808315440665 ; l2 norm of weights:  7.192138016948493\n",
            "Iteration#:  5784 ; Loss:  3.039869804868819 ; l2 norm of gradient:  1.1176634544205073 ; l2 norm of weights:  7.192120449825549\n",
            "Iteration#:  5788 ; Loss:  3.039819578925422 ; l2 norm of gradient:  1.1176460773510446 ; l2 norm of weights:  7.1921028829567355\n",
            "Iteration#:  5792 ; Loss:  3.0397693550906597 ; l2 norm of gradient:  1.1176287003356196 ; l2 norm of weights:  7.192085316342043\n",
            "Iteration#:  5796 ; Loss:  3.0397191323964257 ; l2 norm of gradient:  1.117611323374172 ; l2 norm of weights:  7.192067749981466\n",
            "Iteration#:  5800 ; Loss:  3.0396689112693926 ; l2 norm of gradient:  1.1175939464666445 ; l2 norm of weights:  7.192050183874995\n",
            "Iteration#:  5804 ; Loss:  3.0396186917469574 ; l2 norm of gradient:  1.1175765696129758 ; l2 norm of weights:  7.192032618022628\n",
            "Iteration#:  5808 ; Loss:  3.0395684739581124 ; l2 norm of gradient:  1.1175591928131066 ; l2 norm of weights:  7.192015052424353\n",
            "Iteration#:  5812 ; Loss:  3.0395182568244854 ; l2 norm of gradient:  1.1175418160669792 ; l2 norm of weights:  7.191997487080165\n",
            "Iteration#:  5816 ; Loss:  3.0394680419863023 ; l2 norm of gradient:  1.1175244393745334 ; l2 norm of weights:  7.1919799219900575\n",
            "Iteration#:  5820 ; Loss:  3.0394178288139733 ; l2 norm of gradient:  1.11750706273571 ; l2 norm of weights:  7.191962357154019\n",
            "Iteration#:  5824 ; Loss:  3.0393676172153565 ; l2 norm of gradient:  1.1174896861504497 ; l2 norm of weights:  7.191944792572049\n",
            "Iteration#:  5828 ; Loss:  3.039317406844714 ; l2 norm of gradient:  1.1174723096186945 ; l2 norm of weights:  7.191927228244136\n",
            "Iteration#:  5832 ; Loss:  3.039267198420784 ; l2 norm of gradient:  1.1174549331403836 ; l2 norm of weights:  7.191909664170275\n",
            "Iteration#:  5836 ; Loss:  3.0392169905061817 ; l2 norm of gradient:  1.11743755671546 ; l2 norm of weights:  7.191892100350458\n",
            "Iteration#:  5840 ; Loss:  3.0391667852033297 ; l2 norm of gradient:  1.1174201803438615 ; l2 norm of weights:  7.191874536784677\n",
            "Iteration#:  5844 ; Loss:  3.0391165812427956 ; l2 norm of gradient:  1.1174028040255324 ; l2 norm of weights:  7.191856973472926\n",
            "Iteration#:  5848 ; Loss:  3.0390663791755315 ; l2 norm of gradient:  1.1173854277604116 ; l2 norm of weights:  7.191839410415197\n",
            "Iteration#:  5852 ; Loss:  3.039016178565431 ; l2 norm of gradient:  1.1173680515484412 ; l2 norm of weights:  7.191821847611484\n",
            "Iteration#:  5856 ; Loss:  3.038965979362784 ; l2 norm of gradient:  1.117350675389561 ; l2 norm of weights:  7.19180428506178\n",
            "Iteration#:  5860 ; Loss:  3.0389157815936656 ; l2 norm of gradient:  1.117333299283714 ; l2 norm of weights:  7.191786722766077\n",
            "Iteration#:  5864 ; Loss:  3.038865584795776 ; l2 norm of gradient:  1.117315923230839 ; l2 norm of weights:  7.191769160724369\n",
            "Iteration#:  5868 ; Loss:  3.0388153901402344 ; l2 norm of gradient:  1.117298547230878 ; l2 norm of weights:  7.191751598936646\n",
            "Iteration#:  5872 ; Loss:  3.038765197290372 ; l2 norm of gradient:  1.1172811712837731 ; l2 norm of weights:  7.191734037402904\n",
            "Iteration#:  5876 ; Loss:  3.0387150051680503 ; l2 norm of gradient:  1.117263795389466 ; l2 norm of weights:  7.191716476123137\n",
            "Iteration#:  5880 ; Loss:  3.0386648154272877 ; l2 norm of gradient:  1.1172464195478948 ; l2 norm of weights:  7.191698915097334\n",
            "Iteration#:  5884 ; Loss:  3.0386146271781715 ; l2 norm of gradient:  1.1172290437590033 ; l2 norm of weights:  7.191681354325488\n",
            "Iteration#:  5888 ; Loss:  3.0385644403805556 ; l2 norm of gradient:  1.1172116680227318 ; l2 norm of weights:  7.191663793807598\n",
            "Iteration#:  5892 ; Loss:  3.038514254451721 ; l2 norm of gradient:  1.1171942923390223 ; l2 norm of weights:  7.191646233543648\n",
            "Iteration#:  5896 ; Loss:  3.038464070733497 ; l2 norm of gradient:  1.1171769167078163 ; l2 norm of weights:  7.191628673533637\n",
            "Iteration#:  5900 ; Loss:  3.0384138885284733 ; l2 norm of gradient:  1.1171595411290547 ; l2 norm of weights:  7.191611113777557\n",
            "Iteration#:  5904 ; Loss:  3.038363707857438 ; l2 norm of gradient:  1.1171421656026779 ; l2 norm of weights:  7.191593554275401\n",
            "Iteration#:  5908 ; Loss:  3.0383135289845398 ; l2 norm of gradient:  1.1171247901286283 ; l2 norm of weights:  7.191575995027159\n",
            "Iteration#:  5912 ; Loss:  3.038263351778158 ; l2 norm of gradient:  1.117107414706848 ; l2 norm of weights:  7.191558436032827\n",
            "Iteration#:  5916 ; Loss:  3.038213175148938 ; l2 norm of gradient:  1.1170900393372765 ; l2 norm of weights:  7.191540877292396\n",
            "Iteration#:  5920 ; Loss:  3.0381630005793516 ; l2 norm of gradient:  1.1170726640198578 ; l2 norm of weights:  7.19152331880586\n",
            "Iteration#:  5924 ; Loss:  3.0381128277408536 ; l2 norm of gradient:  1.1170552887545313 ; l2 norm of weights:  7.191505760573213\n",
            "Iteration#:  5928 ; Loss:  3.0380626565858133 ; l2 norm of gradient:  1.1170379135412398 ; l2 norm of weights:  7.191488202594447\n",
            "Iteration#:  5932 ; Loss:  3.038012487004695 ; l2 norm of gradient:  1.117020538379924 ; l2 norm of weights:  7.191470644869552\n",
            "Iteration#:  5936 ; Loss:  3.03796231885625 ; l2 norm of gradient:  1.1170031632705264 ; l2 norm of weights:  7.191453087398525\n",
            "Iteration#:  5940 ; Loss:  3.0379121516704837 ; l2 norm of gradient:  1.1169857882129879 ; l2 norm of weights:  7.191435530181357\n",
            "Iteration#:  5944 ; Loss:  3.0378619864664405 ; l2 norm of gradient:  1.1169684132072502 ; l2 norm of weights:  7.191417973218041\n",
            "Iteration#:  5948 ; Loss:  3.037811823242467 ; l2 norm of gradient:  1.1169510382532564 ; l2 norm of weights:  7.19140041650857\n",
            "Iteration#:  5952 ; Loss:  3.037761660960607 ; l2 norm of gradient:  1.116933663350946 ; l2 norm of weights:  7.1913828600529355\n",
            "Iteration#:  5956 ; Loss:  3.037711500522656 ; l2 norm of gradient:  1.1169162885002608 ; l2 norm of weights:  7.191365303851133\n",
            "Iteration#:  5960 ; Loss:  3.037661341817039 ; l2 norm of gradient:  1.116898913701145 ; l2 norm of weights:  7.191347747903154\n",
            "Iteration#:  5964 ; Loss:  3.0376111845458635 ; l2 norm of gradient:  1.1168815389535391 ; l2 norm of weights:  7.191330192208992\n",
            "Iteration#:  5968 ; Loss:  3.0375610281635175 ; l2 norm of gradient:  1.1168641642573844 ; l2 norm of weights:  7.191312636768639\n",
            "Iteration#:  5972 ; Loss:  3.037510874066011 ; l2 norm of gradient:  1.1168467896126233 ; l2 norm of weights:  7.191295081582088\n",
            "Iteration#:  5976 ; Loss:  3.037460721419098 ; l2 norm of gradient:  1.1168294150191966 ; l2 norm of weights:  7.191277526649332\n",
            "Iteration#:  5980 ; Loss:  3.037410570648947 ; l2 norm of gradient:  1.116812040477048 ; l2 norm of weights:  7.191259971970365\n",
            "Iteration#:  5984 ; Loss:  3.037360421012492 ; l2 norm of gradient:  1.1167946659861185 ; l2 norm of weights:  7.19124241754518\n",
            "Iteration#:  5988 ; Loss:  3.0373102732490924 ; l2 norm of gradient:  1.116777291546351 ; l2 norm of weights:  7.191224863373767\n",
            "Iteration#:  5992 ; Loss:  3.0372601258335146 ; l2 norm of gradient:  1.1167599171576847 ; l2 norm of weights:  7.191207309456122\n",
            "Iteration#:  5996 ; Loss:  3.0372099811051525 ; l2 norm of gradient:  1.1167425428200655 ; l2 norm of weights:  7.191189755792236\n",
            "Iteration#:  6000 ; Loss:  3.037159837810778 ; l2 norm of gradient:  1.1167251685334325 ; l2 norm of weights:  7.191172202382104\n",
            "Iteration#:  6004 ; Loss:  3.0371096963287347 ; l2 norm of gradient:  1.1167077942977293 ; l2 norm of weights:  7.191154649225717\n",
            "Iteration#:  6008 ; Loss:  3.03705955619916 ; l2 norm of gradient:  1.1166904201128964 ; l2 norm of weights:  7.191137096323067\n",
            "Iteration#:  6012 ; Loss:  3.0370094176641365 ; l2 norm of gradient:  1.1166730459788783 ; l2 norm of weights:  7.191119543674149\n",
            "Iteration#:  6016 ; Loss:  3.0369592797078653 ; l2 norm of gradient:  1.1166556718956149 ; l2 norm of weights:  7.191101991278955\n",
            "Iteration#:  6020 ; Loss:  3.0369091438718177 ; l2 norm of gradient:  1.1166382978630507 ; l2 norm of weights:  7.191084439137478\n",
            "Iteration#:  6024 ; Loss:  3.0368590100947097 ; l2 norm of gradient:  1.1166209238811262 ; l2 norm of weights:  7.191066887249712\n",
            "Iteration#:  6028 ; Loss:  3.036808877889365 ; l2 norm of gradient:  1.1166035499497833 ; l2 norm of weights:  7.191049335615649\n",
            "Iteration#:  6032 ; Loss:  3.0367587472165036 ; l2 norm of gradient:  1.1165861760689666 ; l2 norm of weights:  7.191031784235281\n",
            "Iteration#:  6036 ; Loss:  3.0367086177370335 ; l2 norm of gradient:  1.1165688022386164 ; l2 norm of weights:  7.191014233108601\n",
            "Iteration#:  6040 ; Loss:  3.036658489451091 ; l2 norm of gradient:  1.116551428458675 ; l2 norm of weights:  7.190996682235603\n",
            "Iteration#:  6044 ; Loss:  3.0366083629018092 ; l2 norm of gradient:  1.1165340547290854 ; l2 norm of weights:  7.19097913161628\n",
            "Iteration#:  6048 ; Loss:  3.036558238471512 ; l2 norm of gradient:  1.1165166810497902 ; l2 norm of weights:  7.190961581250624\n",
            "Iteration#:  6052 ; Loss:  3.0365081154269666 ; l2 norm of gradient:  1.1164993074207312 ; l2 norm of weights:  7.190944031138629\n",
            "Iteration#:  6056 ; Loss:  3.036457994105631 ; l2 norm of gradient:  1.1164819338418512 ; l2 norm of weights:  7.190926481280285\n",
            "Iteration#:  6060 ; Loss:  3.0364078733637547 ; l2 norm of gradient:  1.1164645603130932 ; l2 norm of weights:  7.190908931675588\n",
            "Iteration#:  6064 ; Loss:  3.0363577547334333 ; l2 norm of gradient:  1.1164471868343993 ; l2 norm of weights:  7.190891382324529\n",
            "Iteration#:  6068 ; Loss:  3.036307637875801 ; l2 norm of gradient:  1.116429813405711 ; l2 norm of weights:  7.190873833227103\n",
            "Iteration#:  6072 ; Loss:  3.0362575226613235 ; l2 norm of gradient:  1.1164124400269724 ; l2 norm of weights:  7.190856284383301\n",
            "Iteration#:  6076 ; Loss:  3.0362074091250575 ; l2 norm of gradient:  1.116395066698125 ; l2 norm of weights:  7.190838735793118\n",
            "Iteration#:  6080 ; Loss:  3.0361572966858588 ; l2 norm of gradient:  1.1163776934191127 ; l2 norm of weights:  7.190821187456545\n",
            "Iteration#:  6084 ; Loss:  3.036107185158068 ; l2 norm of gradient:  1.1163603201898769 ; l2 norm of weights:  7.1908036393735735\n",
            "Iteration#:  6088 ; Loss:  3.036057076161757 ; l2 norm of gradient:  1.1163429470103605 ; l2 norm of weights:  7.1907860915442\n",
            "Iteration#:  6092 ; Loss:  3.0360069688723446 ; l2 norm of gradient:  1.1163255738805065 ; l2 norm of weights:  7.190768543968415\n",
            "Iteration#:  6096 ; Loss:  3.035956862877682 ; l2 norm of gradient:  1.1163082008002585 ; l2 norm of weights:  7.190750996646211\n",
            "Iteration#:  6100 ; Loss:  3.0359067583021027 ; l2 norm of gradient:  1.1162908277695576 ; l2 norm of weights:  7.190733449577583\n",
            "Iteration#:  6104 ; Loss:  3.035856655472006 ; l2 norm of gradient:  1.1162734547883473 ; l2 norm of weights:  7.190715902762523\n",
            "Iteration#:  6108 ; Loss:  3.035806553454164 ; l2 norm of gradient:  1.1162560818565703 ; l2 norm of weights:  7.190698356201024\n",
            "Iteration#:  6112 ; Loss:  3.035756453965412 ; l2 norm of gradient:  1.1162387089741708 ; l2 norm of weights:  7.190680809893078\n",
            "Iteration#:  6116 ; Loss:  3.0357063554270294 ; l2 norm of gradient:  1.1162213361410886 ; l2 norm of weights:  7.190663263838677\n",
            "Iteration#:  6120 ; Loss:  3.035656258856987 ; l2 norm of gradient:  1.1162039633572698 ; l2 norm of weights:  7.190645718037818\n",
            "Iteration#:  6124 ; Loss:  3.0356061638880716 ; l2 norm of gradient:  1.1161865906226565 ; l2 norm of weights:  7.190628172490489\n",
            "Iteration#:  6128 ; Loss:  3.0355560697217645 ; l2 norm of gradient:  1.1161692179371903 ; l2 norm of weights:  7.190610627196686\n",
            "Iteration#:  6132 ; Loss:  3.035505977373342 ; l2 norm of gradient:  1.1161518453008155 ; l2 norm of weights:  7.190593082156401\n",
            "Iteration#:  6136 ; Loss:  3.0354558869308637 ; l2 norm of gradient:  1.1161344727134752 ; l2 norm of weights:  7.190575537369626\n",
            "Iteration#:  6140 ; Loss:  3.035405798216855 ; l2 norm of gradient:  1.1161171001751125 ; l2 norm of weights:  7.190557992836356\n",
            "Iteration#:  6144 ; Loss:  3.035355710775211 ; l2 norm of gradient:  1.116099727685669 ; l2 norm of weights:  7.190540448556583\n",
            "Iteration#:  6148 ; Loss:  3.0353056249388644 ; l2 norm of gradient:  1.1160823552450883 ; l2 norm of weights:  7.190522904530299\n",
            "Iteration#:  6152 ; Loss:  3.0352555402790946 ; l2 norm of gradient:  1.1160649828533145 ; l2 norm of weights:  7.190505360757497\n",
            "Iteration#:  6156 ; Loss:  3.0352054576070695 ; l2 norm of gradient:  1.116047610510291 ; l2 norm of weights:  7.1904878172381705\n",
            "Iteration#:  6160 ; Loss:  3.0351553760546253 ; l2 norm of gradient:  1.1160302382159608 ; l2 norm of weights:  7.190470273972314\n",
            "Iteration#:  6164 ; Loss:  3.0351052969391352 ; l2 norm of gradient:  1.1160128659702648 ; l2 norm of weights:  7.190452730959915\n",
            "Iteration#:  6168 ; Loss:  3.0350552190100477 ; l2 norm of gradient:  1.1159954937731487 ; l2 norm of weights:  7.190435188200973\n",
            "Iteration#:  6172 ; Loss:  3.0350051414410624 ; l2 norm of gradient:  1.1159781216245555 ; l2 norm of weights:  7.190417645695478\n",
            "Iteration#:  6176 ; Loss:  3.0349550666845215 ; l2 norm of gradient:  1.1159607495244295 ; l2 norm of weights:  7.190400103443422\n",
            "Iteration#:  6180 ; Loss:  3.0349049934568972 ; l2 norm of gradient:  1.1159433774727114 ; l2 norm of weights:  7.190382561444798\n",
            "Iteration#:  6184 ; Loss:  3.0348549211760005 ; l2 norm of gradient:  1.1159260054693463 ; l2 norm of weights:  7.190365019699599\n",
            "Iteration#:  6188 ; Loss:  3.0348048511807018 ; l2 norm of gradient:  1.1159086335142767 ; l2 norm of weights:  7.19034747820782\n",
            "Iteration#:  6192 ; Loss:  3.0347547827048693 ; l2 norm of gradient:  1.115891261607448 ; l2 norm of weights:  7.1903299369694516\n",
            "Iteration#:  6196 ; Loss:  3.0347047144024755 ; l2 norm of gradient:  1.1158738897488012 ; l2 norm of weights:  7.1903123959844875\n",
            "Iteration#:  6200 ; Loss:  3.034654649027977 ; l2 norm of gradient:  1.1158565179382811 ; l2 norm of weights:  7.190294855252922\n",
            "Iteration#:  6204 ; Loss:  3.0346045850731658 ; l2 norm of gradient:  1.1158391461758308 ; l2 norm of weights:  7.190277314774744\n",
            "Iteration#:  6208 ; Loss:  3.034554522375933 ; l2 norm of gradient:  1.1158217744613939 ; l2 norm of weights:  7.19025977454995\n",
            "Iteration#:  6212 ; Loss:  3.0345044615937424 ; l2 norm of gradient:  1.1158044027949148 ; l2 norm of weights:  7.190242234578531\n",
            "Iteration#:  6216 ; Loss:  3.034454401612777 ; l2 norm of gradient:  1.1157870311763358 ; l2 norm of weights:  7.190224694860483\n",
            "Iteration#:  6220 ; Loss:  3.0344043433110537 ; l2 norm of gradient:  1.1157696596056015 ; l2 norm of weights:  7.190207155395795\n",
            "Iteration#:  6224 ; Loss:  3.0343542875036777 ; l2 norm of gradient:  1.1157522880826551 ; l2 norm of weights:  7.190189616184461\n",
            "Iteration#:  6228 ; Loss:  3.0343042330535734 ; l2 norm of gradient:  1.1157349166074397 ; l2 norm of weights:  7.1901720772264754\n",
            "Iteration#:  6232 ; Loss:  3.0342541793390345 ; l2 norm of gradient:  1.1157175451799013 ; l2 norm of weights:  7.1901545385218295\n",
            "Iteration#:  6236 ; Loss:  3.034204127061512 ; l2 norm of gradient:  1.1157001737999808 ; l2 norm of weights:  7.190137000070517\n",
            "Iteration#:  6240 ; Loss:  3.034154076902862 ; l2 norm of gradient:  1.1156828024676237 ; l2 norm of weights:  7.190119461872531\n",
            "Iteration#:  6244 ; Loss:  3.0341040286214094 ; l2 norm of gradient:  1.1156654311827738 ; l2 norm of weights:  7.1901019239278625\n",
            "Iteration#:  6248 ; Loss:  3.034053981737498 ; l2 norm of gradient:  1.1156480599453744 ; l2 norm of weights:  7.1900843862365065\n",
            "Iteration#:  6252 ; Loss:  3.034003936174599 ; l2 norm of gradient:  1.1156306887553686 ; l2 norm of weights:  7.190066848798456\n",
            "Iteration#:  6256 ; Loss:  3.0339538923033245 ; l2 norm of gradient:  1.1156133176127019 ; l2 norm of weights:  7.190049311613702\n",
            "Iteration#:  6260 ; Loss:  3.033903849187897 ; l2 norm of gradient:  1.1155959465173175 ; l2 norm of weights:  7.190031774682239\n",
            "Iteration#:  6264 ; Loss:  3.033853808549469 ; l2 norm of gradient:  1.1155785754691598 ; l2 norm of weights:  7.190014238004058\n",
            "Iteration#:  6268 ; Loss:  3.033803769418318 ; l2 norm of gradient:  1.115561204468172 ; l2 norm of weights:  7.189996701579155\n",
            "Iteration#:  6272 ; Loss:  3.033753731548434 ; l2 norm of gradient:  1.115543833514298 ; l2 norm of weights:  7.189979165407522\n",
            "Iteration#:  6276 ; Loss:  3.0337036955229317 ; l2 norm of gradient:  1.1155264626074823 ; l2 norm of weights:  7.189961629489147\n",
            "Iteration#:  6280 ; Loss:  3.0336536600424244 ; l2 norm of gradient:  1.11550909174767 ; l2 norm of weights:  7.18994409382403\n",
            "Iteration#:  6284 ; Loss:  3.033603627253477 ; l2 norm of gradient:  1.1154917209348034 ; l2 norm of weights:  7.18992655841216\n",
            "Iteration#:  6288 ; Loss:  3.033553596115486 ; l2 norm of gradient:  1.1154743501688273 ; l2 norm of weights:  7.18990902325353\n",
            "Iteration#:  6292 ; Loss:  3.0335035660268432 ; l2 norm of gradient:  1.1154569794496874 ; l2 norm of weights:  7.189891488348135\n",
            "Iteration#:  6296 ; Loss:  3.033453537758474 ; l2 norm of gradient:  1.1154396087773257 ; l2 norm of weights:  7.189873953695965\n",
            "Iteration#:  6300 ; Loss:  3.0334035100426777 ; l2 norm of gradient:  1.1154222381516872 ; l2 norm of weights:  7.189856419297015\n",
            "Iteration#:  6304 ; Loss:  3.033353484877622 ; l2 norm of gradient:  1.1154048675727157 ; l2 norm of weights:  7.189838885151278\n",
            "Iteration#:  6308 ; Loss:  3.03330346112548 ; l2 norm of gradient:  1.1153874970403574 ; l2 norm of weights:  7.189821351258745\n",
            "Iteration#:  6312 ; Loss:  3.0332534391012898 ; l2 norm of gradient:  1.1153701265545541 ; l2 norm of weights:  7.18980381761941\n",
            "Iteration#:  6316 ; Loss:  3.033203418523721 ; l2 norm of gradient:  1.1153527561152519 ; l2 norm of weights:  7.189786284233265\n",
            "Iteration#:  6320 ; Loss:  3.033153398748494 ; l2 norm of gradient:  1.115335385722394 ; l2 norm of weights:  7.189768751100306\n",
            "Iteration#:  6324 ; Loss:  3.033103381577313 ; l2 norm of gradient:  1.1153180153759252 ; l2 norm of weights:  7.189751218220522\n",
            "Iteration#:  6328 ; Loss:  3.0330533653537346 ; l2 norm of gradient:  1.1153006450757905 ; l2 norm of weights:  7.189733685593909\n",
            "Iteration#:  6332 ; Loss:  3.033003351270078 ; l2 norm of gradient:  1.1152832748219337 ; l2 norm of weights:  7.1897161532204565\n",
            "Iteration#:  6336 ; Loss:  3.032953338231817 ; l2 norm of gradient:  1.1152659046143003 ; l2 norm of weights:  7.189698621100161\n",
            "Iteration#:  6340 ; Loss:  3.0329033264392975 ; l2 norm of gradient:  1.1152485344528333 ; l2 norm of weights:  7.189681089233012\n",
            "Iteration#:  6344 ; Loss:  3.0328533164830436 ; l2 norm of gradient:  1.1152311643374777 ; l2 norm of weights:  7.189663557619005\n",
            "Iteration#:  6348 ; Loss:  3.0328033085842745 ; l2 norm of gradient:  1.1152137942681797 ; l2 norm of weights:  7.189646026258132\n",
            "Iteration#:  6352 ; Loss:  3.0327533018687163 ; l2 norm of gradient:  1.1151964242448802 ; l2 norm of weights:  7.189628495150386\n",
            "Iteration#:  6356 ; Loss:  3.0327032968426257 ; l2 norm of gradient:  1.1151790542675284 ; l2 norm of weights:  7.189610964295759\n",
            "Iteration#:  6360 ; Loss:  3.032653292285349 ; l2 norm of gradient:  1.1151616843360652 ; l2 norm of weights:  7.189593433694245\n",
            "Iteration#:  6364 ; Loss:  3.0326032906476152 ; l2 norm of gradient:  1.1151443144504387 ; l2 norm of weights:  7.189575903345837\n",
            "Iteration#:  6368 ; Loss:  3.032553290038069 ; l2 norm of gradient:  1.11512694461059 ; l2 norm of weights:  7.189558373250526\n",
            "Iteration#:  6372 ; Loss:  3.0325032914087204 ; l2 norm of gradient:  1.115109574816467 ; l2 norm of weights:  7.189540843408308\n",
            "Iteration#:  6376 ; Loss:  3.0324532939015114 ; l2 norm of gradient:  1.1150922050680123 ; l2 norm of weights:  7.189523313819173\n",
            "Iteration#:  6380 ; Loss:  3.032403297655075 ; l2 norm of gradient:  1.1150748353651727 ; l2 norm of weights:  7.1895057844831145\n",
            "Iteration#:  6384 ; Loss:  3.0323533031537755 ; l2 norm of gradient:  1.115057465707891 ; l2 norm of weights:  7.189488255400128\n",
            "Iteration#:  6388 ; Loss:  3.032303310789456 ; l2 norm of gradient:  1.115040096096113 ; l2 norm of weights:  7.189470726570202\n",
            "Iteration#:  6392 ; Loss:  3.0322533196151293 ; l2 norm of gradient:  1.1150227265297847 ; l2 norm of weights:  7.189453197993334\n",
            "Iteration#:  6396 ; Loss:  3.0322033290193935 ; l2 norm of gradient:  1.1150053570088483 ; l2 norm of weights:  7.189435669669513\n",
            "Iteration#:  6400 ; Loss:  3.0321533411697654 ; l2 norm of gradient:  1.1149879875332518 ; l2 norm of weights:  7.189418141598734\n",
            "Iteration#:  6404 ; Loss:  3.032103354739848 ; l2 norm of gradient:  1.1149706181029382 ; l2 norm of weights:  7.189400613780988\n",
            "Iteration#:  6408 ; Loss:  3.032053370144696 ; l2 norm of gradient:  1.1149532487178533 ; l2 norm of weights:  7.189383086216272\n",
            "Iteration#:  6412 ; Loss:  3.0320033866547904 ; l2 norm of gradient:  1.1149358793779416 ; l2 norm of weights:  7.189365558904575\n",
            "Iteration#:  6416 ; Loss:  3.0319534041445415 ; l2 norm of gradient:  1.11491851008315 ; l2 norm of weights:  7.189348031845891\n",
            "Iteration#:  6420 ; Loss:  3.0319034238885747 ; l2 norm of gradient:  1.1149011408334206 ; l2 norm of weights:  7.189330505040211\n",
            "Iteration#:  6424 ; Loss:  3.0318534455433794 ; l2 norm of gradient:  1.1148837716287008 ; l2 norm of weights:  7.189312978487532\n",
            "Iteration#:  6428 ; Loss:  3.0318034680975843 ; l2 norm of gradient:  1.1148664024689348 ; l2 norm of weights:  7.189295452187844\n",
            "Iteration#:  6432 ; Loss:  3.0317534923738685 ; l2 norm of gradient:  1.1148490333540686 ; l2 norm of weights:  7.18927792614114\n",
            "Iteration#:  6436 ; Loss:  3.031703517773355 ; l2 norm of gradient:  1.1148316642840477 ; l2 norm of weights:  7.189260400347413\n",
            "Iteration#:  6440 ; Loss:  3.031653545298495 ; l2 norm of gradient:  1.1148142952588151 ; l2 norm of weights:  7.189242874806658\n",
            "Iteration#:  6444 ; Loss:  3.031603574421145 ; l2 norm of gradient:  1.1147969262783195 ; l2 norm of weights:  7.189225349518865\n",
            "Iteration#:  6448 ; Loss:  3.031553604947371 ; l2 norm of gradient:  1.1147795573425032 ; l2 norm of weights:  7.189207824484028\n",
            "Iteration#:  6452 ; Loss:  3.031503637069584 ; l2 norm of gradient:  1.114762188451313 ; l2 norm of weights:  7.18919029970214\n",
            "Iteration#:  6456 ; Loss:  3.031453670088233 ; l2 norm of gradient:  1.1147448196046938 ; l2 norm of weights:  7.189172775173196\n",
            "Iteration#:  6460 ; Loss:  3.0314037053765217 ; l2 norm of gradient:  1.1147274508025922 ; l2 norm of weights:  7.189155250897184\n",
            "Iteration#:  6464 ; Loss:  3.0313537424069033 ; l2 norm of gradient:  1.114710082044952 ; l2 norm of weights:  7.189137726874101\n",
            "Iteration#:  6468 ; Loss:  3.0313037806387464 ; l2 norm of gradient:  1.1146927133317186 ; l2 norm of weights:  7.189120203103939\n",
            "Iteration#:  6472 ; Loss:  3.0312538196579544 ; l2 norm of gradient:  1.114675344662839 ; l2 norm of weights:  7.189102679586689\n",
            "Iteration#:  6476 ; Loss:  3.0312038613046393 ; l2 norm of gradient:  1.1146579760382578 ; l2 norm of weights:  7.189085156322345\n",
            "Iteration#:  6480 ; Loss:  3.0311539042790097 ; l2 norm of gradient:  1.1146406074579216 ; l2 norm of weights:  7.189067633310901\n",
            "Iteration#:  6484 ; Loss:  3.0311039490046587 ; l2 norm of gradient:  1.1146232389217747 ; l2 norm of weights:  7.189050110552349\n",
            "Iteration#:  6488 ; Loss:  3.031053995070833 ; l2 norm of gradient:  1.1146058704297637 ; l2 norm of weights:  7.1890325880466825\n",
            "Iteration#:  6492 ; Loss:  3.0310040419451747 ; l2 norm of gradient:  1.1145885019818338 ; l2 norm of weights:  7.1890150657938925\n",
            "Iteration#:  6496 ; Loss:  3.0309540914141273 ; l2 norm of gradient:  1.1145711335779303 ; l2 norm of weights:  7.188997543793975\n",
            "Iteration#:  6500 ; Loss:  3.0309041419969573 ; l2 norm of gradient:  1.114553765217999 ; l2 norm of weights:  7.188980022046919\n",
            "Iteration#:  6504 ; Loss:  3.030854194245519 ; l2 norm of gradient:  1.114536396901987 ; l2 norm of weights:  7.18896250055272\n",
            "Iteration#:  6508 ; Loss:  3.030804247391394 ; l2 norm of gradient:  1.1145190286298379 ; l2 norm of weights:  7.188944979311371\n",
            "Iteration#:  6512 ; Loss:  3.0307543030195383 ; l2 norm of gradient:  1.1145016604014994 ; l2 norm of weights:  7.188927458322864\n",
            "Iteration#:  6516 ; Loss:  3.03070435971391 ; l2 norm of gradient:  1.1144842922169167 ; l2 norm of weights:  7.188909937587191\n",
            "Iteration#:  6520 ; Loss:  3.030654418610678 ; l2 norm of gradient:  1.114466924076035 ; l2 norm of weights:  7.188892417104348\n",
            "Iteration#:  6524 ; Loss:  3.030604478537206 ; l2 norm of gradient:  1.114449555978801 ; l2 norm of weights:  7.188874896874324\n",
            "Iteration#:  6528 ; Loss:  3.0305545393542763 ; l2 norm of gradient:  1.1144321879251609 ; l2 norm of weights:  7.188857376897116\n",
            "Iteration#:  6532 ; Loss:  3.0305046027775635 ; l2 norm of gradient:  1.11441481991506 ; l2 norm of weights:  7.188839857172713\n",
            "Iteration#:  6536 ; Loss:  3.030454667379277 ; l2 norm of gradient:  1.1143974519484445 ; l2 norm of weights:  7.188822337701109\n",
            "Iteration#:  6540 ; Loss:  3.030404733569095 ; l2 norm of gradient:  1.114380084025261 ; l2 norm of weights:  7.188804818482298\n",
            "Iteration#:  6544 ; Loss:  3.030354800960903 ; l2 norm of gradient:  1.1143627161454523 ; l2 norm of weights:  7.188787299516274\n",
            "Iteration#:  6548 ; Loss:  3.0303048703062614 ; l2 norm of gradient:  1.1143453483089703 ; l2 norm of weights:  7.188769780803025\n",
            "Iteration#:  6552 ; Loss:  3.030254941020282 ; l2 norm of gradient:  1.1143279805157569 ; l2 norm of weights:  7.1887522623425495\n",
            "Iteration#:  6556 ; Loss:  3.030205013773439 ; l2 norm of gradient:  1.114310612765759 ; l2 norm of weights:  7.188734744134837\n",
            "Iteration#:  6560 ; Loss:  3.030155087579368 ; l2 norm of gradient:  1.1142932450589231 ; l2 norm of weights:  7.1887172261798815\n",
            "Iteration#:  6564 ; Loss:  3.0301051624174553 ; l2 norm of gradient:  1.1142758773951955 ; l2 norm of weights:  7.188699708477675\n",
            "Iteration#:  6568 ; Loss:  3.030055239446254 ; l2 norm of gradient:  1.114258509774523 ; l2 norm of weights:  7.188682191028212\n",
            "Iteration#:  6572 ; Loss:  3.030005318479317 ; l2 norm of gradient:  1.114241142196851 ; l2 norm of weights:  7.188664673831484\n",
            "Iteration#:  6576 ; Loss:  3.029955398608069 ; l2 norm of gradient:  1.1142237746621257 ; l2 norm of weights:  7.188647156887486\n",
            "Iteration#:  6580 ; Loss:  3.029905479384677 ; l2 norm of gradient:  1.1142064071702942 ; l2 norm of weights:  7.188629640196208\n",
            "Iteration#:  6584 ; Loss:  3.0298555632131623 ; l2 norm of gradient:  1.1141890397213023 ; l2 norm of weights:  7.1886121237576415\n",
            "Iteration#:  6588 ; Loss:  3.0298056479429363 ; l2 norm of gradient:  1.1141716723150952 ; l2 norm of weights:  7.188594607571786\n",
            "Iteration#:  6592 ; Loss:  3.0297557343393255 ; l2 norm of gradient:  1.1141543049516223 ; l2 norm of weights:  7.188577091638628\n",
            "Iteration#:  6596 ; Loss:  3.029705822224658 ; l2 norm of gradient:  1.114136937630828 ; l2 norm of weights:  7.1885595759581635\n",
            "Iteration#:  6600 ; Loss:  3.0296559112322705 ; l2 norm of gradient:  1.1141195703526592 ; l2 norm of weights:  7.188542060530383\n",
            "Iteration#:  6604 ; Loss:  3.0296060022853184 ; l2 norm of gradient:  1.114102203117062 ; l2 norm of weights:  7.188524545355283\n",
            "Iteration#:  6608 ; Loss:  3.0295560949252747 ; l2 norm of gradient:  1.1140848359239834 ; l2 norm of weights:  7.188507030432854\n",
            "Iteration#:  6612 ; Loss:  3.029506188777336 ; l2 norm of gradient:  1.1140674687733707 ; l2 norm of weights:  7.18848951576309\n",
            "Iteration#:  6616 ; Loss:  3.0294562843514496 ; l2 norm of gradient:  1.1140501016651685 ; l2 norm of weights:  7.188472001345982\n",
            "Iteration#:  6620 ; Loss:  3.029406381434669 ; l2 norm of gradient:  1.1140327345993248 ; l2 norm of weights:  7.188454487181525\n",
            "Iteration#:  6624 ; Loss:  3.029356480167272 ; l2 norm of gradient:  1.1140153675757867 ; l2 norm of weights:  7.1884369732697095\n",
            "Iteration#:  6628 ; Loss:  3.0293065804154007 ; l2 norm of gradient:  1.1139980005945012 ; l2 norm of weights:  7.188419459610531\n",
            "Iteration#:  6632 ; Loss:  3.029256681537005 ; l2 norm of gradient:  1.1139806336554137 ; l2 norm of weights:  7.18840194620398\n",
            "Iteration#:  6636 ; Loss:  3.0292067855624496 ; l2 norm of gradient:  1.1139632667584707 ; l2 norm of weights:  7.188384433050053\n",
            "Iteration#:  6640 ; Loss:  3.0291568902629966 ; l2 norm of gradient:  1.1139458999036203 ; l2 norm of weights:  7.188366920148739\n",
            "Iteration#:  6644 ; Loss:  3.0291069966808957 ; l2 norm of gradient:  1.1139285330908089 ; l2 norm of weights:  7.188349407500031\n",
            "Iteration#:  6648 ; Loss:  3.029057104844175 ; l2 norm of gradient:  1.113911166319982 ; l2 norm of weights:  7.188331895103923\n",
            "Iteration#:  6652 ; Loss:  3.0290072137997957 ; l2 norm of gradient:  1.1138937995910898 ; l2 norm of weights:  7.18831438296041\n",
            "Iteration#:  6656 ; Loss:  3.02895732545417 ; l2 norm of gradient:  1.1138764329040745 ; l2 norm of weights:  7.188296871069483\n",
            "Iteration#:  6660 ; Loss:  3.0289074379750263 ; l2 norm of gradient:  1.1138590662588876 ; l2 norm of weights:  7.1882793594311325\n",
            "Iteration#:  6664 ; Loss:  3.028857552453088 ; l2 norm of gradient:  1.1138416996554739 ; l2 norm of weights:  7.188261848045356\n",
            "Iteration#:  6668 ; Loss:  3.0288076675016193 ; l2 norm of gradient:  1.11382433309378 ; l2 norm of weights:  7.188244336912144\n",
            "Iteration#:  6672 ; Loss:  3.028757784635551 ; l2 norm of gradient:  1.1138069665737536 ; l2 norm of weights:  7.188226826031489\n",
            "Iteration#:  6676 ; Loss:  3.028707904251683 ; l2 norm of gradient:  1.1137896000953424 ; l2 norm of weights:  7.188209315403384\n",
            "Iteration#:  6680 ; Loss:  3.0286580245508232 ; l2 norm of gradient:  1.1137722336584925 ; l2 norm of weights:  7.188191805027823\n",
            "Iteration#:  6684 ; Loss:  3.0286081457784126 ; l2 norm of gradient:  1.1137548672631514 ; l2 norm of weights:  7.188174294904797\n",
            "Iteration#:  6688 ; Loss:  3.0285582694519544 ; l2 norm of gradient:  1.1137375009092665 ; l2 norm of weights:  7.188156785034301\n",
            "Iteration#:  6692 ; Loss:  3.0285083945633255 ; l2 norm of gradient:  1.1137201345967842 ; l2 norm of weights:  7.188139275416326\n",
            "Iteration#:  6696 ; Loss:  3.0284585212401733 ; l2 norm of gradient:  1.1137027683256526 ; l2 norm of weights:  7.188121766050866\n",
            "Iteration#:  6700 ; Loss:  3.028408648835136 ; l2 norm of gradient:  1.1136854020958191 ; l2 norm of weights:  7.188104256937914\n",
            "Iteration#:  6704 ; Loss:  3.0283587788931112 ; l2 norm of gradient:  1.1136680359072302 ; l2 norm of weights:  7.188086748077462\n",
            "Iteration#:  6708 ; Loss:  3.028308910261241 ; l2 norm of gradient:  1.113650669759833 ; l2 norm of weights:  7.188069239469503\n",
            "Iteration#:  6712 ; Loss:  3.0282590430380543 ; l2 norm of gradient:  1.113633303653576 ; l2 norm of weights:  7.188051731114032\n",
            "Iteration#:  6716 ; Loss:  3.0282091767856683 ; l2 norm of gradient:  1.113615937588406 ; l2 norm of weights:  7.18803422301104\n",
            "Iteration#:  6720 ; Loss:  3.0281593127288136 ; l2 norm of gradient:  1.1135985715642698 ; l2 norm of weights:  7.188016715160518\n",
            "Iteration#:  6724 ; Loss:  3.0281094501880816 ; l2 norm of gradient:  1.1135812055811165 ; l2 norm of weights:  7.187999207562464\n",
            "Iteration#:  6728 ; Loss:  3.0280595897888394 ; l2 norm of gradient:  1.1135638396388918 ; l2 norm of weights:  7.187981700216865\n",
            "Iteration#:  6732 ; Loss:  3.0280097303202878 ; l2 norm of gradient:  1.1135464737375445 ; l2 norm of weights:  7.187964193123718\n",
            "Iteration#:  6736 ; Loss:  3.0279598717445046 ; l2 norm of gradient:  1.1135291078770206 ; l2 norm of weights:  7.187946686283013\n",
            "Iteration#:  6740 ; Loss:  3.0279100154525613 ; l2 norm of gradient:  1.113511742057269 ; l2 norm of weights:  7.187929179694745\n",
            "Iteration#:  6744 ; Loss:  3.0278601608821005 ; l2 norm of gradient:  1.1134943762782379 ; l2 norm of weights:  7.187911673358908\n",
            "Iteration#:  6748 ; Loss:  3.0278103076176253 ; l2 norm of gradient:  1.1134770105398732 ; l2 norm of weights:  7.187894167275491\n",
            "Iteration#:  6752 ; Loss:  3.0277604551277846 ; l2 norm of gradient:  1.1134596448421228 ; l2 norm of weights:  7.1878766614444904\n",
            "Iteration#:  6756 ; Loss:  3.0277106052317935 ; l2 norm of gradient:  1.1134422791849354 ; l2 norm of weights:  7.187859155865897\n",
            "Iteration#:  6760 ; Loss:  3.027660756768014 ; l2 norm of gradient:  1.1134249135682592 ; l2 norm of weights:  7.1878416505397045\n",
            "Iteration#:  6764 ; Loss:  3.0276109097398303 ; l2 norm of gradient:  1.1134075479920396 ; l2 norm of weights:  7.187824145465906\n",
            "Iteration#:  6768 ; Loss:  3.0275610639916968 ; l2 norm of gradient:  1.1133901824562271 ; l2 norm of weights:  7.187806640644494\n",
            "Iteration#:  6772 ; Loss:  3.0275112199661933 ; l2 norm of gradient:  1.113372816960767 ; l2 norm of weights:  7.1877891360754615\n",
            "Iteration#:  6776 ; Loss:  3.027461377756215 ; l2 norm of gradient:  1.1133554515056094 ; l2 norm of weights:  7.1877716317588005\n",
            "Iteration#:  6780 ; Loss:  3.027411536896791 ; l2 norm of gradient:  1.1133380860907 ; l2 norm of weights:  7.1877541276945065\n",
            "Iteration#:  6784 ; Loss:  3.0273616970700115 ; l2 norm of gradient:  1.1133207207159896 ; l2 norm of weights:  7.18773662388257\n",
            "Iteration#:  6788 ; Loss:  3.0273118593746227 ; l2 norm of gradient:  1.1133033553814233 ; l2 norm of weights:  7.187719120322983\n",
            "Iteration#:  6792 ; Loss:  3.0272620234150693 ; l2 norm of gradient:  1.1132859900869496 ; l2 norm of weights:  7.187701617015742\n",
            "Iteration#:  6796 ; Loss:  3.0272121886486563 ; l2 norm of gradient:  1.1132686248325188 ; l2 norm of weights:  7.187684113960836\n",
            "Iteration#:  6800 ; Loss:  3.027162354857263 ; l2 norm of gradient:  1.1132512596180757 ; l2 norm of weights:  7.18766661115826\n",
            "Iteration#:  6804 ; Loss:  3.0271125234802874 ; l2 norm of gradient:  1.1132338944435705 ; l2 norm of weights:  7.1876491086080065\n",
            "Iteration#:  6808 ; Loss:  3.027062693621284 ; l2 norm of gradient:  1.1132165293089507 ; l2 norm of weights:  7.187631606310069\n",
            "Iteration#:  6812 ; Loss:  3.0270128651809523 ; l2 norm of gradient:  1.113199164214165 ; l2 norm of weights:  7.187614104264439\n",
            "Iteration#:  6816 ; Loss:  3.026963037710158 ; l2 norm of gradient:  1.1131817991591613 ; l2 norm of weights:  7.187596602471111\n",
            "Iteration#:  6820 ; Loss:  3.0269132122691564 ; l2 norm of gradient:  1.1131644341438853 ; l2 norm of weights:  7.187579100930075\n",
            "Iteration#:  6824 ; Loss:  3.02686338850394 ; l2 norm of gradient:  1.1131470691682892 ; l2 norm of weights:  7.187561599641327\n",
            "Iteration#:  6828 ; Loss:  3.0268135665485603 ; l2 norm of gradient:  1.1131297042323196 ; l2 norm of weights:  7.187544098604859\n",
            "Iteration#:  6832 ; Loss:  3.0267637451687457 ; l2 norm of gradient:  1.1131123393359241 ; l2 norm of weights:  7.187526597820663\n",
            "Iteration#:  6836 ; Loss:  3.026713925990883 ; l2 norm of gradient:  1.1130949744790517 ; l2 norm of weights:  7.187509097288734\n",
            "Iteration#:  6840 ; Loss:  3.026664108542508 ; l2 norm of gradient:  1.1130776096616508 ; l2 norm of weights:  7.187491597009061\n",
            "Iteration#:  6844 ; Loss:  3.026614292537071 ; l2 norm of gradient:  1.11306024488367 ; l2 norm of weights:  7.187474096981641\n",
            "Iteration#:  6848 ; Loss:  3.026564477404855 ; l2 norm of gradient:  1.113042880145057 ; l2 norm of weights:  7.187456597206466\n",
            "Iteration#:  6852 ; Loss:  3.0265146645459606 ; l2 norm of gradient:  1.113025515445761 ; l2 norm of weights:  7.187439097683526\n",
            "Iteration#:  6856 ; Loss:  3.026464853203981 ; l2 norm of gradient:  1.1130081507857303 ; l2 norm of weights:  7.187421598412816\n",
            "Iteration#:  6860 ; Loss:  3.026415043503849 ; l2 norm of gradient:  1.1129907861649126 ; l2 norm of weights:  7.187404099394329\n",
            "Iteration#:  6864 ; Loss:  3.0263652344746643 ; l2 norm of gradient:  1.1129734215832576 ; l2 norm of weights:  7.1873866006280585\n",
            "Iteration#:  6868 ; Loss:  3.026315427865227 ; l2 norm of gradient:  1.1129560570407129 ; l2 norm of weights:  7.187369102113996\n",
            "Iteration#:  6872 ; Loss:  3.026265622689282 ; l2 norm of gradient:  1.1129386925372284 ; l2 norm of weights:  7.187351603852134\n",
            "Iteration#:  6876 ; Loss:  3.026215818471119 ; l2 norm of gradient:  1.1129213280727506 ; l2 norm of weights:  7.187334105842467\n",
            "Iteration#:  6880 ; Loss:  3.0261660163101656 ; l2 norm of gradient:  1.1129039636472307 ; l2 norm of weights:  7.187316608084988\n",
            "Iteration#:  6884 ; Loss:  3.026116216024201 ; l2 norm of gradient:  1.1128865992606154 ; l2 norm of weights:  7.187299110579688\n",
            "Iteration#:  6888 ; Loss:  3.026066417099929 ; l2 norm of gradient:  1.112869234912855 ; l2 norm of weights:  7.18728161332656\n",
            "Iteration#:  6892 ; Loss:  3.026016618906162 ; l2 norm of gradient:  1.1128518706038975 ; l2 norm of weights:  7.187264116325599\n",
            "Iteration#:  6896 ; Loss:  3.025966823141672 ; l2 norm of gradient:  1.1128345063336913 ; l2 norm of weights:  7.187246619576796\n",
            "Iteration#:  6900 ; Loss:  3.025917029013897 ; l2 norm of gradient:  1.1128171421021853 ; l2 norm of weights:  7.187229123080145\n",
            "Iteration#:  6904 ; Loss:  3.0258672361873185 ; l2 norm of gradient:  1.1127997779093293 ; l2 norm of weights:  7.187211626835638\n",
            "Iteration#:  6908 ; Loss:  3.0258174441541827 ; l2 norm of gradient:  1.1127824137550713 ; l2 norm of weights:  7.1871941308432685\n",
            "Iteration#:  6912 ; Loss:  3.0257676545782983 ; l2 norm of gradient:  1.1127650496393608 ; l2 norm of weights:  7.187176635103029\n",
            "Iteration#:  6916 ; Loss:  3.025717866479449 ; l2 norm of gradient:  1.1127476855621463 ; l2 norm of weights:  7.187159139614912\n",
            "Iteration#:  6920 ; Loss:  3.025668079969719 ; l2 norm of gradient:  1.112730321523376 ; l2 norm of weights:  7.1871416443789125\n",
            "Iteration#:  6924 ; Loss:  3.0256182938620557 ; l2 norm of gradient:  1.1127129575230021 ; l2 norm of weights:  7.18712414939502\n",
            "Iteration#:  6928 ; Loss:  3.025568510374073 ; l2 norm of gradient:  1.1126955935609704 ; l2 norm of weights:  7.18710665466323\n",
            "Iteration#:  6932 ; Loss:  3.025518728671691 ; l2 norm of gradient:  1.1126782296372302 ; l2 norm of weights:  7.187089160183533\n",
            "Iteration#:  6936 ; Loss:  3.025468948409799 ; l2 norm of gradient:  1.1126608657517325 ; l2 norm of weights:  7.187071665955926\n",
            "Iteration#:  6940 ; Loss:  3.02541916872892 ; l2 norm of gradient:  1.1126435019044256 ; l2 norm of weights:  7.187054171980398\n",
            "Iteration#:  6944 ; Loss:  3.0253693916213718 ; l2 norm of gradient:  1.1126261380952578 ; l2 norm of weights:  7.187036678256943\n",
            "Iteration#:  6948 ; Loss:  3.02531961594544 ; l2 norm of gradient:  1.1126087743241786 ; l2 norm of weights:  7.187019184785554\n",
            "Iteration#:  6952 ; Loss:  3.0252698408416685 ; l2 norm of gradient:  1.1125914105911385 ; l2 norm of weights:  7.187001691566224\n",
            "Iteration#:  6956 ; Loss:  3.025220068413328 ; l2 norm of gradient:  1.1125740468960856 ; l2 norm of weights:  7.186984198598946\n",
            "Iteration#:  6960 ; Loss:  3.025170297410681 ; l2 norm of gradient:  1.1125566832389693 ; l2 norm of weights:  7.1869667058837114\n",
            "Iteration#:  6964 ; Loss:  3.025120527430039 ; l2 norm of gradient:  1.1125393196197395 ; l2 norm of weights:  7.186949213420517\n",
            "Iteration#:  6968 ; Loss:  3.0250707588105525 ; l2 norm of gradient:  1.1125219560383464 ; l2 norm of weights:  7.18693172120935\n",
            "Iteration#:  6972 ; Loss:  3.025020992319703 ; l2 norm of gradient:  1.1125045924947365 ; l2 norm of weights:  7.186914229250207\n",
            "Iteration#:  6976 ; Loss:  3.0249712276274283 ; l2 norm of gradient:  1.1124872289888619 ; l2 norm of weights:  7.186896737543081\n",
            "Iteration#:  6980 ; Loss:  3.0249214643564613 ; l2 norm of gradient:  1.112469865520671 ; l2 norm of weights:  7.186879246087964\n",
            "Iteration#:  6984 ; Loss:  3.024871701822283 ; l2 norm of gradient:  1.1124525020901137 ; l2 norm of weights:  7.1868617548848475\n",
            "Iteration#:  6988 ; Loss:  3.024821941653899 ; l2 norm of gradient:  1.1124351386971392 ; l2 norm of weights:  7.186844263933726\n",
            "Iteration#:  6992 ; Loss:  3.0247721829850747 ; l2 norm of gradient:  1.1124177753416977 ; l2 norm of weights:  7.186826773234593\n",
            "Iteration#:  6996 ; Loss:  3.0247224255784366 ; l2 norm of gradient:  1.1124004120237376 ; l2 norm of weights:  7.186809282787439\n",
            "Iteration#:  7000 ; Loss:  3.0246726692948203 ; l2 norm of gradient:  1.1123830487432091 ; l2 norm of weights:  7.18679179259226\n",
            "Iteration#:  7004 ; Loss:  3.0246229152808457 ; l2 norm of gradient:  1.1123656855000625 ; l2 norm of weights:  7.186774302649047\n",
            "Iteration#:  7008 ; Loss:  3.0245731627754964 ; l2 norm of gradient:  1.1123483222942474 ; l2 norm of weights:  7.186756812957792\n",
            "Iteration#:  7012 ; Loss:  3.02452341128921 ; l2 norm of gradient:  1.1123309591257122 ; l2 norm of weights:  7.18673932351849\n",
            "Iteration#:  7016 ; Loss:  3.0244736617319283 ; l2 norm of gradient:  1.1123135959944077 ; l2 norm of weights:  7.1867218343311325\n",
            "Iteration#:  7020 ; Loss:  3.0244239137282656 ; l2 norm of gradient:  1.1122962329002841 ; l2 norm of weights:  7.186704345395713\n",
            "Iteration#:  7024 ; Loss:  3.024374167321803 ; l2 norm of gradient:  1.1122788698432908 ; l2 norm of weights:  7.186686856712223\n",
            "Iteration#:  7028 ; Loss:  3.024324422094701 ; l2 norm of gradient:  1.1122615068233774 ; l2 norm of weights:  7.186669368280657\n",
            "Iteration#:  7032 ; Loss:  3.024274678847953 ; l2 norm of gradient:  1.1122441438404944 ; l2 norm of weights:  7.186651880101008\n",
            "Iteration#:  7036 ; Loss:  3.0242249371778684 ; l2 norm of gradient:  1.1122267808945905 ; l2 norm of weights:  7.186634392173268\n",
            "Iteration#:  7040 ; Loss:  3.0241751968514583 ; l2 norm of gradient:  1.1122094179856168 ; l2 norm of weights:  7.18661690449743\n",
            "Iteration#:  7044 ; Loss:  3.0241254575602197 ; l2 norm of gradient:  1.1121920551135227 ; l2 norm of weights:  7.186599417073486\n",
            "Iteration#:  7048 ; Loss:  3.0240757205618656 ; l2 norm of gradient:  1.112174692278259 ; l2 norm of weights:  7.186581929901431\n",
            "Iteration#:  7052 ; Loss:  3.0240259851436684 ; l2 norm of gradient:  1.1121573294797742 ; l2 norm of weights:  7.186564442981256\n",
            "Iteration#:  7056 ; Loss:  3.0239762505093877 ; l2 norm of gradient:  1.1121399667180198 ; l2 norm of weights:  7.186546956312956\n",
            "Iteration#:  7060 ; Loss:  3.0239265178577224 ; l2 norm of gradient:  1.112122603992947 ; l2 norm of weights:  7.18652946989652\n",
            "Iteration#:  7064 ; Loss:  3.023876787233477 ; l2 norm of gradient:  1.1121052413045027 ; l2 norm of weights:  7.186511983731946\n",
            "Iteration#:  7068 ; Loss:  3.0238270579045397 ; l2 norm of gradient:  1.11208787865264 ; l2 norm of weights:  7.186494497819222\n",
            "Iteration#:  7072 ; Loss:  3.023777329369423 ; l2 norm of gradient:  1.112070516037307 ; l2 norm of weights:  7.186477012158345\n",
            "Iteration#:  7076 ; Loss:  3.02372760302924 ; l2 norm of gradient:  1.1120531534584546 ; l2 norm of weights:  7.186459526749305\n",
            "Iteration#:  7080 ; Loss:  3.0236778782882423 ; l2 norm of gradient:  1.112035790916034 ; l2 norm of weights:  7.186442041592095\n",
            "Iteration#:  7084 ; Loss:  3.023628154492123 ; l2 norm of gradient:  1.1120184284099952 ; l2 norm of weights:  7.186424556686709\n",
            "Iteration#:  7088 ; Loss:  3.023578433055912 ; l2 norm of gradient:  1.1120010659402881 ; l2 norm of weights:  7.18640707203314\n",
            "Iteration#:  7092 ; Loss:  3.0235287128042834 ; l2 norm of gradient:  1.1119837035068627 ; l2 norm of weights:  7.18638958763138\n",
            "Iteration#:  7096 ; Loss:  3.023478994370503 ; l2 norm of gradient:  1.1119663411096714 ; l2 norm of weights:  7.186372103481424\n",
            "Iteration#:  7100 ; Loss:  3.0234292768209485 ; l2 norm of gradient:  1.1119489787486623 ; l2 norm of weights:  7.186354619583261\n",
            "Iteration#:  7104 ; Loss:  3.023379561636745 ; l2 norm of gradient:  1.1119316164237856 ; l2 norm of weights:  7.186337135936887\n",
            "Iteration#:  7108 ; Loss:  3.0233298475507486 ; l2 norm of gradient:  1.1119142541349951 ; l2 norm of weights:  7.186319652542292\n",
            "Iteration#:  7112 ; Loss:  3.023280134511672 ; l2 norm of gradient:  1.1118968918822385 ; l2 norm of weights:  7.1863021693994735\n",
            "Iteration#:  7116 ; Loss:  3.023230423969392 ; l2 norm of gradient:  1.1118795296654658 ; l2 norm of weights:  7.1862846865084204\n",
            "Iteration#:  7120 ; Loss:  3.0231807147192957 ; l2 norm of gradient:  1.1118621674846307 ; l2 norm of weights:  7.186267203869127\n",
            "Iteration#:  7124 ; Loss:  3.0231310070302935 ; l2 norm of gradient:  1.1118448053396803 ; l2 norm of weights:  7.186249721481585\n",
            "Iteration#:  7128 ; Loss:  3.0230813001584864 ; l2 norm of gradient:  1.111827443230568 ; l2 norm of weights:  7.186232239345789\n",
            "Iteration#:  7132 ; Loss:  3.0230315956363674 ; l2 norm of gradient:  1.1118100811572433 ; l2 norm of weights:  7.186214757461732\n",
            "Iteration#:  7136 ; Loss:  3.022981892300005 ; l2 norm of gradient:  1.1117927191196577 ; l2 norm of weights:  7.1861972758294055\n",
            "Iteration#:  7140 ; Loss:  3.022932190254804 ; l2 norm of gradient:  1.1117753571177607 ; l2 norm of weights:  7.186179794448802\n",
            "Iteration#:  7144 ; Loss:  3.0228824904678273 ; l2 norm of gradient:  1.1117579951515042 ; l2 norm of weights:  7.186162313319915\n",
            "Iteration#:  7148 ; Loss:  3.022832791875154 ; l2 norm of gradient:  1.111740633220838 ; l2 norm of weights:  7.186144832442739\n",
            "Iteration#:  7152 ; Loss:  3.022783095183179 ; l2 norm of gradient:  1.1117232713257141 ; l2 norm of weights:  7.186127351817265\n",
            "Iteration#:  7156 ; Loss:  3.022733399352696 ; l2 norm of gradient:  1.1117059094660822 ; l2 norm of weights:  7.186109871443487\n",
            "Iteration#:  7160 ; Loss:  3.0226837055900266 ; l2 norm of gradient:  1.1116885476418956 ; l2 norm of weights:  7.186092391321395\n",
            "Iteration#:  7164 ; Loss:  3.022634013302466 ; l2 norm of gradient:  1.1116711858531023 ; l2 norm of weights:  7.186074911450986\n",
            "Iteration#:  7168 ; Loss:  3.0225843217064705 ; l2 norm of gradient:  1.1116538240996552 ; l2 norm of weights:  7.186057431832251\n",
            "Iteration#:  7172 ; Loss:  3.02253463291388 ; l2 norm of gradient:  1.1116364623815047 ; l2 norm of weights:  7.186039952465182\n",
            "Iteration#:  7176 ; Loss:  3.022484945138024 ; l2 norm of gradient:  1.1116191006986023 ; l2 norm of weights:  7.186022473349772\n",
            "Iteration#:  7180 ; Loss:  3.022435259264419 ; l2 norm of gradient:  1.1116017390508985 ; l2 norm of weights:  7.186004994486016\n",
            "Iteration#:  7184 ; Loss:  3.022385573805286 ; l2 norm of gradient:  1.1115843774383438 ; l2 norm of weights:  7.185987515873905\n",
            "Iteration#:  7188 ; Loss:  3.0223358910920375 ; l2 norm of gradient:  1.1115670158608908 ; l2 norm of weights:  7.1859700375134326\n",
            "Iteration#:  7192 ; Loss:  3.0222862098021843 ; l2 norm of gradient:  1.111549654318491 ; l2 norm of weights:  7.1859525594045905\n",
            "Iteration#:  7196 ; Loss:  3.022236529012103 ; l2 norm of gradient:  1.1115322928110933 ; l2 norm of weights:  7.185935081547372\n",
            "Iteration#:  7200 ; Loss:  3.0221868506526586 ; l2 norm of gradient:  1.1115149313386508 ; l2 norm of weights:  7.185917603941773\n",
            "Iteration#:  7204 ; Loss:  3.0221371741902576 ; l2 norm of gradient:  1.1114975699011143 ; l2 norm of weights:  7.18590012658778\n",
            "Iteration#:  7208 ; Loss:  3.0220874987540824 ; l2 norm of gradient:  1.1114802084984357 ; l2 norm of weights:  7.185882649485392\n",
            "Iteration#:  7212 ; Loss:  3.0220378247260804 ; l2 norm of gradient:  1.111462847130566 ; l2 norm of weights:  7.1858651726346\n",
            "Iteration#:  7216 ; Loss:  3.021988152614857 ; l2 norm of gradient:  1.111445485797456 ; l2 norm of weights:  7.1858476960353945\n",
            "Iteration#:  7220 ; Loss:  3.0219384819063997 ; l2 norm of gradient:  1.1114281244990576 ; l2 norm of weights:  7.185830219687771\n",
            "Iteration#:  7224 ; Loss:  3.0218888121749954 ; l2 norm of gradient:  1.1114107632353227 ; l2 norm of weights:  7.185812743591722\n",
            "Iteration#:  7228 ; Loss:  3.02183914441102 ; l2 norm of gradient:  1.1113934020062015 ; l2 norm of weights:  7.18579526774724\n",
            "Iteration#:  7232 ; Loss:  3.0217894786144885 ; l2 norm of gradient:  1.1113760408116475 ; l2 norm of weights:  7.185777792154318\n",
            "Iteration#:  7236 ; Loss:  3.02173981397971 ; l2 norm of gradient:  1.111358679651611 ; l2 norm of weights:  7.185760316812948\n",
            "Iteration#:  7240 ; Loss:  3.0216901509389507 ; l2 norm of gradient:  1.1113413185260428 ; l2 norm of weights:  7.1857428417231235\n",
            "Iteration#:  7244 ; Loss:  3.021640489661769 ; l2 norm of gradient:  1.1113239574348963 ; l2 norm of weights:  7.185725366884838\n",
            "Iteration#:  7248 ; Loss:  3.0215908295474656 ; l2 norm of gradient:  1.1113065963781217 ; l2 norm of weights:  7.185707892298083\n",
            "Iteration#:  7252 ; Loss:  3.0215411708009388 ; l2 norm of gradient:  1.1112892353556716 ; l2 norm of weights:  7.185690417962853\n",
            "Iteration#:  7256 ; Loss:  3.0214915144674346 ; l2 norm of gradient:  1.1112718743674979 ; l2 norm of weights:  7.185672943879141\n",
            "Iteration#:  7260 ; Loss:  3.0214418592509613 ; l2 norm of gradient:  1.111254513413551 ; l2 norm of weights:  7.185655470046937\n",
            "Iteration#:  7264 ; Loss:  3.021392205005399 ; l2 norm of gradient:  1.1112371524937845 ; l2 norm of weights:  7.185637996466238\n",
            "Iteration#:  7268 ; Loss:  3.0213425528532962 ; l2 norm of gradient:  1.1112197916081492 ; l2 norm of weights:  7.1856205231370325\n",
            "Iteration#:  7272 ; Loss:  3.0212929025406257 ; l2 norm of gradient:  1.1112024307565962 ; l2 norm of weights:  7.185603050059316\n",
            "Iteration#:  7276 ; Loss:  3.021243252416762 ; l2 norm of gradient:  1.1111850699390788 ; l2 norm of weights:  7.1855855772330814\n",
            "Iteration#:  7280 ; Loss:  3.021193605475029 ; l2 norm of gradient:  1.111167709155549 ; l2 norm of weights:  7.185568104658321\n",
            "Iteration#:  7284 ; Loss:  3.0211439593570666 ; l2 norm of gradient:  1.1111503484059568 ; l2 norm of weights:  7.185550632335027\n",
            "Iteration#:  7288 ; Loss:  3.0210943153489653 ; l2 norm of gradient:  1.111132987690257 ; l2 norm of weights:  7.185533160263193\n",
            "Iteration#:  7292 ; Loss:  3.0210446719871493 ; l2 norm of gradient:  1.111115627008399 ; l2 norm of weights:  7.185515688442813\n",
            "Iteration#:  7296 ; Loss:  3.0209950306795226 ; l2 norm of gradient:  1.1110982663603364 ; l2 norm of weights:  7.185498216873878\n",
            "Iteration#:  7300 ; Loss:  3.020945391190548 ; l2 norm of gradient:  1.1110809057460207 ; l2 norm of weights:  7.185480745556382\n",
            "Iteration#:  7304 ; Loss:  3.0208957521661586 ; l2 norm of gradient:  1.1110635451654052 ; l2 norm of weights:  7.185463274490318\n",
            "Iteration#:  7308 ; Loss:  3.0208461156742543 ; l2 norm of gradient:  1.1110461846184396 ; l2 norm of weights:  7.185445803675676\n",
            "Iteration#:  7312 ; Loss:  3.020796480599368 ; l2 norm of gradient:  1.1110288241050783 ; l2 norm of weights:  7.185428333112454\n",
            "Iteration#:  7316 ; Loss:  3.020746846495749 ; l2 norm of gradient:  1.111011463625273 ; l2 norm of weights:  7.185410862800641\n",
            "Iteration#:  7320 ; Loss:  3.0206972144908004 ; l2 norm of gradient:  1.1109941031789758 ; l2 norm of weights:  7.185393392740229\n",
            "Iteration#:  7324 ; Loss:  3.0206475841731257 ; l2 norm of gradient:  1.1109767427661388 ; l2 norm of weights:  7.185375922931215\n",
            "Iteration#:  7328 ; Loss:  3.020597955174665 ; l2 norm of gradient:  1.1109593823867143 ; l2 norm of weights:  7.185358453373589\n",
            "Iteration#:  7332 ; Loss:  3.0205483274514964 ; l2 norm of gradient:  1.1109420220406552 ; l2 norm of weights:  7.185340984067345\n",
            "Iteration#:  7336 ; Loss:  3.0204987015392217 ; l2 norm of gradient:  1.1109246617279123 ; l2 norm of weights:  7.185323515012474\n",
            "Iteration#:  7340 ; Loss:  3.020449077442377 ; l2 norm of gradient:  1.1109073014484414 ; l2 norm of weights:  7.1853060462089715\n",
            "Iteration#:  7344 ; Loss:  3.0203994538427446 ; l2 norm of gradient:  1.1108899412021915 ; l2 norm of weights:  7.185288577656828\n",
            "Iteration#:  7348 ; Loss:  3.020349833051165 ; l2 norm of gradient:  1.110872580989116 ; l2 norm of weights:  7.185271109356039\n",
            "Iteration#:  7352 ; Loss:  3.0203002131568812 ; l2 norm of gradient:  1.1108552208091682 ; l2 norm of weights:  7.185253641306594\n",
            "Iteration#:  7356 ; Loss:  3.020250594592469 ; l2 norm of gradient:  1.1108378606623004 ; l2 norm of weights:  7.185236173508489\n",
            "Iteration#:  7360 ; Loss:  3.020200978072842 ; l2 norm of gradient:  1.1108205005484648 ; l2 norm of weights:  7.185218705961716\n",
            "Iteration#:  7364 ; Loss:  3.0201513633006987 ; l2 norm of gradient:  1.1108031404676149 ; l2 norm of weights:  7.1852012386662665\n",
            "Iteration#:  7368 ; Loss:  3.0201017490108426 ; l2 norm of gradient:  1.1107857804197028 ; l2 norm of weights:  7.185183771622134\n",
            "Iteration#:  7372 ; Loss:  3.020052136789286 ; l2 norm of gradient:  1.1107684204046804 ; l2 norm of weights:  7.185166304829313\n",
            "Iteration#:  7376 ; Loss:  3.0200025270284803 ; l2 norm of gradient:  1.110751060422502 ; l2 norm of weights:  7.185148838287794\n",
            "Iteration#:  7380 ; Loss:  3.0199529171741353 ; l2 norm of gradient:  1.1107337004731186 ; l2 norm of weights:  7.185131371997571\n",
            "Iteration#:  7384 ; Loss:  3.0199033103542052 ; l2 norm of gradient:  1.1107163405564842 ; l2 norm of weights:  7.185113905958636\n",
            "Iteration#:  7388 ; Loss:  3.0198537044298135 ; l2 norm of gradient:  1.1106989806725516 ; l2 norm of weights:  7.185096440170984\n",
            "Iteration#:  7392 ; Loss:  3.0198041004873017 ; l2 norm of gradient:  1.110681620821273 ; l2 norm of weights:  7.185078974634606\n",
            "Iteration#:  7396 ; Loss:  3.0197544970047447 ; l2 norm of gradient:  1.1106642610026014 ; l2 norm of weights:  7.185061509349494\n",
            "Iteration#:  7400 ; Loss:  3.0197048962705253 ; l2 norm of gradient:  1.110646901216491 ; l2 norm of weights:  7.185044044315644\n",
            "Iteration#:  7404 ; Loss:  3.019655296803137 ; l2 norm of gradient:  1.1106295414628926 ; l2 norm of weights:  7.185026579533048\n",
            "Iteration#:  7408 ; Loss:  3.019605697769063 ; l2 norm of gradient:  1.1106121817417611 ; l2 norm of weights:  7.185009115001696\n",
            "Iteration#:  7412 ; Loss:  3.0195561016975168 ; l2 norm of gradient:  1.1105948220530495 ; l2 norm of weights:  7.184991650721583\n",
            "Iteration#:  7416 ; Loss:  3.019506506652073 ; l2 norm of gradient:  1.1105774623967097 ; l2 norm of weights:  7.184974186692702\n",
            "Iteration#:  7420 ; Loss:  3.019456912744864 ; l2 norm of gradient:  1.1105601027726952 ; l2 norm of weights:  7.184956722915047\n",
            "Iteration#:  7424 ; Loss:  3.0194073207891043 ; l2 norm of gradient:  1.110542743180958 ; l2 norm of weights:  7.184939259388607\n",
            "Iteration#:  7428 ; Loss:  3.019357730416519 ; l2 norm of gradient:  1.1105253836214535 ; l2 norm of weights:  7.184921796113378\n",
            "Iteration#:  7432 ; Loss:  3.0193081413782137 ; l2 norm of gradient:  1.1105080240941343 ; l2 norm of weights:  7.184904333089352\n",
            "Iteration#:  7436 ; Loss:  3.0192585541522514 ; l2 norm of gradient:  1.1104906645989518 ; l2 norm of weights:  7.184886870316522\n",
            "Iteration#:  7440 ; Loss:  3.0192089687527153 ; l2 norm of gradient:  1.110473305135862 ; l2 norm of weights:  7.184869407794882\n",
            "Iteration#:  7444 ; Loss:  3.019159383893899 ; l2 norm of gradient:  1.1104559457048162 ; l2 norm of weights:  7.184851945524422\n",
            "Iteration#:  7448 ; Loss:  3.0191098011845483 ; l2 norm of gradient:  1.1104385863057686 ; l2 norm of weights:  7.1848344835051385\n",
            "Iteration#:  7452 ; Loss:  3.019060220435729 ; l2 norm of gradient:  1.1104212269386722 ; l2 norm of weights:  7.184817021737022\n",
            "Iteration#:  7456 ; Loss:  3.0190106402398795 ; l2 norm of gradient:  1.1104038676034806 ; l2 norm of weights:  7.184799560220065\n",
            "Iteration#:  7460 ; Loss:  3.018961062648664 ; l2 norm of gradient:  1.1103865083001472 ; l2 norm of weights:  7.184782098954261\n",
            "Iteration#:  7464 ; Loss:  3.0189114860259645 ; l2 norm of gradient:  1.110369149028626 ; l2 norm of weights:  7.184764637939603\n",
            "Iteration#:  7468 ; Loss:  3.0188619105618586 ; l2 norm of gradient:  1.1103517897888686 ; l2 norm of weights:  7.184747177176085\n",
            "Iteration#:  7472 ; Loss:  3.0188123374768265 ; l2 norm of gradient:  1.1103344305808305 ; l2 norm of weights:  7.184729716663698\n",
            "Iteration#:  7476 ; Loss:  3.0187627655809868 ; l2 norm of gradient:  1.1103170714044646 ; l2 norm of weights:  7.1847122564024355\n",
            "Iteration#:  7480 ; Loss:  3.018713195470635 ; l2 norm of gradient:  1.1102997122597242 ; l2 norm of weights:  7.18469479639229\n",
            "Iteration#:  7484 ; Loss:  3.0186636264178937 ; l2 norm of gradient:  1.110282353146565 ; l2 norm of weights:  7.184677336633257\n",
            "Iteration#:  7488 ; Loss:  3.018614059178342 ; l2 norm of gradient:  1.110264994064937 ; l2 norm of weights:  7.184659877125326\n",
            "Iteration#:  7492 ; Loss:  3.018564493583758 ; l2 norm of gradient:  1.1102476350147965 ; l2 norm of weights:  7.184642417868489\n",
            "Iteration#:  7496 ; Loss:  3.018514929175669 ; l2 norm of gradient:  1.1102302759960962 ; l2 norm of weights:  7.184624958862743\n",
            "Iteration#:  7500 ; Loss:  3.0184653664368204 ; l2 norm of gradient:  1.1102129170087913 ; l2 norm of weights:  7.184607500108079\n",
            "Iteration#:  7504 ; Loss:  3.01841580555904 ; l2 norm of gradient:  1.1101955580528335 ; l2 norm of weights:  7.18459004160449\n",
            "Iteration#:  7508 ; Loss:  3.018366245748861 ; l2 norm of gradient:  1.110178199128178 ; l2 norm of weights:  7.184572583351966\n",
            "Iteration#:  7512 ; Loss:  3.0183166876043384 ; l2 norm of gradient:  1.1101608402347787 ; l2 norm of weights:  7.1845551253505056\n",
            "Iteration#:  7516 ; Loss:  3.018267131454142 ; l2 norm of gradient:  1.1101434813725892 ; l2 norm of weights:  7.184537667600096\n",
            "Iteration#:  7520 ; Loss:  3.0182175762134795 ; l2 norm of gradient:  1.110126122541563 ; l2 norm of weights:  7.184520210100733\n",
            "Iteration#:  7524 ; Loss:  3.0181680230884997 ; l2 norm of gradient:  1.1101087637416547 ; l2 norm of weights:  7.184502752852409\n",
            "Iteration#:  7528 ; Loss:  3.018118471226664 ; l2 norm of gradient:  1.1100914049728183 ; l2 norm of weights:  7.184485295855118\n",
            "Iteration#:  7532 ; Loss:  3.018068920324353 ; l2 norm of gradient:  1.1100740462350074 ; l2 norm of weights:  7.1844678391088515\n",
            "Iteration#:  7536 ; Loss:  3.0180193719480237 ; l2 norm of gradient:  1.1100566875281768 ; l2 norm of weights:  7.184450382613601\n",
            "Iteration#:  7540 ; Loss:  3.0179698249781914 ; l2 norm of gradient:  1.1100393288522794 ; l2 norm of weights:  7.184432926369361\n",
            "Iteration#:  7544 ; Loss:  3.0179202786504775 ; l2 norm of gradient:  1.1100219702072704 ; l2 norm of weights:  7.184415470376125\n",
            "Iteration#:  7548 ; Loss:  3.0178707350793053 ; l2 norm of gradient:  1.1100046115931037 ; l2 norm of weights:  7.184398014633886\n",
            "Iteration#:  7552 ; Loss:  3.0178211926783716 ; l2 norm of gradient:  1.1099872530097332 ; l2 norm of weights:  7.184380559142634\n",
            "Iteration#:  7556 ; Loss:  3.0177716507919685 ; l2 norm of gradient:  1.109969894457114 ; l2 norm of weights:  7.184363103902366\n",
            "Iteration#:  7560 ; Loss:  3.017722111627939 ; l2 norm of gradient:  1.1099525359351998 ; l2 norm of weights:  7.184345648913071\n",
            "Iteration#:  7564 ; Loss:  3.0176725740555996 ; l2 norm of gradient:  1.1099351774439445 ; l2 norm of weights:  7.1843281941747446\n",
            "Iteration#:  7568 ; Loss:  3.017623037110865 ; l2 norm of gradient:  1.1099178189833039 ; l2 norm of weights:  7.184310739687378\n",
            "Iteration#:  7572 ; Loss:  3.017573502237579 ; l2 norm of gradient:  1.1099004605532303 ; l2 norm of weights:  7.1842932854509645\n",
            "Iteration#:  7576 ; Loss:  3.017523969068084 ; l2 norm of gradient:  1.10988310215368 ; l2 norm of weights:  7.184275831465498\n",
            "Iteration#:  7580 ; Loss:  3.017474437173708 ; l2 norm of gradient:  1.1098657437846056 ; l2 norm of weights:  7.184258377730971\n",
            "Iteration#:  7584 ; Loss:  3.0174249070937122 ; l2 norm of gradient:  1.1098483854459638 ; l2 norm of weights:  7.184240924247375\n",
            "Iteration#:  7588 ; Loss:  3.017375378518517 ; l2 norm of gradient:  1.1098310271377072 ; l2 norm of weights:  7.184223471014703\n",
            "Iteration#:  7592 ; Loss:  3.017325850811109 ; l2 norm of gradient:  1.109813668859791 ; l2 norm of weights:  7.184206018032949\n",
            "Iteration#:  7596 ; Loss:  3.017276325383722 ; l2 norm of gradient:  1.10979631061217 ; l2 norm of weights:  7.184188565302105\n",
            "Iteration#:  7600 ; Loss:  3.0172268017066717 ; l2 norm of gradient:  1.1097789523947994 ; l2 norm of weights:  7.184171112822167\n",
            "Iteration#:  7604 ; Loss:  3.017177278724163 ; l2 norm of gradient:  1.1097615942076329 ; l2 norm of weights:  7.184153660593122\n",
            "Iteration#:  7608 ; Loss:  3.017127757871421 ; l2 norm of gradient:  1.1097442360506249 ; l2 norm of weights:  7.184136208614968\n",
            "Iteration#:  7612 ; Loss:  3.017078238529356 ; l2 norm of gradient:  1.1097268779237297 ; l2 norm of weights:  7.1841187568876945\n",
            "Iteration#:  7616 ; Loss:  3.0170287205856967 ; l2 norm of gradient:  1.109709519826906 ; l2 norm of weights:  7.184101305411295\n",
            "Iteration#:  7620 ; Loss:  3.0169792042458337 ; l2 norm of gradient:  1.1096921617601037 ; l2 norm of weights:  7.184083854185765\n",
            "Iteration#:  7624 ; Loss:  3.0169296895610973 ; l2 norm of gradient:  1.1096748037232804 ; l2 norm of weights:  7.184066403211095\n",
            "Iteration#:  7628 ; Loss:  3.016880175811528 ; l2 norm of gradient:  1.1096574457163901 ; l2 norm of weights:  7.184048952487277\n",
            "Iteration#:  7632 ; Loss:  3.0168306643436558 ; l2 norm of gradient:  1.109640087739387 ; l2 norm of weights:  7.1840315020143075\n",
            "Iteration#:  7636 ; Loss:  3.0167811542967753 ; l2 norm of gradient:  1.1096227297922274 ; l2 norm of weights:  7.1840140517921744\n",
            "Iteration#:  7640 ; Loss:  3.0167316453757707 ; l2 norm of gradient:  1.1096053718748653 ; l2 norm of weights:  7.183996601820875\n",
            "Iteration#:  7644 ; Loss:  3.0166821385626825 ; l2 norm of gradient:  1.1095880139872567 ; l2 norm of weights:  7.1839791521004\n",
            "Iteration#:  7648 ; Loss:  3.0166326329618207 ; l2 norm of gradient:  1.109570656129356 ; l2 norm of weights:  7.183961702630742\n",
            "Iteration#:  7652 ; Loss:  3.016583128381667 ; l2 norm of gradient:  1.1095532983011183 ; l2 norm of weights:  7.183944253411896\n",
            "Iteration#:  7656 ; Loss:  3.0165336261388145 ; l2 norm of gradient:  1.1095359405024985 ; l2 norm of weights:  7.1839268044438525\n",
            "Iteration#:  7660 ; Loss:  3.0164841257230504 ; l2 norm of gradient:  1.1095185827334515 ; l2 norm of weights:  7.1839093557266045\n",
            "Iteration#:  7664 ; Loss:  3.016434625853056 ; l2 norm of gradient:  1.1095012249939336 ; l2 norm of weights:  7.183891907260146\n",
            "Iteration#:  7668 ; Loss:  3.01638512812954 ; l2 norm of gradient:  1.1094838672838983 ; l2 norm of weights:  7.1838744590444685\n",
            "Iteration#:  7672 ; Loss:  3.016335632184896 ; l2 norm of gradient:  1.109466509603303 ; l2 norm of weights:  7.183857011079567\n",
            "Iteration#:  7676 ; Loss:  3.016286136759593 ; l2 norm of gradient:  1.1094491519521015 ; l2 norm of weights:  7.183839563365433\n",
            "Iteration#:  7680 ; Loss:  3.01623664375809 ; l2 norm of gradient:  1.1094317943302494 ; l2 norm of weights:  7.183822115902059\n",
            "Iteration#:  7684 ; Loss:  3.016187151481153 ; l2 norm of gradient:  1.1094144367377015 ; l2 norm of weights:  7.183804668689438\n",
            "Iteration#:  7688 ; Loss:  3.0161376616190116 ; l2 norm of gradient:  1.1093970791744145 ; l2 norm of weights:  7.183787221727564\n",
            "Iteration#:  7692 ; Loss:  3.0160881735173977 ; l2 norm of gradient:  1.109379721640344 ; l2 norm of weights:  7.183769775016429\n",
            "Iteration#:  7696 ; Loss:  3.0160386859611474 ; l2 norm of gradient:  1.109362364135443 ; l2 norm of weights:  7.183752328556025\n",
            "Iteration#:  7700 ; Loss:  3.0159892007612608 ; l2 norm of gradient:  1.1093450066596702 ; l2 norm of weights:  7.183734882346347\n",
            "Iteration#:  7704 ; Loss:  3.0159397167712436 ; l2 norm of gradient:  1.1093276492129782 ; l2 norm of weights:  7.1837174363873855\n",
            "Iteration#:  7708 ; Loss:  3.0158902339387943 ; l2 norm of gradient:  1.1093102917953235 ; l2 norm of weights:  7.183699990679136\n",
            "Iteration#:  7712 ; Loss:  3.0158407531622977 ; l2 norm of gradient:  1.109292934406663 ; l2 norm of weights:  7.183682545221588\n",
            "Iteration#:  7716 ; Loss:  3.0157912742507906 ; l2 norm of gradient:  1.1092755770469511 ; l2 norm of weights:  7.183665100014737\n",
            "Iteration#:  7720 ; Loss:  3.015741795784134 ; l2 norm of gradient:  1.1092582197161436 ; l2 norm of weights:  7.183647655058575\n",
            "Iteration#:  7724 ; Loss:  3.0156923195284833 ; l2 norm of gradient:  1.1092408624141967 ; l2 norm of weights:  7.183630210353094\n",
            "Iteration#:  7728 ; Loss:  3.01564284573955 ; l2 norm of gradient:  1.1092235051410653 ; l2 norm of weights:  7.183612765898289\n",
            "Iteration#:  7732 ; Loss:  3.015593371986716 ; l2 norm of gradient:  1.109206147896705 ; l2 norm of weights:  7.183595321694152\n",
            "Iteration#:  7736 ; Loss:  3.0155439005935594 ; l2 norm of gradient:  1.1091887906810738 ; l2 norm of weights:  7.183577877740674\n",
            "Iteration#:  7740 ; Loss:  3.0154944306331655 ; l2 norm of gradient:  1.1091714334941247 ; l2 norm of weights:  7.18356043403785\n",
            "Iteration#:  7744 ; Loss:  3.0154449613858443 ; l2 norm of gradient:  1.1091540763358156 ; l2 norm of weights:  7.183542990585672\n",
            "Iteration#:  7748 ; Loss:  3.0153954947073736 ; l2 norm of gradient:  1.109136719206101 ; l2 norm of weights:  7.183525547384133\n",
            "Iteration#:  7752 ; Loss:  3.015346029483325 ; l2 norm of gradient:  1.1091193621049371 ; l2 norm of weights:  7.183508104433225\n",
            "Iteration#:  7756 ; Loss:  3.015296564887376 ; l2 norm of gradient:  1.1091020050322813 ; l2 norm of weights:  7.183490661732943\n",
            "Iteration#:  7760 ; Loss:  3.0152471029277836 ; l2 norm of gradient:  1.1090846479880883 ; l2 norm of weights:  7.183473219283278\n",
            "Iteration#:  7764 ; Loss:  3.01519764142697 ; l2 norm of gradient:  1.1090672909723145 ; l2 norm of weights:  7.183455777084224\n",
            "Iteration#:  7768 ; Loss:  3.015148182553032 ; l2 norm of gradient:  1.1090499339849147 ; l2 norm of weights:  7.183438335135774\n",
            "Iteration#:  7772 ; Loss:  3.0150987249676584 ; l2 norm of gradient:  1.1090325770258473 ; l2 norm of weights:  7.18342089343792\n",
            "Iteration#:  7776 ; Loss:  3.0150492682455545 ; l2 norm of gradient:  1.1090152200950671 ; l2 norm of weights:  7.183403451990653\n",
            "Iteration#:  7780 ; Loss:  3.0149998137313583 ; l2 norm of gradient:  1.10899786319253 ; l2 norm of weights:  7.18338601079397\n",
            "Iteration#:  7784 ; Loss:  3.0149503608774118 ; l2 norm of gradient:  1.1089805063181928 ; l2 norm of weights:  7.18336856984786\n",
            "Iteration#:  7788 ; Loss:  3.014900908727838 ; l2 norm of gradient:  1.1089631494720118 ; l2 norm of weights:  7.183351129152319\n",
            "Iteration#:  7792 ; Loss:  3.014851459004874 ; l2 norm of gradient:  1.1089457926539439 ; l2 norm of weights:  7.183333688707338\n",
            "Iteration#:  7796 ; Loss:  3.01480201078609 ; l2 norm of gradient:  1.1089284358639444 ; l2 norm of weights:  7.183316248512909\n",
            "Iteration#:  7800 ; Loss:  3.0147525633822476 ; l2 norm of gradient:  1.1089110791019698 ; l2 norm of weights:  7.1832988085690275\n",
            "Iteration#:  7804 ; Loss:  3.0147031180239328 ; l2 norm of gradient:  1.1088937223679753 ; l2 norm of weights:  7.183281368875686\n",
            "Iteration#:  7808 ; Loss:  3.0146536746692574 ; l2 norm of gradient:  1.10887636566192 ; l2 norm of weights:  7.183263929432874\n",
            "Iteration#:  7812 ; Loss:  3.0146042316108526 ; l2 norm of gradient:  1.1088590089837596 ; l2 norm of weights:  7.183246490240588\n",
            "Iteration#:  7816 ; Loss:  3.014554791260772 ; l2 norm of gradient:  1.1088416523334488 ; l2 norm of weights:  7.183229051298819\n",
            "Iteration#:  7820 ; Loss:  3.0145053513572795 ; l2 norm of gradient:  1.108824295710946 ; l2 norm of weights:  7.183211612607561\n",
            "Iteration#:  7824 ; Loss:  3.014455913943716 ; l2 norm of gradient:  1.1088069391162068 ; l2 norm of weights:  7.183194174166807\n",
            "Iteration#:  7828 ; Loss:  3.0144064780242146 ; l2 norm of gradient:  1.1087895825491876 ; l2 norm of weights:  7.183176735976548\n",
            "Iteration#:  7832 ; Loss:  3.0143570429048516 ; l2 norm of gradient:  1.1087722260098465 ; l2 norm of weights:  7.183159298036777\n",
            "Iteration#:  7836 ; Loss:  3.0143076100694617 ; l2 norm of gradient:  1.108754869498139 ; l2 norm of weights:  7.183141860347489\n",
            "Iteration#:  7840 ; Loss:  3.01425817881522 ; l2 norm of gradient:  1.108737513014022 ; l2 norm of weights:  7.183124422908676\n",
            "Iteration#:  7844 ; Loss:  3.0142087484099145 ; l2 norm of gradient:  1.108720156557452 ; l2 norm of weights:  7.183106985720332\n",
            "Iteration#:  7848 ; Loss:  3.01415932029592 ; l2 norm of gradient:  1.108702800128386 ; l2 norm of weights:  7.183089548782445\n",
            "Iteration#:  7852 ; Loss:  3.014109893325622 ; l2 norm of gradient:  1.1086854437267815 ; l2 norm of weights:  7.1830721120950125\n",
            "Iteration#:  7856 ; Loss:  3.0140604675781053 ; l2 norm of gradient:  1.1086680873525954 ; l2 norm of weights:  7.183054675658027\n",
            "Iteration#:  7860 ; Loss:  3.014011043964966 ; l2 norm of gradient:  1.108650731005782 ; l2 norm of weights:  7.183037239471478\n",
            "Iteration#:  7864 ; Loss:  3.0139616213750653 ; l2 norm of gradient:  1.1086333746863006 ; l2 norm of weights:  7.183019803535363\n",
            "Iteration#:  7868 ; Loss:  3.0139122007494294 ; l2 norm of gradient:  1.1086160183941083 ; l2 norm of weights:  7.183002367849672\n",
            "Iteration#:  7872 ; Loss:  3.0138627818596118 ; l2 norm of gradient:  1.1085986621291612 ; l2 norm of weights:  7.182984932414399\n",
            "Iteration#:  7876 ; Loss:  3.0138133637764484 ; l2 norm of gradient:  1.1085813058914171 ; l2 norm of weights:  7.182967497229535\n",
            "Iteration#:  7880 ; Loss:  3.013763947954276 ; l2 norm of gradient:  1.108563949680832 ; l2 norm of weights:  7.182950062295077\n",
            "Iteration#:  7884 ; Loss:  3.013714533636593 ; l2 norm of gradient:  1.1085465934973637 ; l2 norm of weights:  7.182932627611013\n",
            "Iteration#:  7888 ; Loss:  3.0136651199752387 ; l2 norm of gradient:  1.1085292373409683 ; l2 norm of weights:  7.1829151931773385\n",
            "Iteration#:  7892 ; Loss:  3.013615708348372 ; l2 norm of gradient:  1.1085118812116046 ; l2 norm of weights:  7.182897758994045\n",
            "Iteration#:  7896 ; Loss:  3.0135662986859035 ; l2 norm of gradient:  1.1084945251092286 ; l2 norm of weights:  7.182880325061127\n",
            "Iteration#:  7900 ; Loss:  3.013516889964799 ; l2 norm of gradient:  1.108477169033799 ; l2 norm of weights:  7.1828628913785755\n",
            "Iteration#:  7904 ; Loss:  3.0134674832191832 ; l2 norm of gradient:  1.1084598129852714 ; l2 norm of weights:  7.182845457946386\n",
            "Iteration#:  7908 ; Loss:  3.0134180775002783 ; l2 norm of gradient:  1.108442456963604 ; l2 norm of weights:  7.1828280247645475\n",
            "Iteration#:  7912 ; Loss:  3.0133686739041554 ; l2 norm of gradient:  1.1084251009687538 ; l2 norm of weights:  7.182810591833055\n",
            "Iteration#:  7916 ; Loss:  3.0133192718936903 ; l2 norm of gradient:  1.108407745000678 ; l2 norm of weights:  7.182793159151903\n",
            "Iteration#:  7920 ; Loss:  3.0132698708859538 ; l2 norm of gradient:  1.1083903890593343 ; l2 norm of weights:  7.182775726721083\n",
            "Iteration#:  7924 ; Loss:  3.0132204719503277 ; l2 norm of gradient:  1.1083730331446802 ; l2 norm of weights:  7.1827582945405855\n",
            "Iteration#:  7928 ; Loss:  3.01317107419127 ; l2 norm of gradient:  1.1083556772566732 ; l2 norm of weights:  7.182740862610406\n",
            "Iteration#:  7932 ; Loss:  3.0131216776261107 ; l2 norm of gradient:  1.1083383213952696 ; l2 norm of weights:  7.182723430930538\n",
            "Iteration#:  7936 ; Loss:  3.0130722834700343 ; l2 norm of gradient:  1.1083209655604307 ; l2 norm of weights:  7.182705999500972\n",
            "Iteration#:  7940 ; Loss:  3.013022890037699 ; l2 norm of gradient:  1.1083036097521088 ; l2 norm of weights:  7.182688568321703\n",
            "Iteration#:  7944 ; Loss:  3.0129734986741097 ; l2 norm of gradient:  1.1082862539702656 ; l2 norm of weights:  7.182671137392722\n",
            "Iteration#:  7948 ; Loss:  3.0129241091064256 ; l2 norm of gradient:  1.1082688982148574 ; l2 norm of weights:  7.1826537067140235\n",
            "Iteration#:  7952 ; Loss:  3.012874720394316 ; l2 norm of gradient:  1.1082515424858403 ; l2 norm of weights:  7.182636276285599\n",
            "Iteration#:  7956 ; Loss:  3.0128253336812554 ; l2 norm of gradient:  1.1082341867831753 ; l2 norm of weights:  7.182618846107442\n",
            "Iteration#:  7960 ; Loss:  3.0127759485108183 ; l2 norm of gradient:  1.108216831106818 ; l2 norm of weights:  7.182601416179545\n",
            "Iteration#:  7964 ; Loss:  3.012726564088598 ; l2 norm of gradient:  1.1081994754567266 ; l2 norm of weights:  7.182583986501902\n",
            "Iteration#:  7968 ; Loss:  3.012677182234272 ; l2 norm of gradient:  1.108182119832859 ; l2 norm of weights:  7.182566557074503\n",
            "Iteration#:  7972 ; Loss:  3.0126278010265333 ; l2 norm of gradient:  1.108164764235173 ; l2 norm of weights:  7.182549127897344\n",
            "Iteration#:  7976 ; Loss:  3.012578422251039 ; l2 norm of gradient:  1.1081474086636256 ; l2 norm of weights:  7.1825316989704175\n",
            "Iteration#:  7980 ; Loss:  3.0125290451840057 ; l2 norm of gradient:  1.1081300531181772 ; l2 norm of weights:  7.182514270293714\n",
            "Iteration#:  7984 ; Loss:  3.0124796680287327 ; l2 norm of gradient:  1.1081126975987832 ; l2 norm of weights:  7.182496841867229\n",
            "Iteration#:  7988 ; Loss:  3.0124302938237237 ; l2 norm of gradient:  1.1080953421054038 ; l2 norm of weights:  7.182479413690953\n",
            "Iteration#:  7992 ; Loss:  3.0123809214005646 ; l2 norm of gradient:  1.1080779866379944 ; l2 norm of weights:  7.182461985764882\n",
            "Iteration#:  7996 ; Loss:  3.0123315495007628 ; l2 norm of gradient:  1.1080606311965158 ; l2 norm of weights:  7.182444558089005\n",
            "Iteration#:  8000 ; Loss:  3.0122821798594472 ; l2 norm of gradient:  1.1080432757809249 ; l2 norm of weights:  7.182427130663316\n",
            "Iteration#:  8004 ; Loss:  3.0122328107114367 ; l2 norm of gradient:  1.1080259203911798 ; l2 norm of weights:  7.18240970348781\n",
            "Iteration#:  8008 ; Loss:  3.0121834441534583 ; l2 norm of gradient:  1.1080085650272384 ; l2 norm of weights:  7.182392276562479\n",
            "Iteration#:  8012 ; Loss:  3.0121340793203077 ; l2 norm of gradient:  1.1079912096890605 ; l2 norm of weights:  7.182374849887314\n",
            "Iteration#:  8016 ; Loss:  3.012084715188088 ; l2 norm of gradient:  1.1079738543766011 ; l2 norm of weights:  7.18235742346231\n",
            "Iteration#:  8020 ; Loss:  3.012035352749037 ; l2 norm of gradient:  1.1079564990898223 ; l2 norm of weights:  7.182339997287458\n",
            "Iteration#:  8024 ; Loss:  3.011985992480468 ; l2 norm of gradient:  1.1079391438286805 ; l2 norm of weights:  7.182322571362753\n",
            "Iteration#:  8028 ; Loss:  3.0119366329393915 ; l2 norm of gradient:  1.1079217885931334 ; l2 norm of weights:  7.182305145688185\n",
            "Iteration#:  8032 ; Loss:  3.0118872757527786 ; l2 norm of gradient:  1.1079044333831414 ; l2 norm of weights:  7.182287720263749\n",
            "Iteration#:  8036 ; Loss:  3.0118379192117435 ; l2 norm of gradient:  1.107887078198661 ; l2 norm of weights:  7.182270295089437\n",
            "Iteration#:  8040 ; Loss:  3.0117885647756077 ; l2 norm of gradient:  1.1078697230396517 ; l2 norm of weights:  7.182252870165244\n",
            "Iteration#:  8044 ; Loss:  3.0117392119306343 ; l2 norm of gradient:  1.1078523679060712 ; l2 norm of weights:  7.182235445491161\n",
            "Iteration#:  8048 ; Loss:  3.0116898603189854 ; l2 norm of gradient:  1.1078350127978798 ; l2 norm of weights:  7.18221802106718\n",
            "Iteration#:  8052 ; Loss:  3.0116405107759916 ; l2 norm of gradient:  1.1078176577150354 ; l2 norm of weights:  7.182200596893294\n",
            "Iteration#:  8056 ; Loss:  3.0115911616398696 ; l2 norm of gradient:  1.1078003026574954 ; l2 norm of weights:  7.182183172969498\n",
            "Iteration#:  8060 ; Loss:  3.0115418150962903 ; l2 norm of gradient:  1.1077829476252183 ; l2 norm of weights:  7.182165749295783\n",
            "Iteration#:  8064 ; Loss:  3.011492470328993 ; l2 norm of gradient:  1.1077655926181647 ; l2 norm of weights:  7.182148325872142\n",
            "Iteration#:  8068 ; Loss:  3.011443126228842 ; l2 norm of gradient:  1.1077482376362933 ; l2 norm of weights:  7.182130902698569\n",
            "Iteration#:  8072 ; Loss:  3.0113937839466907 ; l2 norm of gradient:  1.10773088267956 ; l2 norm of weights:  7.1821134797750545\n",
            "Iteration#:  8076 ; Loss:  3.011344443615328 ; l2 norm of gradient:  1.1077135277479269 ; l2 norm of weights:  7.182096057101594\n",
            "Iteration#:  8080 ; Loss:  3.01129510415009 ; l2 norm of gradient:  1.107696172841351 ; l2 norm of weights:  7.182078634678179\n",
            "Iteration#:  8084 ; Loss:  3.0112457665826304 ; l2 norm of gradient:  1.1076788179597916 ; l2 norm of weights:  7.182061212504802\n",
            "Iteration#:  8088 ; Loss:  3.01119643019104 ; l2 norm of gradient:  1.1076614631032071 ; l2 norm of weights:  7.1820437905814565\n",
            "Iteration#:  8092 ; Loss:  3.0111470959364666 ; l2 norm of gradient:  1.1076441082715585 ; l2 norm of weights:  7.1820263689081365\n",
            "Iteration#:  8096 ; Loss:  3.0110977630816085 ; l2 norm of gradient:  1.1076267534648019 ; l2 norm of weights:  7.1820089474848325\n",
            "Iteration#:  8100 ; Loss:  3.011048431196339 ; l2 norm of gradient:  1.107609398682898 ; l2 norm of weights:  7.1819915263115375\n",
            "Iteration#:  8104 ; Loss:  3.0109991018675584 ; l2 norm of gradient:  1.1075920439258062 ; l2 norm of weights:  7.181974105388247\n",
            "Iteration#:  8108 ; Loss:  3.0109497730586527 ; l2 norm of gradient:  1.107574689193484 ; l2 norm of weights:  7.181956684714951\n",
            "Iteration#:  8112 ; Loss:  3.010900446124242 ; l2 norm of gradient:  1.107557334485892 ; l2 norm of weights:  7.181939264291644\n",
            "Iteration#:  8116 ; Loss:  3.010851121517799 ; l2 norm of gradient:  1.1075399798029888 ; l2 norm of weights:  7.181921844118319\n",
            "Iteration#:  8120 ; Loss:  3.010801797423747 ; l2 norm of gradient:  1.1075226251447325 ; l2 norm of weights:  7.1819044241949666\n",
            "Iteration#:  8124 ; Loss:  3.0107524751347743 ; l2 norm of gradient:  1.1075052705110848 ; l2 norm of weights:  7.181887004521581\n",
            "Iteration#:  8128 ; Loss:  3.01070315425796 ; l2 norm of gradient:  1.1074879159020024 ; l2 norm of weights:  7.181869585098158\n",
            "Iteration#:  8132 ; Loss:  3.0106538355040486 ; l2 norm of gradient:  1.107470561317446 ; l2 norm of weights:  7.181852165924686\n",
            "Iteration#:  8136 ; Loss:  3.01060451786806 ; l2 norm of gradient:  1.1074532067573757 ; l2 norm of weights:  7.181834747001159\n",
            "Iteration#:  8140 ; Loss:  3.010555201490502 ; l2 norm of gradient:  1.1074358522217491 ; l2 norm of weights:  7.181817328327571\n",
            "Iteration#:  8144 ; Loss:  3.010505887598035 ; l2 norm of gradient:  1.1074184977105266 ; l2 norm of weights:  7.181799909903913\n",
            "Iteration#:  8148 ; Loss:  3.0104565738180264 ; l2 norm of gradient:  1.1074011432236675 ; l2 norm of weights:  7.18178249173018\n",
            "Iteration#:  8152 ; Loss:  3.0104072627971163 ; l2 norm of gradient:  1.1073837887611315 ; l2 norm of weights:  7.181765073806364\n",
            "Iteration#:  8156 ; Loss:  3.0103579528756104 ; l2 norm of gradient:  1.1073664343228768 ; l2 norm of weights:  7.181747656132458\n",
            "Iteration#:  8160 ; Loss:  3.0103086442807028 ; l2 norm of gradient:  1.1073490799088654 ; l2 norm of weights:  7.181730238708455\n",
            "Iteration#:  8164 ; Loss:  3.0102593380956044 ; l2 norm of gradient:  1.1073317255190545 ; l2 norm of weights:  7.1817128215343455\n",
            "Iteration#:  8168 ; Loss:  3.010210032109252 ; l2 norm of gradient:  1.1073143711534057 ; l2 norm of weights:  7.181695404610125\n",
            "Iteration#:  8172 ; Loss:  3.010160728726176 ; l2 norm of gradient:  1.107297016811877 ; l2 norm of weights:  7.181677987935786\n",
            "Iteration#:  8176 ; Loss:  3.01011142704653 ; l2 norm of gradient:  1.1072796624944286 ; l2 norm of weights:  7.181660571511322\n",
            "Iteration#:  8180 ; Loss:  3.0100621257090627 ; l2 norm of gradient:  1.1072623082010207 ; l2 norm of weights:  7.181643155336724\n",
            "Iteration#:  8184 ; Loss:  3.010012826894094 ; l2 norm of gradient:  1.1072449539316127 ; l2 norm of weights:  7.181625739411985\n",
            "Iteration#:  8188 ; Loss:  3.009963528548545 ; l2 norm of gradient:  1.107227599686165 ; l2 norm of weights:  7.181608323737099\n",
            "Iteration#:  8192 ; Loss:  3.0099142330868283 ; l2 norm of gradient:  1.1072102454646366 ; l2 norm of weights:  7.181590908312059\n",
            "Iteration#:  8196 ; Loss:  3.0098649389882803 ; l2 norm of gradient:  1.107192891266987 ; l2 norm of weights:  7.181573493136858\n",
            "Iteration#:  8200 ; Loss:  3.0098156454990947 ; l2 norm of gradient:  1.1071755370931775 ; l2 norm of weights:  7.181556078211486\n",
            "Iteration#:  8204 ; Loss:  3.0097663544020774 ; l2 norm of gradient:  1.1071581829431674 ; l2 norm of weights:  7.181538663535939\n",
            "Iteration#:  8208 ; Loss:  3.0097170637701938 ; l2 norm of gradient:  1.1071408288169173 ; l2 norm of weights:  7.181521249110209\n",
            "Iteration#:  8212 ; Loss:  3.009667776019225 ; l2 norm of gradient:  1.1071234747143848 ; l2 norm of weights:  7.181503834934286\n",
            "Iteration#:  8216 ; Loss:  3.009618489316191 ; l2 norm of gradient:  1.1071061206355333 ; l2 norm of weights:  7.181486421008168\n",
            "Iteration#:  8220 ; Loss:  3.0095692036631005 ; l2 norm of gradient:  1.1070887665803213 ; l2 norm of weights:  7.181469007331845\n",
            "Iteration#:  8224 ; Loss:  3.0095199204784366 ; l2 norm of gradient:  1.1070714125487089 ; l2 norm of weights:  7.181451593905311\n",
            "Iteration#:  8228 ; Loss:  3.009470637634575 ; l2 norm of gradient:  1.1070540585406559 ; l2 norm of weights:  7.181434180728556\n",
            "Iteration#:  8232 ; Loss:  3.0094213572452437 ; l2 norm of gradient:  1.1070367045561227 ; l2 norm of weights:  7.181416767801577\n",
            "Iteration#:  8236 ; Loss:  3.0093720783992 ; l2 norm of gradient:  1.1070193505950707 ; l2 norm of weights:  7.181399355124363\n",
            "Iteration#:  8240 ; Loss:  3.009322800588473 ; l2 norm of gradient:  1.107001996657459 ; l2 norm of weights:  7.181381942696908\n",
            "Iteration#:  8244 ; Loss:  3.0092735245277638 ; l2 norm of gradient:  1.1069846427432477 ; l2 norm of weights:  7.181364530519206\n",
            "Iteration#:  8248 ; Loss:  3.009224249786606 ; l2 norm of gradient:  1.1069672888523978 ; l2 norm of weights:  7.18134711859125\n",
            "Iteration#:  8252 ; Loss:  3.00917497686328 ; l2 norm of gradient:  1.1069499349848697 ; l2 norm of weights:  7.18132970691303\n",
            "Iteration#:  8256 ; Loss:  3.009125705896989 ; l2 norm of gradient:  1.106932581140624 ; l2 norm of weights:  7.181312295484543\n",
            "Iteration#:  8260 ; Loss:  3.009076435872844 ; l2 norm of gradient:  1.10691522731962 ; l2 norm of weights:  7.181294884305778\n",
            "Iteration#:  8264 ; Loss:  3.009027167597238 ; l2 norm of gradient:  1.1068978735218196 ; l2 norm of weights:  7.18127747337673\n",
            "Iteration#:  8268 ; Loss:  3.008977900563307 ; l2 norm of gradient:  1.1068805197471827 ; l2 norm of weights:  7.181260062697392\n",
            "Iteration#:  8272 ; Loss:  3.008928635429786 ; l2 norm of gradient:  1.1068631659956694 ; l2 norm of weights:  7.1812426522677555\n",
            "Iteration#:  8276 ; Loss:  3.008879372251564 ; l2 norm of gradient:  1.1068458122672424 ; l2 norm of weights:  7.181225242087814\n",
            "Iteration#:  8280 ; Loss:  3.008830109190105 ; l2 norm of gradient:  1.1068284585618582 ; l2 norm of weights:  7.181207832157561\n",
            "Iteration#:  8284 ; Loss:  3.008780849149299 ; l2 norm of gradient:  1.1068111048794822 ; l2 norm of weights:  7.18119042247699\n",
            "Iteration#:  8288 ; Loss:  3.008731589509603 ; l2 norm of gradient:  1.1067937512200714 ; l2 norm of weights:  7.18117301304609\n",
            "Iteration#:  8292 ; Loss:  3.008682332193955 ; l2 norm of gradient:  1.1067763975835898 ; l2 norm of weights:  7.181155603864858\n",
            "Iteration#:  8296 ; Loss:  3.0086330757065065 ; l2 norm of gradient:  1.1067590439699957 ; l2 norm of weights:  7.181138194933285\n",
            "Iteration#:  8300 ; Loss:  3.008583821595128 ; l2 norm of gradient:  1.1067416903792509 ; l2 norm of weights:  7.181120786251365\n",
            "Iteration#:  8304 ; Loss:  3.0085345688096488 ; l2 norm of gradient:  1.1067243368113155 ; l2 norm of weights:  7.181103377819088\n",
            "Iteration#:  8308 ; Loss:  3.00848531682131 ; l2 norm of gradient:  1.1067069832661511 ; l2 norm of weights:  7.181085969636451\n",
            "Iteration#:  8312 ; Loss:  3.0084360674123056 ; l2 norm of gradient:  1.1066896297437185 ; l2 norm of weights:  7.181068561703442\n",
            "Iteration#:  8316 ; Loss:  3.0083868184970566 ; l2 norm of gradient:  1.1066722762439791 ; l2 norm of weights:  7.181051154020058\n",
            "Iteration#:  8320 ; Loss:  3.008337572168462 ; l2 norm of gradient:  1.1066549227668931 ; l2 norm of weights:  7.181033746586291\n",
            "Iteration#:  8324 ; Loss:  3.0082883270241045 ; l2 norm of gradient:  1.1066375693124224 ; l2 norm of weights:  7.181016339402132\n",
            "Iteration#:  8328 ; Loss:  3.0082390828509595 ; l2 norm of gradient:  1.1066202158805276 ; l2 norm of weights:  7.180998932467575\n",
            "Iteration#:  8332 ; Loss:  3.0081898408456436 ; l2 norm of gradient:  1.1066028624711688 ; l2 norm of weights:  7.180981525782614\n",
            "Iteration#:  8336 ; Loss:  3.008140600020795 ; l2 norm of gradient:  1.1065855090843093 ; l2 norm of weights:  7.180964119347239\n",
            "Iteration#:  8340 ; Loss:  3.0080913609383804 ; l2 norm of gradient:  1.106568155719908 ; l2 norm of weights:  7.180946713161446\n",
            "Iteration#:  8344 ; Loss:  3.008042123259853 ; l2 norm of gradient:  1.106550802377929 ; l2 norm of weights:  7.180929307225225\n",
            "Iteration#:  8348 ; Loss:  3.0079928872527963 ; l2 norm of gradient:  1.1065334490583312 ; l2 norm of weights:  7.180911901538572\n",
            "Iteration#:  8352 ; Loss:  3.0079436528887213 ; l2 norm of gradient:  1.1065160957610767 ; l2 norm of weights:  7.180894496101476\n",
            "Iteration#:  8356 ; Loss:  3.007894419704369 ; l2 norm of gradient:  1.1064987424861263 ; l2 norm of weights:  7.180877090913932\n",
            "Iteration#:  8360 ; Loss:  3.0078451886273134 ; l2 norm of gradient:  1.1064813892334409 ; l2 norm of weights:  7.180859685975934\n",
            "Iteration#:  8364 ; Loss:  3.0077959584284955 ; l2 norm of gradient:  1.1064640360029832 ; l2 norm of weights:  7.1808422812874735\n",
            "Iteration#:  8368 ; Loss:  3.007746730216685 ; l2 norm of gradient:  1.1064466827947148 ; l2 norm of weights:  7.180824876848542\n",
            "Iteration#:  8372 ; Loss:  3.00769750381277 ; l2 norm of gradient:  1.1064293296085963 ; l2 norm of weights:  7.180807472659135\n",
            "Iteration#:  8376 ; Loss:  3.0076482779356852 ; l2 norm of gradient:  1.1064119764445903 ; l2 norm of weights:  7.180790068719244\n",
            "Iteration#:  8380 ; Loss:  3.0075990544987516 ; l2 norm of gradient:  1.1063946233026563 ; l2 norm of weights:  7.180772665028862\n",
            "Iteration#:  8384 ; Loss:  3.0075498320903886 ; l2 norm of gradient:  1.1063772701827577 ; l2 norm of weights:  7.180755261587981\n",
            "Iteration#:  8388 ; Loss:  3.0075006115098986 ; l2 norm of gradient:  1.1063599170848561 ; l2 norm of weights:  7.180737858396595\n",
            "Iteration#:  8392 ; Loss:  3.007451391856797 ; l2 norm of gradient:  1.106342564008912 ; l2 norm of weights:  7.180720455454698\n",
            "Iteration#:  8396 ; Loss:  3.0074021747197204 ; l2 norm of gradient:  1.106325210954888 ; l2 norm of weights:  7.180703052762279\n",
            "Iteration#:  8400 ; Loss:  3.007352958918809 ; l2 norm of gradient:  1.1063078579227457 ; l2 norm of weights:  7.180685650319333\n",
            "Iteration#:  8404 ; Loss:  3.007303744080818 ; l2 norm of gradient:  1.1062905049124474 ; l2 norm of weights:  7.180668248125855\n",
            "Iteration#:  8408 ; Loss:  3.0072545314186847 ; l2 norm of gradient:  1.1062731519239533 ; l2 norm of weights:  7.180650846181835\n",
            "Iteration#:  8412 ; Loss:  3.0072053193950627 ; l2 norm of gradient:  1.1062557989572264 ; l2 norm of weights:  7.180633444487265\n",
            "Iteration#:  8416 ; Loss:  3.0071561100386477 ; l2 norm of gradient:  1.1062384460122283 ; l2 norm of weights:  7.180616043042142\n",
            "Iteration#:  8420 ; Loss:  3.0071069008924183 ; l2 norm of gradient:  1.1062210930889214 ; l2 norm of weights:  7.180598641846455\n",
            "Iteration#:  8424 ; Loss:  3.007057694356975 ; l2 norm of gradient:  1.106203740187267 ; l2 norm of weights:  7.180581240900197\n",
            "Iteration#:  8428 ; Loss:  3.007008489647161 ; l2 norm of gradient:  1.1061863873072266 ; l2 norm of weights:  7.180563840203364\n",
            "Iteration#:  8432 ; Loss:  3.0069592854753364 ; l2 norm of gradient:  1.106169034448765 ; l2 norm of weights:  7.180546439755946\n",
            "Iteration#:  8436 ; Loss:  3.006910083309049 ; l2 norm of gradient:  1.1061516816118413 ; l2 norm of weights:  7.1805290395579355\n",
            "Iteration#:  8440 ; Loss:  3.0068608825986614 ; l2 norm of gradient:  1.1061343287964183 ; l2 norm of weights:  7.180511639609328\n",
            "Iteration#:  8444 ; Loss:  3.0068116835739427 ; l2 norm of gradient:  1.1061169760024596 ; l2 norm of weights:  7.180494239910114\n",
            "Iteration#:  8448 ; Loss:  3.006762485252925 ; l2 norm of gradient:  1.1060996232299254 ; l2 norm of weights:  7.180476840460289\n",
            "Iteration#:  8452 ; Loss:  3.0067132897573887 ; l2 norm of gradient:  1.106082270478779 ; l2 norm of weights:  7.180459441259841\n",
            "Iteration#:  8456 ; Loss:  3.006664095499413 ; l2 norm of gradient:  1.1060649177489825 ; l2 norm of weights:  7.180442042308767\n",
            "Iteration#:  8460 ; Loss:  3.006614901828907 ; l2 norm of gradient:  1.1060475650404982 ; l2 norm of weights:  7.1804246436070605\n",
            "Iteration#:  8464 ; Loss:  3.006565711102338 ; l2 norm of gradient:  1.1060302123532884 ; l2 norm of weights:  7.18040724515471\n",
            "Iteration#:  8468 ; Loss:  3.0065165205673163 ; l2 norm of gradient:  1.1060128596873149 ; l2 norm of weights:  7.180389846951712\n",
            "Iteration#:  8472 ; Loss:  3.0064673324649225 ; l2 norm of gradient:  1.1059955070425411 ; l2 norm of weights:  7.180372448998059\n",
            "Iteration#:  8476 ; Loss:  3.006418145515858 ; l2 norm of gradient:  1.1059781544189284 ; l2 norm of weights:  7.180355051293742\n",
            "Iteration#:  8480 ; Loss:  3.00636896025505 ; l2 norm of gradient:  1.1059608018164402 ; l2 norm of weights:  7.180337653838755\n",
            "Iteration#:  8484 ; Loss:  3.0063197767172767 ; l2 norm of gradient:  1.1059434492350382 ; l2 norm of weights:  7.180320256633091\n",
            "Iteration#:  8488 ; Loss:  3.00627059396626 ; l2 norm of gradient:  1.1059260966746862 ; l2 norm of weights:  7.180302859676741\n",
            "Iteration#:  8492 ; Loss:  3.0062214140035266 ; l2 norm of gradient:  1.105908744135346 ; l2 norm of weights:  7.1802854629696995\n",
            "Iteration#:  8496 ; Loss:  3.0061722341849966 ; l2 norm of gradient:  1.10589139161698 ; l2 norm of weights:  7.180268066511961\n",
            "Iteration#:  8500 ; Loss:  3.0061230566951713 ; l2 norm of gradient:  1.1058740391195505 ; l2 norm of weights:  7.180250670303517\n",
            "Iteration#:  8504 ; Loss:  3.0060738804716687 ; l2 norm of gradient:  1.1058566866430213 ; l2 norm of weights:  7.180233274344356\n",
            "Iteration#:  8508 ; Loss:  3.0060247061873744 ; l2 norm of gradient:  1.1058393341873545 ; l2 norm of weights:  7.180215878634477\n",
            "Iteration#:  8512 ; Loss:  3.0059755334176863 ; l2 norm of gradient:  1.1058219817525137 ; l2 norm of weights:  7.180198483173872\n",
            "Iteration#:  8516 ; Loss:  3.005926361435223 ; l2 norm of gradient:  1.1058046293384594 ; l2 norm of weights:  7.180181087962531\n",
            "Iteration#:  8520 ; Loss:  3.005877192126944 ; l2 norm of gradient:  1.1057872769451578 ; l2 norm of weights:  7.180163693000448\n",
            "Iteration#:  8524 ; Loss:  3.0058280232065835 ; l2 norm of gradient:  1.1057699245725685 ; l2 norm of weights:  7.1801462982876165\n",
            "Iteration#:  8528 ; Loss:  3.0057788564923333 ; l2 norm of gradient:  1.1057525722206563 ; l2 norm of weights:  7.18012890382403\n",
            "Iteration#:  8532 ; Loss:  3.0057296907834745 ; l2 norm of gradient:  1.1057352198893848 ; l2 norm of weights:  7.180111509609678\n",
            "Iteration#:  8536 ; Loss:  3.0056805275462337 ; l2 norm of gradient:  1.1057178675787154 ; l2 norm of weights:  7.180094115644556\n",
            "Iteration#:  8540 ; Loss:  3.0056313646212978 ; l2 norm of gradient:  1.1057005152886126 ; l2 norm of weights:  7.180076721928657\n",
            "Iteration#:  8544 ; Loss:  3.005582204333218 ; l2 norm of gradient:  1.1056831630190371 ; l2 norm of weights:  7.180059328461973\n",
            "Iteration#:  8548 ; Loss:  3.005533045320862 ; l2 norm of gradient:  1.105665810769954 ; l2 norm of weights:  7.180041935244498\n",
            "Iteration#:  8552 ; Loss:  3.0054838871859566 ; l2 norm of gradient:  1.1056484585413262 ; l2 norm of weights:  7.180024542276221\n",
            "Iteration#:  8556 ; Loss:  3.005434731698177 ; l2 norm of gradient:  1.105631106333116 ; l2 norm of weights:  7.180007149557141\n",
            "Iteration#:  8560 ; Loss:  3.0053855765580835 ; l2 norm of gradient:  1.1056137541452882 ; l2 norm of weights:  7.179989757087247\n",
            "Iteration#:  8564 ; Loss:  3.0053364239002818 ; l2 norm of gradient:  1.1055964019778044 ; l2 norm of weights:  7.179972364866531\n",
            "Iteration#:  8568 ; Loss:  3.005287271896117 ; l2 norm of gradient:  1.1055790498306288 ; l2 norm of weights:  7.179954972894988\n",
            "Iteration#:  8572 ; Loss:  3.005238122029518 ; l2 norm of gradient:  1.105561697703725 ; l2 norm of weights:  7.179937581172609\n",
            "Iteration#:  8576 ; Loss:  3.0051889743212175 ; l2 norm of gradient:  1.1055443455970562 ; l2 norm of weights:  7.17992018969939\n",
            "Iteration#:  8580 ; Loss:  3.0051398268108858 ; l2 norm of gradient:  1.105526993510585 ; l2 norm of weights:  7.179902798475319\n",
            "Iteration#:  8584 ; Loss:  3.0050906818527525 ; l2 norm of gradient:  1.1055096414442753 ; l2 norm of weights:  7.179885407500393\n",
            "Iteration#:  8588 ; Loss:  3.0050415374132444 ; l2 norm of gradient:  1.1054922893980912 ; l2 norm of weights:  7.179868016774605\n",
            "Iteration#:  8592 ; Loss:  3.004992395323824 ; l2 norm of gradient:  1.1054749373719945 ; l2 norm of weights:  7.179850626297943\n",
            "Iteration#:  8596 ; Loss:  3.004943254297566 ; l2 norm of gradient:  1.1054575853659503 ; l2 norm of weights:  7.179833236070405\n",
            "Iteration#:  8600 ; Loss:  3.0048941157192637 ; l2 norm of gradient:  1.1054402333799223 ; l2 norm of weights:  7.179815846091981\n",
            "Iteration#:  8604 ; Loss:  3.004844977623334 ; l2 norm of gradient:  1.1054228814138731 ; l2 norm of weights:  7.179798456362667\n",
            "Iteration#:  8608 ; Loss:  3.0047958417336216 ; l2 norm of gradient:  1.105405529467768 ; l2 norm of weights:  7.179781066882451\n",
            "Iteration#:  8612 ; Loss:  3.0047467071969627 ; l2 norm of gradient:  1.1053881775415686 ; l2 norm of weights:  7.179763677651329\n",
            "Iteration#:  8616 ; Loss:  3.004697573730116 ; l2 norm of gradient:  1.1053708256352397 ; l2 norm of weights:  7.179746288669294\n",
            "Iteration#:  8620 ; Loss:  3.0046484426220035 ; l2 norm of gradient:  1.1053534737487452 ; l2 norm of weights:  7.179728899936337\n",
            "Iteration#:  8624 ; Loss:  3.0045993122660395 ; l2 norm of gradient:  1.105336121882049 ; l2 norm of weights:  7.179711511452453\n",
            "Iteration#:  8628 ; Loss:  3.0045501843498075 ; l2 norm of gradient:  1.1053187700351146 ; l2 norm of weights:  7.179694123217632\n",
            "Iteration#:  8632 ; Loss:  3.004501056806358 ; l2 norm of gradient:  1.1053014182079053 ; l2 norm of weights:  7.17967673523187\n",
            "Iteration#:  8636 ; Loss:  3.004451931895408 ; l2 norm of gradient:  1.105284066400386 ; l2 norm of weights:  7.179659347495158\n",
            "Iteration#:  8640 ; Loss:  3.0044028074386935 ; l2 norm of gradient:  1.1052667146125215 ; l2 norm of weights:  7.179641960007489\n",
            "Iteration#:  8644 ; Loss:  3.0043536853705306 ; l2 norm of gradient:  1.1052493628442739 ; l2 norm of weights:  7.179624572768856\n",
            "Iteration#:  8648 ; Loss:  3.004304564183102 ; l2 norm of gradient:  1.1052320110956075 ; l2 norm of weights:  7.179607185779253\n",
            "Iteration#:  8652 ; Loss:  3.0042554452557155 ; l2 norm of gradient:  1.105214659366488 ; l2 norm of weights:  7.179589799038671\n",
            "Iteration#:  8656 ; Loss:  3.0042063277733257 ; l2 norm of gradient:  1.1051973076568773 ; l2 norm of weights:  7.1795724125471025\n",
            "Iteration#:  8660 ; Loss:  3.004157211593656 ; l2 norm of gradient:  1.1051799559667417 ; l2 norm of weights:  7.179555026304543\n",
            "Iteration#:  8664 ; Loss:  3.0041080973048855 ; l2 norm of gradient:  1.105162604296044 ; l2 norm of weights:  7.179537640310982\n",
            "Iteration#:  8668 ; Loss:  3.0040589836059746 ; l2 norm of gradient:  1.1051452526447483 ; l2 norm of weights:  7.179520254566414\n",
            "Iteration#:  8672 ; Loss:  3.0040098723926274 ; l2 norm of gradient:  1.10512790101282 ; l2 norm of weights:  7.179502869070834\n",
            "Iteration#:  8676 ; Loss:  3.003960761888829 ; l2 norm of gradient:  1.1051105494002225 ; l2 norm of weights:  7.179485483824231\n",
            "Iteration#:  8680 ; Loss:  3.003911653804906 ; l2 norm of gradient:  1.1050931978069205 ; l2 norm of weights:  7.1794680988266\n",
            "Iteration#:  8684 ; Loss:  3.0038625463757755 ; l2 norm of gradient:  1.105075846232879 ; l2 norm of weights:  7.179450714077933\n",
            "Iteration#:  8688 ; Loss:  3.0038134412168445 ; l2 norm of gradient:  1.1050584946780608 ; l2 norm of weights:  7.179433329578224\n",
            "Iteration#:  8692 ; Loss:  3.0037643368634392 ; l2 norm of gradient:  1.1050411431424316 ; l2 norm of weights:  7.179415945327464\n",
            "Iteration#:  8696 ; Loss:  3.003715234765833 ; l2 norm of gradient:  1.1050237916259549 ; l2 norm of weights:  7.179398561325648\n",
            "Iteration#:  8700 ; Loss:  3.0036661343179274 ; l2 norm of gradient:  1.1050064401285962 ; l2 norm of weights:  7.179381177572767\n",
            "Iteration#:  8704 ; Loss:  3.0036170345168016 ; l2 norm of gradient:  1.1049890886503198 ; l2 norm of weights:  7.179363794068814\n",
            "Iteration#:  8708 ; Loss:  3.003567937265621 ; l2 norm of gradient:  1.104971737191091 ; l2 norm of weights:  7.179346410813784\n",
            "Iteration#:  8712 ; Loss:  3.003518840677839 ; l2 norm of gradient:  1.1049543857508726 ; l2 norm of weights:  7.179329027807666\n",
            "Iteration#:  8716 ; Loss:  3.003469746541466 ; l2 norm of gradient:  1.1049370343296308 ; l2 norm of weights:  7.179311645050455\n",
            "Iteration#:  8720 ; Loss:  3.0034206530690684 ; l2 norm of gradient:  1.1049196829273304 ; l2 norm of weights:  7.179294262542145\n",
            "Iteration#:  8724 ; Loss:  3.0033715617561145 ; l2 norm of gradient:  1.1049023315439346 ; l2 norm of weights:  7.179276880282727\n",
            "Iteration#:  8728 ; Loss:  3.0033224713554842 ; l2 norm of gradient:  1.1048849801794098 ; l2 norm of weights:  7.179259498272195\n",
            "Iteration#:  8732 ; Loss:  3.0032733830332257 ; l2 norm of gradient:  1.10486762883372 ; l2 norm of weights:  7.179242116510543\n",
            "Iteration#:  8736 ; Loss:  3.0032242956410085 ; l2 norm of gradient:  1.1048502775068307 ; l2 norm of weights:  7.179224734997759\n",
            "Iteration#:  8740 ; Loss:  3.0031752105781 ; l2 norm of gradient:  1.1048329261987064 ; l2 norm of weights:  7.179207353733841\n",
            "Iteration#:  8744 ; Loss:  3.0031261261823365 ; l2 norm of gradient:  1.1048155749093123 ; l2 norm of weights:  7.1791899727187785\n",
            "Iteration#:  8748 ; Loss:  3.0030770441208645 ; l2 norm of gradient:  1.1047982236386131 ; l2 norm of weights:  7.179172591952566\n",
            "Iteration#:  8752 ; Loss:  3.0030279636339654 ; l2 norm of gradient:  1.104780872386573 ; l2 norm of weights:  7.179155211435197\n",
            "Iteration#:  8756 ; Loss:  3.002978883940894 ; l2 norm of gradient:  1.1047635211531583 ; l2 norm of weights:  7.179137831166662\n",
            "Iteration#:  8760 ; Loss:  3.0029298065837007 ; l2 norm of gradient:  1.1047461699383339 ; l2 norm of weights:  7.1791204511469555\n",
            "Iteration#:  8764 ; Loss:  3.0028807298116726 ; l2 norm of gradient:  1.104728818742065 ; l2 norm of weights:  7.179103071376071\n",
            "Iteration#:  8768 ; Loss:  3.0028316553867733 ; l2 norm of gradient:  1.1047114675643162 ; l2 norm of weights:  7.179085691853999\n",
            "Iteration#:  8772 ; Loss:  3.00278258175486 ; l2 norm of gradient:  1.104694116405053 ; l2 norm of weights:  7.179068312580733\n",
            "Iteration#:  8776 ; Loss:  3.0027335106624298 ; l2 norm of gradient:  1.1046767652642413 ; l2 norm of weights:  7.179050933556266\n",
            "Iteration#:  8780 ; Loss:  3.0026844400993262 ; l2 norm of gradient:  1.104659414141845 ; l2 norm of weights:  7.1790335547805935\n",
            "Iteration#:  8784 ; Loss:  3.002635371801222 ; l2 norm of gradient:  1.1046420630378309 ; l2 norm of weights:  7.179016176253705\n",
            "Iteration#:  8788 ; Loss:  3.002586304368278 ; l2 norm of gradient:  1.104624711952163 ; l2 norm of weights:  7.178998797975594\n",
            "Iteration#:  8792 ; Loss:  3.0025372388166796 ; l2 norm of gradient:  1.1046073608848077 ; l2 norm of weights:  7.178981419946254\n",
            "Iteration#:  8796 ; Loss:  3.002488174516817 ; l2 norm of gradient:  1.1045900098357297 ; l2 norm of weights:  7.178964042165677\n",
            "Iteration#:  8800 ; Loss:  3.002439112412447 ; l2 norm of gradient:  1.1045726588048952 ; l2 norm of weights:  7.178946664633857\n",
            "Iteration#:  8804 ; Loss:  3.0023900520308997 ; l2 norm of gradient:  1.1045553077922692 ; l2 norm of weights:  7.178929287350786\n",
            "Iteration#:  8808 ; Loss:  3.002340992347317 ; l2 norm of gradient:  1.1045379567978175 ; l2 norm of weights:  7.178911910316458\n",
            "Iteration#:  8812 ; Loss:  3.002291934747494 ; l2 norm of gradient:  1.1045206058215058 ; l2 norm of weights:  7.178894533530864\n",
            "Iteration#:  8816 ; Loss:  3.002242878077501 ; l2 norm of gradient:  1.1045032548632991 ; l2 norm of weights:  7.178877156993997\n",
            "Iteration#:  8820 ; Loss:  3.0021938237451375 ; l2 norm of gradient:  1.1044859039231638 ; l2 norm of weights:  7.178859780705851\n",
            "Iteration#:  8824 ; Loss:  3.002144770212993 ; l2 norm of gradient:  1.1044685530010652 ; l2 norm of weights:  7.17884240466642\n",
            "Iteration#:  8828 ; Loss:  3.00209571887479 ; l2 norm of gradient:  1.1044512020969686 ; l2 norm of weights:  7.178825028875694\n",
            "Iteration#:  8832 ; Loss:  3.0020466682604487 ; l2 norm of gradient:  1.104433851210841 ; l2 norm of weights:  7.178807653333665\n",
            "Iteration#:  8836 ; Loss:  3.0019976200597185 ; l2 norm of gradient:  1.1044165003426465 ; l2 norm of weights:  7.17879027804033\n",
            "Iteration#:  8840 ; Loss:  3.00194857258571 ; l2 norm of gradient:  1.104399149492353 ; l2 norm of weights:  7.17877290299568\n",
            "Iteration#:  8844 ; Loss:  3.001899527507875 ; l2 norm of gradient:  1.1043817986599251 ; l2 norm of weights:  7.178755528199706\n",
            "Iteration#:  8848 ; Loss:  3.0018504825782117 ; l2 norm of gradient:  1.104364447845328 ; l2 norm of weights:  7.178738153652404\n",
            "Iteration#:  8852 ; Loss:  3.0018014405757683 ; l2 norm of gradient:  1.1043470970485305 ; l2 norm of weights:  7.178720779353763\n",
            "Iteration#:  8856 ; Loss:  3.0017523992181157 ; l2 norm of gradient:  1.1043297462694952 ; l2 norm of weights:  7.1787034053037795\n",
            "Iteration#:  8860 ; Loss:  3.0017033602882814 ; l2 norm of gradient:  1.1043123955081902 ; l2 norm of weights:  7.178686031502443\n",
            "Iteration#:  8864 ; Loss:  3.0016543218590086 ; l2 norm of gradient:  1.104295044764581 ; l2 norm of weights:  7.178668657949751\n",
            "Iteration#:  8868 ; Loss:  3.0016052860556153 ; l2 norm of gradient:  1.1042776940386334 ; l2 norm of weights:  7.178651284645691\n",
            "Iteration#:  8872 ; Loss:  3.001556250767142 ; l2 norm of gradient:  1.104260343330315 ; l2 norm of weights:  7.17863391159026\n",
            "Iteration#:  8876 ; Loss:  3.001507217952213 ; l2 norm of gradient:  1.10424299263959 ; l2 norm of weights:  7.178616538783447\n",
            "Iteration#:  8880 ; Loss:  3.001458186125909 ; l2 norm of gradient:  1.1042256419664251 ; l2 norm of weights:  7.178599166225248\n",
            "Iteration#:  8884 ; Loss:  3.001409155877098 ; l2 norm of gradient:  1.1042082913107882 ; l2 norm of weights:  7.178581793915654\n",
            "Iteration#:  8888 ; Loss:  3.0013601275576023 ; l2 norm of gradient:  1.104190940672644 ; l2 norm of weights:  7.17856442185466\n",
            "Iteration#:  8892 ; Loss:  3.001311099886447 ; l2 norm of gradient:  1.104173590051959 ; l2 norm of weights:  7.178547050042256\n",
            "Iteration#:  8896 ; Loss:  3.001262074782941 ; l2 norm of gradient:  1.1041562394487001 ; l2 norm of weights:  7.178529678478437\n",
            "Iteration#:  8900 ; Loss:  3.001213050450109 ; l2 norm of gradient:  1.1041388888628332 ; l2 norm of weights:  7.178512307163195\n",
            "Iteration#:  8904 ; Loss:  3.0011640283367362 ; l2 norm of gradient:  1.104121538294325 ; l2 norm of weights:  7.178494936096524\n",
            "Iteration#:  8908 ; Loss:  3.0011150064811245 ; l2 norm of gradient:  1.1041041877431426 ; l2 norm of weights:  7.178477565278413\n",
            "Iteration#:  8912 ; Loss:  3.001065987568219 ; l2 norm of gradient:  1.1040868372092516 ; l2 norm of weights:  7.178460194708859\n",
            "Iteration#:  8916 ; Loss:  3.001016969225142 ; l2 norm of gradient:  1.1040694866926193 ; l2 norm of weights:  7.178442824387853\n",
            "Iteration#:  8920 ; Loss:  3.0009679532334856 ; l2 norm of gradient:  1.1040521361932103 ; l2 norm of weights:  7.178425454315387\n",
            "Iteration#:  8924 ; Loss:  3.0009189381765378 ; l2 norm of gradient:  1.1040347857109951 ; l2 norm of weights:  7.178408084491457\n",
            "Iteration#:  8928 ; Loss:  3.0008699248661963 ; l2 norm of gradient:  1.1040174352459367 ; l2 norm of weights:  7.178390714916052\n",
            "Iteration#:  8932 ; Loss:  3.0008209127325087 ; l2 norm of gradient:  1.1040000847980047 ; l2 norm of weights:  7.178373345589168\n",
            "Iteration#:  8936 ; Loss:  3.00077190300512 ; l2 norm of gradient:  1.1039827343671629 ; l2 norm of weights:  7.178355976510795\n",
            "Iteration#:  8940 ; Loss:  3.000722893860493 ; l2 norm of gradient:  1.103965383953381 ; l2 norm of weights:  7.178338607680929\n",
            "Iteration#:  8944 ; Loss:  3.000673886989336 ; l2 norm of gradient:  1.1039480335566239 ; l2 norm of weights:  7.178321239099559\n",
            "Iteration#:  8948 ; Loss:  3.0006248805881612 ; l2 norm of gradient:  1.103930683176859 ; l2 norm of weights:  7.178303870766681\n",
            "Iteration#:  8952 ; Loss:  3.000575877062486 ; l2 norm of gradient:  1.1039133328140536 ; l2 norm of weights:  7.178286502682287\n",
            "Iteration#:  8956 ; Loss:  3.0005268741136035 ; l2 norm of gradient:  1.1038959824681738 ; l2 norm of weights:  7.178269134846369\n",
            "Iteration#:  8960 ; Loss:  3.000477873579411 ; l2 norm of gradient:  1.1038786321391876 ; l2 norm of weights:  7.178251767258922\n",
            "Iteration#:  8964 ; Loss:  3.0004288735581723 ; l2 norm of gradient:  1.1038612818270621 ; l2 norm of weights:  7.178234399919935\n",
            "Iteration#:  8968 ; Loss:  3.00037987569812 ; l2 norm of gradient:  1.103843931531763 ; l2 norm of weights:  7.178217032829403\n",
            "Iteration#:  8972 ; Loss:  3.0003308788097125 ; l2 norm of gradient:  1.103826581253259 ; l2 norm of weights:  7.178199665987321\n",
            "Iteration#:  8976 ; Loss:  3.0002818845387953 ; l2 norm of gradient:  1.1038092309915162 ; l2 norm of weights:  7.1781822993936775\n",
            "Iteration#:  8980 ; Loss:  3.000232890928979 ; l2 norm of gradient:  1.1037918807465026 ; l2 norm of weights:  7.178164933048468\n",
            "Iteration#:  8984 ; Loss:  3.0001838991846568 ; l2 norm of gradient:  1.103774530518184 ; l2 norm of weights:  7.178147566951685\n",
            "Iteration#:  8988 ; Loss:  3.0001349083584525 ; l2 norm of gradient:  1.1037571803065294 ; l2 norm of weights:  7.178130201103322\n",
            "Iteration#:  8992 ; Loss:  3.0000859200753993 ; l2 norm of gradient:  1.103739830111505 ; l2 norm of weights:  7.178112835503369\n",
            "Iteration#:  8996 ; Loss:  3.0000369325787215 ; l2 norm of gradient:  1.1037224799330778 ; l2 norm of weights:  7.178095470151822\n",
            "Iteration#:  9000 ; Loss:  2.9999879468436537 ; l2 norm of gradient:  1.1037051297712166 ; l2 norm of weights:  7.178078105048673\n",
            "Iteration#:  9004 ; Loss:  2.999938963249898 ; l2 norm of gradient:  1.103687779625889 ; l2 norm of weights:  7.178060740193914\n",
            "Iteration#:  9008 ; Loss:  2.9998899801924557 ; l2 norm of gradient:  1.1036704294970592 ; l2 norm of weights:  7.178043375587537\n",
            "Iteration#:  9012 ; Loss:  2.999840999733597 ; l2 norm of gradient:  1.1036530793846981 ; l2 norm of weights:  7.178026011229537\n",
            "Iteration#:  9016 ; Loss:  2.9997920193368275 ; l2 norm of gradient:  1.1036357292887733 ; l2 norm of weights:  7.178008647119906\n",
            "Iteration#:  9020 ; Loss:  2.999743041745206 ; l2 norm of gradient:  1.1036183792092495 ; l2 norm of weights:  7.177991283258636\n",
            "Iteration#:  9024 ; Loss:  2.999694064936258 ; l2 norm of gradient:  1.1036010291460963 ; l2 norm of weights:  7.177973919645721\n",
            "Iteration#:  9028 ; Loss:  2.9996450899525025 ; l2 norm of gradient:  1.1035836790992815 ; l2 norm of weights:  7.177956556281154\n",
            "Iteration#:  9032 ; Loss:  2.999596116335922 ; l2 norm of gradient:  1.103566329068772 ; l2 norm of weights:  7.177939193164927\n",
            "Iteration#:  9036 ; Loss:  2.99954714487055 ; l2 norm of gradient:  1.1035489790545365 ; l2 norm of weights:  7.177921830297032\n",
            "Iteration#:  9040 ; Loss:  2.99949817394586 ; l2 norm of gradient:  1.1035316290565418 ; l2 norm of weights:  7.177904467677464\n",
            "Iteration#:  9044 ; Loss:  2.999449205391859 ; l2 norm of gradient:  1.1035142790747554 ; l2 norm of weights:  7.177887105306214\n",
            "Iteration#:  9048 ; Loss:  2.9994002379191964 ; l2 norm of gradient:  1.103496929109147 ; l2 norm of weights:  7.177869743183275\n",
            "Iteration#:  9052 ; Loss:  2.9993512725006894 ; l2 norm of gradient:  1.1034795791596819 ; l2 norm of weights:  7.17785238130864\n",
            "Iteration#:  9056 ; Loss:  2.9993023076937306 ; l2 norm of gradient:  1.1034622292263292 ; l2 norm of weights:  7.177835019682303\n",
            "Iteration#:  9060 ; Loss:  2.999253345550485 ; l2 norm of gradient:  1.1034448793090585 ; l2 norm of weights:  7.1778176583042566\n",
            "Iteration#:  9064 ; Loss:  2.9992043839812 ; l2 norm of gradient:  1.1034275294078348 ; l2 norm of weights:  7.177800297174491\n",
            "Iteration#:  9068 ; Loss:  2.9991554244483725 ; l2 norm of gradient:  1.1034101795226292 ; l2 norm of weights:  7.177782936293002\n",
            "Iteration#:  9072 ; Loss:  2.9991064662898053 ; l2 norm of gradient:  1.1033928296534068 ; l2 norm of weights:  7.17776557565978\n",
            "Iteration#:  9076 ; Loss:  2.999057509925166 ; l2 norm of gradient:  1.103375479800137 ; l2 norm of weights:  7.17774821527482\n",
            "Iteration#:  9080 ; Loss:  2.9990085542980864 ; l2 norm of gradient:  1.103358129962789 ; l2 norm of weights:  7.1777308551381145\n",
            "Iteration#:  9084 ; Loss:  2.998959601348551 ; l2 norm of gradient:  1.1033407801413295 ; l2 norm of weights:  7.177713495249654\n",
            "Iteration#:  9088 ; Loss:  2.9989106487903374 ; l2 norm of gradient:  1.1033234303357267 ; l2 norm of weights:  7.177696135609436\n",
            "Iteration#:  9092 ; Loss:  2.9988616987515884 ; l2 norm of gradient:  1.103306080545951 ; l2 norm of weights:  7.177678776217449\n",
            "Iteration#:  9096 ; Loss:  2.9988127496498627 ; l2 norm of gradient:  1.1032887307719679 ; l2 norm of weights:  7.177661417073686\n",
            "Iteration#:  9100 ; Loss:  2.998763802283054 ; l2 norm of gradient:  1.1032713810137469 ; l2 norm of weights:  7.177644058178143\n",
            "Iteration#:  9104 ; Loss:  2.998714856311908 ; l2 norm of gradient:  1.1032540312712569 ; l2 norm of weights:  7.17762669953081\n",
            "Iteration#:  9108 ; Loss:  2.998665912538973 ; l2 norm of gradient:  1.1032366815444659 ; l2 norm of weights:  7.17760934113168\n",
            "Iteration#:  9112 ; Loss:  2.998616968959179 ; l2 norm of gradient:  1.1032193318333416 ; l2 norm of weights:  7.177591982980748\n",
            "Iteration#:  9116 ; Loss:  2.9985680283072838 ; l2 norm of gradient:  1.103201982137854 ; l2 norm of weights:  7.177574625078004\n",
            "Iteration#:  9120 ; Loss:  2.9985190878686496 ; l2 norm of gradient:  1.1031846324579706 ; l2 norm of weights:  7.177557267423443\n",
            "Iteration#:  9124 ; Loss:  2.9984701503566704 ; l2 norm of gradient:  1.1031672827936612 ; l2 norm of weights:  7.177539910017057\n",
            "Iteration#:  9128 ; Loss:  2.998421213352932 ; l2 norm of gradient:  1.103149933144891 ; l2 norm of weights:  7.1775225528588384\n",
            "Iteration#:  9132 ; Loss:  2.9983722784544344 ; l2 norm of gradient:  1.1031325835116332 ; l2 norm of weights:  7.177505195948781\n",
            "Iteration#:  9136 ; Loss:  2.9983233447905 ; l2 norm of gradient:  1.1031152338938526 ; l2 norm of weights:  7.177487839286876\n",
            "Iteration#:  9140 ; Loss:  2.9982744126742595 ; l2 norm of gradient:  1.103097884291521 ; l2 norm of weights:  7.177470482873118\n",
            "Iteration#:  9144 ; Loss:  2.9982254820628045 ; l2 norm of gradient:  1.103080534704605 ; l2 norm of weights:  7.177453126707499\n",
            "Iteration#:  9148 ; Loss:  2.9981765534702443 ; l2 norm of gradient:  1.1030631851330748 ; l2 norm of weights:  7.1774357707900105\n",
            "Iteration#:  9152 ; Loss:  2.998127625541645 ; l2 norm of gradient:  1.1030458355768988 ; l2 norm of weights:  7.1774184151206475\n",
            "Iteration#:  9156 ; Loss:  2.9980787001353657 ; l2 norm of gradient:  1.1030284860360462 ; l2 norm of weights:  7.1774010596994025\n",
            "Iteration#:  9160 ; Loss:  2.9980297750727094 ; l2 norm of gradient:  1.1030111365104847 ; l2 norm of weights:  7.177383704526267\n",
            "Iteration#:  9164 ; Loss:  2.997980851958077 ; l2 norm of gradient:  1.1029937870001845 ; l2 norm of weights:  7.177366349601236\n",
            "Iteration#:  9168 ; Loss:  2.9979319307951577 ; l2 norm of gradient:  1.1029764375051136 ; l2 norm of weights:  7.1773489949243\n",
            "Iteration#:  9172 ; Loss:  2.9978830108712384 ; l2 norm of gradient:  1.1029590880252418 ; l2 norm of weights:  7.177331640495452\n",
            "Iteration#:  9176 ; Loss:  2.9978340928360723 ; l2 norm of gradient:  1.1029417385605387 ; l2 norm of weights:  7.1773142863146875\n",
            "Iteration#:  9180 ; Loss:  2.9977851757711065 ; l2 norm of gradient:  1.102924389110971 ; l2 norm of weights:  7.177296932381997\n",
            "Iteration#:  9184 ; Loss:  2.9977362611121228 ; l2 norm of gradient:  1.1029070396765106 ; l2 norm of weights:  7.177279578697373\n",
            "Iteration#:  9188 ; Loss:  2.997687346995843 ; l2 norm of gradient:  1.1028896902571252 ; l2 norm of weights:  7.177262225260809\n",
            "Iteration#:  9192 ; Loss:  2.9976384354028798 ; l2 norm of gradient:  1.1028723408527854 ; l2 norm of weights:  7.1772448720723\n",
            "Iteration#:  9196 ; Loss:  2.9975895242788155 ; l2 norm of gradient:  1.102854991463459 ; l2 norm of weights:  7.177227519131834\n",
            "Iteration#:  9200 ; Loss:  2.9975406158192803 ; l2 norm of gradient:  1.102837642089116 ; l2 norm of weights:  7.177210166439407\n",
            "Iteration#:  9204 ; Loss:  2.9974917075459064 ; l2 norm of gradient:  1.1028202927297248 ; l2 norm of weights:  7.177192813995012\n",
            "Iteration#:  9208 ; Loss:  2.9974428022103763 ; l2 norm of gradient:  1.1028029433852564 ; l2 norm of weights:  7.177175461798639\n",
            "Iteration#:  9212 ; Loss:  2.9973938970691116 ; l2 norm of gradient:  1.1027855940556792 ; l2 norm of weights:  7.177158109850286\n",
            "Iteration#:  9216 ; Loss:  2.9973449946675115 ; l2 norm of gradient:  1.102768244740962 ; l2 norm of weights:  7.177140758149942\n",
            "Iteration#:  9220 ; Loss:  2.997296092652252 ; l2 norm of gradient:  1.1027508954410754 ; l2 norm of weights:  7.177123406697599\n",
            "Iteration#:  9224 ; Loss:  2.9972471935659475 ; l2 norm of gradient:  1.102733546155989 ; l2 norm of weights:  7.177106055493252\n",
            "Iteration#:  9228 ; Loss:  2.997198294698638 ; l2 norm of gradient:  1.1027161968856727 ; l2 norm of weights:  7.177088704536893\n",
            "Iteration#:  9232 ; Loss:  2.997149398328542 ; l2 norm of gradient:  1.1026988476300943 ; l2 norm of weights:  7.177071353828516\n",
            "Iteration#:  9236 ; Loss:  2.997100502579542 ; l2 norm of gradient:  1.1026814983892255 ; l2 norm of weights:  7.177054003368111\n",
            "Iteration#:  9240 ; Loss:  2.9970516092842288 ; l2 norm of gradient:  1.1026641491630353 ; l2 norm of weights:  7.177036653155674\n",
            "Iteration#:  9244 ; Loss:  2.997002716662317 ; l2 norm of gradient:  1.1026467999514926 ; l2 norm of weights:  7.177019303191195\n",
            "Iteration#:  9248 ; Loss:  2.9969538261125908 ; l2 norm of gradient:  1.102629450754568 ; l2 norm of weights:  7.1770019534746705\n",
            "Iteration#:  9252 ; Loss:  2.996904936796405 ; l2 norm of gradient:  1.1026121015722308 ; l2 norm of weights:  7.17698460400609\n",
            "Iteration#:  9256 ; Loss:  2.996856049231572 ; l2 norm of gradient:  1.1025947524044517 ; l2 norm of weights:  7.176967254785446\n",
            "Iteration#:  9260 ; Loss:  2.9968071630486994 ; l2 norm of gradient:  1.1025774032512006 ; l2 norm of weights:  7.1769499058127355\n",
            "Iteration#:  9264 ; Loss:  2.9967582787499305 ; l2 norm of gradient:  1.102560054112446 ; l2 norm of weights:  7.176932557087946\n",
            "Iteration#:  9268 ; Loss:  2.99670939568784 ; l2 norm of gradient:  1.1025427049881589 ; l2 norm of weights:  7.176915208611074\n",
            "Iteration#:  9272 ; Loss:  2.996660514325834 ; l2 norm of gradient:  1.1025253558783106 ; l2 norm of weights:  7.176897860382111\n",
            "Iteration#:  9276 ; Loss:  2.996611634113567 ; l2 norm of gradient:  1.1025080067828679 ; l2 norm of weights:  7.17688051240105\n",
            "Iteration#:  9280 ; Loss:  2.9965627560068464 ; l2 norm of gradient:  1.1024906577018034 ; l2 norm of weights:  7.176863164667884\n",
            "Iteration#:  9284 ; Loss:  2.9965138785434275 ; l2 norm of gradient:  1.102473308635087 ; l2 norm of weights:  7.176845817182605\n",
            "Iteration#:  9288 ; Loss:  2.996465003043226 ; l2 norm of gradient:  1.1024559595826875 ; l2 norm of weights:  7.176828469945206\n",
            "Iteration#:  9292 ; Loss:  2.9964161294879794 ; l2 norm of gradient:  1.1024386105445767 ; l2 norm of weights:  7.17681112295568\n",
            "Iteration#:  9296 ; Loss:  2.9963672569724347 ; l2 norm of gradient:  1.1024212615207245 ; l2 norm of weights:  7.176793776214021\n",
            "Iteration#:  9300 ; Loss:  2.9963183863430873 ; l2 norm of gradient:  1.1024039125111007 ; l2 norm of weights:  7.1767764297202215\n",
            "Iteration#:  9304 ; Loss:  2.9962695168901607 ; l2 norm of gradient:  1.1023865635156742 ; l2 norm of weights:  7.1767590834742725\n",
            "Iteration#:  9308 ; Loss:  2.996220649448191 ; l2 norm of gradient:  1.102369214534419 ; l2 norm of weights:  7.176741737476167\n",
            "Iteration#:  9312 ; Loss:  2.9961717828733687 ; l2 norm of gradient:  1.1023518655673028 ; l2 norm of weights:  7.1767243917259\n",
            "Iteration#:  9316 ; Loss:  2.996122918883713 ; l2 norm of gradient:  1.102334516614297 ; l2 norm of weights:  7.176707046223464\n",
            "Iteration#:  9320 ; Loss:  2.996074055297996 ; l2 norm of gradient:  1.1023171676753711 ; l2 norm of weights:  7.176689700968848\n",
            "Iteration#:  9324 ; Loss:  2.9960251942300147 ; l2 norm of gradient:  1.102299818750496 ; l2 norm of weights:  7.176672355962049\n",
            "Iteration#:  9328 ; Loss:  2.9959763337761474 ; l2 norm of gradient:  1.102282469839644 ; l2 norm of weights:  7.17665501120306\n",
            "Iteration#:  9332 ; Loss:  2.9959274753134313 ; l2 norm of gradient:  1.1022651209427836 ; l2 norm of weights:  7.176637666691871\n",
            "Iteration#:  9336 ; Loss:  2.995878618299756 ; l2 norm of gradient:  1.1022477720598851 ; l2 norm of weights:  7.176620322428476\n",
            "Iteration#:  9340 ; Loss:  2.995829763109936 ; l2 norm of gradient:  1.1022304231909208 ; l2 norm of weights:  7.176602978412868\n",
            "Iteration#:  9344 ; Loss:  2.9957809086961 ; l2 norm of gradient:  1.1022130743358607 ; l2 norm of weights:  7.176585634645039\n",
            "Iteration#:  9348 ; Loss:  2.9957320568726993 ; l2 norm of gradient:  1.1021957254946755 ; l2 norm of weights:  7.176568291124984\n",
            "Iteration#:  9352 ; Loss:  2.9956832055325004 ; l2 norm of gradient:  1.1021783766673359 ; l2 norm of weights:  7.176550947852694\n",
            "Iteration#:  9356 ; Loss:  2.9956343565901533 ; l2 norm of gradient:  1.1021610278538139 ; l2 norm of weights:  7.176533604828162\n",
            "Iteration#:  9360 ; Loss:  2.995585508551999 ; l2 norm of gradient:  1.1021436790540793 ; l2 norm of weights:  7.176516262051381\n",
            "Iteration#:  9364 ; Loss:  2.995536661899926 ; l2 norm of gradient:  1.102126330268102 ; l2 norm of weights:  7.176498919522343\n",
            "Iteration#:  9368 ; Loss:  2.995487817506197 ; l2 norm of gradient:  1.102108981495855 ; l2 norm of weights:  7.176481577241042\n",
            "Iteration#:  9372 ; Loss:  2.9954389740146072 ; l2 norm of gradient:  1.102091632737307 ; l2 norm of weights:  7.176464235207471\n",
            "Iteration#:  9376 ; Loss:  2.9953901327635073 ; l2 norm of gradient:  1.1020742839924313 ; l2 norm of weights:  7.176446893421622\n",
            "Iteration#:  9380 ; Loss:  2.9953412920853895 ; l2 norm of gradient:  1.1020569352611984 ; l2 norm of weights:  7.176429551883488\n",
            "Iteration#:  9384 ; Loss:  2.995292453994029 ; l2 norm of gradient:  1.1020395865435777 ; l2 norm of weights:  7.176412210593061\n",
            "Iteration#:  9388 ; Loss:  2.9952436165779903 ; l2 norm of gradient:  1.102022237839543 ; l2 norm of weights:  7.176394869550336\n",
            "Iteration#:  9392 ; Loss:  2.9951947812404622 ; l2 norm of gradient:  1.1020048891490637 ; l2 norm of weights:  7.176377528755304\n",
            "Iteration#:  9396 ; Loss:  2.9951459471320483 ; l2 norm of gradient:  1.1019875404721111 ; l2 norm of weights:  7.1763601882079575\n",
            "Iteration#:  9400 ; Loss:  2.9950971149213768 ; l2 norm of gradient:  1.1019701918086577 ; l2 norm of weights:  7.176342847908291\n",
            "Iteration#:  9404 ; Loss:  2.9950482834855374 ; l2 norm of gradient:  1.101952843158673 ; l2 norm of weights:  7.176325507856296\n",
            "Iteration#:  9408 ; Loss:  2.994999454784648 ; l2 norm of gradient:  1.1019354945221298 ; l2 norm of weights:  7.176308168051965\n",
            "Iteration#:  9412 ; Loss:  2.994950626276422 ; l2 norm of gradient:  1.1019181458989986 ; l2 norm of weights:  7.176290828495293\n",
            "Iteration#:  9416 ; Loss:  2.9949018003080603 ; l2 norm of gradient:  1.101900797289252 ; l2 norm of weights:  7.17627348918627\n",
            "Iteration#:  9420 ; Loss:  2.994852974935596 ; l2 norm of gradient:  1.1018834486928595 ; l2 norm of weights:  7.176256150124891\n",
            "Iteration#:  9424 ; Loss:  2.9948041517656336 ; l2 norm of gradient:  1.1018661001097954 ; l2 norm of weights:  7.176238811311147\n",
            "Iteration#:  9428 ; Loss:  2.9947553301454124 ; l2 norm of gradient:  1.1018487515400277 ; l2 norm of weights:  7.176221472745032\n",
            "Iteration#:  9432 ; Loss:  2.9947065096007317 ; l2 norm of gradient:  1.1018314029835303 ; l2 norm of weights:  7.1762041344265395\n",
            "Iteration#:  9436 ; Loss:  2.9946576914905405 ; l2 norm of gradient:  1.1018140544402746 ; l2 norm of weights:  7.176186796355661\n",
            "Iteration#:  9440 ; Loss:  2.9946088738668752 ; l2 norm of gradient:  1.101796705910233 ; l2 norm of weights:  7.176169458532389\n",
            "Iteration#:  9444 ; Loss:  2.9945600584964414 ; l2 norm of gradient:  1.1017793573933747 ; l2 norm of weights:  7.176152120956716\n",
            "Iteration#:  9448 ; Loss:  2.994511243992755 ; l2 norm of gradient:  1.1017620088896736 ; l2 norm of weights:  7.176134783628638\n",
            "Iteration#:  9452 ; Loss:  2.994462432077848 ; l2 norm of gradient:  1.1017446603991008 ; l2 norm of weights:  7.176117446548145\n",
            "Iteration#:  9456 ; Loss:  2.9944136204189973 ; l2 norm of gradient:  1.1017273119216275 ; l2 norm of weights:  7.17610010971523\n",
            "Iteration#:  9460 ; Loss:  2.9943648113799197 ; l2 norm of gradient:  1.1017099634572272 ; l2 norm of weights:  7.176082773129885\n",
            "Iteration#:  9464 ; Loss:  2.994316002923833 ; l2 norm of gradient:  1.1016926150058695 ; l2 norm of weights:  7.176065436792104\n",
            "Iteration#:  9468 ; Loss:  2.9942671958916893 ; l2 norm of gradient:  1.1016752665675282 ; l2 norm of weights:  7.176048100701882\n",
            "Iteration#:  9472 ; Loss:  2.994218391783888 ; l2 norm of gradient:  1.1016579181421753 ; l2 norm of weights:  7.176030764859207\n",
            "Iteration#:  9476 ; Loss:  2.9941695878863834 ; l2 norm of gradient:  1.101640569729781 ; l2 norm of weights:  7.176013429264077\n",
            "Iteration#:  9480 ; Loss:  2.99412078631145 ; l2 norm of gradient:  1.1016232213303196 ; l2 norm of weights:  7.1759960939164795\n",
            "Iteration#:  9484 ; Loss:  2.994071985475261 ; l2 norm of gradient:  1.1016058729437614 ; l2 norm of weights:  7.175978758816412\n",
            "Iteration#:  9488 ; Loss:  2.9940231874083865 ; l2 norm of gradient:  1.10158852457008 ; l2 norm of weights:  7.175961423963865\n",
            "Iteration#:  9492 ; Loss:  2.9939743896899076 ; l2 norm of gradient:  1.1015711762092457 ; l2 norm of weights:  7.175944089358832\n",
            "Iteration#:  9496 ; Loss:  2.9939255943175223 ; l2 norm of gradient:  1.101553827861233 ; l2 norm of weights:  7.175926755001305\n",
            "Iteration#:  9500 ; Loss:  2.9938767997850677 ; l2 norm of gradient:  1.1015364795260127 ; l2 norm of weights:  7.175909420891277\n",
            "Iteration#:  9504 ; Loss:  2.9938280073881076 ; l2 norm of gradient:  1.1015191312035568 ; l2 norm of weights:  7.175892087028741\n",
            "Iteration#:  9508 ; Loss:  2.9937792156565655 ; l2 norm of gradient:  1.1015017828938383 ; l2 norm of weights:  7.175874753413691\n",
            "Iteration#:  9512 ; Loss:  2.9937304261956545 ; l2 norm of gradient:  1.10148443459683 ; l2 norm of weights:  7.175857420046117\n",
            "Iteration#:  9516 ; Loss:  2.993681638284412 ; l2 norm of gradient:  1.1014670863125027 ; l2 norm of weights:  7.175840086926015\n",
            "Iteration#:  9520 ; Loss:  2.9936328514438144 ; l2 norm of gradient:  1.1014497380408317 ; l2 norm of weights:  7.175822754053374\n",
            "Iteration#:  9524 ; Loss:  2.993584066653828 ; l2 norm of gradient:  1.1014323897817861 ; l2 norm of weights:  7.17580542142819\n",
            "Iteration#:  9528 ; Loss:  2.993535282798245 ; l2 norm of gradient:  1.1014150415353403 ; l2 norm of weights:  7.175788089050456\n",
            "Iteration#:  9532 ; Loss:  2.9934865014554655 ; l2 norm of gradient:  1.1013976933014669 ; l2 norm of weights:  7.175770756920164\n",
            "Iteration#:  9536 ; Loss:  2.9934377205849803 ; l2 norm of gradient:  1.1013803450801376 ; l2 norm of weights:  7.175753425037305\n",
            "Iteration#:  9540 ; Loss:  2.9933889421219724 ; l2 norm of gradient:  1.101362996871326 ; l2 norm of weights:  7.175736093401874\n",
            "Iteration#:  9544 ; Loss:  2.9933401643714976 ; l2 norm of gradient:  1.1013456486750042 ; l2 norm of weights:  7.1757187620138625\n",
            "Iteration#:  9548 ; Loss:  2.993291388897224 ; l2 norm of gradient:  1.1013283004911458 ; l2 norm of weights:  7.175701430873263\n",
            "Iteration#:  9552 ; Loss:  2.9932426142126185 ; l2 norm of gradient:  1.1013109523197218 ; l2 norm of weights:  7.175684099980072\n",
            "Iteration#:  9556 ; Loss:  2.993193840882347 ; l2 norm of gradient:  1.1012936041607073 ; l2 norm of weights:  7.175666769334279\n",
            "Iteration#:  9560 ; Loss:  2.9931450700956357 ; l2 norm of gradient:  1.1012762560140728 ; l2 norm of weights:  7.175649438935875\n",
            "Iteration#:  9564 ; Loss:  2.9930963002687414 ; l2 norm of gradient:  1.1012589078797927 ; l2 norm of weights:  7.175632108784857\n",
            "Iteration#:  9568 ; Loss:  2.9930475323984274 ; l2 norm of gradient:  1.1012415597578395 ; l2 norm of weights:  7.175614778881216\n",
            "Iteration#:  9572 ; Loss:  2.9929987654654346 ; l2 norm of gradient:  1.1012242116481865 ; l2 norm of weights:  7.175597449224943\n",
            "Iteration#:  9576 ; Loss:  2.992950000721912 ; l2 norm of gradient:  1.1012068635508054 ; l2 norm of weights:  7.175580119816034\n",
            "Iteration#:  9580 ; Loss:  2.9929012365659244 ; l2 norm of gradient:  1.1011895154656715 ; l2 norm of weights:  7.17556279065448\n",
            "Iteration#:  9584 ; Loss:  2.9928524749486005 ; l2 norm of gradient:  1.101172167392756 ; l2 norm of weights:  7.175545461740274\n",
            "Iteration#:  9588 ; Loss:  2.9928037140616266 ; l2 norm of gradient:  1.101154819332032 ; l2 norm of weights:  7.17552813307341\n",
            "Iteration#:  9592 ; Loss:  2.9927549545858767 ; l2 norm of gradient:  1.101137471283474 ; l2 norm of weights:  7.175510804653877\n",
            "Iteration#:  9596 ; Loss:  2.992706197445957 ; l2 norm of gradient:  1.1011201232470544 ; l2 norm of weights:  7.175493476481673\n",
            "Iteration#:  9600 ; Loss:  2.992657441485871 ; l2 norm of gradient:  1.1011027752227462 ; l2 norm of weights:  7.175476148556789\n",
            "Iteration#:  9604 ; Loss:  2.99260868734315 ; l2 norm of gradient:  1.1010854272105235 ; l2 norm of weights:  7.175458820879215\n",
            "Iteration#:  9608 ; Loss:  2.9925599341246167 ; l2 norm of gradient:  1.1010680792103584 ; l2 norm of weights:  7.175441493448946\n",
            "Iteration#:  9612 ; Loss:  2.9925111832326583 ; l2 norm of gradient:  1.1010507312222257 ; l2 norm of weights:  7.175424166265976\n",
            "Iteration#:  9616 ; Loss:  2.9924624330798055 ; l2 norm of gradient:  1.1010333832460981 ; l2 norm of weights:  7.175406839330296\n",
            "Iteration#:  9620 ; Loss:  2.992413685104725 ; l2 norm of gradient:  1.1010160352819487 ; l2 norm of weights:  7.1753895126419005\n",
            "Iteration#:  9624 ; Loss:  2.9923649380029684 ; l2 norm of gradient:  1.1009986873297508 ; l2 norm of weights:  7.175372186200779\n",
            "Iteration#:  9628 ; Loss:  2.992316192396228 ; l2 norm of gradient:  1.1009813393894796 ; l2 norm of weights:  7.175354860006928\n",
            "Iteration#:  9632 ; Loss:  2.99226744910431 ; l2 norm of gradient:  1.1009639914611062 ; l2 norm of weights:  7.175337534060339\n",
            "Iteration#:  9636 ; Loss:  2.992218706552403 ; l2 norm of gradient:  1.1009466435446056 ; l2 norm of weights:  7.175320208361003\n",
            "Iteration#:  9640 ; Loss:  2.9921699661273564 ; l2 norm of gradient:  1.1009292956399515 ; l2 norm of weights:  7.175302882908914\n",
            "Iteration#:  9644 ; Loss:  2.9921212267017245 ; l2 norm of gradient:  1.1009119477471165 ; l2 norm of weights:  7.1752855577040675\n",
            "Iteration#:  9648 ; Loss:  2.9920724896013757 ; l2 norm of gradient:  1.1008945998660757 ; l2 norm of weights:  7.175268232746453\n",
            "Iteration#:  9652 ; Loss:  2.9920237530820075 ; l2 norm of gradient:  1.1008772519968029 ; l2 norm of weights:  7.175250908036063\n",
            "Iteration#:  9656 ; Loss:  2.9919750179999345 ; l2 norm of gradient:  1.1008599041392713 ; l2 norm of weights:  7.1752335835728935\n",
            "Iteration#:  9660 ; Loss:  2.9919262855256963 ; l2 norm of gradient:  1.100842556293454 ; l2 norm of weights:  7.175216259356934\n",
            "Iteration#:  9664 ; Loss:  2.991877553699187 ; l2 norm of gradient:  1.1008252084593253 ; l2 norm of weights:  7.175198935388179\n",
            "Iteration#:  9668 ; Loss:  2.9918288242068902 ; l2 norm of gradient:  1.1008078606368603 ; l2 norm of weights:  7.175181611666622\n",
            "Iteration#:  9672 ; Loss:  2.991780095298833 ; l2 norm of gradient:  1.1007905128260322 ; l2 norm of weights:  7.175164288192254\n",
            "Iteration#:  9676 ; Loss:  2.991731368929355 ; l2 norm of gradient:  1.1007731650268142 ; l2 norm of weights:  7.175146964965067\n",
            "Iteration#:  9680 ; Loss:  2.9916826431466648 ; l2 norm of gradient:  1.1007558172391823 ; l2 norm of weights:  7.175129641985057\n",
            "Iteration#:  9684 ; Loss:  2.9916339190677346 ; l2 norm of gradient:  1.100738469463107 ; l2 norm of weights:  7.175112319252216\n",
            "Iteration#:  9688 ; Loss:  2.991585197245559 ; l2 norm of gradient:  1.1007211216985657 ; l2 norm of weights:  7.175094996766536\n",
            "Iteration#:  9692 ; Loss:  2.9915364759453142 ; l2 norm of gradient:  1.1007037739455328 ; l2 norm of weights:  7.175077674528007\n",
            "Iteration#:  9696 ; Loss:  2.9914877573254315 ; l2 norm of gradient:  1.1006864262039802 ; l2 norm of weights:  7.175060352536626\n",
            "Iteration#:  9700 ; Loss:  2.9914390391445718 ; l2 norm of gradient:  1.100669078473883 ; l2 norm of weights:  7.175043030792386\n",
            "Iteration#:  9704 ; Loss:  2.991390323299124 ; l2 norm of gradient:  1.1006517307552162 ; l2 norm of weights:  7.1750257092952765\n",
            "Iteration#:  9708 ; Loss:  2.991341608381846 ; l2 norm of gradient:  1.1006343830479526 ; l2 norm of weights:  7.175008388045291\n",
            "Iteration#:  9712 ; Loss:  2.9912928948901345 ; l2 norm of gradient:  1.1006170353520688 ; l2 norm of weights:  7.174991067042425\n",
            "Iteration#:  9716 ; Loss:  2.9912441837291417 ; l2 norm of gradient:  1.100599687667537 ; l2 norm of weights:  7.174973746286671\n",
            "Iteration#:  9720 ; Loss:  2.991195473361949 ; l2 norm of gradient:  1.1005823399943324 ; l2 norm of weights:  7.174956425778017\n",
            "Iteration#:  9724 ; Loss:  2.9911467652635753 ; l2 norm of gradient:  1.1005649923324297 ; l2 norm of weights:  7.1749391055164615\n",
            "Iteration#:  9728 ; Loss:  2.9910980579485043 ; l2 norm of gradient:  1.100547644681804 ; l2 norm of weights:  7.1749217855019936\n",
            "Iteration#:  9732 ; Loss:  2.99104935277094 ; l2 norm of gradient:  1.1005302970424282 ; l2 norm of weights:  7.174904465734609\n",
            "Iteration#:  9736 ; Loss:  2.9910006486402874 ; l2 norm of gradient:  1.1005129494142787 ; l2 norm of weights:  7.174887146214297\n",
            "Iteration#:  9740 ; Loss:  2.9909519459533485 ; l2 norm of gradient:  1.1004956017973293 ; l2 norm of weights:  7.174869826941053\n",
            "Iteration#:  9744 ; Loss:  2.9909032453833357 ; l2 norm of gradient:  1.1004782541915539 ; l2 norm of weights:  7.174852507914871\n",
            "Iteration#:  9748 ; Loss:  2.9908545456784976 ; l2 norm of gradient:  1.1004609065969275 ; l2 norm of weights:  7.174835189135739\n",
            "Iteration#:  9752 ; Loss:  2.9908058480602495 ; l2 norm of gradient:  1.1004435590134267 ; l2 norm of weights:  7.174817870603654\n",
            "Iteration#:  9756 ; Loss:  2.9907571513353823 ; l2 norm of gradient:  1.100426211441025 ; l2 norm of weights:  7.174800552318607\n",
            "Iteration#:  9760 ; Loss:  2.9907084570813938 ; l2 norm of gradient:  1.1004088638796967 ; l2 norm of weights:  7.174783234280593\n",
            "Iteration#:  9764 ; Loss:  2.99065976341675 ; l2 norm of gradient:  1.100391516329417 ; l2 norm of weights:  7.174765916489602\n",
            "Iteration#:  9768 ; Loss:  2.990611071513276 ; l2 norm of gradient:  1.100374168790161 ; l2 norm of weights:  7.174748598945628\n",
            "Iteration#:  9772 ; Loss:  2.9905623817357565 ; l2 norm of gradient:  1.100356821261904 ; l2 norm of weights:  7.174731281648663\n",
            "Iteration#:  9776 ; Loss:  2.99051369282009 ; l2 norm of gradient:  1.1003394737446197 ; l2 norm of weights:  7.174713964598702\n",
            "Iteration#:  9780 ; Loss:  2.9904650059029145 ; l2 norm of gradient:  1.1003221262382845 ; l2 norm of weights:  7.174696647795735\n",
            "Iteration#:  9784 ; Loss:  2.9904163202528085 ; l2 norm of gradient:  1.1003047787428737 ; l2 norm of weights:  7.174679331239758\n",
            "Iteration#:  9788 ; Loss:  2.9903676365137524 ; l2 norm of gradient:  1.1002874312583604 ; l2 norm of weights:  7.17466201493076\n",
            "Iteration#:  9792 ; Loss:  2.9903189533993406 ; l2 norm of gradient:  1.100270083784722 ; l2 norm of weights:  7.174644698868736\n",
            "Iteration#:  9796 ; Loss:  2.9902702720241208 ; l2 norm of gradient:  1.1002527363219328 ; l2 norm of weights:  7.1746273830536795\n",
            "Iteration#:  9800 ; Loss:  2.990221592973422 ; l2 norm of gradient:  1.1002353888699674 ; l2 norm of weights:  7.174610067485581\n",
            "Iteration#:  9804 ; Loss:  2.9901729148562217 ; l2 norm of gradient:  1.1002180414288014 ; l2 norm of weights:  7.174592752164436\n",
            "Iteration#:  9808 ; Loss:  2.990124238928143 ; l2 norm of gradient:  1.1002006939984108 ; l2 norm of weights:  7.174575437090237\n",
            "Iteration#:  9812 ; Loss:  2.9900755637909446 ; l2 norm of gradient:  1.1001833465787714 ; l2 norm of weights:  7.1745581222629715\n",
            "Iteration#:  9816 ; Loss:  2.990026890011729 ; l2 norm of gradient:  1.1001659991698567 ; l2 norm of weights:  7.174540807682639\n",
            "Iteration#:  9820 ; Loss:  2.9899782181208474 ; l2 norm of gradient:  1.1001486517716434 ; l2 norm of weights:  7.17452349334923\n",
            "Iteration#:  9824 ; Loss:  2.9899295476714394 ; l2 norm of gradient:  1.100131304384107 ; l2 norm of weights:  7.174506179262736\n",
            "Iteration#:  9828 ; Loss:  2.9898808793550637 ; l2 norm of gradient:  1.1001139570072227 ; l2 norm of weights:  7.174488865423151\n",
            "Iteration#:  9832 ; Loss:  2.989832211742144 ; l2 norm of gradient:  1.100096609640966 ; l2 norm of weights:  7.174471551830469\n",
            "Iteration#:  9836 ; Loss:  2.9897835466241336 ; l2 norm of gradient:  1.100079262285312 ; l2 norm of weights:  7.174454238484681\n",
            "Iteration#:  9840 ; Loss:  2.989734882072974 ; l2 norm of gradient:  1.1000619149402378 ; l2 norm of weights:  7.17443692538578\n",
            "Iteration#:  9844 ; Loss:  2.989686218765168 ; l2 norm of gradient:  1.1000445676057178 ; l2 norm of weights:  7.174419612533759\n",
            "Iteration#:  9848 ; Loss:  2.98963755798448 ; l2 norm of gradient:  1.1000272202817287 ; l2 norm of weights:  7.174402299928611\n",
            "Iteration#:  9852 ; Loss:  2.9895888982518666 ; l2 norm of gradient:  1.1000098729682448 ; l2 norm of weights:  7.1743849875703285\n",
            "Iteration#:  9856 ; Loss:  2.989540240729613 ; l2 norm of gradient:  1.0999925256652434 ; l2 norm of weights:  7.174367675458904\n",
            "Iteration#:  9860 ; Loss:  2.989491583720162 ; l2 norm of gradient:  1.0999751783726996 ; l2 norm of weights:  7.174350363594331\n",
            "Iteration#:  9864 ; Loss:  2.9894429280829717 ; l2 norm of gradient:  1.0999578310905889 ; l2 norm of weights:  7.174333051976602\n",
            "Iteration#:  9868 ; Loss:  2.9893942750987508 ; l2 norm of gradient:  1.0999404838188878 ; l2 norm of weights:  7.17431574060571\n",
            "Iteration#:  9872 ; Loss:  2.9893456229022752 ; l2 norm of gradient:  1.099923136557573 ; l2 norm of weights:  7.174298429481649\n",
            "Iteration#:  9876 ; Loss:  2.989296972835303 ; l2 norm of gradient:  1.09990578930662 ; l2 norm of weights:  7.174281118604408\n",
            "Iteration#:  9880 ; Loss:  2.9892483237690035 ; l2 norm of gradient:  1.099888442066003 ; l2 norm of weights:  7.1742638079739836\n",
            "Iteration#:  9884 ; Loss:  2.989199675666156 ; l2 norm of gradient:  1.0998710948357007 ; l2 norm of weights:  7.1742464975903655\n",
            "Iteration#:  9888 ; Loss:  2.9891510302871676 ; l2 norm of gradient:  1.099853747615688 ; l2 norm of weights:  7.17422918745355\n",
            "Iteration#:  9892 ; Loss:  2.989102385889703 ; l2 norm of gradient:  1.0998364004059402 ; l2 norm of weights:  7.174211877563527\n",
            "Iteration#:  9896 ; Loss:  2.9890537435712994 ; l2 norm of gradient:  1.0998190532064358 ; l2 norm of weights:  7.174194567920292\n",
            "Iteration#:  9900 ; Loss:  2.9890051015758363 ; l2 norm of gradient:  1.099801706017149 ; l2 norm of weights:  7.1741772585238355\n",
            "Iteration#:  9904 ; Loss:  2.9889564624266187 ; l2 norm of gradient:  1.0997843588380565 ; l2 norm of weights:  7.17415994937415\n",
            "Iteration#:  9908 ; Loss:  2.9889078240093454 ; l2 norm of gradient:  1.0997670116691356 ; l2 norm of weights:  7.17414264047123\n",
            "Iteration#:  9912 ; Loss:  2.988859186875389 ; l2 norm of gradient:  1.0997496645103615 ; l2 norm of weights:  7.174125331815067\n",
            "Iteration#:  9916 ; Loss:  2.9888105519784047 ; l2 norm of gradient:  1.0997323173617113 ; l2 norm of weights:  7.174108023405655\n",
            "Iteration#:  9920 ; Loss:  2.9887619181058755 ; l2 norm of gradient:  1.0997149702231612 ; l2 norm of weights:  7.174090715242986\n",
            "Iteration#:  9924 ; Loss:  2.9887132865608326 ; l2 norm of gradient:  1.0996976230946875 ; l2 norm of weights:  7.174073407327053\n",
            "Iteration#:  9928 ; Loss:  2.988664655153637 ; l2 norm of gradient:  1.0996802759762676 ; l2 norm of weights:  7.174056099657848\n",
            "Iteration#:  9932 ; Loss:  2.9886160259565826 ; l2 norm of gradient:  1.0996629288678765 ; l2 norm of weights:  7.174038792235365\n",
            "Iteration#:  9936 ; Loss:  2.988567399033775 ; l2 norm of gradient:  1.0996455817694923 ; l2 norm of weights:  7.174021485059596\n",
            "Iteration#:  9940 ; Loss:  2.9885187727592117 ; l2 norm of gradient:  1.0996282346810904 ; l2 norm of weights:  7.174004178130534\n",
            "Iteration#:  9944 ; Loss:  2.9884701484354403 ; l2 norm of gradient:  1.0996108876026485 ; l2 norm of weights:  7.1739868714481725\n",
            "Iteration#:  9948 ; Loss:  2.9884215252122135 ; l2 norm of gradient:  1.0995935405341424 ; l2 norm of weights:  7.173969565012503\n",
            "Iteration#:  9952 ; Loss:  2.9883729035523023 ; l2 norm of gradient:  1.0995761934755506 ; l2 norm of weights:  7.17395225882352\n",
            "Iteration#:  9956 ; Loss:  2.9883242839152087 ; l2 norm of gradient:  1.0995588464268478 ; l2 norm of weights:  7.173934952881214\n",
            "Iteration#:  9960 ; Loss:  2.9882756654383957 ; l2 norm of gradient:  1.0995414993880126 ; l2 norm of weights:  7.173917647185579\n",
            "Iteration#:  9964 ; Loss:  2.988227048967236 ; l2 norm of gradient:  1.09952415235902 ; l2 norm of weights:  7.1739003417366085\n",
            "Iteration#:  9968 ; Loss:  2.988178433225441 ; l2 norm of gradient:  1.0995068053398482 ; l2 norm of weights:  7.173883036534295\n",
            "Iteration#:  9972 ; Loss:  2.988129819172232 ; l2 norm of gradient:  1.0994894583304737 ; l2 norm of weights:  7.17386573157863\n",
            "Iteration#:  9976 ; Loss:  2.9880812074955796 ; l2 norm of gradient:  1.099472111330875 ; l2 norm of weights:  7.1738484268696086\n",
            "Iteration#:  9980 ; Loss:  2.9880325961767302 ; l2 norm of gradient:  1.0994547643410266 ; l2 norm of weights:  7.173831122407221\n",
            "Iteration#:  9984 ; Loss:  2.9879839875023486 ; l2 norm of gradient:  1.099437417360907 ; l2 norm of weights:  7.173813818191461\n",
            "Iteration#:  9988 ; Loss:  2.9879353795478134 ; l2 norm of gradient:  1.0994200703904937 ; l2 norm of weights:  7.173796514222323\n",
            "Iteration#:  9992 ; Loss:  2.9878867729001612 ; l2 norm of gradient:  1.0994027234297628 ; l2 norm of weights:  7.173779210499797\n",
            "Iteration#:  9996 ; Loss:  2.9878381689084312 ; l2 norm of gradient:  1.0993853764786923 ; l2 norm of weights:  7.173761907023877\n",
            "1e-05 0.01 0.46353646353646355\n"
          ]
        }
      ],
      "source": [
        "#grid search for finding the best hyperparams and model\n",
        "\n",
        "best_model = None\n",
        "best_val = -1\n",
        "for lr in [0.01, 0.001, 0.0001, 0.00001]:\n",
        "    for la in [5, 2, 1, 0.1, 0.01]:\n",
        "        model = fit(xtrain_normal, ytrain, lr, la, 10000, verbose=0)\n",
        "        val_acc = accuracy(xval_normal, yval, model)\n",
        "        print(lr, la, val_acc)\n",
        "        if val_acc > best_val:\n",
        "            best_val = val_acc\n",
        "            best_model = model\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 92,
      "metadata": {
        "id": "n5zNZCNVhIvE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3032559f-8575-4847-ea98-3d0487945ac7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test accuracy:  0.942\n"
          ]
        }
      ],
      "source": [
        "print(\"Test accuracy: \", accuracy(xtest_normal, ytest, best_model))"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ngEf-kJQbqa4"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.6"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}